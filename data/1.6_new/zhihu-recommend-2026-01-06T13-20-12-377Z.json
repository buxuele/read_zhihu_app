{
  "data": [
    {
      "id": "84_1767705673.617",
      "type": "feed",
      "offset": 84,
      "verb": "TOPIC_ACKNOWLEDGED_ARTICLE",
      "created_time": 1767705673,
      "updated_time": 1767705673,
      "target": {
        "is_labeled": false,
        "visited_count": 1,
        "navigator_vote": false,
        "title": "CS336 Assignment2 Systems and Parallelism 完整解答参考-Part II",
        "linkbox": {
          "category": "",
          "pic": "",
          "title": "",
          "url": ""
        },
        "excerpt_new": "完整code source： https://github.com/Lumieshone/CS336-Assignment2 ，有帮助多多star哦！1.2 Optimizing Attention with FlashAttention-2 请在不同规模下对注意力机制的实现进行benchmark。编写一个脚本，要求： 将 batch size 固定为 8，且不使用多头注意力机制（即移除 head 维度）；遍历 [16, 32, 64, 128] 的笛卡尔积作为 d_model，遍历 [256, 1024, 4096, 8192, 16384] 的笛卡尔积作为 seq_len。为相应的 size 生成随机输入 Q、K、V；使用这些输入进…",
        "created": 1767703858,
        "preview_type": "default",
        "content": "<p data-pid=\"xFvjg3j_\">完整code source：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Lumieshone/CS336-Assignment2\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Lumieshone/C</span><span class=\"invisible\">S336-Assignment2</span><span class=\"ellipsis\"></span></a>，有帮助多多star哦！</p><h2><b>1.2 Optimizing Attention with FlashAttention-2</b><br/></h2><p data-pid=\"pXcXl0R2\">请在不同规模下对注意力机制的实现进行benchmark。编写一个脚本，要求：</p><ol><li data-pid=\"9UR72kS3\">将 batch size 固定为 8，且不使用多头注意力机制（即移除 head 维度）；</li><li data-pid=\"ULfct19d\">遍历 [16, 32, 64, 128] 的笛卡尔积作为 d_model，遍历 [256, 1024, 4096, 8192, 16384] 的笛卡尔积作为 seq_len。</li><li data-pid=\"_uZ0OYdJ\">为相应的 size 生成随机输入 Q、K、V；</li><li data-pid=\"8GTfUgwl\">使用这些输入进行 100 次 forward 并记录耗时；</li><li data-pid=\"fg553BNW\">在 backward 开始前测量内存使用量，并记录 100 次 backward 耗时；</li><li data-pid=\"zk7fxMJA\">确保进行 warmup 操作，并在每次 forward/backward 后调用 <code>torch.cuda.synchronize()</code>函数。</li></ol><p data-pid=\"VKX4TXnx\">报告这些配置下的运行时间（或 OOM Errors）。在哪个size下会出现 OOM Error？在你发现的最小配置中，是否需要计算注意力机制的内存占用（可使用第一项作业中Transformer 模型的内存使用公式）？backward节省的内存随序列长度如何变化？如何消除这种内存成本？<br/>我们添加<code>benchmark_attn.py</code>文件：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">timeit</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">itertools</span>\n<span class=\"kn\">from</span> <span class=\"nn\">cs336_basics.model</span> <span class=\"k\">import</span> <span class=\"n\">scaled_dot_product_attention</span>\n\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">)</span>\n<span class=\"n\">handler</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">StreamHandler</span><span class=\"p\">()</span>\n<span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">formatter</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">Formatter</span><span class=\"p\">(</span><span class=\"s1\">&#39;</span><span class=\"si\">%(asctime)s</span><span class=\"s1\"> - </span><span class=\"si\">%(levelname)s</span><span class=\"s1\"> - </span><span class=\"si\">%(message)s</span><span class=\"s1\">&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">setFormatter</span><span class=\"p\">(</span><span class=\"n\">formatter</span><span class=\"p\">)</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">addHandler</span><span class=\"p\">(</span><span class=\"n\">handler</span><span class=\"p\">)</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n\n<span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;cuda&#34;</span> <span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">()</span> <span class=\"k\">else</span> <span class=\"s2\">&#34;cpu&#34;</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">ScaledDotProductAttention</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">):</span>\n        <span class=\"n\">seq_len</span> <span class=\"o\">=</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tril</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bool</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">scaled_dot_product_attention</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">measure</span><span class=\"p\">(</span><span class=\"n\">d_model</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">warmup_steps</span><span class=\"p\">):</span>\n    <span class=\"n\">B</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>\n    <span class=\"n\">S</span><span class=\"p\">,</span> <span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">d_model</span>\n    <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">S</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n               <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)]</span>\n    <span class=\"n\">f_time</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">b_time</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">f_mem</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">b_mem</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">attn</span> <span class=\"o\">=</span> <span class=\"n\">ScaledDotProductAttention</span><span class=\"p\">()</span>\n    <span class=\"c1\"># attn = torch.compile(attn)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">warmup_steps</span> <span class=\"o\">+</span> <span class=\"mi\">100</span><span class=\"p\">):</span>\n        <span class=\"n\">start_time</span> <span class=\"o\">=</span> <span class=\"n\">timeit</span><span class=\"o\">.</span><span class=\"n\">default_timer</span><span class=\"p\">()</span>\n        <span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">attn</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">)</span>\n        <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">synchronize</span><span class=\"p\">()</span>\n        <span class=\"n\">mid_time</span> <span class=\"o\">=</span> <span class=\"n\">timeit</span><span class=\"o\">.</span><span class=\"n\">default_timer</span><span class=\"p\">()</span>\n        <span class=\"n\">mid_mem</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">memory_allocated</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1024</span> <span class=\"o\">**</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">out</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>\n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">synchronize</span><span class=\"p\">()</span>\n        <span class=\"n\">end_time</span> <span class=\"o\">=</span> <span class=\"n\">timeit</span><span class=\"o\">.</span><span class=\"n\">default_timer</span><span class=\"p\">()</span>\n        <span class=\"n\">end_mem</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">memory_allocated</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1024</span> <span class=\"o\">**</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">&gt;=</span> <span class=\"n\">warmup_steps</span><span class=\"p\">:</span>\n            <span class=\"n\">f_time</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">mid_time</span> <span class=\"o\">-</span> <span class=\"n\">start_time</span><span class=\"p\">)</span>\n            <span class=\"n\">b_time</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">end_time</span> <span class=\"o\">-</span> <span class=\"n\">mid_time</span><span class=\"p\">)</span>\n            <span class=\"n\">f_mem</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">mid_mem</span><span class=\"p\">)</span>\n            <span class=\"n\">b_mem</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">end_mem</span><span class=\"p\">)</span>\n        <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">reset_peak_memory_stats</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">f_time</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">b_time</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">f_mem</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">b_mem</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">():</span>\n    <span class=\"n\">d_models</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">]</span>\n    <span class=\"n\">seq_lens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"mi\">8192</span><span class=\"p\">,</span> <span class=\"mi\">16384</span><span class=\"p\">]</span>\n    <span class=\"n\">warmup_steps</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n    <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">d_model</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span> <span class=\"ow\">in</span> <span class=\"n\">itertools</span><span class=\"o\">.</span><span class=\"n\">product</span><span class=\"p\">(</span><span class=\"n\">d_models</span><span class=\"p\">,</span> <span class=\"n\">seq_lens</span><span class=\"p\">):</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;Running benchmark: d_model </span><span class=\"si\">{d_model}</span><span class=\"s2\"> seq_len </span><span class=\"si\">{seq_len}</span><span class=\"s2\">&#34;</span><span class=\"p\">)</span>\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"n\">f_time</span><span class=\"p\">,</span> <span class=\"n\">b_time</span><span class=\"p\">,</span> <span class=\"n\">f_mem</span><span class=\"p\">,</span> <span class=\"n\">b_mem</span> <span class=\"o\">=</span> <span class=\"n\">measure</span><span class=\"p\">(</span><span class=\"n\">d_model</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">warmup_steps</span><span class=\"p\">)</span>\n            <span class=\"n\">rows</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">({</span>\n                <span class=\"s1\">&#39;d_model&#39;</span><span class=\"p\">:</span> <span class=\"n\">d_model</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;seq_len&#39;</span><span class=\"p\">:</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;f_time&#39;</span><span class=\"p\">:</span> <span class=\"n\">f_time</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;b_time&#39;</span><span class=\"p\">:</span> <span class=\"n\">b_time</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;f_memory&#39;</span><span class=\"p\">:</span> <span class=\"n\">f_mem</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;b_memory&#39;</span><span class=\"p\">:</span> <span class=\"n\">b_mem</span>\n            <span class=\"p\">})</span>\n        <span class=\"k\">except</span> <span class=\"ne\">RuntimeError</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"s2\">&#34;out of memory&#34;</span> <span class=\"ow\">in</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">):</span>\n                <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">warning</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;OOM for d_model=</span><span class=\"si\">{d_model}</span><span class=\"s2\">, seq_len=</span><span class=\"si\">{seq_len}</span><span class=\"s2\">&#34;</span><span class=\"p\">)</span>\n                <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">empty_cache</span><span class=\"p\">()</span>\n                <span class=\"n\">rows</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">({</span>\n                    <span class=\"s1\">&#39;d_model&#39;</span><span class=\"p\">:</span> <span class=\"n\">d_model</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;seq_len&#39;</span><span class=\"p\">:</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;f_time&#39;</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;b_time&#39;</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;f_memory&#39;</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;b_memory&#39;</span><span class=\"p\">:</span> <span class=\"kc\">None</span>\n                <span class=\"p\">})</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"k\">raise</span>\n    <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">rows</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">to_markdown</span><span class=\"p\">(</span><span class=\"n\">index</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">floatfmt</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&#34;.0f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.0f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.2f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.2f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.2f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.2f&#34;</span><span class=\"p\">]))</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;__main__&#34;</span><span class=\"p\">:</span>\n    <span class=\"n\">benchmark</span><span class=\"p\">()</span></code></pre></div><p data-pid=\"TC8s8M0U\">我们运行该脚本即可得到以下结果：</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-a8793609a8175c38178518b77e90a1de_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1462\" data-rawheight=\"1410\" data-original-token=\"v2-e3dc8e4b426267b89153fdd1b3691067\" class=\"origin_image zh-lightbox-thumb\" width=\"1462\" data-original=\"https://pic3.zhimg.com/v2-a8793609a8175c38178518b77e90a1de_r.jpg\"/></figure><p data-pid=\"XgG93t7A\">可以看到，无论哪个 size，都会在 seq_len 达到 16384 时 OOM，这是因为GPU需要存储的最大注意力矩阵大小是 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=O%28N%5E2%29\" alt=\"O(N^2)\"/> 的，随着 seq_len 增大，GPU 内存占用率急剧上升。此外，随 seq_len 线性增加，backward 节省的内存以其平方速度增加。可以通过 tiling 的方式减少访存次数，来应对内存成本。</p><h2>1.3 Benchmarking JIT-Compiled Attention </h2><p data-pid=\"jjTyt3wN\">2.0之后的 pytorch 还配备了一个 compiler，它会通过动态分析计算图来尝试自动生成融合的 Triton 内核，直接在我们想要应用的 layer 或者 model 上包裹 torch.compile 即可。</p><ol><li data-pid=\"o87AOZDk\">将<code>benchmark_attn.py</code>扩展至包含 PyTorch 注意力机制的编译版本，并将其性能与上述 pytorch_attention 问题中相同配置的未编译版本进行对比。</li></ol><p data-pid=\"tMELxM6n\">将之前注释掉的<code>attn = torch.compile(attn)</code>加回来即可，重新运行，得到以下结果： </p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-7d061b01f005da61d3d02639ef633150_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"703\" data-original-token=\"v2-bfb402a317d1314632af043df571440d\" class=\"origin_image zh-lightbox-thumb\" width=\"741\" data-original=\"https://pic1.zhimg.com/v2-7d061b01f005da61d3d02639ef633150_r.jpg\"/></figure><p data-pid=\"s-16AC92\">观察结果可以得出以下结论：</p><ul><li data-pid=\"iO1REh_O\">在 seq_len 大于等于 4096 时，torch.compile对运行时间的节省变得显著，且 seq_len 越大越显著。</li><li data-pid=\"dPWPUK2O\">使用 torch.compile 后，seq_len = 16384 时不会 OOM，但是在其他情况下，f/b 所用内存与朴素版本基本一致 </li></ul><p data-pid=\"FT-JncCd\">2. 在<code>benchmark.py</code>中用 torch.compile 编译整个 Transformer 模型。forward 的性能会如何变化？forward+backward+optimer steps 的性能又会如何变化？</p><p data-pid=\"_WaO-fH_\">在<code>benchmark.py</code>中初始化完 model 后，添加<code>model = torch.compile(model)</code>，为快速得到结果，我们在 <b>d_model = 128，d_ff = 512，num_layers = num_heads = 4 </b>的小规模配置下进行实验，seq_len 选取 2k，4k，8k 来表示长序列输入，然后分别运行朴素版本跟 torch.compile 版本，得到的结果如下： </p><table data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"><tbody><tr><th>seq_len</th><th>origin_f</th><th>compile_f</th><th>origin_b</th><th>compile_b</th></tr><tr><td>2048</td><td>0.0186</td><td>0.0106</td><td>0.0708</td><td>0.0422</td></tr><tr><td>4096</td><td>0.0697</td><td>0.0403</td><td>0.2486</td><td>0.1381</td></tr><tr><td>8192</td><td>OOM</td><td>OOM</td><td>OOM</td><td>OOM</td></tr></tbody></table><p data-pid=\"PjLZT15A\">从上述结果可以看出，torch.compile 对长序列的优化效果在完整 model 上亦有体现，但当seq_len 来到 8k 时我们看到，即使使用 torch.compile，依然会 OOM，因此我们需要进一步优化我们的访存，也就引出了下文的 flash-attention 实现。</p><h3>1.3.2 FlashAttention-2 Forward Pass Forward pass </h3><p data-pid=\"Zj9SZkRz\"><b>Forward pass</b></p><p data-pid=\"F1OkRB63\">在该模块我们将分别使用 Pytorch 与 Triron 实现 Flashattention-2的 forward 过程，关于 FlashAttention 算法的部分公式推导，本文不再赘述，具体来说可以参照下图： </p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-f61ed804f7c27937e55f617dadaeb878_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"877\" data-original-token=\"v2-30a299a69637cde99c0e3fc724e5a1fc\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic1.zhimg.com/v2-f61ed804f7c27937e55f617dadaeb878_r.jpg\"/></figure><ol><li data-pid=\"Zo2NvpDc\">请编写一个纯 PyTorch（不含Triton）的<code>autograd.Function</code>，实现 FlashAttention-2 的 forward 过程。该函数需接收输入 Q、K、V 及是否启用因果掩码 is_causal，并计算 output O 和 logsumexp L。此任务中可忽略 is_causal 标志。函数需保存 L、Q、K、V、O 作为 backward 参数并返回 O。所有<code>autograd.Function</code>子类都需实现 backward 方法，当前可直接抛出 NotImplementedError。</li></ol><p data-pid=\"bC9KNfsS\">我们新建<code>flashattn2-torch.py</code>，并实现一个<code>FlashAttnWithTorch</code>类，分块大小采用 16 * 16，为了保持泛化性，我们假设Q的形状不限定为<code>batch_size, Nq, D</code>，而是假设最后两维前还有若干维（例如 head 等）：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">math</span>\n<span class=\"kn\">from</span> <span class=\"nn\">einops</span> <span class=\"k\">import</span> <span class=\"n\">einsum</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">FlashAttnWithTorch</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">autograd</span><span class=\"o\">.</span><span class=\"n\">Function</span><span class=\"p\">):</span>\n    <span class=\"nd\">@staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">is_causal</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">):</span>\n        <span class=\"c1\"># 分块大小</span>\n        <span class=\"n\">Bq</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n        <span class=\"n\">Bk</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n        <span class=\"n\">Nq</span><span class=\"p\">,</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">],</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n        <span class=\"n\">Nk</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n        <span class=\"n\">Tq</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Nq</span> <span class=\"o\">+</span> <span class=\"n\">Bq</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">Bq</span>\n        <span class=\"n\">Tk</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Nk</span> <span class=\"o\">+</span> <span class=\"n\">Bk</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">Bk</span>\n        <span class=\"n\">scale</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">)</span>\n        <span class=\"n\">O</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">)</span>\n        <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"n\">all_q_pos</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">Nq</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"n\">all_k_pos</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">Nk</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">Tq</span><span class=\"p\">):</span>\n            <span class=\"n\">start_q</span> <span class=\"o\">=</span> <span class=\"n\">i</span> <span class=\"o\">*</span> <span class=\"n\">Bq</span>\n            <span class=\"n\">end_q</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">((</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">Bq</span><span class=\"p\">,</span> <span class=\"n\">Nq</span><span class=\"p\">)</span>\n            <span class=\"n\">Qi</span><span class=\"o\">=</span> <span class=\"n\">Q</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n            <span class=\"n\">Oi</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">Qi</span><span class=\"p\">)</span>\n            <span class=\"n\">mi</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">(</span><span class=\"n\">Qi</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">&#39;-inf&#39;</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n            <span class=\"n\">Li</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">mi</span><span class=\"p\">)</span>\n            <span class=\"n\">q_pos</span> <span class=\"o\">=</span> <span class=\"n\">all_q_pos</span><span class=\"p\">[</span><span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">]</span>\n            <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">Tk</span><span class=\"p\">):</span>\n                <span class=\"n\">start_k</span> <span class=\"o\">=</span> <span class=\"n\">j</span> <span class=\"o\">*</span> <span class=\"n\">Bk</span>\n                <span class=\"n\">end_k</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">((</span><span class=\"n\">j</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">Bk</span><span class=\"p\">,</span> <span class=\"n\">Nk</span><span class=\"p\">)</span>\n                <span class=\"n\">Kj</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_k</span><span class=\"p\">:</span><span class=\"n\">end_k</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n                <span class=\"n\">Vj</span> <span class=\"o\">=</span> <span class=\"n\">V</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_k</span><span class=\"p\">:</span><span class=\"n\">end_k</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n                <span class=\"n\">Sij</span> <span class=\"o\">=</span> <span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"n\">Qi</span><span class=\"p\">,</span> <span class=\"n\">Kj</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... q d, ... k d -&gt; ... q k&#34;</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n                <span class=\"n\">k_pos</span> <span class=\"o\">=</span> <span class=\"n\">all_k_pos</span><span class=\"p\">[</span><span class=\"n\">start_k</span><span class=\"p\">:</span><span class=\"n\">end_k</span><span class=\"p\">]</span>\n                <span class=\"k\">if</span> <span class=\"n\">is_causal</span><span class=\"p\">:</span>\n                    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">q_pos</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"n\">k_pos</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n                    <span class=\"n\">Sij</span> <span class=\"o\">=</span> <span class=\"n\">Sij</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">1.0e6</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">Sij</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">Sij</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">))</span>\n                <span class=\"n\">mij</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"n\">mi</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">Sij</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n                <span class=\"n\">Pij</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">Sij</span> <span class=\"o\">-</span> <span class=\"n\">mij</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n                <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">mi</span> <span class=\"o\">-</span> <span class=\"n\">mij</span><span class=\"p\">)</span>\n                <span class=\"n\">mi</span> <span class=\"o\">=</span> <span class=\"n\">mij</span>\n                <span class=\"n\">Li</span> <span class=\"o\">=</span> <span class=\"n\">s</span> <span class=\"o\">*</span> <span class=\"n\">Li</span> <span class=\"o\">+</span> <span class=\"n\">Pij</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n                <span class=\"n\">Pij</span> <span class=\"o\">=</span> <span class=\"n\">Pij</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Vj</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n                <span class=\"n\">Oi</span> <span class=\"o\">=</span> <span class=\"n\">Oi</span> <span class=\"o\">*</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"n\">Pij</span><span class=\"p\">,</span> <span class=\"n\">Vj</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... q k, ... k d -&gt; ... q d&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">O</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Oi</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">Li</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mf\">1e-6</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n            <span class=\"n\">L</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">mi</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">Li</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">save_for_backward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">O</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">)</span>\n        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">is_causal</span> <span class=\"o\">=</span> <span class=\"n\">is_causal</span>\n        <span class=\"k\">return</span> <span class=\"n\">O</span>\n\n    <span class=\"nd\">@staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">backward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">dO</span><span class=\"p\">):</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">NotImplementedError</span></code></pre></div><p data-pid=\"UUjTrSWF\">在<code>adapter.py</code>中修改<code>get_flashattention_autograd_function_pytorch</code>函数后，运行<code>uv run pytest -k test_flash_forward_pass_pytorch</code>，然后 pass。</p><p data-pid=\"cvnAm2y7\">2. 编写一个用于 FlashAttention-2 forward 的 Triton kernel。然后，编写另一个<code>autograd.Function</code>子类，在 forward 中调用此（融合）kernel。以下 tips 可以帮助避免精度问题：</p><ul><li data-pid=\"qMmtgeWD\">On Chip 的 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=%28O_i%2C+l%2Cm%29\" alt=\"(O_i, l,m)\"/> 应是<code>tl.float32</code> 类型。如果正在向 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=O_i\" alt=\"O_i\"/> 累加数据，请使用 <code>O_i = tl.dot（...，acc=O_i)</code>。</li><li data-pid=\"0O2_Mxof\">在乘法运算前，将 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=P_%7Bij%7D\" alt=\"P_{ij}\"/> 转换为 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=V_j\" alt=\"V_j\"/> 的 dtype，并在将 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=O_i\" alt=\"O_i\"/> 写入全局内存前将其转换为适当的 dtype。转换通过<code>tensor.to</code>完成。可以使用<code>tensor.dtype</code>获取张量的dtype，而使用<code>* _block_ptr.type.element_ty</code>获取块指针/指针的 dtype。</li></ul><p data-pid=\"bESJohXS\"> 我们新建<code>flashattn2-triton.py</code>，并实现一个<code>flash_fwd_kernel</code>函数以及<code>FlashAttnWithTriton</code>类：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n<span class=\"kn\">from</span> <span class=\"nn\">einops</span> <span class=\"k\">import</span> <span class=\"n\">rearrange</span>\n\n<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n<span class=\"k\">def</span> <span class=\"nf\">flash_fwd_kernel</span><span class=\"p\">(</span>\n    <span class=\"n\">Q_ptr</span><span class=\"p\">,</span> <span class=\"n\">K_ptr</span><span class=\"p\">,</span> <span class=\"n\">V_ptr</span><span class=\"p\">,</span>\n    <span class=\"n\">O_ptr</span><span class=\"p\">,</span> <span class=\"n\">L_ptr</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_qb</span><span class=\"p\">,</span> <span class=\"n\">stride_qq</span><span class=\"p\">,</span> <span class=\"n\">stride_qd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_kb</span><span class=\"p\">,</span> <span class=\"n\">stride_kk</span><span class=\"p\">,</span> <span class=\"n\">stride_kd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_vb</span><span class=\"p\">,</span> <span class=\"n\">stride_vk</span><span class=\"p\">,</span> <span class=\"n\">stride_vd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_ob</span><span class=\"p\">,</span> <span class=\"n\">stride_oq</span><span class=\"p\">,</span> <span class=\"n\">stride_od</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_lb</span><span class=\"p\">,</span> <span class=\"n\">stride_lq</span><span class=\"p\">,</span>\n    <span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">N_KEYS</span><span class=\"p\">,</span>\n    <span class=\"n\">scale</span><span class=\"p\">,</span>\n    <span class=\"n\">D</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">K_TILE_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">is_causal</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n<span class=\"p\">):</span>\n    <span class=\"c1\"># Program indices</span>\n    <span class=\"n\">query_tile_index</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">batch_index</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Offset each pointer with the corresponding batch index</span>\n    <span class=\"c1\"># multiplied with the batch stride for each tensor</span>\n    <span class=\"n\">Q_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n        <span class=\"n\">Q_ptr</span> <span class=\"o\">+</span> <span class=\"n\">batch_index</span> <span class=\"o\">*</span> <span class=\"n\">stride_qb</span><span class=\"p\">,</span>\n        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_qq</span><span class=\"p\">,</span> <span class=\"n\">stride_qd</span><span class=\"p\">),</span>\n        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">query_tile_index</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">K_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n        <span class=\"n\">K_ptr</span> <span class=\"o\">+</span> <span class=\"n\">batch_index</span> <span class=\"o\">*</span> <span class=\"n\">stride_kb</span><span class=\"p\">,</span>\n        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_KEYS</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_kk</span><span class=\"p\">,</span> <span class=\"n\">stride_kd</span><span class=\"p\">),</span>\n        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">K_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">V_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n        <span class=\"n\">V_ptr</span> <span class=\"o\">+</span> <span class=\"n\">batch_index</span> <span class=\"o\">*</span> <span class=\"n\">stride_vb</span><span class=\"p\">,</span>\n        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_KEYS</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_vk</span><span class=\"p\">,</span> <span class=\"n\">stride_vd</span><span class=\"p\">),</span>\n        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">K_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">O_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n        <span class=\"n\">O_ptr</span> <span class=\"o\">+</span> <span class=\"n\">batch_index</span> <span class=\"o\">*</span> <span class=\"n\">stride_ob</span><span class=\"p\">,</span>\n        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_oq</span><span class=\"p\">,</span> <span class=\"n\">stride_od</span><span class=\"p\">),</span>\n        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">query_tile_index</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">L_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n        <span class=\"n\">L_ptr</span> <span class=\"o\">+</span> <span class=\"n\">batch_index</span> <span class=\"o\">*</span> <span class=\"n\">stride_lb</span><span class=\"p\">,</span>\n        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,),</span>\n        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_lq</span><span class=\"p\">,),</span>\n        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">query_tile_index</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,),</span>\n        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,),</span>\n        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">Q_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Q_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">O_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"n\">l_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"n\">m_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,),</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">&#39;-inf&#39;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"n\">q_pos</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">query_tile_index</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N_KEYS</span><span class=\"p\">,</span> <span class=\"n\">K_TILE_SIZE</span><span class=\"p\">)):</span>\n        <span class=\"n\">K_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">K_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">V_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">V_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">S_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Q_i</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">K_j</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n        <span class=\"n\">k_pos</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K_TILE_SIZE</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">i</span> <span class=\"o\">*</span> <span class=\"n\">K_TILE_SIZE</span>\n        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">q_pos</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">N_QUERIES</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">k_pos</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">N_KEYS</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">is_causal</span><span class=\"p\">:</span>\n            <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">q_pos</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"n\">k_pos</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:])</span> <span class=\"o\">&amp;</span> <span class=\"n\">mask</span>\n        <span class=\"n\">S_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">S_ij</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.0e6</span><span class=\"p\">)</span>\n        <span class=\"n\">m_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"n\">m_i</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">S_ij</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n        <span class=\"n\">P_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">S_ij</span> <span class=\"o\">-</span> <span class=\"n\">m_ij</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n        <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">m_i</span> <span class=\"o\">-</span> <span class=\"n\">m_ij</span><span class=\"p\">)</span>\n        <span class=\"n\">m_i</span> <span class=\"o\">=</span> <span class=\"n\">m_ij</span>\n        <span class=\"n\">l_i</span> <span class=\"o\">=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">l_i</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">P_ij</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">P_ij</span> <span class=\"o\">=</span> <span class=\"n\">P_ij</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">V_j</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n        <span class=\"n\">O_i</span> <span class=\"o\">*=</span> <span class=\"n\">alpha</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n        <span class=\"n\">O_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">P_ij</span><span class=\"p\">,</span> <span class=\"n\">V_j</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"o\">=</span><span class=\"n\">O_i</span><span class=\"p\">)</span>\n        <span class=\"n\">K_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">K_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"n\">K_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n        <span class=\"n\">V_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">V_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"n\">K_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">O_i</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">O_i</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">l_i</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"mf\">1e-6</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">O_block_ptr</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">)</span>\n    <span class=\"n\">l_i</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">m_i</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">l_i</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">L_block_ptr</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">)</span>\n    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">O_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">O_i</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,))</span>\n    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">L_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">l_i</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,))</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">FlashAttnWithTriton</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">autograd</span><span class=\"o\">.</span><span class=\"n\">Function</span><span class=\"p\">):</span>\n    <span class=\"nd\">@staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">is_causal</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">):</span>\n        <span class=\"c1\"># 分块大小</span>\n        <span class=\"n\">Bq</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n        <span class=\"n\">Bk</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n        <span class=\"n\">input_shape</span> <span class=\"o\">=</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n        <span class=\"n\">Q</span> <span class=\"o\">=</span> <span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... q d -&gt; (...) q d&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... k d -&gt; (...) k d&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">V</span> <span class=\"o\">=</span> <span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... k d -&gt; (...) k d&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n        <span class=\"n\">N_KEYS</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n        <span class=\"n\">scale</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">d</span> <span class=\"o\">**</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n        <span class=\"n\">Tq</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">N_QUERIES</span> <span class=\"o\">+</span> <span class=\"n\">Bq</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">Bq</span>\n        <span class=\"n\">O</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">)</span>\n        <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"n\">flash_fwd_kernel</span><span class=\"p\">[(</span><span class=\"n\">Tq</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">)](</span>\n            <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span>\n            <span class=\"n\">O</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span>\n            <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"n\">O</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">O</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">O</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n            <span class=\"n\">N_QUERIES</span><span class=\"o\">=</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">N_KEYS</span><span class=\"o\">=</span><span class=\"n\">N_KEYS</span><span class=\"p\">,</span>\n            <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"n\">scale</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"o\">=</span><span class=\"n\">Bq</span><span class=\"p\">,</span> <span class=\"n\">K_TILE_SIZE</span><span class=\"o\">=</span><span class=\"n\">Bk</span><span class=\"p\">,</span>\n            <span class=\"n\">is_causal</span> <span class=\"o\">=</span> <span class=\"n\">is_causal</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">save_for_backward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">O</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">)</span>\n        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">Bq</span> <span class=\"o\">=</span> <span class=\"n\">Bq</span>\n        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">Bk</span> <span class=\"o\">=</span> <span class=\"n\">Bk</span>\n        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">is_causal</span> <span class=\"o\">=</span> <span class=\"n\">is_causal</span>\n        <span class=\"k\">return</span> <span class=\"n\">O</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"p\">)</span>\n\n    <span class=\"nd\">@staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">backward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">dO</span><span class=\"p\">):</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">NotImplementedError</span></code></pre></div><p data-pid=\"gBIVz1BI\">该代码的实现过程有以下几点需要注意：</p><ul><li data-pid=\"Gu9MGvjZ\">关于边界掩码 mask：事实上，就算不计算<code>q_pos</code>和<code>p_pos</code>然后生成边界掩码，我们仍然能够通过官方给定的测试，但是如果你将<code>test_attention.py</code>的<code>_make_attn_inputs</code>函数做一些修改，比如令合成数据的<code>n_queries = 131，n_keys = 137</code>，你就会发现你的朴素代码被卡死了。这是因为如果矩阵行数不能够被 tile_size 整除，则最后一个分块将只有部分有效值，其余元素填充为0，此时不加入边界掩码的话，对于跟有效值在同一行的其他 0 元素而言，他们依然会通过 exp 参与对分母 L 的贡献，进而影响有效位置的最终值精度。可以从下图看出，如果不提前用极小值将填充 0 的位置mask掉，它们会在后续计算中影响有效元素结果的精度：</li></ul><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-c97c7f1472183bf99055db42073ddc63_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"559\" data-original-token=\"v2-ea639da8a0cc7f725e25f3433153ba52\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://picx.zhimg.com/v2-c97c7f1472183bf99055db42073ddc63_r.jpg\"/></figure><ul><li data-pid=\"2PG944r-\">下面所示的 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=O_i\" alt=\"O_i\"/> 更新操作实际上与<code>O_i = O_i * alpha[:, None] + tl.dot(P_ij, V_j)</code>在数学上等价，但是使用 acc 参数更有利于 Tensor Core 指令融合与保持数值精度一致性（此例中与 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=O_i\" alt=\"O_i\"/> 一致，都是 fp32），此外，使用 acc 参数时数据流是原地更新的， <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=PV\" alt=\"PV\"/> 的结果直接写入了存储 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=O_i\" alt=\"O_i\"/> 的寄存器中，可以降低寄存器压力。</li></ul><div class=\"highlight\"><pre><code class=\"language-python3\">  <span class=\"n\">O_i</span> <span class=\"o\">*=</span> <span class=\"n\">alpha</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n  <span class=\"n\">O_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">P_ij</span><span class=\"p\">,</span> <span class=\"n\">V_j</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"o\">=</span><span class=\"n\">O_i</span><span class=\"p\">)</span></code></pre></div><p data-pid=\"1VSUuaPt\">3. 将<code>is_casual：tl_constexp</code>作为最后一个参数添加到你的 kernel 中，当其设置为True时，启用索引比较以实现因果掩码。在Triton中，构建适当的索引向量用于 query 和 key，并将它们进行比较以形成大小为 Bq × Bk 的方形掩码。对于被掩码掉的元素，将常数值<code>-1e6</code>添加到注意力分数矩阵 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=S_%7Bij%7D\" alt=\"S_{ij}\"/> 的相应元素中。使用<code>ctx.is_causal = is_causal</code>保存掩码标志以供 backward 使用。</p><p data-pid=\"acMF_6sY\">只需要在上述代码块添加一个下三角区域判定即可，可以与之前的边界 mask 无缝集成，这也是我们的写法精妙之处：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">if</span> <span class=\"n\">is_causal</span><span class=\"p\">:</span>\n    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">q_pos</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"n\">k_pos</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:])</span> <span class=\"o\">&amp;</span> <span class=\"n\">mask</span></code></pre></div><p data-pid=\"IUTLPxAw\"><b>Backward pass with recomputation</b></p><ol><li data-pid=\"e2Jxd4iS\">使用PyTorch（而非Triton）和<code>torch.compile</code>实现 FlashAttention-2 <code>autograd.Function</code>的 backward 过程。该实现需输出 Q、K、V、O、dO 和 L 张量，并返回 dQ、dK 和 dV。请务必计算并使用 D 向量。</li></ol><p data-pid=\"Hq78XhLS\">我们写一个<code>flash_backward</code>函数并用 torch.compile 编译：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">flash_backward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">O</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">dO</span><span class=\"p\">,</span> <span class=\"n\">is_causal</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">):</span>\n    <span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">O</span> <span class=\"o\">*</span> <span class=\"n\">dO</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">Bq</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n    <span class=\"n\">Bk</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n    <span class=\"n\">Nq</span><span class=\"p\">,</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">Nk</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n    <span class=\"n\">Tq</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Nq</span> <span class=\"o\">+</span> <span class=\"n\">Bq</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">Bq</span>\n    <span class=\"n\">Tk</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Nk</span> <span class=\"o\">+</span> <span class=\"n\">Bk</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">Bk</span>\n    <span class=\"n\">scale</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">)</span>\n    <span class=\"n\">dQ</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"n\">dK</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"n\">dV</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"n\">all_q_pos</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">Nq</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"n\">all_k_pos</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">Nk</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">Tk</span><span class=\"p\">):</span>\n        <span class=\"n\">start_k</span> <span class=\"o\">=</span> <span class=\"n\">j</span> <span class=\"o\">*</span> <span class=\"n\">Bk</span>\n        <span class=\"n\">end_k</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">((</span><span class=\"n\">j</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">Bk</span><span class=\"p\">,</span> <span class=\"n\">Nk</span><span class=\"p\">)</span>\n        <span class=\"n\">Kj</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_k</span><span class=\"p\">:</span><span class=\"n\">end_k</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n        <span class=\"n\">Vj</span> <span class=\"o\">=</span> <span class=\"n\">V</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_k</span><span class=\"p\">:</span><span class=\"n\">end_k</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n        <span class=\"n\">dKj</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">Kj</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n        <span class=\"n\">dVj</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">Vj</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n        <span class=\"n\">k_pos</span> <span class=\"o\">=</span> <span class=\"n\">all_k_pos</span><span class=\"p\">[</span><span class=\"n\">start_k</span><span class=\"p\">:</span><span class=\"n\">end_k</span><span class=\"p\">]</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">Tq</span><span class=\"p\">):</span>\n            <span class=\"n\">start_q</span> <span class=\"o\">=</span> <span class=\"n\">i</span> <span class=\"o\">*</span> <span class=\"n\">Bq</span>\n            <span class=\"n\">end_q</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">((</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">Bq</span><span class=\"p\">,</span> <span class=\"n\">Nq</span><span class=\"p\">)</span>\n            <span class=\"n\">Li</span> <span class=\"o\">=</span> <span class=\"n\">L</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">]</span>\n            <span class=\"n\">Di</span> <span class=\"o\">=</span> <span class=\"n\">D</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">]</span>\n            <span class=\"n\">Qi</span> <span class=\"o\">=</span> <span class=\"n\">Q</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n            <span class=\"n\">dQi</span> <span class=\"o\">=</span> <span class=\"n\">dQ</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n            <span class=\"n\">dOi</span> <span class=\"o\">=</span> <span class=\"n\">dO</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n            <span class=\"n\">Sij</span> <span class=\"o\">=</span> <span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"n\">Qi</span><span class=\"p\">,</span> <span class=\"n\">Kj</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... q d, ... k d -&gt; ... q k&#34;</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n            <span class=\"n\">q_pos</span> <span class=\"o\">=</span> <span class=\"n\">all_q_pos</span><span class=\"p\">[</span><span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">]</span>\n            <span class=\"k\">if</span> <span class=\"n\">is_causal</span><span class=\"p\">:</span>\n                <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">q_pos</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"n\">k_pos</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n                <span class=\"n\">Sij</span> <span class=\"o\">=</span> <span class=\"n\">Sij</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">1.0e6</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">Sij</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">Sij</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">))</span>\n            <span class=\"n\">Pij</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">Sij</span> <span class=\"o\">-</span> <span class=\"n\">Li</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n            <span class=\"n\">Pij</span> <span class=\"o\">=</span> <span class=\"n\">Pij</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">dOi</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n            <span class=\"n\">dVj</span> <span class=\"o\">+=</span> <span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"n\">Pij</span><span class=\"p\">,</span> <span class=\"n\">dOi</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... q k, ... q d -&gt; ... k d&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">dPij</span> <span class=\"o\">=</span> <span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"n\">dOi</span><span class=\"p\">,</span> <span class=\"n\">Vj</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... q d, ... k d -&gt; ... q k&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">dSij</span> <span class=\"o\">=</span> <span class=\"n\">Pij</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">dPij</span> <span class=\"o\">-</span> <span class=\"n\">Di</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n            <span class=\"n\">dQi</span> <span class=\"o\">+=</span> <span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"n\">dSij</span><span class=\"p\">,</span> <span class=\"n\">Kj</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... q k, ... k d -&gt; ... q d&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">dQ</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_q</span><span class=\"p\">:</span><span class=\"n\">end_q</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">dQi</span>\n            <span class=\"n\">dKj</span> <span class=\"o\">+=</span> <span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"n\">dSij</span><span class=\"p\">,</span> <span class=\"n\">Qi</span><span class=\"p\">,</span> <span class=\"s2\">&#34;... q k, ... q d -&gt; ... k d&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">dK</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_k</span><span class=\"p\">:</span><span class=\"n\">end_k</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">dKj</span>\n        <span class=\"n\">dV</span><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">start_k</span><span class=\"p\">:</span><span class=\"n\">end_k</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">dVj</span>\n    <span class=\"k\">return</span> <span class=\"n\">dQ</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">),</span> <span class=\"n\">dK</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">),</span> <span class=\"n\">dV</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">),</span> <span class=\"kc\">None</span>\n\n<span class=\"n\">compiled_backward</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">flash_backward</span><span class=\"p\">)</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">FlashAttnWithTorch</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">autograd</span><span class=\"o\">.</span><span class=\"n\">Function</span><span class=\"p\">):</span>\n    <span class=\"nd\">@staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">backward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">dO</span><span class=\"p\">):</span>\n        <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">O</span><span class=\"p\">,</span> <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">saved_tensors</span>\n        <span class=\"n\">is_causal</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">is_causal</span>\n        <span class=\"k\">return</span> <span class=\"n\">compiled_backward</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">O</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">dO</span><span class=\"p\">,</span> <span class=\"n\">is_causal</span><span class=\"p\">)</span></code></pre></div><p data-pid=\"RlVmURk8\">在实现该函数的过程中，有以下几个细节需要注意：</p><ul><li data-pid=\"zntuwfKR\">backward 是 KV 外循环，Q 内循环，与 forward 正好相反。</li><li data-pid=\"M0y3ffld\">dQ 的初始化要用<code>zeros_like</code>而不是<code>empty_like</code>，因为<code>empty_like</code>分配的显存是<b>未初始化</b>的，里面包含的是上一次使用该显存留下的随机值（垃圾值），这些值可能非常大（如 <code>1e37</code>）或为 NaN。第一次读取 dQi 时，我们就会把这些垃圾值读进来了，导致梯度瞬间爆炸。</li><li data-pid=\"gBq73o1x\"><code>flash_backward</code>函数需要返回四个梯度，因为<code>backward</code> 函数返回的梯度数量，必须严格等于 <code>forward</code> 函数接收的参数数量，因此<code>is_causal</code>也需要一个占位梯度 None。</li><li data-pid=\"iO7QrcY9\">如果使用<code>Sij.mask_fill</code>而不是<code>Sij.where</code>，<code>mask = q_pos[:, None] &lt; k_pos[None, :]</code>。</li></ul><p data-pid=\"HE1_Lciq\">2. 用 triton 实现 Flash Attention-2 的 backward：</p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-99c71ec130eea5678bfd05f2c5a75f1e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"896\" data-original-token=\"v2-2a3ecc87500c08e8d33dafc1713724e5\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic1.zhimg.com/v2-99c71ec130eea5678bfd05f2c5a75f1e_r.jpg\"/></figure><p data-pid=\"WrEOLclA\">  依据上述算法，我们需要做三次分块计算，分别是计算 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=D%3Drowsum%28dO%5Ccirc+O%29\" alt=\"D=rowsum(dO\\circ O)\"/> （<code>flash_bwd_preprocess</code>），计算 dQ（<code>flash_bwd_dq_kernel</code>）和计算 dK、dV（<code>flash_bwd_dkdv_kernel</code>）</p><ul><li data-pid=\"otAVuzFv\">分块计算 D 是因为 O 与 dO 的矩阵规模仍然很大，且在上图所示算法里需要预计算，因此我们需要沿用flash attention的核心思想，即对 O 和 dO 沿着最后两维度进行分块矩阵运算来节省显存和访存开销。</li><li data-pid=\"QjH_zzVq\"><b>将 dQ 的计算与 dK、dV 的计算封装成两个独立的 kernel</b>，其核心在于，如果仅将上图所示的算法封装在一个 kernel 内进行，即以 KV 为外循环，Q 为内循环，此时我们观察内循环中 dQ 的计算，会发现同一个 dQ_i 需要从多个 j 累加而来，会发生多个 block 写同一 dQ tile 的情况，导致写冲突（kernel 在主函数调用时以 tile j 为并行单位），因此<code>dQ_i += ...</code>必须是 atomicAdd，否则会写坏，这就是为什么上图算法中写到：&#34;Must be atomic for correctness!&#34;。因此，为了避免使用效率低的 atomicAdd，我们可以将 dQ 和 dK、dV 的计算封装到两个不同的 kernel 中，但 kernel 之间不能共享 P（因为不保存 NxN，非常大），所以只能在两个 kernel 中分别重算一次 P，因为重算代价相对便宜（softmax tile 是 Bq×Bk），显存访问更贵。这也就解释了原文中的 tips：</li></ul><blockquote data-pid=\"DOudUYUm\">“A key trick here is to compute P twice, once for the backward pass for dQ and another time for dK and dV”。</blockquote><p data-pid=\"fbMo-4_1\">  下面是代码实现部分，我们在<code>flashattn2_triton.py</code>中添加新的kernel，分别是：</p><ul><li data-pid=\"a33DXECr\"><code>flash_bwd_preprocess</code>：</li></ul><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n<span class=\"k\">def</span> <span class=\"nf\">flash_bwd_preprocess</span><span class=\"p\">(</span>\n    <span class=\"n\">O_ptr</span><span class=\"p\">,</span> <span class=\"n\">dO_ptr</span><span class=\"p\">,</span> <span class=\"n\">D_ptr</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_ob</span><span class=\"p\">,</span> <span class=\"n\">stride_oq</span><span class=\"p\">,</span> <span class=\"n\">stride_od</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_dob</span><span class=\"p\">,</span> <span class=\"n\">stride_doq</span><span class=\"p\">,</span> <span class=\"n\">stride_dod</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_db</span><span class=\"p\">,</span> <span class=\"n\">stride_dq</span><span class=\"p\">,</span>\n    <span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span>\n    <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">D_TILE_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n<span class=\"p\">):</span>\n    <span class=\"c1\"># Program indices</span>\n    <span class=\"n\">query_tile_index</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">batch_index</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">O_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n        <span class=\"n\">O_ptr</span> <span class=\"o\">+</span> <span class=\"n\">batch_index</span> <span class=\"o\">*</span> <span class=\"n\">stride_ob</span><span class=\"p\">,</span>\n        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_oq</span><span class=\"p\">,</span> <span class=\"n\">stride_od</span><span class=\"p\">),</span>\n        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">query_tile_index</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D_TILE_SIZE</span><span class=\"p\">),</span>\n        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">dO_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n        <span class=\"n\">dO_ptr</span> <span class=\"o\">+</span> <span class=\"n\">batch_index</span> <span class=\"o\">*</span> <span class=\"n\">stride_dob</span><span class=\"p\">,</span>\n        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span>\n        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_doq</span><span class=\"p\">,</span> <span class=\"n\">stride_dod</span><span class=\"p\">),</span>\n        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">query_tile_index</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D_TILE_SIZE</span><span class=\"p\">),</span>\n        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">D_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n        <span class=\"n\">D_ptr</span> <span class=\"o\">+</span> <span class=\"n\">batch_index</span> <span class=\"o\">*</span> <span class=\"n\">stride_db</span><span class=\"p\">,</span>\n        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,),</span>\n        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_dq</span><span class=\"p\">,),</span>\n        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">query_tile_index</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,),</span>\n        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,),</span>\n        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">D_TILE</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">D_TILE_SIZE</span><span class=\"p\">)):</span>\n        <span class=\"n\">O</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">O_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n        <span class=\"n\">dO</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">dO_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n        <span class=\"n\">D_TILE</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">O</span> <span class=\"o\">*</span> <span class=\"n\">dO</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">O_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">O_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">D_TILE_SIZE</span><span class=\"p\">))</span>\n        <span class=\"n\">dO_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">dO_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">D_TILE_SIZE</span><span class=\"p\">))</span>\n    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">D_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">D_TILE</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,))</span></code></pre></div><p data-pid=\"nvnmfXuD\">该函数用于预计算伪代码里的 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=D%3Drowsum%28dO%5Ccirc+O%29\" alt=\"D=rowsum(dO\\circ O)\"/> ，为了贯彻分块的核心思想，我们将大矩阵 O 和 dO 也按照最后两维度进行分块，Q 维度为外循环，D 维度为内循环，这里还需要注意不要将分块的 D 维度与前面提到的 <img eeimg=\"1\" src=\"https://www.zhihu.com/equation?tex=D%3Drowsum%28dO%5Ccirc+O%29\" alt=\"D=rowsum(dO\\circ O)\"/> 混淆。</p><ul><li data-pid=\"j0RZey9d\"><code>flash_bwd_dq_kernel</code>：</li></ul><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n<span class=\"k\">def</span> <span class=\"nf\">flash_bwd_dq_kernel</span><span class=\"p\">(</span>\n    <span class=\"n\">Q_ptr</span><span class=\"p\">,</span> <span class=\"n\">K_ptr</span><span class=\"p\">,</span> <span class=\"n\">V_ptr</span><span class=\"p\">,</span>\n    <span class=\"n\">D_ptr</span><span class=\"p\">,</span> <span class=\"n\">L_ptr</span><span class=\"p\">,</span> <span class=\"n\">dO_ptr</span><span class=\"p\">,</span>\n    <span class=\"n\">dQ_ptr</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_qb</span><span class=\"p\">,</span> <span class=\"n\">stride_qq</span><span class=\"p\">,</span> <span class=\"n\">stride_qd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_kb</span><span class=\"p\">,</span> <span class=\"n\">stride_kk</span><span class=\"p\">,</span> <span class=\"n\">stride_kd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_vb</span><span class=\"p\">,</span> <span class=\"n\">stride_vk</span><span class=\"p\">,</span> <span class=\"n\">stride_vd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_dob</span><span class=\"p\">,</span> <span class=\"n\">stride_doq</span><span class=\"p\">,</span> <span class=\"n\">stride_dod</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_dqb</span><span class=\"p\">,</span> <span class=\"n\">stride_dqq</span><span class=\"p\">,</span> <span class=\"n\">stride_dqd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_db</span><span class=\"p\">,</span> <span class=\"n\">stride_dq</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_lb</span><span class=\"p\">,</span> <span class=\"n\">stride_lq</span><span class=\"p\">,</span>\n    <span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">N_KEYS</span><span class=\"p\">,</span>\n    <span class=\"n\">scale</span><span class=\"p\">,</span>\n    <span class=\"n\">D</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">K_TILE_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">is_causal</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n<span class=\"p\">):</span>\n    <span class=\"n\">query_tile_index</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">batch_index</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 省略初始化block</span>\n    <span class=\"n\">Q_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Q_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">dO_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">dO_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">L_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">L_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">D_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">D_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">dQ_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"n\">q_start</span> <span class=\"o\">=</span> <span class=\"n\">query_tile_index</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span>\n    <span class=\"n\">q_pos</span> <span class=\"o\">=</span> <span class=\"n\">q_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">is_causal</span><span class=\"p\">:</span>\n        <span class=\"n\">j_stop</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">q_start</span> <span class=\"o\">+</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">K_TILE_SIZE</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">j_stop</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N_KEYS</span><span class=\"p\">,</span> <span class=\"n\">K_TILE_SIZE</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">j_stop</span><span class=\"p\">):</span>\n        <span class=\"n\">K_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">K_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">V_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">V_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">S_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Q_i</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">K_j</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n        <span class=\"k\">if</span> <span class=\"n\">is_causal</span><span class=\"p\">:</span>\n            <span class=\"n\">k_pos</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K_TILE_SIZE</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">j</span> <span class=\"o\">*</span> <span class=\"n\">K_TILE_SIZE</span>\n            <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">q_pos</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"n\">k_pos</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n            <span class=\"n\">S_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">S_ij</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.0e6</span><span class=\"p\">)</span>\n        <span class=\"n\">P_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">S_ij</span> <span class=\"o\">-</span> <span class=\"n\">L_i</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n        <span class=\"n\">dP_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dO_i</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">V_j</span><span class=\"p\">))</span>\n        <span class=\"n\">dS_ij</span> <span class=\"o\">=</span> <span class=\"n\">P_ij</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">dP_ij</span> <span class=\"o\">-</span> <span class=\"n\">D_i</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n        <span class=\"n\">dS_ij</span> <span class=\"o\">=</span> <span class=\"n\">dS_ij</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">K_j</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n        <span class=\"n\">dQ_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dS_ij</span><span class=\"p\">,</span> <span class=\"n\">K_j</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"o\">=</span><span class=\"n\">dQ_i</span><span class=\"p\">)</span>\n        <span class=\"n\">K_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">K_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"n\">K_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n        <span class=\"n\">V_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">V_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"n\">K_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">dQ_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">dQ_i</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">dQ_block_ptr</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">),</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,))</span></code></pre></div><p data-pid=\"vwPCK8jK\">这里我们以 Q 为外循环，K 为内循环，因此在 kernel 里作为内循环是使用 K 循环，迭代的是 KV tile，计算的是 dQ。同时需要注意我们的 mask，此时不需要考虑 forward 中提到的边界 mask，因为我们可以直接使用 forward 传过来的 L，不处理边界不会影响结果精度。虽然不需要处理边界mask，但是我们可以通过精准控制内循环 range 的手段来提前终止循环，避免空矩阵计算，大大提升资源利用率。</p><ul><li data-pid=\"_vvKD5H0\"><code>flash_bwd_dkdv_kernel</code>：</li></ul><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n<span class=\"k\">def</span> <span class=\"nf\">flash_bwd_dkdv_kernel</span><span class=\"p\">(</span>\n    <span class=\"n\">Q_ptr</span><span class=\"p\">,</span> <span class=\"n\">K_ptr</span><span class=\"p\">,</span> <span class=\"n\">V_ptr</span><span class=\"p\">,</span>\n    <span class=\"n\">D_ptr</span><span class=\"p\">,</span> <span class=\"n\">L_ptr</span><span class=\"p\">,</span> <span class=\"n\">dO_ptr</span><span class=\"p\">,</span>\n    <span class=\"n\">dK_ptr</span><span class=\"p\">,</span> <span class=\"n\">dV_ptr</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_qb</span><span class=\"p\">,</span> <span class=\"n\">stride_qq</span><span class=\"p\">,</span> <span class=\"n\">stride_qd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_kb</span><span class=\"p\">,</span> <span class=\"n\">stride_kk</span><span class=\"p\">,</span> <span class=\"n\">stride_kd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_vb</span><span class=\"p\">,</span> <span class=\"n\">stride_vk</span><span class=\"p\">,</span> <span class=\"n\">stride_vd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_dob</span><span class=\"p\">,</span> <span class=\"n\">stride_doq</span><span class=\"p\">,</span> <span class=\"n\">stride_dod</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_dkb</span><span class=\"p\">,</span> <span class=\"n\">stride_dkk</span><span class=\"p\">,</span> <span class=\"n\">stride_dkd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_dvb</span><span class=\"p\">,</span> <span class=\"n\">stride_dvk</span><span class=\"p\">,</span> <span class=\"n\">stride_dvd</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_db</span><span class=\"p\">,</span> <span class=\"n\">stride_dq</span><span class=\"p\">,</span>\n    <span class=\"n\">stride_lb</span><span class=\"p\">,</span> <span class=\"n\">stride_lq</span><span class=\"p\">,</span>\n    <span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">N_KEYS</span><span class=\"p\">,</span>\n    <span class=\"n\">scale</span><span class=\"p\">,</span>\n    <span class=\"n\">D</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">K_TILE_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n    <span class=\"n\">is_causal</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n<span class=\"p\">):</span>\n    <span class=\"n\">key_tile_index</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">batch_index</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 省去初始化各个block的代码</span>\n    <span class=\"n\">K_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">K_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">V_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">V_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">k_start</span> <span class=\"o\">=</span> <span class=\"n\">key_tile_index</span> <span class=\"o\">*</span> <span class=\"n\">K_TILE_SIZE</span>\n    <span class=\"n\">k_pos</span> <span class=\"o\">=</span> <span class=\"n\">k_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K_TILE_SIZE</span><span class=\"p\">)</span>\n    <span class=\"n\">dK_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">K_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"n\">dV_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">K_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">)):</span>\n        <span class=\"n\">should_compute</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n        <span class=\"k\">if</span> <span class=\"n\">is_causal</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span> <span class=\"o\">&lt;=</span> <span class=\"n\">k_start</span><span class=\"p\">:</span>\n                <span class=\"n\">should_compute</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n        <span class=\"k\">if</span> <span class=\"n\">should_compute</span><span class=\"p\">:</span>\n            <span class=\"n\">Q_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Q_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">dO_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">dO_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">L_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">L_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">D_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">D_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,),</span> <span class=\"n\">padding_option</span><span class=\"o\">=</span><span class=\"s2\">&#34;zero&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">S_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Q_i</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">K_j</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n            <span class=\"k\">if</span> <span class=\"n\">is_causal</span><span class=\"p\">:</span>\n                <span class=\"n\">q_pos</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">i</span> <span class=\"o\">*</span> <span class=\"n\">Q_TILE_SIZE</span>\n                <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">q_pos</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"n\">k_pos</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n                <span class=\"n\">S_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">S_ij</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.0e6</span><span class=\"p\">)</span>\n            <span class=\"n\">P_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">S_ij</span> <span class=\"o\">-</span> <span class=\"n\">L_i</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n            <span class=\"n\">P_ij</span> <span class=\"o\">=</span> <span class=\"n\">P_ij</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">dO_i</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n            <span class=\"n\">dV_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">P_ij</span><span class=\"p\">),</span> <span class=\"n\">dO_i</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"o\">=</span><span class=\"n\">dV_j</span><span class=\"p\">)</span>\n            <span class=\"n\">dP_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dO_i</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">V_j</span><span class=\"p\">))</span>\n            <span class=\"n\">dS_ij</span> <span class=\"o\">=</span> <span class=\"n\">P_ij</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">dP_ij</span> <span class=\"o\">-</span> <span class=\"n\">D_i</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n            <span class=\"n\">dS_ij</span> <span class=\"o\">=</span> <span class=\"n\">dS_ij</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Q_i</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n            <span class=\"n\">dK_j</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">dS_ij</span><span class=\"p\">),</span> <span class=\"n\">Q_i</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"o\">=</span><span class=\"n\">dK_j</span><span class=\"p\">)</span>\n        <span class=\"n\">Q_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">Q_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n        <span class=\"n\">dO_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">dO_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n        <span class=\"n\">L_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">L_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,))</span>\n        <span class=\"n\">D_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">D_block_ptr</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">((</span><span class=\"n\">Q_TILE_SIZE</span><span class=\"p\">,))</span>\n\n    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">dK_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">dK_j</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">dK_block_ptr</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">),</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,))</span>\n    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">dV_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">dV_j</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">dV_block_ptr</span><span class=\"o\">.</span><span class=\"n\">type</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">),</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,))</span></code></pre></div><p data-pid=\"1ZdAH0tR\">这里我们以 K 为外循环，Q 为内循环，因此在 kernel 里作为内循环是使用 Q 循环，迭代的是 Q tile，计算的是 dK、dV。这里采用内部跳过的方式避免计算空矩阵，其逻辑是：如果 Q 块全在 K 块左上方，则该 Q 块被完全掩码，所以不予以计算。</p><ul><li data-pid=\"OR_JZBDv\"><code>FlashAttnWithTriton.backward</code>：</li></ul><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"nd\">@staticmethod</span>\n<span class=\"k\">def</span> <span class=\"nf\">backward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">dO</span><span class=\"p\">):</span>\n    <span class=\"c1\"># 分块大小</span>\n    <span class=\"n\">Bq</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n    <span class=\"n\">Bk</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n    <span class=\"n\">Bd</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n    <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">O</span><span class=\"p\">,</span> <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">saved_tensors</span>\n    <span class=\"n\">is_causal</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">is_causal</span>\n    <span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    <span class=\"n\">N_KEYS</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n    <span class=\"n\">scale</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">d</span> <span class=\"o\">**</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n    <span class=\"n\">Tq</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">N_QUERIES</span> <span class=\"o\">+</span> <span class=\"n\">Bq</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">Bq</span>\n    <span class=\"n\">Tk</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">N_KEYS</span> <span class=\"o\">+</span> <span class=\"n\">Bk</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">Bk</span>\n    <span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">)</span>\n    <span class=\"n\">dQ</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">)</span>\n    <span class=\"n\">dK</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">)</span>\n    <span class=\"n\">dV</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">V</span><span class=\"p\">)</span>\n    <span class=\"n\">flash_bwd_preprocess</span><span class=\"p\">[(</span><span class=\"n\">Tq</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">)](</span>\n        <span class=\"n\">O</span><span class=\"p\">,</span> <span class=\"n\">dO</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span>\n        <span class=\"n\">O</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">O</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">O</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">dO</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">dO</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dO</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"n\">N_QUERIES</span><span class=\"o\">=</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span>\n        <span class=\"n\">Q_TILE_SIZE</span><span class=\"o\">=</span><span class=\"n\">Bq</span><span class=\"p\">,</span>\n        <span class=\"n\">D_TILE_SIZE</span><span class=\"o\">=</span><span class=\"n\">Bd</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">flash_bwd_dq_kernel</span><span class=\"p\">[(</span><span class=\"n\">Tq</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">)](</span>\n        <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span>\n        <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">dO</span><span class=\"p\">,</span> <span class=\"n\">dQ</span><span class=\"p\">,</span>\n        <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">dO</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">dO</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dO</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">dQ</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">dQ</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dQ</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"n\">N_QUERIES</span><span class=\"o\">=</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">N_KEYS</span><span class=\"o\">=</span><span class=\"n\">N_KEYS</span><span class=\"p\">,</span>\n        <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"n\">scale</span><span class=\"p\">,</span>\n        <span class=\"n\">D</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span>\n        <span class=\"n\">Q_TILE_SIZE</span><span class=\"o\">=</span><span class=\"n\">Bq</span><span class=\"p\">,</span>\n        <span class=\"n\">K_TILE_SIZE</span><span class=\"o\">=</span><span class=\"n\">Bk</span><span class=\"p\">,</span>\n        <span class=\"n\">is_causal</span><span class=\"o\">=</span><span class=\"n\">is_causal</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">flash_bwd_dkdv_kernel</span><span class=\"p\">[(</span><span class=\"n\">Tk</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">)](</span>\n        <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span>\n        <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">dO</span><span class=\"p\">,</span> <span class=\"n\">dK</span><span class=\"p\">,</span> <span class=\"n\">dV</span><span class=\"p\">,</span>\n        <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">V</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">dO</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">dO</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dO</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">dK</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">dK</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dK</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">dV</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">dV</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dV</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"n\">N_QUERIES</span><span class=\"o\">=</span><span class=\"n\">N_QUERIES</span><span class=\"p\">,</span> <span class=\"n\">N_KEYS</span><span class=\"o\">=</span><span class=\"n\">N_KEYS</span><span class=\"p\">,</span>\n        <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"n\">scale</span><span class=\"p\">,</span>\n        <span class=\"n\">D</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span>\n        <span class=\"n\">Q_TILE_SIZE</span><span class=\"o\">=</span><span class=\"n\">Bq</span><span class=\"p\">,</span>\n        <span class=\"n\">K_TILE_SIZE</span><span class=\"o\">=</span><span class=\"n\">Bk</span><span class=\"p\">,</span>\n        <span class=\"n\">is_causal</span><span class=\"o\">=</span><span class=\"n\">is_causal</span>\n    <span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">dQ</span><span class=\"p\">,</span> <span class=\"n\">dK</span><span class=\"p\">,</span> <span class=\"n\">dV</span><span class=\"p\">,</span> <span class=\"kc\">None</span></code></pre></div><p data-pid=\"H5qyLTJQ\">主函数没什么好说的，按顺序调用前面三个 kernel 即可，返回值依然需要四个。</p><p data-pid=\"0bUGsZF1\">3. 使用<code>triton.testing.do_bench</code>编写 benchmark，对比基于 Triton 实现的FlashAttention-2 forward/backward 与常规 PyTorch 实现的性能表现。具体要求：生成包含 Triton 与 PyTorch 实现的 forward/backward/forward+backward 延迟的对比表格。测试前需随机生成输入数据，并在单个计算节点上运行 benchmark。实验参数设置：batch_size=1，is_causal=True，Nq = Nk = [128、256...65536]，dim =[16、32、64、128]，精度：[<code>torch.bfloat16</code>、<code>torch.float32</code>]。根据输入数据量可能需要调整数据块尺寸。</p><p data-pid=\"pRW6GGRw\">我们创建<code>benchmark_triton.py</code>，为节省时间，我们缩小一下需要遍历的 Nq/Nk 范围：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">itertools</span>\n<span class=\"kn\">from</span> <span class=\"nn\">flashattn2_torch</span> <span class=\"k\">import</span> <span class=\"n\">FlashAttnWithTorch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">flashattn2_triton</span> <span class=\"k\">import</span> <span class=\"n\">FlashAttnWithTriton</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">set_float32_matmul_precision</span><span class=\"p\">(</span><span class=\"s1\">&#39;high&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">_dynamo</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">cache_size_limit</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">)</span>\n<span class=\"n\">handler</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">StreamHandler</span><span class=\"p\">()</span>\n<span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">formatter</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">Formatter</span><span class=\"p\">(</span><span class=\"s1\">&#39;</span><span class=\"si\">%(asctime)s</span><span class=\"s1\"> - </span><span class=\"si\">%(levelname)s</span><span class=\"s1\"> - </span><span class=\"si\">%(message)s</span><span class=\"s1\">&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">setFormatter</span><span class=\"p\">(</span><span class=\"n\">formatter</span><span class=\"p\">)</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">addHandler</span><span class=\"p\">(</span><span class=\"n\">handler</span><span class=\"p\">)</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">make_attn_inputs</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">Nq</span><span class=\"p\">,</span> <span class=\"n\">Nk</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&#34;cuda&#34;</span><span class=\"p\">):</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">Nq</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">Nk</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">Nk</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">Nq</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">do</span>\n<span class=\"n\">torch_flash_compiled</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">FlashAttnWithTorch</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">,</span> <span class=\"n\">dynamic</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">test_timing_flash_forward_backward</span><span class=\"p\">(</span><span class=\"n\">Nq</span><span class=\"p\">,</span> <span class=\"n\">Nk</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span><span class=\"p\">,</span> <span class=\"n\">is_triton</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&#34;cuda&#34;</span><span class=\"p\">):</span>\n    <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">make_attn_inputs</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">Nq</span><span class=\"p\">,</span> <span class=\"n\">Nk</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">is_triton</span><span class=\"p\">:</span>\n        <span class=\"n\">flash</span> <span class=\"o\">=</span> <span class=\"n\">FlashAttnWithTriton</span><span class=\"o\">.</span><span class=\"n\">apply</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">flash</span> <span class=\"o\">=</span> <span class=\"n\">torch_flash_compiled</span>\n    <span class=\"k\">def</span> <span class=\"nf\">run_fwd</span><span class=\"p\">():</span>\n        <span class=\"n\">flash</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"k\">def</span> <span class=\"nf\">run_fwd_bwd</span><span class=\"p\">():</span>\n        <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">flash</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">)</span>\n    <span class=\"n\">run_fwd</span><span class=\"p\">()</span>\n    <span class=\"n\">f_time</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">run_fwd</span><span class=\"p\">(),</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n    <span class=\"n\">fb_time</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">run_fwd_bwd</span><span class=\"p\">(),</span> <span class=\"n\">grad_to_none</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">],</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">f_time</span><span class=\"p\">,</span> <span class=\"n\">fb_time</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">():</span>\n    <span class=\"n\">d_models</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">]</span>\n    <span class=\"n\">seq_lens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"mi\">2048</span><span class=\"p\">]</span>\n    <span class=\"n\">dtypes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">]</span>\n    <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">configs</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">itertools</span><span class=\"o\">.</span><span class=\"n\">product</span><span class=\"p\">(</span><span class=\"n\">d_models</span><span class=\"p\">,</span> <span class=\"n\">seq_lens</span><span class=\"p\">,</span> <span class=\"n\">dtypes</span><span class=\"p\">))</span>\n    <span class=\"n\">total</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">configs</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">d_model</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">configs</span><span class=\"p\">):</span>\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;[{i + 1}/</span><span class=\"si\">{total}</span><span class=\"s2\">] Running: d_model=</span><span class=\"si\">{d_model}</span><span class=\"s2\">, seq_len=</span><span class=\"si\">{seq_len}</span><span class=\"s2\">, dtype=</span><span class=\"si\">{dtype}</span><span class=\"s2\">&#34;</span><span class=\"p\">)</span>\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"n\">torch_f_time</span><span class=\"p\">,</span> <span class=\"n\">torch_fb_time</span> <span class=\"o\">=</span> <span class=\"n\">test_timing_flash_forward_backward</span><span class=\"p\">(</span><span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">d_model</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">)</span>\n            <span class=\"n\">triton_f_time</span><span class=\"p\">,</span> <span class=\"n\">triton_fb_time</span> <span class=\"o\">=</span> <span class=\"n\">test_timing_flash_forward_backward</span><span class=\"p\">(</span><span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span> <span class=\"n\">d_model</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">is_triton</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n            <span class=\"n\">rows</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">({</span>\n                <span class=\"s1\">&#39;d_model&#39;</span><span class=\"p\">:</span> <span class=\"n\">d_model</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;seq_len&#39;</span><span class=\"p\">:</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;dtype&#39;</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">dtype</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">)[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span>\n                <span class=\"s1\">&#39;torch_f_time&#39;</span><span class=\"p\">:</span> <span class=\"n\">torch_f_time</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;torch_fb_time&#39;</span><span class=\"p\">:</span> <span class=\"n\">torch_fb_time</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;triton_f_time&#39;</span><span class=\"p\">:</span> <span class=\"n\">triton_f_time</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;triton_fb_time&#39;</span><span class=\"p\">:</span> <span class=\"n\">triton_fb_time</span><span class=\"p\">,</span>\n            <span class=\"p\">})</span>\n        <span class=\"k\">except</span> <span class=\"ne\">RuntimeError</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"s2\">&#34;out of memory&#34;</span> <span class=\"ow\">in</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">):</span>\n                <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">warning</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;OOM for d_model=</span><span class=\"si\">{d_model}</span><span class=\"s2\">, seq_len=</span><span class=\"si\">{seq_len}</span><span class=\"s2\">, dtype=</span><span class=\"si\">{dtype}</span><span class=\"s2\">&#34;</span><span class=\"p\">)</span>\n                <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">empty_cache</span><span class=\"p\">()</span>\n                <span class=\"n\">rows</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">({</span>\n                    <span class=\"s1\">&#39;d_model&#39;</span><span class=\"p\">:</span> <span class=\"n\">d_model</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;seq_len&#39;</span><span class=\"p\">:</span> <span class=\"n\">seq_len</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;dtype&#39;</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">dtype</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">)[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span>\n                    <span class=\"s1\">&#39;torch_f_time&#39;</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;torch_fb_time&#39;</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;triton_f_time&#39;</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n                    <span class=\"s1\">&#39;triton_fb_time&#39;</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n                <span class=\"p\">})</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"k\">raise</span>\n    <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">rows</span><span class=\"p\">)</span>\n    <span class=\"n\">float_fmt</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">&#34;.0f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.0f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.2f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.2f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.2f&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;.2f&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">md_output</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">to_markdown</span><span class=\"p\">(</span><span class=\"n\">index</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">floatfmt</span><span class=\"o\">=</span><span class=\"n\">float_fmt</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;</span><span class=\"se\">\\n</span><span class=\"s2\">Benchmark Results:&#34;</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">md_output</span><span class=\"p\">)</span>\n    <span class=\"n\">output_file</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;reports/torch_vs_triton.md&#34;</span>\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">output_file</span><span class=\"p\">,</span> <span class=\"s2\">&#34;w&#34;</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s2\">&#34;utf-8&#34;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;# Flash Attention Benchmark Results</span><span class=\"se\">\\n\\n</span><span class=\"s2\">&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;Device: {torch.cuda.get_device_name(0)}</span><span class=\"se\">\\n\\n</span><span class=\"s2\">&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">md_output</span><span class=\"p\">)</span>\n\n    <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;Results saved to </span><span class=\"si\">{output_file}</span><span class=\"s2\">&#34;</span><span class=\"p\">)</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;__main__&#34;</span><span class=\"p\">:</span>\n    <span class=\"n\">benchmark</span><span class=\"p\">()</span></code></pre></div><p data-pid=\"YIogKv36\">这里需要注意两点：</p><ul><li data-pid=\"HbjFv9MQ\">torch.compile 语句要放在<code>test_timing_flash_forward_backward</code>函数之前，不然会在主函数循环中多次调用浪费时间。</li><li data-pid=\"6ioDxDNH\">测量端到端时延的时候，要在 do_bench 函数中通过 grad_to_none 参数重置 qkv 的梯度，避免循环中梯度累积影响结果。</li></ul><p data-pid=\"nIYfRSdx\">最后我测量出的结果如下图所示，可以看到相比torch的朴素实现，triton 实现节省的时间是相当夸张的：</p><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-8fa35ee9632309d807ed96d79a01ff33_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"883\" data-rawheight=\"1026\" data-original-token=\"v2-39dc93c1dba184f3f1637af03fa3a836\" class=\"origin_image zh-lightbox-thumb\" width=\"883\" data-original=\"https://pic2.zhimg.com/v2-8fa35ee9632309d807ed96d79a01ff33_r.jpg\"/></figure>",
        "thumbnails": [
          "https://picx.zhimg.com/50/v2-e3dc8e4b426267b89153fdd1b3691067_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-bfb402a317d1314632af043df571440d_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-30a299a69637cde99c0e3fc724e5a1fc_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-2a3ecc87500c08e8d33dafc1713724e5_720w.jpg?source=b6762063",
          "https://pica.zhimg.com/50/v2-39dc93c1dba184f3f1637af03fa3a836_720w.jpg?source=b6762063"
        ],
        "is_navigator": false,
        "id": "1991954891556542266",
        "type": "article",
        "url": "https://api.zhihu.com/articles/1991954891556542266",
        "allow_segment_interaction": true,
        "excerpt": "完整code source： https://github.com/Lumieshone/CS336-Assignment2 ，有帮助多多star哦！1.2 Optimizing Attention with FlashAttention-2 请在不同规模下对注意力机制的实现进行benchmark。编写一个脚本，要求： 将 batch size 固定为 8，且不使用多头注意力机制（即移除 head 维度）；遍历 [16, 32, 64, 128] 的笛卡尔积作为 d_model，遍历 [256, 1024, 4096, 8192, 16384] 的笛卡尔积作为 seq_len。为相应的 size 生成随机输入 Q、K、V；使用这些输入进…",
        "preview_text": "",
        "vote_next_step": "vote",
        "updated": 1767703858,
        "voting": 0,
        "comment_count": 0,
        "favorite_count": 1,
        "article_type": "normal",
        "author": {
          "id": "a1fd8692959945ef95717b2afe5ef1e8",
          "url": "https://api.zhihu.com/people/a1fd8692959945ef95717b2afe5ef1e8",
          "is_org": false,
          "gender": 1,
          "followers_count": 7,
          "is_following": false,
          "user_type": "people",
          "url_token": "don-foanhewu",
          "name": "图灵鸣泣之时",
          "headline": "用冰冷的理性温暖世界～",
          "avatar_url": "https://pic1.zhimg.com/50/v2-84d9124b11485c788a74b813c194cf94_l.jpg?source=b6762063",
          "is_followed": false
        },
        "comment_permission": "all",
        "voteup_count": 0
      },
      "brief": "{\"source\": \"TS\", \"type\": \"article\", \"id\": 1991954891556542266}",
      "attached_info": "CtIHCKf2uOaz/qfLhQEQBxoJMjY4NjUyOTMwILKK9MoGKAAwAEBUSiQKGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDESATAYACAAOgBKLAohVFNfU09VUkNFX1dBUk1VUF9aSElfRklORVRVTkVfSTJJEgEwGAAgADoAYiBlYWVhNTQwNjZjODE5YTZhNzliOWZjYmUxZDkyMjMzNHITMTk5MTk1NDg5MTU1NjU0MjI2NqoBCXJlY29tbWVuZMIBIGExZmQ4NjkyOTU5OTQ1ZWY5NTcxN2IyYWZlNWVmMWU48gEKCAwSBk5vcm1hbPIBKAgKEiQxZDc5ODVlNi1iOTZlLTQyNjQtOTczMC1lZDNiZDZkYzVmNWTyAQYICxICMTWCAgCIAsv/35u5M5ICIGExZmQ4NjkyOTU5OTQ1ZWY5NTcxN2IyYWZlNWVmMWU4mgIAygIURmlyc3RCcnVzaFdlaWdodFJ1bGXKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxl2gIZVFNfU09VUkNFX1dBUk1fVVBfTk9STUFMMegCA/oCC05PUk1BTF9GTE9XigMgYzNiYWQ1ZDg3OTU0NDBlMWI3ODFkMjk2Zjk0NzllYjCaAw0KAnYyEAAaBW90aGVyqAMB2AMA6gMaYWlfemhpX2ZpbmV0dW5yX2kyaV9yZWNhbGz6A7kCEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAAQtgsYggsiI3YyLWUzZGM4ZTRiNDI2MjY3Yjg5MTUzZmRkMWIzNjkxMDY3Oi0IABDlBRi/BSIjdjItYmZiNDAyYTMxN2QxMzE0NjMyYWYwNDNkZjU3MTQ0MGQ6LQgAEIAKGO0GIiN2Mi0zMGEyOTlhNjk2MzdjZGU5OWMwZTNmYzcyNGU1YTFmYzotCAAQgAgYrwQiI3YyLWVhNjM5ZGE4YTBjYzdmNzI1ZTI1ZjM0MzMxNTNiYTUyOi0IABCAChiAByIjdjItMmEzZWNjODc1MDBjMDhlOGQzM2RhZmMxNzEzNzI0ZTU6LQgAEPMGGIIIIiN2Mi0zOWRjOTNjMWRiYTE4NGYzZjE2MzdhZjAzZmEzYTgzNoAEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAQBxBZD+BBQAAAAAAAAAAiQU5vuVUe3myP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUPkAYAoAZVqAYBkgIuCgkyNjg2NTI5MzASEzE5OTE5NTQ4OTE1NTY1NDIyNjYYByIKSU1BR0VfVEVYVA==",
      "action_card": false
    },
    {
      "id": "85_1767705673.874",
      "type": "feed",
      "offset": 85,
      "verb": "TOPIC_ACKNOWLEDGED_ANSWER",
      "created_time": 1767705673,
      "updated_time": 1767705673,
      "target": {
        "vote_next_step": "vote",
        "voteup_count": 292,
        "excerpt": "1，冬季的办公室都是保暖达人     2，看到这个明码标价的，我肯定会买。     3，找到座椅加热的平替了     4，亲眼目睹了大姨从超市进货     5，出国了但没完全出，不用看标签我都知道made in哪里。       6，嘘，小声点别把们吓跑了，小鸟绿树       7，找同事处理工作前要先取号     8，加热马桶吸引了错误的家伙，猫果然能精准找到家里最暖和的地方。   9，公司你真的好懂年会抽奖#了胜于无吧#       10，哈尔的移动城堡   11，当你点外卖忘记把默认备注改了…   12，女厕…",
        "answer_type": "normal",
        "is_navigator": false,
        "allow_segment_interaction": true,
        "relationship": {
          "is_thanked": false,
          "is_nothelp": false,
          "voting": 0
        },
        "visited_count": 37641,
        "comment_count": 13,
        "thumbnail": "https://pic1.zhimg.com/50/v2-f0a6741d291fe533a58c149ebb1f92c8_720w.jpg?source=b6762063",
        "preview_type": "default",
        "favorite_count": 14,
        "navigator_vote": false,
        "author": {
          "url_token": "66-23-57-84",
          "headline": "文案美学｜快乐分享者｜女性成长",
          "gender": 0,
          "followers_count": 157,
          "id": "238b6d2be42e12cce2f20c6951953fa5",
          "user_type": "people",
          "avatar_url": "https://pica.zhimg.com/50/v2-2c6ce3ad6b5182d6c6bca2a471b387bb_l.jpg?source=b6762063",
          "is_org": false,
          "is_following": false,
          "is_followed": false,
          "url": "https://api.zhihu.com/people/238b6d2be42e12cce2f20c6951953fa5",
          "name": "夏南"
        },
        "updated_time": 1766304610,
        "thanks_count": 52,
        "created_time": 1766304610,
        "content": "<p></p><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-9c6f1f390085b267cdc7c135aafcb30b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"724\" data-rawheight=\"952\" data-original-token=\"v2-f0a6741d291fe533a58c149ebb1f92c8\" class=\"origin_image zh-lightbox-thumb\" width=\"724\" data-original=\"https://pic2.zhimg.com/v2-9c6f1f390085b267cdc7c135aafcb30b_r.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p data-pid=\"wtmLFuMp\">1，冬季的办公室都是保暖达人</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-8fa7aafdcd88f71bf28f151d0de92bd3_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1280\" data-original-token=\"v2-4d245a738262f312e69c613a054a8569\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-8fa7aafdcd88f71bf28f151d0de92bd3_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-8249a4d235400fe6b32039e885851cdd_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1284\" data-original-token=\"v2-d5c5f07a43b6b2ad3fdeea0a191f1406\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-8249a4d235400fe6b32039e885851cdd_r.jpg\"/></figure><p data-pid=\"XYXiJHma\">2，看到这个明码标价的，我肯定会买。</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-d960d0e6a5d80e0090c11bc169d86159_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"1616\" data-original-token=\"v2-08c222b22e674376c366c9ab0a155c0c\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://picx.zhimg.com/v2-d960d0e6a5d80e0090c11bc169d86159_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-b52199ff15aeb8c26779a157c17888c3_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"1634\" data-original-token=\"v2-31742e27695b4605342dec6c870d4539\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic4.zhimg.com/v2-b52199ff15aeb8c26779a157c17888c3_r.jpg\"/></figure><p data-pid=\"UDRmYZMF\">3，找到座椅加热的平替了</p><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-a839f850b299f8da2a12220880953c4d_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"828\" data-rawheight=\"1039\" data-original-token=\"v2-08ba6151910cd09bedbfb57c40f3e7df\" class=\"origin_image zh-lightbox-thumb\" width=\"828\" data-original=\"https://pic2.zhimg.com/v2-a839f850b299f8da2a12220880953c4d_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-61dc63254f4da9a92d87689935184969_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"722\" data-rawheight=\"901\" data-original-token=\"v2-ada1525b69419caaf863f9ebc56165b3\" class=\"origin_image zh-lightbox-thumb\" width=\"722\" data-original=\"https://pic4.zhimg.com/v2-61dc63254f4da9a92d87689935184969_r.jpg\"/></figure><p data-pid=\"w5e7V6Ws\">4，亲眼目睹了大姨从超市进货</p><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-e46c095eb455c6689e21175db52c1ce4_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1239\" data-original-token=\"v2-099c548e45a0d3f111461c5901aed81a\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-e46c095eb455c6689e21175db52c1ce4_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-d3996874ed6568f53b6af752296034c2_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1212\" data-original-token=\"v2-4d49704ec30f7ccde4573f5f441034e8\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-d3996874ed6568f53b6af752296034c2_r.jpg\"/></figure><p data-pid=\"AhsC9ocW\">5，出国了但没完全出，不用看标签我都知道made in哪里。</p><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-aef96c561ff6098e358237666969f32d_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1704\" data-rawheight=\"1278\" data-original-token=\"v2-1ef8fbd07bd96115b159e67fd0a4c602\" class=\"origin_image zh-lightbox-thumb\" width=\"1704\" data-original=\"https://pic4.zhimg.com/v2-aef96c561ff6098e358237666969f32d_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-2f0bd860d59cda7da2c831452adc04f9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1704\" data-rawheight=\"1278\" data-original-token=\"v2-16d3bcf4b992f94490317ad0fe461b7b\" class=\"origin_image zh-lightbox-thumb\" width=\"1704\" data-original=\"https://pic2.zhimg.com/v2-2f0bd860d59cda7da2c831452adc04f9_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-e4443a28c00e9e261dba1fe3175c2c33_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1278\" data-rawheight=\"960\" data-original-token=\"v2-7dcb0822cdcd5730e014b53477bc39b4\" class=\"origin_image zh-lightbox-thumb\" width=\"1278\" data-original=\"https://picx.zhimg.com/v2-e4443a28c00e9e261dba1fe3175c2c33_r.jpg\"/></figure><p data-pid=\"zh8fyZRg\">6，嘘，小声点别把们吓跑了，小鸟绿树</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-a555b554e57ca371ff4ba60eb2e873ef_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1047\" data-rawheight=\"1280\" data-original-token=\"v2-9670dced2c51615ce3e18a82c2f4f69c\" class=\"origin_image zh-lightbox-thumb\" width=\"1047\" data-original=\"https://picx.zhimg.com/v2-a555b554e57ca371ff4ba60eb2e873ef_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-3c68e2ceeeaa1d708eae2081274f2f47_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1253\" data-original-token=\"v2-76d98ecbfeba35188cb22640bdd30dd1\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-3c68e2ceeeaa1d708eae2081274f2f47_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-9bf7615e5ff2daf3255e0f742f2bd5ff_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1258\" data-original-token=\"v2-428720a35ca3525c58b25897eefd5912\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-9bf7615e5ff2daf3255e0f742f2bd5ff_r.jpg\"/></figure><p data-pid=\"PnkKFZhy\">7，找同事处理工作前要先取号</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-83df188cfb8e54c4eb2ff4e291c5a602_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1395\" data-original-token=\"v2-4f641d1ea13b7a1a0f730710a53c2fba\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-83df188cfb8e54c4eb2ff4e291c5a602_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-9b863973dd324f6e4419a3dc5ee9ff49_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"1563\" data-original-token=\"v2-ebd0905a6c0b4786da8ac21e665aa3b3\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic2.zhimg.com/v2-9b863973dd324f6e4419a3dc5ee9ff49_r.jpg\"/></figure><p data-pid=\"8SV831ZI\">8，加热马桶吸引了错误的家伙，猫果然能精准找到家里最暖和的地方。</p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-d15466c55a5d5235ef5f78af1c33c468_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"828\" data-rawheight=\"1042\" data-original-token=\"v2-a5ffdcf985a18d2c13208475d857d9d6\" class=\"origin_image zh-lightbox-thumb\" width=\"828\" data-original=\"https://pic1.zhimg.com/v2-d15466c55a5d5235ef5f78af1c33c468_r.jpg\"/></figure><p data-pid=\"6_ZyXchW\">9，公司你真的好懂年会抽奖#了胜于无吧#</p><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-fb16644aab03c66f503e533ada7acc03_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"1280\" data-original-token=\"v2-b13960f89c2cab04b27d4a21574a5bed\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-fb16644aab03c66f503e533ada7acc03_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-c90c77a32074a21483306dcd25813f57_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"1280\" data-original-token=\"v2-7d74a0d277f2b92fbb5384d662fe726a\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-c90c77a32074a21483306dcd25813f57_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-4a275eba3b31baa751a372c589b2ce1b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"1280\" data-original-token=\"v2-396ba4bc4a8e1561b853afb999517627\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-4a275eba3b31baa751a372c589b2ce1b_r.jpg\"/></figure><p data-pid=\"OemPGGQi\">10，哈尔的移动城堡</p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-6df75a074242154d390a4f712f59c04e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"1280\" data-original-token=\"v2-7393988037d6ab245a32a2bf56ce4065\" class=\"origin_image zh-lightbox-thumb\" width=\"964\" data-original=\"https://pic1.zhimg.com/v2-6df75a074242154d390a4f712f59c04e_r.jpg\"/></figure><p data-pid=\"nd8i2FEP\">11，当你点外卖忘记把默认备注改了… </p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-2f84cc4d542618d79a9a353534ddf8de_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"614\" data-rawheight=\"770\" data-original-token=\"v2-79c01a408c90c505ae23820d7e1db345\" class=\"origin_image zh-lightbox-thumb\" width=\"614\" data-original=\"https://pic1.zhimg.com/v2-2f84cc4d542618d79a9a353534ddf8de_r.jpg\"/></figure><p data-pid=\"9u56114B\">12，女厕所标识上为什么要插一根钢筋</p><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-82edc472630cf5586b5034c63b88ac16_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"920\" data-original-token=\"v2-9be1a98ec75d663f639208bf915509e9\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pica.zhimg.com/v2-82edc472630cf5586b5034c63b88ac16_r.jpg\"/></figure><p data-pid=\"t4yIOljn\">13，猫咪:人，看我不迷死你’</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-1f809047a608118dcb049d74090d6bdd_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"920\" data-original-token=\"v2-aefa87f94aa413c7107126c7a0d8182b\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://picx.zhimg.com/v2-1f809047a608118dcb049d74090d6bdd_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-d7f0c4540a3b4d1253f31df65a31e859_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"920\" data-original-token=\"v2-d378af89bd85ed19e6869ddb06e4b506\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic2.zhimg.com/v2-d7f0c4540a3b4d1253f31df65a31e859_r.jpg\"/></figure><p data-pid=\"PYIRUKg4\">14，在飞机上看电影,看得津津有味的时候,航班插播了一波八段锦#睡觉醒来还以为被劫机了#</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-d2e1ab9ab4e3e187dc15bc306cab3e85_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"676\" data-original-token=\"v2-12a72b288671490d087df33c584daed3\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://picx.zhimg.com/v2-d2e1ab9ab4e3e187dc15bc306cab3e85_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-6072115dc8c9f86d52afed463858a9be_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"780\" data-original-token=\"v2-def269a13bfc743e54506ca7df3edaae\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pica.zhimg.com/v2-6072115dc8c9f86d52afed463858a9be_r.jpg\"/></figure><p data-pid=\"ZBIwSZ5s\">15，小区的保安，人模狗样的</p><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-d292f7a49d300296a903fcdcae87e53e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"928\" data-original-token=\"v2-061b83bd5c5163416c81ebb006191ffc\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pica.zhimg.com/v2-d292f7a49d300296a903fcdcae87e53e_r.jpg\"/></figure><p data-pid=\"hEYY6mxO\">16，老师:  别说了，我都是被带的那个</p><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-cdc1cddb51b53bab4761ee29b3731e4b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"830\" data-original-token=\"v2-153a0538572ecf4ebb7e8ade79f49e98\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic4.zhimg.com/v2-cdc1cddb51b53bab4761ee29b3731e4b_r.jpg\"/></figure><p data-pid=\"7DzPrEF6\">17，我有证据：英语发源地是山东</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-d6b337ffb2c63ab07ba6459876c8218a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"388\" data-original-token=\"v2-7e17135d8bc3bb2adb7f6b587e86e002\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-d6b337ffb2c63ab07ba6459876c8218a_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-33e328076bd6713e79c9680de04b229c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"388\" data-original-token=\"v2-fb080d9030393ee138a9bd1bb84d9d12\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic1.zhimg.com/v2-33e328076bd6713e79c9680de04b229c_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-36041d93064d5dbeba947921ef34bfe0_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"388\" data-original-token=\"v2-19fee6de5b2d0a3de045a117a79af639\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-36041d93064d5dbeba947921ef34bfe0_r.jpg\"/></figure><p data-pid=\"5X28E4HA\">18，打工人有属于自己的圣诞树</p><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-f1966ee4ef33c2097d8ffc5213a7bba3_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"883\" data-original-token=\"v2-e28b8e364625a018901b8a7414552063\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic4.zhimg.com/v2-f1966ee4ef33c2097d8ffc5213a7bba3_r.jpg\"/></figure><p data-pid=\"gGgYyF2D\">19，带棕熊去医院不打麻醉，这操作也只有战斗民族才干得出来了</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-d98db19fd966a09dfde67c80e5c66500_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"902\" data-original-token=\"v2-a44f16dc68f6af86eddd842920dc02c4\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-d98db19fd966a09dfde67c80e5c66500_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-d4382ae42ee3b0cb92521560d41bb9b9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"889\" data-original-token=\"v2-6e9b8cba600e27a6b888ccbf795e05ee\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic4.zhimg.com/v2-d4382ae42ee3b0cb92521560d41bb9b9_r.jpg\"/></figure><p data-pid=\"rwSsXgeg\">20，宝见你别吃酷了，幺幺哒</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-929659da749cb7623f53ad087e724947_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"678\" data-original-token=\"v2-1a468ab4da2456c42e04598052ee3ed9\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://picx.zhimg.com/v2-929659da749cb7623f53ad087e724947_r.jpg\"/></figure><p data-pid=\"M-L-lubQ\">21，终于知道孔雀为什么不爱开屏了。后面孔雀： 坏点子已然生成……</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-46d8992ac01e84f547595e2aed837a7e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"554\" data-original-token=\"v2-35ddfba2725b6b2e5d15b8a1115caca6\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-46d8992ac01e84f547595e2aed837a7e_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-90c5e094297215e8fe5c37ff34f6dcfb_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"533\" data-original-token=\"v2-e90a45a195a498af4e646f055d1d7394\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic4.zhimg.com/v2-90c5e094297215e8fe5c37ff34f6dcfb_r.jpg\"/></figure><p data-pid=\"frzsvIzf\">22，能把围巾织成洗脸巾也是一种天赋</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-366679b7b3334fc4446d5f014da9aceb_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"892\" data-original-token=\"v2-8727622318c91b8809e1ac9f8e90e595\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://picx.zhimg.com/v2-366679b7b3334fc4446d5f014da9aceb_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-89764259ca6c69d709f27389815e1906_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"859\" data-original-token=\"v2-b8d14c56269f87d9f435a261ebb283cc\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic1.zhimg.com/v2-89764259ca6c69d709f27389815e1906_r.jpg\"/></figure><p data-pid=\"AyS3-mqb\">23，放了一年的吉他被拿来种多肉了</p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-d0c5fe000bc2a0f312d707e86d0e8914_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"948\" data-original-token=\"v2-86db0c25876ccc762d8d986a219c4e10\" class=\"origin_image zh-lightbox-thumb\" width=\"1440\" data-original=\"https://pic1.zhimg.com/v2-d0c5fe000bc2a0f312d707e86d0e8914_r.jpg\"/></figure><p data-pid=\"KmU0zX0K\">24，我都不敢靠近 </p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-ac13aa11015768264085f353e9475f50_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1282\" data-rawheight=\"1708\" data-original-token=\"v2-c7d92aa1c43e48ad904a51c77350e0e1\" class=\"origin_image zh-lightbox-thumb\" width=\"1282\" data-original=\"https://pic3.zhimg.com/v2-ac13aa11015768264085f353e9475f50_r.jpg\"/></figure><p data-pid=\"4oq-5VCk\">25，沈阳的甜品现在都这么抽象了吗？？？</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-87607e5dfef2a74471e38997f6d30b00_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"1176\" data-original-token=\"v2-d5eb3c06ee39f5980e6d76a82f447159\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-87607e5dfef2a74471e38997f6d30b00_r.jpg\"/></figure><p data-pid=\"Gsx5bGtX\">26，事实证明，跑步鞋只能跑步</p><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-dc90d2328b2aad3060224d4bd5b87947_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"546\" data-original-token=\"v2-00d6d347499a84bd745bc0c4a59fac9c\" class=\"origin_image zh-lightbox-thumb\" width=\"502\" data-original=\"https://picx.zhimg.com/v2-dc90d2328b2aad3060224d4bd5b87947_r.jpg\"/></figure><p data-pid=\"SXtRtGsf\">27，现在买夏威夷果都配钥匙了吗？#你夏威夷别墅的钥匙#</p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/v2-690cbeefc6691d0e72858a1d87a6fd0e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"828\" data-rawheight=\"1104\" data-original-token=\"v2-bbd2bdecdf4c0389acc3e99741a0cde5\" class=\"origin_image zh-lightbox-thumb\" width=\"828\" data-original=\"https://pic1.zhimg.com/v2-690cbeefc6691d0e72858a1d87a6fd0e_r.jpg\"/></figure><p data-pid=\"MeE9Q6DD\">28，螃蟹从180平方米的蒸锅里醒来</p><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-5c0525b1fc52f40c46ede2f5853cb541_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1282\" data-rawheight=\"1710\" data-original-token=\"v2-ea6522120d1155fb4ad3b193d2a400b8\" class=\"origin_image zh-lightbox-thumb\" width=\"1282\" data-original=\"https://pic4.zhimg.com/v2-5c0525b1fc52f40c46ede2f5853cb541_r.jpg\"/></figure><p data-pid=\"OAB_qPpm\">29，孩子的阿贝贝退休了</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/v2-c71f24804ac29aafe0d0c065e0e37d24_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1704\" data-rawheight=\"1278\" data-original-token=\"v2-eef843c4eec36c91684466739b120f88\" class=\"origin_image zh-lightbox-thumb\" width=\"1704\" data-original=\"https://pic3.zhimg.com/v2-c71f24804ac29aafe0d0c065e0e37d24_r.jpg\"/></figure><p data-pid=\"Hpc1Rja_\">30，蛋糕翻车了又好像没翻</p><figure data-size=\"normal\"><img src=\"https://pic2.zhimg.com/v2-7b978f5425cc4a6fe1345b158c5fd921_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"643\" data-original-token=\"v2-0a60091f11a51401f24c1eb155facbbf\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic2.zhimg.com/v2-7b978f5425cc4a6fe1345b158c5fd921_r.jpg\"/></figure><figure data-size=\"normal\"><img src=\"https://picx.zhimg.com/v2-a6a3e149c6e1765468be28f586cc1145_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"669\" data-original-token=\"v2-3156157de4360c55bb31af016b5a8bc7\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://picx.zhimg.com/v2-a6a3e149c6e1765468be28f586cc1145_r.jpg\"/></figure><p data-pid=\"UQ1hINTi\">感谢阅读~您的阅读是我持续更新的动力</p><p data-pid=\"Rz0Tyd6M\">我是<a href=\"https://www.zhihu.com/people/238b6d2be42e12cce2f20c6951953fa5\" class=\"internal\" target=\"_blank\">@夏南</a>，一个爱分享又带点假正经的创作者，走过路过期待您的点赞一个</p><p data-pid=\"jUPvKlEs\">更多精彩敬请期待哦~</p><a href=\"https://zhuanlan.zhihu.com/p/1985161339283460998\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-draft-title=\"听说有点搞笑的文案有哪些？\" data-draft-cover=\"https://pic1.zhimg.com/v2-732e7321688764d72cbcd53801856882.jpg?source=7e7ef6e2&amp;needBackground=1\" class=\"internal\">听说有点搞笑的文案有哪些？</a><a href=\"https://zhuanlan.zhihu.com/p/1985440080442184600\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-draft-title=\"有什么让你一看就笑的文案吗\" data-draft-cover=\"https://pic1.zhimg.com/v2-5ac893a0668c6e415cb7aea539c1fc8a.jpg?source=7e7ef6e2&amp;needBackground=1\" class=\"internal\">有什么让你一看就笑的文案吗</a><p></p>",
        "excerpt_new": "1，冬季的办公室都是保暖达人     2，看到这个明码标价的，我肯定会买。     3，找到座椅加热的平替了     4，亲眼目睹了大姨从超市进货     5，出国了但没完全出，不用看标签我都知道made in哪里。       6，嘘，小声点别把们吓跑了，小鸟绿树       7，找同事处理工作前要先取号     8，加热马桶吸引了错误的家伙，猫果然能精准找到家里最暖和的地方。   9，公司你真的好懂年会抽奖#了胜于无吧#       10，哈尔的移动城堡   11，当你点外卖忘记把默认备注改了…   12，女厕…",
        "id": "1986106167768921723",
        "type": "answer",
        "reshipment_settings": "allowed",
        "is_labeled": false,
        "url": "https://api.zhihu.com/answers/1986106167768921723",
        "question": {
          "title": "有哪些令人捧腹的笑话？",
          "relationship": {
            "is_author": false
          },
          "answer_count": 0,
          "bound_topic_ids": [
            1307,
            4447,
            4453,
            11772,
            95644
          ],
          "question_type": "normal",
          "type": "question",
          "author": {
            "name": "知乎用户wc9ydd",
            "avatar_url": "https://picx.zhimg.com/50/v2-ddc9cc13a4938674c49af1aee11433a0_l.jpg?source=b6762063",
            "is_org": false,
            "gender": 0,
            "followers_count": 1,
            "url": "https://api.zhihu.com/people/0e5ae00508dc1a23414ccdda93902fe1",
            "user_type": "people",
            "url_token": "k-silent",
            "is_following": false,
            "id": "0e5ae00508dc1a23414ccdda93902fe1",
            "headline": "",
            "is_followed": false
          },
          "comment_count": 57,
          "created": 1555572736,
          "follower_count": 0,
          "is_following": false,
          "excerpt": "",
          "detail": "",
          "id": "320669736",
          "url": "https://api.zhihu.com/questions/320669736"
        },
        "thumbnails": [
          "https://pic1.zhimg.com/50/v2-f0a6741d291fe533a58c149ebb1f92c8_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-4d245a738262f312e69c613a054a8569_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-d5c5f07a43b6b2ad3fdeea0a191f1406_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-08c222b22e674376c366c9ab0a155c0c_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-31742e27695b4605342dec6c870d4539_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-08ba6151910cd09bedbfb57c40f3e7df_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-ada1525b69419caaf863f9ebc56165b3_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-099c548e45a0d3f111461c5901aed81a_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-4d49704ec30f7ccde4573f5f441034e8_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-1ef8fbd07bd96115b159e67fd0a4c602_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-16d3bcf4b992f94490317ad0fe461b7b_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-7dcb0822cdcd5730e014b53477bc39b4_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-9670dced2c51615ce3e18a82c2f4f69c_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-76d98ecbfeba35188cb22640bdd30dd1_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-428720a35ca3525c58b25897eefd5912_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-4f641d1ea13b7a1a0f730710a53c2fba_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-ebd0905a6c0b4786da8ac21e665aa3b3_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-a5ffdcf985a18d2c13208475d857d9d6_720w.jpg?source=b6762063",
          "https://pica.zhimg.com/50/v2-b13960f89c2cab04b27d4a21574a5bed_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-7d74a0d277f2b92fbb5384d662fe726a_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-396ba4bc4a8e1561b853afb999517627_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-7393988037d6ab245a32a2bf56ce4065_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-9be1a98ec75d663f639208bf915509e9_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-aefa87f94aa413c7107126c7a0d8182b_720w.jpg?source=b6762063",
          "https://pica.zhimg.com/50/v2-d378af89bd85ed19e6869ddb06e4b506_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-12a72b288671490d087df33c584daed3_720w.jpg?source=b6762063",
          "https://pica.zhimg.com/50/v2-def269a13bfc743e54506ca7df3edaae_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-061b83bd5c5163416c81ebb006191ffc_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-153a0538572ecf4ebb7e8ade79f49e98_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-e28b8e364625a018901b8a7414552063_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-a44f16dc68f6af86eddd842920dc02c4_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-6e9b8cba600e27a6b888ccbf795e05ee_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-1a468ab4da2456c42e04598052ee3ed9_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-8727622318c91b8809e1ac9f8e90e595_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-b8d14c56269f87d9f435a261ebb283cc_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-86db0c25876ccc762d8d986a219c4e10_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-c7d92aa1c43e48ad904a51c77350e0e1_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-d5eb3c06ee39f5980e6d76a82f447159_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-bbd2bdecdf4c0389acc3e99741a0cde5_720w.jpg?source=b6762063",
          "https://pic1.zhimg.com/50/v2-ea6522120d1155fb4ad3b193d2a400b8_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-eef843c4eec36c91684466739b120f88_720w.jpg?source=b6762063",
          "https://picx.zhimg.com/50/v2-0a60091f11a51401f24c1eb155facbbf_720w.jpg?source=b6762063",
          "https://pica.zhimg.com/50/v2-3156157de4360c55bb31af016b5a8bc7_720w.jpg?source=b6762063"
        ],
        "is_copyable": true,
        "preview_text": ""
      },
      "brief": "{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1986106167768921723}",
      "attached_info": "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",
      "action_card": false
    },
    {
      "id": "86_1767705673.652",
      "type": "feed",
      "offset": 86,
      "verb": "TOPIC_ACKNOWLEDGED_ANSWER",
      "created_time": 1767705673,
      "updated_time": 1767705673,
      "target": {
        "excerpt_new": "这不就讲一个做人的道理么， 话要说给听得进去的人，听不进去你话的人，你的苦口婆心，好心好意，只会显得你又蠢又贱。所以讲话之前要先会看人，掂量下你在人心里啥分量 重视你的人才会听得进去，主动请教你的人，拿你当回事的人，可以试着说几句， 判断你在别人心里的分量，有时候短时间看不出来。 得细节观察，听进去了，能不能做到。 能做到的人才是听劝的，光嘴上说，你说的有理，但又不去做，后面就不用再说了。 真拿你当回…",
        "preview_type": "default",
        "visited_count": 33299,
        "answer_type": "normal",
        "is_navigator": false,
        "type": "answer",
        "is_copyable": false,
        "voteup_count": 1042,
        "thanks_count": 20,
        "comment_count": 40,
        "preview_text": "",
        "content": "<p data-pid=\"w8Ax4F9i\">这不就讲一个做人的道理么，</p><h3>话要说给听得进去的人，听不进去你话的人，你的苦口婆心，好心好意，只会显得你又蠢又贱。</h3><p data-pid=\"zRRwYer7\">所以讲话之前要先会看人，掂量下你在人心里啥分量</p><p data-pid=\"X36BAHNR\"><b>重视你的人才会听得进去，</b></p><p data-pid=\"yugJFWpA\">主动请教你的人，拿你当回事的人，可以试着说几句，</p><p data-pid=\"R37o-E3g\"><b>判断你在别人心里的分量，有时候短时间看不出来。</b></p><p data-pid=\"TKQda_Nk\"><b>得细节观察，听进去了，能不能做到。</b></p><p data-pid=\"n-7ubzHZ\"><b>能做到的人才是听劝的，光嘴上说，你说的有理，但又不去做，后面就不用再说了。</b></p><p data-pid=\"LrFNDbnf\"><b>真拿你当回事，那种对你的重视，你能感受到。</b></p><p data-pid=\"wYx40JpD\">这种人，你和他说一些建议，他只要能把你说的情况对得上，他会小心在意</p><p data-pid=\"-JeAXz9F\">不拿你当回事的人，你这人在他心里都不算什么，更何况你说的话呢，更没意义</p><p data-pid=\"4Jl6EX7W\"><b>不教年羹尧是因为年羹尧也没拿邬思道当回事。</b></p><p data-pid=\"8IFTXrkX\"><b>江夏镇那一件事，年羹尧就可以在邬思道心里定性了：心狠手辣，自命甚高，行事独断。他都不把雍正的话放心上。</b></p><p data-pid=\"sWD7ZNN9\"><b>张廷玉对年羹尧的告诫，年羹尧也没听进去。</b></p><p data-pid=\"TMAkocSn\">这种人一般是不听劝，也不甘于人下。</p><p data-pid=\"swV1DrRI\">年羹尧对邬思道客气不过是给雍正面子，</p><p data-pid=\"JOn6GFZm\">如果邬思道背景不是雍正，他也不敢去送粮去了。</p><p data-pid=\"hdGn63S1\"><b>年羹尧心里是看不上邬思道的，因为他没觉得他比邬思道差在哪。年羹尧进士出身，主管西北军政，邬思道一个连进士都考不上的幕僚，办不了实事的货色，够格给年大将军提意见么？</b></p><p data-pid=\"VUTj0tlU\">年羹尧听他说叛军在法轮寺，实际上叛军在哪年羹尧早就知道了</p><p data-pid=\"16YFQxYn\">不过是两个人一唱一和，暗地那意思很明白</p><p data-pid=\"nalpYAom\"><b>邬思道：别拖了，差不多得了，再养寇自重，皇帝那边要干不下去。</b></p><p data-pid=\"hRWmVSrL\"><b>年羹尧：行，那就办。</b></p><p data-pid=\"iTSJPjKf\">李卫和老十三不同，</p><p data-pid=\"8hGRRSfw\">李卫在剧里面是穷苦出身，邬思道神奇的地方，厉害的地方，李卫是见识过的，对李卫来说那是不一个维度的认知，在李卫看来，邬思道就是个神仙，所以邬思道讲话，李卫一定听，且听得进去。</p><p data-pid=\"o7WM_eV2\">老十三是个忠厚的人，老十三通过掌军和年羹尧有不同，年羹尧靠钱，老十三靠恩义，谈恩义的人一般都会重视别人的心思，懂得施恩，靠钱的人，不在意他们的想法，谈价就好了。<b>所以邬思道教老十三，他肯定也能听进去，做不做那就是另一码事了，邬思道也不管他听不听得进去，只是还他礼送的恩情而已。</b></p><p data-pid=\"vGByg4-5\"><b>所以好为人师的人最傻的地方就是在于不会看人，高估了自己在别人心里的分量，交浅言深。</b></p><p data-pid=\"vp4gyNfZ\"><b>相信很多人像我一样，反感一些人说：我是为你好，才会和你说什么什么。。。。</b></p><p data-pid=\"Wo8BCJn_\"><b>这就是好为人师人的共性。没拿你当回事，没想听你建议，还那么多废话。</b></p><p data-pid=\"cBw7ZtP2\"><b>邬思道是个人精，早就看明白年羹尧了，所以自然也不会多话。</b></p>",
        "relationship": {
          "is_thanked": false,
          "is_nothelp": false,
          "voting": 0
        },
        "url": "https://api.zhihu.com/answers/1987550568344998826",
        "created_time": 1766648982,
        "favorite_count": 387,
        "vote_next_step": "vote",
        "is_labeled": false,
        "navigator_vote": false,
        "allow_segment_interaction": true,
        "excerpt": "这不就讲一个做人的道理么， 话要说给听得进去的人，听不进去你话的人，你的苦口婆心，好心好意，只会显得你又蠢又贱。所以讲话之前要先会看人，掂量下你在人心里啥分量 重视你的人才会听得进去，主动请教你的人，拿你当回事的人，可以试着说几句， 判断你在别人心里的分量，有时候短时间看不出来。 得细节观察，听进去了，能不能做到。 能做到的人才是听劝的，光嘴上说，你说的有理，但又不去做，后面就不用再说了。 真拿你当回…",
        "reshipment_settings": "disallowed",
        "updated_time": 1766975170,
        "question": {
          "answer_count": 0,
          "title": "《雍正王朝》里的邬先生为什么只教老十三如何自保，却不教年羹尧？",
          "created": 1698848740,
          "follower_count": 0,
          "relationship": {
            "is_author": false
          },
          "detail": "",
          "question_type": "normal",
          "id": "628723221",
          "type": "question",
          "author": {
            "avatar_url": "https://picx.zhimg.com/50/v2-71ec55d109989756469bb92bc9cf18ab_l.jpg?source=b6762063",
            "gender": -1,
            "followers_count": 3,
            "id": "4021e3137a277c00a28ca06829c6ed49",
            "url": "https://api.zhihu.com/people/4021e3137a277c00a28ca06829c6ed49",
            "user_type": "people",
            "url_token": "ting-ting-40-85-53",
            "name": "婷婷",
            "headline": "",
            "is_org": false,
            "is_following": false,
            "is_followed": false
          },
          "is_following": false,
          "bound_topic_ids": [
            2102,
            5510,
            39455,
            176433,
            196775
          ],
          "excerpt": "",
          "url": "https://api.zhihu.com/questions/628723221",
          "comment_count": 2
        },
        "id": "1987550568344998826",
        "author": {
          "is_following": false,
          "is_followed": false,
          "id": "ee658969a40762ace79d1cf9f301176c",
          "user_type": "people",
          "url_token": "198805266911",
          "headline": "ENFJ，编程的，撸铁十多年了其中健力三年，健美6年，爱看史",
          "gender": 1,
          "followers_count": 51075,
          "url": "https://api.zhihu.com/people/ee658969a40762ace79d1cf9f301176c",
          "name": "实践出真知",
          "avatar_url": "https://picx.zhimg.com/50/v2-15edb52bf4f31434ba54c48792bc4584_l.jpg?source=b6762063",
          "is_org": false,
          "badge": [
            {
              "type": "best_answerer",
              "description": "健身话题下的优秀答主",
              "topic_names": [
                "健身"
              ],
              "topic_ids": [
                658
              ]
            },
            {
              "type": "super_activity",
              "description": "知势榜运动健身领域影响力榜答主"
            }
          ]
        }
      },
      "brief": "{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1987550568344998826}",
      "attached_info": "CsUFCKf2uOaz/qfLhQEQBBoJNzYyNjI0MzY0IJbZs8oGKJIIMChAVko/CipUU19TT1VSQ0VfWlJFQ0FMTF9GRUVEUkVfTkVXQklFX0hPVVJMWV9SVU0SATAYACAAOgp7InJhdyI6IiJ9WgkxMDE5OTQ5MjFiIGVhZWE1NDA2NmM4MTlhNmE3OWI5ZmNiZTFkOTIyMzM0chMxOTg3NTUwNTY4MzQ0OTk4ODI2igEJNjI4NzIzMjIxqgEJcmVjb21tZW5kwgEgZWU2NTg5NjlhNDA3NjJhY2U3OWQxY2Y5ZjMwMTE3NmPyAQoIDBIGTm9ybWFs8gEoCAoSJGYyNGIyYzU0LWY5MjctNGQ3Ni1hYzU1LTFmNjI1NmU3NmQ5ZfIBBggLEgIxNYICAIgCy//fm7kzkgIgZWU2NTg5NjlhNDA3NjJhY2U3OWQxY2Y5ZjMwMTE3NmOaAgDKAhRGaXJzdEJydXNoV2VpZ2h0UnVsZcoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXaAipUU19TT1VSQ0VfWlJFQ0FMTF9GRUVEUkVfTkVXQklFX0hPVVJMWV9SVU3oAgX6AgtOT1JNQUxfRkxPV4oDIGMzYmFkNWQ4Nzk1NDQwZTFiNzgxZDI5NmY5NDc5ZWIwmgMNCgJ2MhAAGgVvdGhlcqgDk4QC2AMA6gMQbmV3YmllX2ZlZWRyZV92MvoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAE1oAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAIBxO5I/gQUAAAAAAAAAAIkFOb7lVHt5sj+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFD5AGAKAGV6gGAJICLgoJNzYyNjI0MzY0EhMxOTg3NTUwNTY4MzQ0OTk4ODI2GAQiCklNQUdFX1RFWFQ=",
      "action_card": false
    },
    {
      "id": "87_1767705673.467",
      "type": "feed",
      "offset": 87,
      "verb": "TOPIC_ACKNOWLEDGED_ANSWER",
      "created_time": 1767705673,
      "updated_time": 1767705673,
      "target": {
        "answer_type": "normal",
        "navigator_vote": false,
        "updated_time": 1763212121,
        "thanks_count": 18,
        "comment_count": 44,
        "question": {
          "follower_count": 0,
          "question_type": "normal",
          "id": "302427662",
          "type": "question",
          "title": "大熊猫究竟有多可爱？",
          "relationship": {
            "is_author": false
          },
          "is_following": false,
          "excerpt": "",
          "url": "https://api.zhihu.com/questions/302427662",
          "created": 1542199841,
          "answer_count": 0,
          "detail": "",
          "author": {
            "is_followed": false,
            "user_type": "people",
            "name": "Mikey",
            "headline": "大家都可以去寻找自己的25号底片",
            "is_org": false,
            "followers_count": 1,
            "is_following": false,
            "id": "8ef347e8a96409fe465e8ec5ab5d2d72",
            "url": "https://api.zhihu.com/people/8ef347e8a96409fe465e8ec5ab5d2d72",
            "url_token": "xing-kong-86-13-68",
            "avatar_url": "https://picx.zhimg.com/50/v2-c3261da6184ed83fc43a8460fc494c0c_l.jpg?source=b6762063",
            "gender": 1
          },
          "comment_count": 23,
          "bound_topic_ids": [
            1571,
            6685,
            11141,
            249184
          ]
        },
        "excerpt_new": "经常看到川西路边看到大熊猫的新闻，于是，我每次川西自驾，都要在车上放一袋苹果（确认没打蜡那种），放一袋胡萝卜，希望能有好运气加持，能碰见野生大熊猫，然后把🍎和🥕拿来喂它们。 结果，到现在为止，一次都未能如愿。",
        "reshipment_settings": "allowed",
        "author": {
          "url": "https://api.zhihu.com/people/db265e51fb45bb1c2c9a0c64b8e62b17",
          "name": "你好我好大家好",
          "avatar_url": "https://picx.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063",
          "is_org": false,
          "gender": 1,
          "followers_count": 2242,
          "is_followed": false,
          "id": "db265e51fb45bb1c2c9a0c64b8e62b17",
          "user_type": "people",
          "url_token": "pa-wa-luo-di-26-19",
          "headline": "生活挺有意思的",
          "is_following": false
        },
        "created_time": 1763212121,
        "voteup_count": 265,
        "is_navigator": false,
        "id": "1973135329398649072",
        "is_copyable": true,
        "content": "<p data-pid=\"LZmTOZ-4\">经常看到川西路边看到大熊猫的新闻，于是，我每次川西自驾，都要在车上放一袋苹果（确认没打蜡那种），放一袋胡萝卜，希望能有好运气加持，能碰见野生大熊猫，然后把🍎和🥕拿来喂它们。</p><p data-pid=\"G0LZ3iBX\">结果，到现在为止，一次都未能如愿。</p>",
        "favorite_count": 6,
        "is_labeled": false,
        "visited_count": 30080,
        "type": "answer",
        "url": "https://api.zhihu.com/answers/1973135329398649072",
        "excerpt": "经常看到川西路边看到大熊猫的新闻，于是，我每次川西自驾，都要在车上放一袋苹果（确认没打蜡那种），放一袋胡萝卜，希望能有好运气加持，能碰见野生大熊猫，然后把🍎和🥕拿来喂它们。 结果，到现在为止，一次都未能如愿。",
        "preview_type": "default",
        "preview_text": "",
        "relationship": {
          "voting": 0,
          "is_thanked": false,
          "is_nothelp": false
        },
        "vote_next_step": "vote",
        "allow_segment_interaction": true
      },
      "brief": "{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1973135329398649072}",
      "attached_info": "CtwFCKf2uOaz/qfLhQEQBBoJNzU2NjczMDAzINn24cgGKIkCMCxAV0ooCh1UU19TT1VSQ0VfTkVBUkxJTkVfQ09OVEVOVF9WMhIBMBgAIAA6AFoIMjk0ODIxNzFiIGVhZWE1NDA2NmM4MTlhNmE3OWI5ZmNiZTFkOTIyMzM0chMxOTczMTM1MzI5Mzk4NjQ5MDcyigEJMzAyNDI3NjYyqgEJcmVjb21tZW5kwgEgZGIyNjVlNTFmYjQ1YmIxYzJjOWEwYzY0YjhlNjJiMTfyAQoIDBIGTm9ybWFs8gEoCAoSJDIwNWZlZjlhLWYyNjUtNDJhZi1hN2E5LWUzNmEyMTE4NTM2ZfIBBggLEgIxNYICAIgCy//fm7kzkgIgZGIyNjVlNTFmYjQ1YmIxYzJjOWEwYzY0YjhlNjJiMTeaAgDKAhRGaXJzdEJydXNoV2VpZ2h0UnVsZcoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhtJbnRlcmFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZcoCF1Rlc3RlZEFuZFdvcmtXZWlnaHRSdWxl2gIdVFNfU09VUkNFX05FQVJMSU5FX0NPTlRFTlRfVjLoAgL6AgtOT1JNQUxfRkxPV4oDIGMzYmFkNWQ4Nzk1NDQwZTFiNzgxZDI5NmY5NDc5ZWIwmgMNCgJ2MhAAGgVvdGhlcqgDgOsB2AMA+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATKgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAgENOrj+BBQAAAAAAAAAAiQU5vuVUe3myP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUPkAYAoAZYqAYAkgIuCgk3NTY2NzMwMDMSEzE5NzMxMzUzMjkzOTg2NDkwNzIYBCIKSU1BR0VfVEVYVA==",
      "action_card": false
    },
    {
      "id": "88_1767705673.934",
      "type": "feed",
      "offset": 88,
      "verb": "TOPIC_ACKNOWLEDGED_ARTICLE",
      "created_time": 1767705673,
      "updated_time": 1767705673,
      "target": {
        "favorite_count": 701,
        "navigator_vote": false,
        "allow_segment_interaction": true,
        "id": "1974238122373046567",
        "preview_text": "",
        "is_labeled": false,
        "content": "<h3>1、发布微头条：</h3><p data-pid=\"rU62kFzj\">截图热门新闻发布到微头条，简单写一下你的观点，一天几十块</p><h3>2、综合任务平台：</h3><p data-pid=\"QM3yripO\">微工社做数据标注、问卷填写，通勤时操作，月入 1500 + 很轻松</p><h3>3、闲鱼卖资料：</h3><p data-pid=\"YjdVZ06p\">某多多上两三块钱的资源，放闲鱼上能卖十几块</p><h3>4、番茄达人中心：</h3><p data-pid=\"OpzdfwOF\">0 粉丝接小说 / 短剧推广，官方授权有保障，新手友好</p><h3>5、小红书虚拟产品：</h3><p data-pid=\"bHO7Q4Um\">去某多多上面买点资源，挂小红书上面开店铺卖</p><h3>6、AI 数据标注：</h3><p data-pid=\"d4rTDaBa\">给图片打标签、语音转文字，熟练后 1 小时挣 20 元</p><h3>7、线上问卷调研：</h3><p data-pid=\"bYTC-7BI\">注册正规平台审问卷，筛选无效问卷两小时挣 120 元</p><h3>8、喜马拉雅讲故事：</h3><p data-pid=\"2DEraIpd\">录制一些讲故事的语音发布到喜马拉雅卖故事转机</p><h3>9、手机知识变现：</h3><p data-pid=\"RG6YeajU\">公综号直接搜：星图笔记，方法非常多</p><h3>10、网盘拉新推广：</h3><p data-pid=\"5F8hjufX\">迅雷、夸克、UC，一大堆网盘都可以推，单价还真不低</p><h3>11、小红书自媒体：</h3><p data-pid=\"XWQ5FMz_\">垂直领域输出（美妆 / 育儿），万粉后接商单，单条报价数千</p><h3>12、抖音星图任务：</h3><p data-pid=\"Bg_9NxP3\">500 粉开通权限，接游戏 / 短剧推广，站内直接提现</p><h3>13、知乎问答变现：</h3><p data-pid=\"Imitob9N\">分享专业经验，一篇优质回答能挣 2000 + 佣金</p><h3>14、头条图文分成：</h3><p data-pid=\"d3Fu0Ejs\">0 粉丝也能做，多平台分发同一内容，收益翻倍</p><h3>15、1 分钟知识胶囊：</h3><p data-pid=\"z6a-quu0\">手机拍 Excel 技巧 / 摄影干货，按点击量分成</p><h3>16、AI 内容变现：</h3><p data-pid=\"Kk0_C47y\">用工具生成短视频脚本，接单卖给博主，省时高效</p><h3>17、虚拟直播带货：</h3><p data-pid=\"pj01H4FA\">设计虚拟形象，AI 驱动 24 小时直播，不用露脸也能卖货</p><h3>18、图虫挣钱：</h3><p data-pid=\"73JZzTY8\">拍原创照片，人物或风景均可，上传到图虫APP卖图</p><h3>19、发布文档挣钱：</h3><p data-pid=\"0E8vj0um\">直接用AI改写别人的文档，再次上传到某度文库挣收益</p><h3>20、文案代写：</h3><p data-pid=\"Bnv1fxN7\">帮小店修改推广文案，一条挣 30 元，批量操作更挣钱</p><h3>21、配音陪玩：</h3><p data-pid=\"lFFl_6AA\">语音平台做游戏陪练 / 故事朗读，声音好听就有优势</p><h3>22、联盟营销：</h3><p data-pid=\"4cFpr6tW\">淘宝联盟选母婴 / 文具，生成专属链接发社群挣佣金</p><h3>23、本地商家服务：</h3><p data-pid=\"hve6KmKI\">拍餐饮店 15 秒视频，按条收费或按新增客户抽成</p><h3>24、知识付费植入：</h3><p data-pid=\"NNk5ykh2\">职场博主推荐课程，精准用户转化率超高</p><h3>25、共享闲置资源：</h3><p data-pid=\"x3yuHIb5\">出租家庭 WiFi 带宽 / 手机算力，APP 自动挣钱</p><h3>26、猪八戒接单：</h3><p data-pid=\"Joh_Hu_d\">可以用AI帮人设计LOGO或者商标，接单挣收益</p>",
        "visited_count": 11309,
        "article_type": "normal",
        "url": "https://api.zhihu.com/articles/1974238122373046567",
        "comment_count": 27,
        "excerpt_new": "1、发布微头条：截图热门新闻发布到微头条，简单写一下你的观点，一天几十块 2、综合任务平台：微工社做数据标注、问卷填写，通勤时操作，月入 1500 + 很轻松 3、闲鱼卖资料：某多多上两三块钱的资源，放闲鱼上能卖十几块 4、番茄达人中心：0 粉丝接小说 / 短剧推广，官方授权有保障，新手友好 5、小红书虚拟产品：去某多多上面买点资源，挂小红书上面开店铺卖 6、AI 数据标注：给图片打标签、语音转文字，熟练后 1 小时挣 20 元…",
        "preview_type": "default",
        "author": {
          "is_followed": false,
          "id": "415537b448f795c31826fd892b313cfa",
          "url": "https://api.zhihu.com/people/415537b448f795c31826fd892b313cfa",
          "gender": 0,
          "followers_count": 1899,
          "is_following": false,
          "is_org": false,
          "user_type": "people",
          "url_token": "95-65-92-57",
          "name": "东方驼子",
          "headline": "",
          "avatar_url": "https://pic1.zhimg.com/50/v2-9f0450777d3c4b498962e814c2c5d359_l.jpg?source=b6762063"
        },
        "comment_permission": "all",
        "created": 1763475337,
        "voteup_count": 148,
        "voting": 0,
        "linkbox": {
          "url": "",
          "category": "",
          "pic": "",
          "title": ""
        },
        "excerpt": "1、发布微头条：截图热门新闻发布到微头条，简单写一下你的观点，一天几十块 2、综合任务平台：微工社做数据标注、问卷填写，通勤时操作，月入 1500 + 很轻松 3、闲鱼卖资料：某多多上两三块钱的资源，放闲鱼上能卖十几块 4、番茄达人中心：0 粉丝接小说 / 短剧推广，官方授权有保障，新手友好 5、小红书虚拟产品：去某多多上面买点资源，挂小红书上面开店铺卖 6、AI 数据标注：给图片打标签、语音转文字，熟练后 1 小时挣 20 元…",
        "is_navigator": false,
        "type": "article",
        "title": "可以在手机上挣钱的26种方法",
        "updated": 1763475337,
        "vote_next_step": "vote"
      },
      "brief": "{\"source\": \"TS\", \"type\": \"article\", \"id\": 1974238122373046567}",
      "attached_info": "CswFCKf2uOaz/qfLhQEQBxoJMjY2NTYwNjEyIIn/8cgGKJQBMBtAWEooCh1UU19TT1VSQ0VfSU5URVJFU1RfV09SRF9NRVJHRRIBMBgAIAA6AGIgZWFlYTU0MDY2YzgxOWE2YTc5YjlmY2JlMWQ5MjIzMzRyEzE5NzQyMzgxMjIzNzMwNDY1NjeqAQlyZWNvbW1lbmTCASA0MTU1MzdiNDQ4Zjc5NWMzMTgyNmZkODkyYjMxM2NmYfIBCggMEgZOb3JtYWzyASgIChIkZWQzMWFlNGUtMTNlZS00OGRiLTlmNjItMzQ4NzM3ZDM5MGY58gEGCAsSAjE1ggIAiALL/9+buTOSAiA0MTU1MzdiNDQ4Zjc5NWMzMTgyNmZkODkyYjMxM2NmYZoCAMoCFEZpcnN0QnJ1c2hXZWlnaHRSdWxlygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxlygIXVGVzdGVkQW5kV29ya1dlaWdodFJ1bGXaAh1UU19TT1VSQ0VfSU5URVJFU1RfV09SRF9NRVJHRegCAvoCC05PUk1BTF9GTE9XigMgYzNiYWQ1ZDg3OTU0NDBlMWI3ODFkMjk2Zjk0NzllYjCaAw0KAnYyEAAaBW90aGVyqAOtWNgDAOoDIkludGVyZXN0V29yZE1lcmdlVjFOZXdQb29sUmVjYWxsZXL6Ax8SDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFgAQAiAQAkgQGTm9ybWFsmgQBMqAEAKgEALAEALoEAmFpwgQDNDAwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAAAAlz6ZP4EFAAAAAAAAAACJBTm+5VR7ebI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQ+QBgCgBlmoBgGSAi4KCTI2NjU2MDYxMhITMTk3NDIzODEyMjM3MzA0NjU2NxgHIgpJTUFHRV9URVhU",
      "action_card": false
    },
    {
      "id": "89_1767705673.348",
      "type": "feed",
      "offset": 89,
      "verb": "TOPIC_ACKNOWLEDGED_ANSWER",
      "created_time": 1767705673,
      "updated_time": 1767705673,
      "target": {
        "author": {
          "followers_count": 214,
          "id": "b7b3d5763175bf59490f3ee2f26b83be",
          "url": "https://api.zhihu.com/people/b7b3d5763175bf59490f3ee2f26b83be",
          "name": "麦门碳水",
          "headline": "渔夫 摸鱼王",
          "avatar_url": "https://pic1.zhimg.com/50/v2-507ebb8e050a43d976217c071ea8d0d1_l.jpg?source=b6762063",
          "is_org": false,
          "gender": 1,
          "is_following": false,
          "is_followed": false,
          "user_type": "people",
          "url_token": "meng-zi-yu-85"
        },
        "thanks_count": 13,
        "preview_type": "default",
        "preview_text": "",
        "is_navigator": false,
        "created_time": 1767638218,
        "voteup_count": 300,
        "comment_count": 88,
        "question": {
          "comment_count": 58,
          "bound_topic_ids": [
            3853,
            401921,
            176890,
            2265330,
            3069249
          ],
          "relationship": {
            "is_author": false
          },
          "author": {
            "avatar_url": "https://pic1.zhimg.com/50/v2-d24ffbc4102e48b6f12d8996682207ae_l.jpg?source=b6762063",
            "gender": -1,
            "followers_count": 1854,
            "is_followed": false,
            "url": "https://api.zhihu.com/people/d371683fd9673e521609ae7a4788e856",
            "name": "liisu",
            "url_token": "bo-ben-er-ba-26",
            "headline": "路人甲",
            "is_org": false,
            "is_following": false,
            "id": "d371683fd9673e521609ae7a4788e856",
            "user_type": "people"
          },
          "title": "马杜罗在美国纽约南区联邦地区法院首次出庭，称自己仍是委内瑞拉总统、「我无罪」，他可能面临怎样的判罚？",
          "created": 1767618090,
          "follower_count": 0,
          "is_following": false,
          "excerpt": "",
          "question_type": "normal",
          "id": "1991615303935813338",
          "type": "question",
          "answer_count": 0,
          "url": "https://api.zhihu.com/questions/1991615303935813338",
          "detail": ""
        },
        "vote_next_step": "vote",
        "id": "1991699724374397241",
        "url": "https://api.zhihu.com/answers/1991699724374397241",
        "content": "<p data-pid=\"LRMVEUjt\">真的给我气笑了。</p><ol><li data-pid=\"sJhEQb4j\">第一次庭审结束</li></ol><ul><li data-pid=\"kgqHB9ND\">该案件的法官是：现年92岁的地区法官阿尔文·海勒斯坦。</li><li data-pid=\"xJwxwvS5\">马杜罗聘请了辩护律师。律师是巴里·乔尔·波拉克。 波拉克曾经是维基解密创始人阿桑奇的辩护律师。</li><li data-pid=\"Gq_K_C0B\">美国法庭指控了。起诉书中列出了四项罪名，包括毒品，恐怖主义，阴谋罪和持有机关枪及爆炸装置罪。马杜罗对所有指控均不不认罪。 (PS: 别的就不说了。最后那个“持有机关枪和爆炸装置罪”是啥意思？人家在委内瑞加拿枪关你啥事啊。脸都不要了呗。)</li></ul><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-e5629082b6d2c3f2efe7fa86a04bd0fa_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"770\" data-rawheight=\"318\" data-original-token=\"v2-ae1e240f889e76621e5e96c6d2cc535a\" class=\"origin_image zh-lightbox-thumb\" width=\"770\" data-original=\"https://pica.zhimg.com/v2-e5629082b6d2c3f2efe7fa86a04bd0fa_r.jpg\"/><figcaption>Cite: BBC News</figcaption></figure><ul><li data-pid=\"hztWnIoV\"><b>美国法律专家都认为特朗普违法。但复杂的是，后面的审理又合法</b>。</li></ul><p data-pid=\"y088uPnn\">（消息来源原文：美国记者<b>凯拉·爱泼斯坦</b>采访了几位美国的法律专家，例如：克利夫兰州立大学法学院的米莱娜·斯特里奥。都认为美国将马杜罗带到纽约的行动明显违反了《联合国宪章》和其他国际法。但专家也表示，因为Maduro（马杜罗）现在身在美国，根据美国国内法，他接受审判几乎肯定是合法的。按照美国的过往判例，即使被告是被绑架、拐骗或强行带到美国的，也不是驳回案件的理由。）</p><p data-pid=\"P4H96Fx-\"><b>下一次开庭时 2026年3月17日。这次暂时就结束了。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p data-pid=\"C7BL9PA9\"><b>2. 附录</b></p><ol><li data-pid=\"8MvJ4shW\">法官<b>海勒斯坦</b>是<b>比尔·克林顿</b>任命的。</li></ol><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/v2-f1718f8703cbfb7175a1fcf3361d2c4b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1166\" data-rawheight=\"428\" data-original-token=\"v2-2b4038d3ed81aec3eec9834535f24ab5\" class=\"origin_image zh-lightbox-thumb\" width=\"1166\" data-original=\"https://pic4.zhimg.com/v2-f1718f8703cbfb7175a1fcf3361d2c4b_r.jpg\"/></figure><ul><li data-pid=\"O-nzFDoW\"><b>海勒斯坦</b>审理了前委内瑞拉军事+情报局局长<b>雨果·卡瓦哈尔</b>，罪名是毒品+恐怖主义。</li></ul><p data-pid=\"vzjFLUvL\">（具体情况：<b>卡瓦哈尔</b>是查韦斯时期的委内瑞拉。2019年，<b>卡瓦哈尔</b>因美国签发的逮捕令在西班牙被捕。该逮捕令指控他在2011年的贩毒罪行。西班牙法院批准将其引渡到美国后，<b>卡瓦哈尔</b>潜逃。2020年 美国国务院悬赏1000万美元。2021年，<b>卡瓦哈尔</b>在马德里被捕，两年后被引渡到美国。2025年，<b>卡瓦哈尔</b>承认了所有刑事指控。）</p><ul><li data-pid=\"1pqBLdwF\"> 他2025年5月裁定，特朗普驱逐委内瑞拉人的做法违宪。</li></ul><p data-pid=\"F5aLfJ2R\">（信源：The Washington Post: N.Y. judge finds Alien Enemies Act use illegal, blocks removals to ‘evil’ jail）</p><p data-pid=\"3ictvEmO\" class=\"ztext-empty-paragraph\"><br/></p><ul><li data-pid=\"NPCpercF\"><b>委内瑞拉副总统等人说可以主动送马杜鲁给美国，特朗普没同意。这事沟通前还和马杜罗汇报过了。</b>看样子今天的剧本是委内瑞拉高官-美国-马杜罗 本人三方早就接受了。只是形式细节上不同。马杜罗看起来内心深处早就半摆烂了...似乎就等着被抓那一天呢。</li></ul><p data-pid=\"bnfZC4Hr\">“以<b>罗德里格斯（现在的委内瑞拉临时总统）</b>及其兄弟（<b>豪尔赫-罗德里格斯</b>）为中心的委内瑞拉官员圈子私下制定了后马杜罗时代委内瑞拉的路线图，<b>罗德里格斯</b>将作为傀儡，流亡将军<b>米格尔·罗德里格斯·托雷斯</b>将领导一个被称为“没有马杜罗的马杜里主义”的“过渡政府”。作为回报，委内瑞拉将欢迎美国投资者，并逐步疏远与美国的竞争对手的关系，如伊朗和俄罗斯。委内瑞拉的提议是通过卡塔尔渠道提交给美国特使<span class=\"nolink\">理查德·格雷内尔</span>的。据报道，特朗普政府拒绝了这些提议。”</p><p data-pid=\"PNkmp6Wf\">另外据报道“以上消息，这些人和马杜罗说过这个事了”（信源：2025年10月16日迈阿密先驱报的报道）</p>",
        "is_labeled": false,
        "visited_count": 78932,
        "favorite_count": 62,
        "relationship": {
          "voting": 0,
          "is_thanked": false,
          "is_nothelp": false
        },
        "answer_type": "normal",
        "type": "answer",
        "updated_time": 1767642375,
        "is_copyable": true,
        "excerpt": "真的给我气笑了。 第一次庭审结束该案件的法官是：现年92岁的地区法官阿尔文·海勒斯坦。马杜罗聘请了辩护律师。律师是巴里·乔尔·波拉克。 波拉克曾经是维基解密创始人阿桑奇的辩护律师。美国法庭指控了。起诉书中列出了四项罪名，包括毒品，恐怖主义，阴谋罪和持有机关枪及爆炸装置罪。马杜罗对所有指控均不不认罪。 (PS: 别的就不说了。最后那个“持有机关枪和爆炸装置罪”是啥意思？人家在委内瑞加拿枪关你啥事啊。脸都不要…",
        "excerpt_new": "真的给我气笑了。 第一次庭审结束该案件的法官是：现年92岁的地区法官阿尔文·海勒斯坦。马杜罗聘请了辩护律师。律师是巴里·乔尔·波拉克。 波拉克曾经是维基解密创始人阿桑奇的辩护律师。美国法庭指控了。起诉书中列出了四项罪名，包括毒品，恐怖主义，阴谋罪和持有机关枪及爆炸装置罪。马杜罗对所有指控均不不认罪。 (PS: 别的就不说了。最后那个“持有机关枪和爆炸装置罪”是啥意思？人家在委内瑞加拿枪关你啥事啊。脸都不要…",
        "reshipment_settings": "allowed",
        "navigator_vote": false,
        "allow_segment_interaction": true
      },
      "brief": "{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1991699724374397241}",
      "attached_info": "CpMGCKf2uOaz/qfLhQEQBBoJNzY0MTgwNTA3IMqJ8MoGKKwCMFhAWUowChtUU19TT1VSQ0VfRkVFRFJFX1RJTUVMSU5FU1MSATAYACAAOgp7InJhdyI6IiJ9WgkxMTc5NjQ2ODRiIGVhZWE1NDA2NmM4MTlhNmE3OWI5ZmNiZTFkOTIyMzM0chMxOTkxNjk5NzI0Mzc0Mzk3MjQxigETMTk5MTYxNTMwMzkzNTgxMzMzOKoBCXJlY29tbWVuZMIBIGI3YjNkNTc2MzE3NWJmNTk0OTBmM2VlMmYyNmI4M2Jl8gEKCAwSBk5vcm1hbPIBKAgKEiRjZmQ0ZDMyMy0xOGU4LTRjY2UtOGJjZC03YzczNTFiMGVjMTDyAQYICxICMTWCAgCIAsv/35u5M5ICIGI3YjNkNTc2MzE3NWJmNTk0OTBmM2VlMmYyNmI4M2JlmgIAygIURmlyc3RCcnVzaFdlaWdodFJ1bGXKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxl2gIbVFNfU09VUkNFX0ZFRURSRV9USU1FTElORVNT6AIC+gILTk9STUFMX0ZMT1eKAyBjM2JhZDVkODc5NTQ0MGUxYjc4MWQyOTZmOTQ3OWViMJoDDQoCdjIQABoFb3RoZXKoA9ToBNgDAOoDFGZlZWRyZV90aW1lbGluZXNzX3Yy+gN9EgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAIQggYYvgIiI3YyLWFlMWUyNDBmODg5ZTc2NjIxZTVlOTZjNmQyY2M1MzVhOi0IAxCOCRisAyIjdjItMmI0MDM4ZDNlZDgxYWVjM2VlYzk4MzQ1MzVmMjRhYjWABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAKDx/pE/gQUAAAAAAAAAAIkFOb7lVHt5sj+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFD5AGAKAGWqgGAJICLgoJNzY0MTgwNTA3EhMxOTkxNjk5NzI0Mzc0Mzk3MjQxGAQiCklNQUdFX1RFWFQ=",
      "action_card": false
    }
  ],
  "paging": {
    "is_end": false,
    "is_start": false,
    "next": "https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down&ad_interval=-10&after_id=89&desktop=true&end_offset=90&page_number=16&session_token=eaea54066c819a6a79b9fcbe1d922334",
    "previous": "https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull&ad_interval=-10&before_id=89&desktop=true&end_offset=90&page_number=16&session_token=eaea54066c819a6a79b9fcbe1d922334",
    "totals": 0
  },
  "fresh_text": "推荐已更新"
}