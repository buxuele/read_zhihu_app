标题: 深度学习调参有哪些技巧？
类型: answer
赞数: 6289
链接: https://www.zhihu.com/question/25097993/answer/2717281021
创建时间: 1665885299
--------------------------------------------------

分享下调参心得：先overfit 再trade off，首先保证你的模型capacity能够过拟合，再尝试减小模型，各种正则化方法；lr ，最重要的参数，一般nlp bert类模型在1e-5级别附近，warmup，衰减；cv类模型在1e-3级别附近，衰减；具体需要多尝试一下。batch size 在表示学习，对比学习领域一般越大越好，显存不够上累计梯度，否则模型可能不收敛… 其他领域看情况；dropout，现在大部分任务都需要使用预训练模型，要注意模型内部dropout ratio是一个很重要的参数，使用默认值不一定最优，有时候dropout reset到0有奇效初始化方法，linear / cnn一般选用kaiming uniform 或者normalize，embedding 一般选择截断 normalize，论文很多，可以去看看。序列输入上LN，非序列上BN基于banckbone 构建层次化的neck 一般都比直接使用最后一层输出要好，reduce function 一般attention 优于简单pooling，多任务需要构建不同的qkv数据增强要结合任务本身来设计随机数种子设定好，否则很多对比实验结论不一定准确cross validation方式要结合任务设计，数据标签设计，其中时序数据要避免未来信息泄漏优化器，nlp，抽象层次较高或目标函数非常不平滑的问题adam优先，其他可以尝试下sgd（一般需要的迭代次数高于sgd）不要过早的early stopping，有时候收敛平台在后段，你会错过，参考1. ，先过拟合train set收藏的同学点个赞啊….需要点义务劳动的动力