{"data":[{"id":"102_1750898493.996","type":"feed","offset":102,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898493,"updated_time":1750898493,"target":{"id":"1916857372900571124","type":"answer","url":"https://api.zhihu.com/answers/1916857372900571124","author":{"id":"892ebcab34773e9488cb8e1d9616f571","url":"https://api.zhihu.com/people/892ebcab34773e9488cb8e1d9616f571","user_type":"people","url_token":"codesdhx","name":"吴师兄学算法","headline":"专注华为OD备考全流程辅导，已帮助数百位同学｜算法｜答疑陪跑","avatar_url":"https://pic1.zhimg.com/50/v2-5349a529ef0a89fd9781b5c1f6d0e654_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":97176,"is_following":true,"is_followed":false},"created_time":1749794411,"updated_time":1749794411,"voteup_count":5,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"560578292","type":"question","url":"https://api.zhihu.com/questions/560578292","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pic1.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"华为OD这么差，为什么985/211还会去？","created":1666080593,"answer_count":0,"follower_count":0,"comment_count":8,"bound_topic_ids":[5263,14734,15114],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"在讨论这个问题前，先简单解释一下 华为OD：这是华为自2018年试点、2020年全面推行的一种用工模式，英文全称 Outsourcing Dispatch（外包派遣）。OD员工与第三方人力公司签约（如外企德科/FESCO等），在华为基地和正式员工一同办公，但身份上属于“非华为正式编制”。 正因为这种特殊身份，网络上对OD位置评价很不客气，甚至流传着“华为正编 \u003e 华为OD \u003e 中小厂正编 \u003e 华为外包”的鄙视链。 那么OD究竟存在哪些问题？在这种情况…","excerpt_new":"在讨论这个问题前，先简单解释一下 华为OD：这是华为自2018年试点、2020年全面推行的一种用工模式，英文全称 Outsourcing Dispatch（外包派遣）。OD员工与第三方人力公司签约（如外企德科/FESCO等），在华为基地和正式员工一同办公，但身份上属于“非华为正式编制”。 正因为这种特殊身份，网络上对OD位置评价很不客气，甚至流传着“华为正编 \u003e 华为OD \u003e 中小厂正编 \u003e 华为外包”的鄙视链。 那么OD究竟存在哪些问题？在这种情况…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"HxStjjaV\"\u003e在讨论这个问题前，先简单解释一下\u003cb\u003e华为OD\u003c/b\u003e：这是华为自2018年试点、2020年全面推行的一种用工模式，英文全称 \u003ci\u003eOutsourcing Dispatch\u003c/i\u003e（外包派遣）。\u003c/p\u003e\u003cp data-pid=\"tI06H4HG\"\u003eOD员工与第三方人力公司签约（如外企德科/FESCO等），在华为基地和正式员工一同办公，但身份上属于“非华为正式编制”。\u003c/p\u003e\u003cp data-pid=\"ao4SlYpU\"\u003e正因为这种特殊身份，网络上对OD位置评价很不客气，甚至流传着“华为正编 \u0026gt; 华为OD \u0026gt; 中小厂正编 \u0026gt; 华为外包”的鄙视链。\u003c/p\u003e\u003cp data-pid=\"MZXG7wiC\"\u003e\u003cb\u003e那么OD究竟存在哪些问题？在这种情况下，为何依然有985/211高校的高材生选择去做OD？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"vONdWV1c\"\u003e下面我们从理性角度分别分析华为OD岗位的弊端，以及近年来促使名校毕业生接受OD的现实背景。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e华为OD岗位存在的问题\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"Sw0-Qxob\"\u003e华为OD饱受争议，原因在于它相对于正式编制有不少劣势。\u003c/p\u003e\u003cp data-pid=\"sXgOalm8\"\u003e归纳主要有以下几点：\u003c/p\u003e\u003ch3\u003e\u003cb\u003e1、工作强度大，加班频繁\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"gpD648d2\"\u003eOD员工的日常工作强度并不比正式员工低，经常需要长时间工作和加班。\u003c/p\u003e\u003cp data-pid=\"NgnwBe4p\"\u003e有2024届应届生分享自己的OD经历称，入职半年“每天9:30上班，晚上9点多下班，一天干10个小时以上”，永远有干不完的活。\u003c/p\u003e\u003cp data-pid=\"ntQ5Pdi1\"\u003e而且OD普遍实行\u003cb\u003e“996+1”\u003c/b\u003e的节奏：周末双休但每月最后一个周六强制加班（有双倍工资）。\u003c/p\u003e\u003cp data-pid=\"hisUY70T\"\u003e高强度、高压力下，OD员工调侃自己“累到只剩下加班和开会，写代码都是抽空在写”。长期如此高负荷工作，对个人身心是巨大考验。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e2、身份尴尬，正式员工待遇享受不到\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"Bfr2fHTZ\"\u003e作为外包派遣，OD员工在华为内部被视为“二等公民”。\u003c/p\u003e\u003cp data-pid=\"K-_J5f6A\"\u003e正如媒体总结的那样：OD员工往往\\“身份‘低人一等’，薪资无分红”。\u003c/p\u003e\u003cp data-pid=\"ubGfvx1t\"\u003e具体来说，OD没有华为员工工号（OD工号以“300”开头，正式员工是“00”开头），进入园区忘带工牌时需要正式员工带领。\u003c/p\u003e\u003cp data-pid=\"8xIPoZDH\"\u003e更重要的是，\u003cb\u003eOD几乎无法参与公司核心技术项目，且没有华为每年的股票分红\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"SaOx29D0\"\u003e在华为内网社区甚至出现过租房、交友帖注明“OD勿扰”的现象，可见OD身份在一些正式员工眼中被贴有标签。这种身份上的隔阂，容易让OD员工缺乏归属感。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e3、同工不同酬，晋升通道受限\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"XcFHjJ_S\"\u003e薪资待遇方面，华为OD与正式员工存在明显差距。虽然OD入职时会按能力定级（D1-D5对应华为13-17级岗位），但实际薪酬往往低一个档次\u003cb\u003e。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"3ghS0xOt\"\u003e\u003cb\u003e例如，OD 的 D1 级对应正编13级岗，但D1月薪一般9000-13000元；而正式13级员工不仅有相近的基本工资，还有年底分红、股权激励等额外收益。OD员工不仅拿不到这些分红股票，年终奖上限也比正式员工少很多（通常2-4个月工资，而正式员工可能更多）。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"WPOoATib\"\u003e\u003cb\u003e更麻烦的是，\u003c/b\u003eOD转正率低，晋升空间狭窄：华为允许OD有机会转为正式员工，但成功比例常年不到5%。\u003c/p\u003e\u003cp data-pid=\"ltwSG17S\"\u003e根据统计数据，2020下半年到2022年底，华为OD在职人数已超过3万人，但同期成功转正的仅约3000多人。\u003c/p\u003e\u003cp data-pid=\"rAsTcsXJ\"\u003e绝大多数OD员工可能长期停留在外包序列，即便能力再突出也很难跻身“00后”正式编制行列。这种天花板效应使得OD更像是华为的人力蓄水池，真正往上流动的人寥寥。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e4、技术成长有限，心理压力大\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"aU6DGfBK\"\u003e不少OD员工反映，在岗位上接触的多是基础性、支撑性工作，真正核心研发的机会不多。\u003c/p\u003e\u003cp data-pid=\"3YTbyF2O\"\u003e有的团队把低价值的活都交给OD做，导致有些OD“干了很多杂事，技术上学不到新东西”，担心这样下去履历变差。同时，由于培训资源向正式员工倾斜，OD新人的培养往往不系统——有员工抱怨导师只是丢给他几份文档自学，几乎得不到手把手指导。\u003c/p\u003e\u003cp data-pid=\"0_4SmSSd\"\u003e在高压环境下，OD普遍有一种“不上不下”的心态：工作既累收入又不算高。一些人在华为背书下勉强维持，但内心对前途感到迷茫和焦虑。\u003c/p\u003e\u003cp data-pid=\"OovkXMYF\"\u003e长此以往，心理上的压力和身份认同的落差会严重影响工作的投入度。\u003c/p\u003e\u003cp data-pid=\"kdp9c5j9\"\u003e综上，华为OD的确存在强度大、身份尴尬、待遇和晋升不如人意等诸多短板。用很多网友的话来说就是\u003cb\u003e“又累又没钱”\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"NhSR8WoU\"\u003e正因为如此，过去一段时间里，OD一度被不少求职者视为下下策。然而，现实并非静止不变的，在当前就业环境下，即使是985/211名校毕业生，有时也不得不权衡利弊，把OD当作权宜之计。\u003c/p\u003e\u003cp data-pid=\"JlCzL035\"\u003e下面重点分析促使高学历人才选择OD的现实背景因素。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e促使985/211高材生选择华为OD的现实背景\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"dDd7Ztnp\"\u003e应届生求职周期拉长，许多毕业生错过了校园招聘的黄金档。很多人毕业数月仍未落实去向，不再像往届那样“毕业即就业”，而是陷入漫长的二次就业市场竞逐。\u003c/p\u003e\u003cp data-pid=\"9i2j_-ut\"\u003e考研“二战”失利人群增多，学业规划落空：近年来“考研热”高涨，不少985/211高校生选择毕业后脱产重考研究生。\u003c/p\u003e\u003cp data-pid=\"3i9CAghk\"\u003e然而考研名额有限，僧多粥少。\u003c/p\u003e\u003cp data-pid=\"2WC2KJzh\"\u003e海量考研落榜生中相当一部分就是前一年信心满满准备“二战”的往届生。\u003c/p\u003e\u003cp data-pid=\"02P_io-g\"\u003e近年\\重考族人数在增加，但成功率在下降\u003cb\u003e，越来越多优秀学子耗费了一年光阴却再度失利。这部分人原本学业计划落空，又因为专注备考错过了校园招聘良机，只能仓促回到就业市场。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"FLzN3ZLL\"\u003e\u003cb\u003e对于这些\u003c/b\u003e考研二战失败的高学历者来说，华为OD提供了一个相对可行的“曲线就业”途径：在无法读研又错过校招的情况下，起码还能先去大厂积累经历，为以后再战做准备。\u003c/p\u003e\u003cp data-pid=\"u6wf42_0\"\u003e应届招聘旺季错过，毕业生“慢就业”现象普遍：上文提到秋招大量人没拿到Offer，这意味着相当多名校毕业生没能在毕业前签约心仪工作。\u003c/p\u003e\u003cp data-pid=\"5svGwf4p\"\u003e\u003cb\u003eOD提供“缓冲带”和“跳板”的现实意义：对于走投无路或陷入长时间待业的高学历者来说，华为OD的吸引力在于它提供了一个**过渡机会\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"_ihKqmu9\"\u003e首先，OD招聘相对高效和包容。去年，我们训练营里面，有很多同学都是秋招offer被毁约的情况，投了华为OD，两周就拿到offer，解了燃眉之急。\u003c/p\u003e\u003cp data-pid=\"-I8Ia_Gc\"\u003e这种\u003cb\u003e“及时雨”式的就业机会\u003c/b\u003e，为那些错过校招的毕业生提供了宝贵的缓冲期。\u003c/p\u003e\u003cp data-pid=\"lzBO3GzP\"\u003e其次，尽管OD转正比例不高，但\u003cb\u003e“边干边等”至少保留了一线希望\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"tWLaUz94\"\u003e华为明确表示OD“有机会”转正，哪怕实际概率不到5%，总有人愿意拼一把努力争取。\u003c/p\u003e\u003cp data-pid=\"irMBBjGm\"\u003e很多投身OD的985/211毕业生心态就是：“只要肯努力，就有概率转正成为华为正式员工，这样的机会在别处很难得”。哪怕最后没能转正，在华为参与项目的经历也为简历增色。\u003c/p\u003e\u003cp data-pid=\"Efs-om6H\"\u003e\u003cb\u003e华为OD经历在求职市场上并非一文不值\u003c/b\u003e，相反，如果学历过硬且积累了实际项目经验，不少人后来跳到了BAT等一线互联网公司。可以说，OD充当了他们通向理想岗位的一个跳板：先凭华为的工作背景站稳脚跟，再伺机“上岸”更好的职业发展轨道。\u003c/p\u003e\u003cp data-pid=\"MqIQqF60\"\u003e当然，OD毕竟带有外包性质，作为第一份工作可能会“污染简历”。但现实中，用人单位更看重的是求职者是否具备名校背景和大厂经验的组合。因此只要合理规划，\u003cb\u003e将OD当作临时过渡而非终点站\u003c/b\u003e，它的正面效用是可以发挥出来的。\u003c/p\u003e\u003cp data-pid=\"wUPnA3ue\"\u003e在当下，\u003cb\u003e“进大厂”本身变得比过去难太多\u003c/b\u003e。高学历毕业生面临的选择题往往不是“大厂正式 vs 中小企业”，而是“曲线进入大厂（OD） vs 长期待业”这样的现实抉择。\u003c/p\u003e\u003cp data-pid=\"Zz_WiiOM\"\u003e相比之下，华为OD虽然不是理想中的高薪优岗，但至少薪资不算太低（D1-D3级OD月薪大致9K~17K区间），福利上该有的加班费、年假、补贴也有，并且工作内容和环境与大厂正职接轨。对比去小厂或做与专业无关的工作，很多985/211毕业生会觉得\u003cb\u003e“退而求其次，先在华为OD积累一下”\u003c/b\u003e是性价比相对更高的选择。\u003c/p\u003e\u003cp data-pid=\"94U9FQCR\"\u003e一句话，\u003cb\u003e大环境不给力，好岗位稀缺，OD就成了无奈之下的次优解\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"zFJJ1G13\"\u003e综上，这些现实因素叠加，使得不少原本眼高手高的名校生，毕业时不得不放下身段选择华为OD。一方面，他们清楚OD有诸多不足，但另一方面，\u003cb\u003e在考研和求职双双碰壁、经济形势又不景气的情况下，OD至少提供了一个体面的缓冲平台\u003c/b\u003e，让他们有机会继续追求后续的发展。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e结语：理性看待“现实选择”OD\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"UNRgKHG_\"\u003e总的来说，华为OD并不是一个完美的工作选择，它有明显短板，难称理想岗位。然而如上所述，在特定的环境和个人处境下，它扮演了“雪中送炭”的角色。\u003c/p\u003e\u003cp data-pid=\"94SCtikQ\"\u003e对于那些考研失利、求职受挫的985/211毕业生而言，\u003cb\u003eOD充当了一个让他们“不掉队”的缓冲区\u003c/b\u003e：既避免长期失业带来的沉没成本，又能借此获得大厂历练的机会。\u003c/p\u003e\u003cp data-pid=\"OHf0z-tl\"\u003e我们应当\u003cb\u003e理性地看待OD\u003c/b\u003e：它绝非最优解，更谈不上什么“香饽饽”，但在职业生涯的某些阶段、对某些陷入困境的高学历求职者来说，却可能是一个性价比较高的“\u003cb\u003e现实选择\u003c/b\u003e”。\u003c/p\u003e\u003cp data-pid=\"8TjXQweJ\"\u003e这种选择并非出于理想，而是现实使然。每个人的境遇不同，职场起点有高有低。选择去华为OD，不代表他们没有追求，只能说明他们愿意务实地积累经验、等待下一次机遇。当宏观环境好转、自身实力提升后，他们依然有机会跳向更好的平台。\u003c/p\u003e\u003cp data-pid=\"TP72X27h\"\u003e简言之，\u003cb\u003e华为OD可以是阶段性的权宜之计，但不应成为最终的归宿\u003c/b\u003e。对于求职者个人而言，重要的是在“现实选择”和“理想追求”之间找到平衡：既懂得务实低头拉车，也不放弃抬头看路。\u003c/p\u003e\u003cp data-pid=\"q4-OJvUg\"\u003e华为OD只是人生的一段经历，如果能善加利用，从中成长，那么日后回望，这段历练或许也将成为逆风翻盘的垫脚石，而不只是无奈的将就。\u003c/p\u003e\u003cp data-pid=\"aTLMmlqm\"\u003e参考链接：\u003ca href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/F0aKjhCZIgeHgqG9wPREeg\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e华为OD这么差，为什么985/211还会去？\u003c/a\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":2083,"favorite_count":5,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1916857372900571124}","attached_info":"CsEGCNu3uMHCytKhqwEQBBoJNzMyMDA0ODI4IOv8rsIGKAUwAEBmSj8KLFRTX1NPVVJDRV9GT0xMT1dfQVVUSE9SX1RWUF9OT19DTElDS19JTVBST1ZFEgkxMDMyNzIxNjEYACAAOgBaCDg2ODU1NzA4YiA1OTAxOTkzODk3YjJiNjcxNTM0YjQxZGZlMTcwMzk0N3ITMTkxNjg1NzM3MjkwMDU3MTEyNIoBCTU2MDU3ODI5MqoBCXJlY29tbWVuZMIBIDg5MmViY2FiMzQ3NzNlOTQ4OGNiOGUxZDk2MTZmNTcx8gEKCAwSBk5vcm1hbPIBKAgKEiQ2NjI0NDM1Ni05NTFjLTRkZmQtOGJhYi1kZWU3NjQ3ZTJhMTPyAQYICxICMTiCAgCIAtKTu836MpICIDg5MmViY2FiMzQ3NzNlOTQ4OGNiOGUxZDk2MTZmNTcxmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFkFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhtJbnRlcmFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhhQZXJpb2RJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZdoCLFRTX1NPVVJDRV9GT0xMT1dfQVVUSE9SX1RWUF9OT19DTElDS19JTVBST1ZF6AID+gILTk9STUFMX0ZMT1eKAyAwMWNmODA5MGM1Nzk0ODk5YTRlZDA5NDBmN2Y5MzEyY5oDDQoCdjIQABoFb3RoZXKoA6MQ2AMB6gMfZmVlZDdGb2xsb3dBdXRob3JOb0NsaWNrSW1wcm92ZfoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQGbWFudWFswgQDMTcwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAACgXEe+P4EFAAAAAAAAAACJBQCofMr1mtI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRKQBgCgBmqoBgGSAi4KCTczMjAwNDgyOBITMTkxNjg1NzM3MjkwMDU3MTEyNBgEIgpJTUFHRV9URVhU","action_card":false},{"id":"103_1750898493.529","type":"feed","offset":103,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750898493,"updated_time":1750898493,"target":{"id":"49358536242","type":"article","url":"https://api.zhihu.com/articles/49358536242","author":{"id":"4df18ee2f3c5fe141cf45389a6a3fa48","url":"https://api.zhihu.com/people/4df18ee2f3c5fe141cf45389a6a3fa48","user_type":"people","url_token":"hai-dao-chuan-da-chu-8800","name":"南门子","headline":"学生","avatar_url":"https://pic1.zhimg.com/50/v2-94878c2bcd831d7b58506c3aa9dbffd8_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"北京大学 软件工程博士在读"}],"followers_count":368,"is_following":false,"is_followed":false},"title":"六万字长文一次性说清 LLM 的后训练技术","comment_permission":"all","created":1749609944,"updated":1749609944,"voteup_count":168,"voting":0,"comment_count":5,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"文章题目：A SURVEY ON POST-TRAINING OF LARGE LANGUAGE MODELS Arxiv 链接：https://arxiv.org/pdf/2503.06072v1 普遍认为，真正的智能赋予我们推理能力，使我们能够检验假设，并为未来的可能性做好准备。—— Jean Khalfa，《什么是智能？》，1994摘要大型语言模型（LLMs）的出现从根本上改变了自然语言处理，使其在从对话系统到科学探索等多个领域不可或缺。然而，它们的预训练架构在特定情境下常常暴露出局限性，包括推理能力有限、伦理不…","excerpt_new":"文章题目：A SURVEY ON POST-TRAINING OF LARGE LANGUAGE MODELS Arxiv 链接：https://arxiv.org/pdf/2503.06072v1 普遍认为，真正的智能赋予我们推理能力，使我们能够检验假设，并为未来的可能性做好准备。—— Jean Khalfa，《什么是智能？》，1994摘要大型语言模型（LLMs）的出现从根本上改变了自然语言处理，使其在从对话系统到科学探索等多个领域不可或缺。然而，它们的预训练架构在特定情境下常常暴露出局限性，包括推理能力有限、伦理不…","preview_type":"default","preview_text":"","column":{"id":"c_1912455755723932411","type":"column","url":"https://api.zhihu.com/columns/c_1912455755723932411","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://picx.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"NLP 论文阅读笔记","imageUrl":"https://pica.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b","comment_permission":"public","intro":"\u003cp\u003e本系列汇总 NLP 领域高分顶会/顶刊论文，提供高质量机翻和阅读笔记\u003c/p\u003e","updated":1748745336,"is_following":false},"content":"\u003cp data-pid=\"-EyaGUoS\"\u003e\u003cb\u003e文章题目\u003c/b\u003e：A SURVEY ON POST-TRAINING OF LARGE LANGUAGE MODELS\u003c/p\u003e\u003cp data-pid=\"TzGTyB2f\"\u003e\u003cb\u003eArxiv 链接\u003c/b\u003e：\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2503.06072v1\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2503.0607\u003c/span\u003e\u003cspan class=\"invisible\"\u003e2v1\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cblockquote data-pid=\"-ElDLlrX\"\u003e普遍认为，真正的智能赋予我们推理能力，使我们能够检验假设，并为未来的可能性做好准备。—— Jean Khalfa，《什么是智能？》，1994\u003c/blockquote\u003e\u003ch2\u003e摘要\u003c/h2\u003e\u003cp data-pid=\"B1Z-M4LV\"\u003e大型语言模型（LLMs）的出现从根本上改变了自然语言处理，使其在从对话系统到科学探索等多个领域不可或缺。然而，它们的预训练架构在特定情境下常常暴露出局限性，包括推理能力有限、伦理不确定性以及领域特定性能不佳等问题。这些挑战需要先进的后训练语言模型（PoLMs）来解决这些不足，例如 OpenAI-o1/o3 和 DeepSeek-R1（统称为大型推理模型，或 LRMs）。本文首次全面综述了 PoLMs，系统地追溯了它们在五个核心范式中的演变：\u003cb\u003e微调\u003c/b\u003e（Fine-tuning），提高任务特定准确性；\u003cb\u003e对齐\u003c/b\u003e（Alignment），确保伦理一致性和与人类偏好的对齐；\u003cb\u003e推理\u003c/b\u003e（Reasoning），尽管在奖励设计方面存在挑战，但仍推进多步推理；\u003cb\u003e效率\u003c/b\u003e（Efficiency），在复杂性不断增加的情况下优化资源利用；以及\u003cb\u003e集成与适应\u003c/b\u003e（Integration and Adaptation），扩展跨多种模态的能力，同时解决一致性问题。从 2018 年 ChatGPT 的基础对齐策略到 2025 年 DeepSeek-R1 的创新推理进展，我们展示了 PoLMs 如何利用数据集减轻偏差、深化推理能力和增强领域适应性。我们的贡献包括对 PoLM 演变的开创性综合、对技术和数据集的结构化分类，以及强调 LRMs 在提高推理能力和领域灵活性方面的战略议程。作为这一范围内的首个综述，本研究整合了最近的 PoLM 进展，并为未来的研究建立了严格的理论框架，促进在科学和社会应用中精确、伦理稳健且多功能的 LLMs 的发展。\u003c/p\u003e\u003ch2\u003e一、引言\u003c/h2\u003e\u003cp data-pid=\"Zz6bE3QS\"\u003e语言模型(LMs) 是设计用于建模和生成人类语言的复杂计算框架。这些模型通过使机器能够以接近人类认知的方式理解、生成和与人类语言互动，彻底改变了自然语言处理(NLP) 领域。与人类通过与环境的交互和接触自然习得语言技能不同，机器必须经过广泛的数据驱动训练才能发展出类似的能力。这提出了一个重要的研究挑战，因为使机器能够理解并生成人类语言，同时进行自然、上下文恰当的对话，不仅需要巨大的计算资源，还需要精细的模型开发方法。\u003c/p\u003e\u003cp data-pid=\"a5u0Eb1N\"\u003e大型语言模型(LLMs) 的出现，如 GPT-3、Instruct GPT 和 GPT-4，标志着语言模型进化的一个变革阶段。这些模型以其广泛的参数化和先进的学习能力为特征，旨在捕捉复杂的语言结构、上下文关系和大规模数据集中的细微模式。这使得 LLMs 不仅能够预测后续单词，还能在包括翻译、问答和摘要在内的各种任务中生成连贯且上下文相关的文本。LLMs 的发展引发了广泛的学术兴趣，可以分为两个主要阶段：预训练(pre-training) 和后训练(post-training)。\u003c/p\u003e\u003ch3\u003e预训练\u003c/h3\u003e\u003cp data-pid=\"D-H-Esu2\"\u003e预训练的概念源自计算机视觉(CV)任务中的迁移学习。其主要目标是使用大量数据集开发一个通用模型，以便轻松微调以适应各种下游应用。预训练的一个重要优势是能够利用任何未标注的文本语料库，从而提供丰富的训练数据来源。然而，早期的静态预训练方法，如神经网络语言模型(NNLM) 和 Word2vec，难以适应不同的文本语义环境，促使了动态预训练技术的发展，如 BERT 和 XLNet。BERT 通过利用 Transformer 架构并在大规模未标注数据集中使用自注意力机制，有效解决了静态方法的局限性。这项研究建立了“预训练和微调”的学习范式，启发了众多后续研究，引入了多种架构，包括 GPT-2 和 BART。\u003c/p\u003e\u003ch3\u003e后训练\u003c/h3\u003e\u003cp data-pid=\"01XlEPVn\"\u003e后训练是指模型经过预训练后所采用的技术和方法，旨在细化和适应特定任务或用户需求。随着具有 1750 亿个参数的 GPT-3 的发布，后训练领域经历了显著的兴趣和创新激增。出现了多种方法来提高模型性能，包括\u003cb\u003e微调\u003c/b\u003e(fine-tuning)，即使用标注数据集或特定任务数据调整模型参数；\u003cb\u003e对齐策略\u003c/b\u003e(alignment strategies)，即优化模型以更好地与用户偏好对齐；\u003cb\u003e知识适配技术\u003c/b\u003e(knowledge adaptation techniques)，即使模型能够纳入领域特定知识；以及推理改进(reasoning improvements)，即增强模型的逻辑推理和决策能力。这些技术统称为后训练语言模型(PoLMs)，催生了如 GPT-4、LLaMA-3、Gemini-2.0 和 Claude-3.5 等模型的开发，标志着 LLM 能力的重大进展。然而，后训练模型通常难以不经重新训练或重大参数调整而适配新任务，这使得 PTM 开发成为活跃的研究领域。\u003c/p\u003e\u003cp data-pid=\"J81GDk7A\"\u003e正如所强调的，预训练语言模型(PLMs) 主要旨在提供一般知识和能力，而 PoLMs 则专注于将这些模型适应特定任务和需求。一个显著的例子是最新一代的 LLM，DeepSeek-R1，它展示了 PoLMs 在增强推理能力、与用户偏好对齐以及提高跨领域适应性方面的演变。此外，开源 LLMs（例如 LLaMA、Gemma 和 Nemotron）和领域特定的大规模数据集（例如 Prompt Source 和 Flan）的日益可用，正在推动学术研究人员和行业从业者开发 PoLMs 的趋势。这一趋势突显了在 PoLMs 领域中定制适应性的重要性。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-95b3553a6482aa3df6c7d92f2d9c83d5_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"502\" data-original-token=\"v2-26d230adfadbbbb52b64bda9c2777d5f\" class=\"origin_image zh-lightbox-thumb\" width=\"1578\" data-original=\"https://picx.zhimg.com/v2-95b3553a6482aa3df6c7d92f2d9c83d5_r.jpg\"/\u003e\u003cfigcaption\u003e图1：大型语言模型后训练技术的演变，展示了从初始方法到先进方法的进展，特别强调 DeepSeek 模型的贡献（用蓝色突出显示）\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"WtxdFyA_\"\u003e在现有文献中，PLMs 已被广泛讨论和综述，而 PoLMs 很少被系统地审查。为了推进这些技术，有必要彻底审查现有的研究成果，以识别关键挑战、差距和进一步改进的机会。本调查旨在填补这一空白，通过提供一个结构化的框架来研究后训练的演变。如图1所示，它探讨了后训练的多个阶段，特别关注从 ChatGPT 到 DeepSeek 所采用的方法。这些技术涵盖了广泛的 方法，包括微调、LLM 对齐、推理增强和效率改进。图中的蓝色部分特别突出了 DeepSeek 应用的一组后训练方法，强调了为其成功适应用户偏好和领域特定需求做出贡献的创新策略。\u003c/p\u003e\u003ch2\u003e1.1 主要贡献\u003c/h2\u003e\u003cp data-pid=\"nd9bxJPy\"\u003e本文是关于PoLMs的首个全面综述，提供了该领域最新进展的详尽、结构化的探索。尽管先前的综述通常集中于LLM开发的具体方面，例如偏好对齐、参数高效的微调[39]和LLM的基础技术[40]，但它们大多集中在狭窄的子主题上。相比之下，本综述采取了整体方法，全面回顾了后训练中常用的核⼼技术，并系统地对这些技术进行了分类。此外，我们研究了这些方法所依赖的数据集和实际应用，如图2所示，并指出了未来研究的开放挑战和有前景的方向。本综述的主要贡献如下：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"DTmvB4eE\"\u003e\u003cb\u003e全面的历史综合\u003c/b\u003e。我们首次深入综合了PoLMs的发展历程，从ChatGPT最初的基于人类反馈的强化学习(RLHF)到DeepSeek-R1创新的冷启动RL方法。这一综合涵盖了关键技术（即微调(Fine-tuning)、对齐(Alignment)、推理(Reasoning)、效率(Efficiency)和集成与适应(Integration and Adaptation)），分析了它们的发展及相关的挑战，如计算复杂性和伦理考虑。通过将这一发展过程呈现为一个连贯的叙述，并辅以必要的参考文献，我们为研究人员提供了近年来后训练发展的全面概述，成为该领域的基础资源。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"-pp5hmNx\"\u003e\u003cb\u003e结构化的分类和框架\u003c/b\u003e。我们引入了一个结构化的分类体系，如图2所示，将后训练方法分为五个不同的类别，并将数据集组织成七种类型，同时在专业、技术和交互领域框定了应用。这一框架明确了这些方法之间的相互关系及其实际意义，提供了对其发展的系统视角。通过提供明确定义的类别和分析见解，我们提高了初学者和专家的访问和理解能力，建立了一本全面的指南，帮助他们应对后训练研究的复杂性。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"oB8guL2g\"\u003e\u003cb\u003e未来方向\u003c/b\u003e。我们突出了新兴趋势，特别是大型推理模型(LRMs)如 o1 和 DeepSeek-R1 的兴起，这些模型利用大规模的强化学习推动了推理能力的边界。我们强调，持续的技术进步对于进一步提升推理能力和领域适应性至关重要。我们的分析识别了关键挑战，包括可扩展性限制、伦理对齐风险和多模态集成障碍。我们提出了自适应RL框架和公平性优化等研究方向。这些方向旨在推动后训练的发展，确保LLMs实现更高的精确度和可信度，以满足未来的需求。\u003cbr/\u003e \u003c/li\u003e\u003c/ul\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-35cf7a184f76f2ec333875f0a9d1ca21_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1546\" data-rawheight=\"832\" data-original-token=\"v2-4a824f72e724aa608155deaab7287429\" class=\"origin_image zh-lightbox-thumb\" width=\"1546\" data-original=\"https://picx.zhimg.com/v2-35cf7a184f76f2ec333875f0a9d1ca21_r.jpg\"/\u003e\u003cfigcaption\u003e图2：本研究调查的后训练技术的结构概述，展示了方法、数据集和应用的组织\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e1.2 本文组织\u003c/h2\u003e\u003cp data-pid=\"VJ962a9b\"\u003e本综述系统地组织，全面探讨了后训练语言模型(Post-training Language Models, PoLMs)，涵盖了其历史演变、方法论、数据集、应用及未来趋势。第2节提供了PoLMs的历史概述。第3节考察了微调技术，包括第3.1节的监督微调(Supervised Fine-Tuning, SFT)和第3.3节的强化微调(Reinforcement Fine-Tuning, RFT)。第4节讨论了对齐问题，涵盖第4.1节的人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)、第4.2节的人工智能反馈强化学习(Reinforcement Learning from AI Feedback, RLAIF)以及第4.3节的直接偏好优化(Direct Preference Optimization, DPO)。第5节聚焦推理能力，包括第5.1节的自精炼方法(Self-Refinement Methods)和第5.2节的推理强化学习(Reinforcement Learning for Reasoning)。第6节调查了提高效率的方法，包括第6.1节的模型压缩(Model Compression)、第6.2节的参数高效微调(Parameter-Efficient Fine-Tuning, PEFT)和第6.3节的知识蒸馏(Knowledge Distillation)。第7节研究了集成与适应技术，涉及多模态方法、领域适应和模型融合。第8节回顾了后训练中使用的数据集。第9节探索了大型语言模型的应用。第10节评估了开放问题和未来方向。最后，第11节以总结和研究展望作为结尾。\u003c/p\u003e\u003ch2\u003e2 概览\u003c/h2\u003e\u003ch2\u003e2.1 PoLMs 的历史\u003c/h2\u003e\u003cp data-pid=\"_E0nuDK2\"\u003e大型语言模型（LLM）的发展构成了自然语言处理（NLP）领域的一个重要篇章，其中后训练方法作为关键催化剂，推动了这些模型从通用预训练架构向专门化任务适应系统的演变。本节概述了后训练语言模型（PoLM）的历史轨迹，追溯其发展从以 BERT 和 GPT 代表的预训练里程碑到现代模型如 o1  和 DeepSeek-R1 所体现的复杂后训练范式。如图3所示，这一进展反映了从建立广泛的语言能力到增强任务特定适应性、伦理一致性、推理复杂性和多模态整合的转变，标志着LLM能力的变革之旅。\u003c/p\u003e\u003cp data-pid=\"-h1eJSSn\"\u003e现代PoLM历史的开端与2018年的预训练革命相吻合，当时 BERT 和 GPT 的发布重新定义了NLP基准。BERT的双向自动编码框架利用了Transformer架构和自注意力机制，在诸如问答等任务中出色地捕捉了上下文依赖关系；而GPT的自回归设计则侧重于生成连贯性，为文本生成设定了先例。这些模型确立了“预训练和微调”范式，随后在2019年通过T5 进一步完善，该模型统一了多种任务的文本到文本框架，促进了多任务学习并为后训练进步奠定了坚实基础。\u003c/p\u003e\u003cp data-pid=\"nJXSIPKz\"\u003e从2020年起，PoLM的格局开始显著演变，这主要是由于需要高效地将预训练模型适应于各种任务并在数据有限的情况下进行。早期创新如前缀调优和提示调优引入了轻量级适应策略，通过修改模型输入而不是重新训练整个架构来实现多任务灵活性，从而节省计算资源并扩大应用范围。这一时期还见证了以用户为中心优化的关键转变，即2021年引入的人类反馈强化学习（RLHF），该技术利用人类评估使模型输出与主观偏好对齐，增强了对话场景中的实用性。到2022年，随着近端策略优化（PPO）的采用，RLHF进一步成熟，改进了对齐稳定性和减轻了对噪声反馈的过拟合。2022年底 ChatGPT 的发布凝聚了这些进步，展示了RLHF在创建响应迅速且用户对齐的LLM方面的变革潜力，并催化了 PoLM 研究的激增。同时，思维链（Chain-of-Thought, CoT）提示作为一种推理增强策略出现，鼓励模型在复杂任务中阐述中间步骤，从而提高了透明度和准确性，特别是在逻辑推理和问题解决领域。\u003c/p\u003e\u003cp data-pid=\"LzAyPm78\"\u003e2022年至2024年间，PoLM多样化发展，以应对领域特异性、伦理稳健性和多模态整合的需求，反映出对LLM改进越来越细致的方法。领域适配技术如检索增强生成（Retrieval-Augmented Generation, RAG）出现，旨在集成外部知识库，使专门领域的输出更加丰富，而无需进行全面再训练——这对于需要最新信息的专业应用至关重要。伦理对齐努力加强，2023年直接偏好优化（Direct Preference Optimization, DPO）简化了RLHF，直接针对人类偏好优化模型输出，绕过了中间奖励建模，提高了效率和稳健性。与此同时，多模态能力的追求也取得了进展，PaLM-E 和 Flamingo 等模型开创了视觉-语言整合的先河，随后 BLIP-2 和 LLaVA  将这些努力扩展到了更广泛的领域，如医学成像。效率创新与这些发展并行，特别是通过专家混合（Mixture of Experts, MoE）架构；2022年，Google 的 Switch-C Transformer 引入了1.6万亿参数跨2048个专家的稀疏激活，而 Mixtral 进一步完善了这一范式，平衡了可扩展性和性能。期间的推理增强，如自我博弈和蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）与CoT的结合，通过模拟迭代推理路径进一步增强了LLM的决策能力，为高级推理导向模型奠定了基础。\u003c/p\u003e\u003cp data-pid=\"U9kB_FQB\"\u003e一个重要的架构进步是专家混合（MoE）模型的兴起，这些模型通过动态激活选择性的参数子集，从传统的密集架构中脱颖而出，从而优化计算效率并容纳庞大的参数规模。这一范式由 Google 的 Switch-C Transformer 在2022年率先提出，该模型拥有1.6万亿参数分布在2048个专家中，这是一种平衡资源需求与性能提升的开创性方法。后续迭代，如Mixtral 和 DeepSeek V2.5 ——后者利用2360亿总参数，其中21亿活跃于160个专家中——进一步完善了这一框架，在LMSYS基准上取得了最先进的结果，并证明稀疏MoE架构可以在可扩展性和效能方面与密集模型相媲美。这些发展突显了向效率导向的PoLM的转变，使LLM能够以较低的计算开销处理复杂任务，这是扩大其实用性的关键一步。到2025年，DeepSeek-R1 成为了PoLM创新的里程碑，它摆脱了传统监督微调（SFT）的依赖，转而采用链式思维（CoT）推理和探索性RL策略。以DeepSeek-R1-Zero为例，该模型集成了自我验证、反思和扩展的CoT生成，验证了在开放研究范式中RL驱动的推理激励，引入了蒸馏技术将复杂的推理模式从较大的架构转移到较小的架构。这种方法不仅比单独的RL训练表现出更高的性能，还预示了一种可扩展的、以推理为中心的LLM范式，旨在解决后训练方法中持续存在的计算效率和任务适应性挑战。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-80c05b029b0f809879c716dbb9680adb_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"1460\" data-original-token=\"v2-510fc43b6f6e706744cdd515b427f8ed\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https://pic4.zhimg.com/v2-80c05b029b0f809879c716dbb9680adb_r.jpg\"/\u003e\u003cfigcaption\u003e图3：大型语言模型训练后技术开发时间表（2018-2025），描绘了其历史进程中的关键里程碑\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e2.2 PoLM的公式基础\u003c/h2\u003e\u003ch2\u003e2.2.1 策略优化原理\u003c/h2\u003e\u003cp data-pid=\"wCd1O0oh\"\u003e近端策略优化(PPO)算法是一种关键的强化学习技术，特别适用于需要保持稳定性和效率的场景，例如基于人类反馈的强化学习(RLHF)。PPO通过限制策略更新的幅度来实现这些目标，确保模型行为的变化是渐进和可控的，从而防止性能的灾难性下降。这在微调大规模语言模型时尤为重要，因为剧烈的策略更新可能导致不可取或不可预测的行为。\u003c/p\u003e\u003cp data-pid=\"HHPIIz9_\"\u003e\u003cb\u003e定义。 \u003c/b\u003e在PPO的上下文中，状态 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt%7D%5Cin%5Cmathcal%7BS%7D\" alt=\"s_{t}\\in\\mathcal{S}\" eeimg=\"1\"/\u003e 表示时间 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 的状态，包括模型做出决策所需的所有相关信息。动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_%7Bt%7D%5Cin%5Cmathcal%7BA%7D%28s_%7Bt%7D%29\" alt=\"a_{t}\\in\\mathcal{A}(s_{t})\" eeimg=\"1\"/\u003e 表示模型在给定状态 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt%7D\" alt=\"s_{t}\" eeimg=\"1\"/\u003e 时所做的选择。这个动作是模型所做的一系列决策的一部分。执行动作后，智能体接收奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_%7Bt%7D%5Cin%5Cmathbb%7BR%7D\" alt=\"r_{t}\\in\\mathbb{R}\" eeimg=\"1\"/\u003e ，这是来自环境的反馈，表明所采取行动的成功或失败。优势函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Cpi%7D%28s%2Ca%29\" alt=\"A^{\\pi}(s,a)\" eeimg=\"1\"/\u003e 衡量在当前策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e 下，在状态 \u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e 中采取动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/\u003e 的优势，相对于该状态下所有动作的期望值。它正式定义为动作价值函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=Q%5E%7B%5Cpi%7D%28s%2Ca%29\" alt=\"Q^{\\pi}(s,a)\" eeimg=\"1\"/\u003e 和状态价值函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=V%5E%7B%5Cpi%7D%28s%29\" alt=\"V^{\\pi}(s)\" eeimg=\"1\"/\u003e 之间的差异：\u003c/p\u003e\u003cp data-pid=\"tvMDyJoB\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+A%5E%7B%5Cpi%7D%28s%2Ca%29%3DQ%5E%7B%5Cpi%7D%28s%2Ca%29-V%5E%7B%5Cpi%7D%28s%29%2C+\" alt=\" A^{\\pi}(s,a)=Q^{\\pi}(s,a)-V^{\\pi}(s), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"iJLV1t0E\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=Q%5E%7B%5Cpi%7D%28s%2Ca%29\" alt=\"Q^{\\pi}(s,a)\" eeimg=\"1\"/\u003e 表示在状态 \u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e 中采取动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/\u003e 并遵循策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e 所获得的预期累积奖励，而 \u003cimg src=\"https://www.zhihu.com/equation?tex=V%5E%7B%5Cpi%7D%28s%29\" alt=\"V^{\\pi}(s)\" eeimg=\"1\"/\u003e 是从状态 \u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e 开始并遵循策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e 所获得的预期累积奖励。这两个函数都考虑了未来的奖励，并通过因子 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/\u003e 进行折现。\u003c/p\u003e\u003cp data-pid=\"eoA209ty\"\u003e策略更新。 PPO算法通过基于优势函数进行增量更新来优化策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/\u003e 。策略更新使用剪切目标函数：\u003c/p\u003e\u003cp data-pid=\"hiLoorys\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+L%5E%7BC+L+I+P%7D%28%5Ctheta%29%3D%5Chat%7B%5Cmathbb%7BE%7D%7D_%7Bt%7D%5Cleft%5B%5Coperatorname%2A%7Bmin%7D%5Cleft%28r_%7Bt%7D%28%5Ctheta%29%5Chat%7BA%7D_%7Bt%7D%2C%5Coperatorname%7Bclip%7D%5Cleft%28r_%7Bt%7D%28%5Ctheta%29%2C1-%5Cepsilon%2C1%2B%5Cepsilon%5Cright%29%5Chat%7BA%7D_%7Bt%7D%5Cright%29%5Cright%5D%2C+\" alt=\" L^{C L I P}(\\theta)=\\hat{\\mathbb{E}}_{t}\\left[\\operatorname*{min}\\left(r_{t}(\\theta)\\hat{A}_{t},\\operatorname{clip}\\left(r_{t}(\\theta),1-\\epsilon,1+\\epsilon\\right)\\hat{A}_{t}\\right)\\right], \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"Y1hJvaZw\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_%7Bt%7D%28%5Ctheta%29\" alt=\"r_{t}(\\theta)\" eeimg=\"1\"/\u003e 表示在当前策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/\u003e 下采取动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_%7Bt%7D\" alt=\"a_{t}\" eeimg=\"1\"/\u003e 的概率与旧策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta_%7B%5Cmathrm%7Bold%7D%7D%7D\" alt=\"\\pi_{\\theta_{\\mathrm{old}}}\" eeimg=\"1\"/\u003e 下采取动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_%7Bt%7D\" alt=\"a_{t}\" eeimg=\"1\"/\u003e 的概率之比。该比率定义为：\u003c/p\u003e\u003cp data-pid=\"9bG5EuAa\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+r_%7Bt%7D%28%5Ctheta%29%3D%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D%28a_%7Bt%7D%7Cs_%7Bt%7D%29%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Cmathrm%7Bold%7D%7D%7D%28a_%7Bt%7D%7Cs_%7Bt%7D%29%7D.+\" alt=\" r_{t}(\\theta)=\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{\\mathrm{old}}}(a_{t}|s_{t})}. \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"BiTW6tXK\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7BA%7D%7Bt%7D\" alt=\"\\hat{A}{t}\" eeimg=\"1\"/\u003e \u003ci\u003e是在时间步\u003c/i\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e \u003ci\u003e的估计优势，而剪切函数\u003c/i\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathrm%7Bclip%7D%28r%7Bt%7D%28%5Ctheta%29%2C1-%5Cepsilon%2C1%2B%5Cepsilon%29\" alt=\"\\mathrm{clip}(r{t}(\\theta),1-\\epsilon,1+\\epsilon)\" eeimg=\"1\"/\u003e 将策略更新限制在一个安全范围内，由超参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/\u003e 控制。这种剪切机制确保更新不会与之前的策略相差太大，从而在训练过程中保持稳定性。\u003c/p\u003e\u003cp data-pid=\"79widWfQ\"\u003e价值函数更新。 价值函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7B%5Cphi%7D\" alt=\"V_{\\phi}\" eeimg=\"1\"/\u003e 估计在给定状态 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt%7D\" alt=\"s_{t}\" eeimg=\"1\"/\u003e 下，根据策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/\u003e 获得的预期累积奖励。为了确保价值函数提供准确的估计，它通过最小化预测值与实际奖励之间的均方误差来优化：\u003c/p\u003e\u003cp data-pid=\"7Mfd6Tlh\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cphi_%7Bk%2B1%7D%3D%5Carg%5Coperatorname%2A%7Bmin%7D_%7B%5Cphi%7D%5Cmathbb%7BE%7D_%7Bs_%7Bt%7D%5Csim%5Cpi_%7B%5Ctheta_%7Bk%7D%7D%7D%5Cleft%5B%5Cleft%28V_%7B%5Cphi%7D%28s_%7Bt%7D%29-R%28s_%7Bt%7D%29%5Cright%29%5E%7B2%7D%5Cright%5D%2C+\" alt=\" \\phi_{k+1}=\\arg\\operatorname*{min}_{\\phi}\\mathbb{E}_{s_{t}\\sim\\pi_{\\theta_{k}}}\\left[\\left(V_{\\phi}(s_{t})-R(s_{t})\\right)^{2}\\right], \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"UXuGp9ck\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28s_%7Bt%7D%29\" alt=\"R(s_{t})\" eeimg=\"1\"/\u003e 是从状态 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt%7D\" alt=\"s_{t}\" eeimg=\"1\"/\u003e 获得的实际累积奖励，而 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7B%5Cphi%7D%28s_%7Bt%7D%29\" alt=\"V_{\\phi}(s_{t})\" eeimg=\"1\"/\u003e 是当前策略下的估计值。目标是调整参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"/\u003e 以最小化预测值与实际奖励之间的差异，提高价值函数的准确性。\u003c/p\u003e\u003ch2\u003e2.2.2 RLHF 原理\u003c/h2\u003e\u003cp data-pid=\"uSdv1ewl\"\u003e强化学习结合人类反馈（Reinforcement Learning with Human Feedback, RLHF）是通过在学习过程中利用人类生成的反馈来使模型与人类偏好对齐的关键方法。这种方法引入了一个奖励函数，该函数显式地捕捉了人类输入，使得模型能够更好地适应用户偏好和实际应用。\u003c/p\u003e\u003cp data-pid=\"ui_xxF5C\"\u003e定义。 在 RLHF 中，语言模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Crho\" alt=\" \\rho\" eeimg=\"1\"/\u003e 生成一个关于词汇表 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CSigma\" alt=\"\\Sigma\" eeimg=\"1\"/\u003e 的序列的概率分布。模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"/\u003e 从输入空间 \u003cimg src=\"https://www.zhihu.com/equation?tex=X%3D%5CSigma%5E%7B%5Cleq+m%7D\" alt=\"X=\\Sigma^{\\leq m}\" eeimg=\"1\"/\u003e 中生成一系列的标记 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_%7B0%7D%2Cx_%7B1%7D%2C%5Cdotsc%2Cx_%7Bn-1%7D\" alt=\"x_{0},x_{1},\\dotsc,x_{n-1}\" eeimg=\"1\"/\u003e ，其中每个标记都条件依赖于之前的标记。模型的输出由以下条件概率分布定义：\u003c/p\u003e\u003cp data-pid=\"F8AaS8GQ\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Crho%5Cleft%28x_%7B0%7D%5Ccdot%5Ccdot%5Ccdot+x_%7Bn-1%7D%5Cright%29%3D%5Cprod_%7B0%5Cleq+k%3Cn%7D%5Crho%5Cleft%28x_%7Bk%7D%5Cmid+x_%7B0%7D%5Ccdot%5Ccdot%5Ccdot+x_%7Bk-1%7D%5Cright%29.+\" alt=\" \\rho\\left(x_{0}\\cdot\\cdot\\cdot x_{n-1}\\right)=\\prod_{0\\leq k\u0026lt;n}\\rho\\left(x_{k}\\mid x_{0}\\cdot\\cdot\\cdot x_{k-1}\\right). \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"Dh4eWsDC\"\u003e模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"/\u003e 在一个由输入空间 \u003cimg src=\"https://www.zhihu.com/equation?tex=+X\" alt=\" X\" eeimg=\"1\"/\u003e 、数据分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/\u003e 和输出空间 \u003cimg src=\"https://www.zhihu.com/equation?tex=Y%3D%5CSigma%5E%7B%5Cleq+n%7D\" alt=\"Y=\\Sigma^{\\leq n}\" eeimg=\"1\"/\u003e 定义的任务上进行训练。例如，在文本摘要任务中，如文献 [16] 所示，GPT-2 模型使用 RLHF 进行训练，任务涉及基于 CNN/DailyMail 和 TL;DR  等数据集预测文本摘要。\u003c/p\u003e\u003cp data-pid=\"X1jpBoB7\"\u003e目标函数。 策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e 是一个与原始模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"/\u003e 结构相同的语言模型。最初，策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e 被设置为等于 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"/\u003e 。目标是通过优化策略来最大化输入输出对 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28x%2Cy%29\" alt=\"(x,y)\" eeimg=\"1\"/\u003e 的预期奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=+R%28x%2Cy%29\" alt=\" R(x,y)\" eeimg=\"1\"/\u003e 。奖励函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28x%2Cy%29%3AX%5Ctimes+Y%5Crightarrow%5Cmathbb%7BR%7D\" alt=\"R(x,y):X\\times Y\\rightarrow\\mathbb{R}\" eeimg=\"1\"/\u003e 为每个输入输出对分配一个标量值，最优策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi%5E%7B%2A%7D\" alt=\"\\pi^{*}\" eeimg=\"1\"/\u003e 通过解决以下最大化问题获得：\u003c/p\u003e\u003cp data-pid=\"RyawHMHt\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cpi%5E%7B%2A%7D%3D%5Coperatorname%2A%7Bmax%7D_%7B%5Cpi%7D%5Cmathbb%7BE%7D%5BR%5D%3D%5Cmathbb%7BE%7D_%7Bx%5Csim%5Cmathcal%7BD%7D%2Cy%5Csim%5Cpi%28%5Ccdot%7Cx%29%7D%5BR%28x%2Cy%29%5D.+\" alt=\" \\pi^{*}=\\operatorname*{max}_{\\pi}\\mathbb{E}[R]=\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi(\\cdot|x)}[R(x,y)]. \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"EqWMjBpn\"\u003e这个目标函数代表了一个标准的强化学习问题，其中模型通过与环境的交互并在人类反馈的引导下学习最大化预期奖励。\u003c/p\u003e\u003ch2\u003e2.2.3 DPO 原理\u003c/h2\u003e\u003cp data-pid=\"gZLD4ajd\"\u003e直接偏好优化(Direct Preference Optimization, DPO) 基于强化学习与人类反馈(RLHF)，通过直接根据人类偏好优化模型的输出来改进模型。这些偏好通常以成对比较的形式表达。DPO 消除了传统奖励函数的需要，而是通过最大化基于偏好的奖励来优化模型行为。\u003c/p\u003e\u003ch3\u003e目标函数\u003c/h3\u003e\u003cp data-pid=\"LNleDaRe\"\u003e在一般奖励函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/\u003e 下，KL 约束下的奖励最大化目标的最优解由下式给出：\u003c/p\u003e\u003cp data-pid=\"FOZVMn1n\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cpi_%7Br%7D%28y%5Cmid+x%29%3D%5Cfrac%7B1%7D%7BZ%28x%29%7D%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y%5Cmid+x%29%5Cexp%5Cleft%28%5Cfrac%7B1%7D%7B%5Cbeta%7Dr%28x%2Cy%29%5Cright%29%2C+\" alt=\" \\pi_{r}(y\\mid x)=\\frac{1}{Z(x)}\\pi_{\\mathrm{ref}}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x,y)\\right), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"mI8DVPHQ\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=Z%28x%29\" alt=\"Z(x)\" eeimg=\"1\"/\u003e 是确保输出在所有可能动作上归一化的分区函数。即使使用真实奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=Z%28x%29\" alt=\"Z(x)\" eeimg=\"1\"/\u003e 的最大似然估计 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_%7B%5Cphi%7D\" alt=\"r_{\\phi}\" eeimg=\"1\"/\u003e ，分区函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=Z%28x%29\" alt=\"Z(x)\" eeimg=\"1\"/\u003e 也可以近似，从而简化优化过程。这种表述通过直接根据人类反馈调整策略，使得偏好优化更加高效。\u003c/p\u003e\u003ch3\u003e偏好模型\u003c/h3\u003e\u003cp data-pid=\"jIlEnVkw\"\u003e使用 Bradley-Terry 模型，该模型描述了两个输出 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7B1%7D\" alt=\"y_{1}\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7B2%7D\" alt=\"y_{2}\" eeimg=\"1\"/\u003e 之间的偏好，最优策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi%5E%7B%2A%7D\" alt=\"\\pi^{*}\" eeimg=\"1\"/\u003e 满足以下偏好模型：\u003c/p\u003e\u003cp data-pid=\"bXQ23ed_\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+p%5E%7B%2A%7D%28y_%7B1%7D%5Csucc+y_%7B2%7D%5Cmid+x%29%3D%7B%5Cfrac%7B1%7D%7B1%2B%5Cexp%5Cleft%28%5Cbeta%5Clog%7B%5Cfrac%7B%5Cpi%5E%7B%2A%7D%28y_%7B2%7D%5Cmid+x%29%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y_%7B2%7D%5Cmid+x%29%7D%7D-%5Cbeta%5Clog%7B%5Cfrac%7B%5Cpi%5E%7B%2A%7D%28y_%7B1%7D%5Cmid+x%29%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y_%7B1%7D%5Cmid+x%29%7D%7D%5Cright%29%7D%7D%2C+\" alt=\" p^{*}(y_{1}\\succ y_{2}\\mid x)={\\frac{1}{1+\\exp\\left(\\beta\\log{\\frac{\\pi^{*}(y_{2}\\mid x)}{\\pi_{\\mathrm{ref}}(y_{2}\\mid x)}}-\\beta\\log{\\frac{\\pi^{*}(y_{1}\\mid x)}{\\pi_{\\mathrm{ref}}(y_{1}\\mid x)}}\\right)}}, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"TrAyUd4l\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=p%5E%7B%2A%7D%28y_%7B1%7D%5Csucc+y_%7B2%7D%5Cmid+x%29\" alt=\"p^{*}(y_{1}\\succ y_{2}\\mid x)\" eeimg=\"1\"/\u003e 表示在给定输入 \u003cimg src=\"https://www.zhihu.com/equation?tex=+x\" alt=\" x\" eeimg=\"1\"/\u003e 的情况下，人类更喜欢输出 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7B1%7D\" alt=\"y_{1}\" eeimg=\"1\"/\u003e 而不是 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7B2%7D\" alt=\"y_{2}\" eeimg=\"1\"/\u003e 的概率。这种方法有效地将人类偏好纳入模型的优化过程中。\u003c/p\u003e\u003ch2\u003e2.2.4 GRPO 原理\u003c/h2\u003e\u003cp data-pid=\"k85Jwq9F\"\u003e组相对策略优化（Group Relative Policy Optimization, GRPO）算法是强化学习中近端策略优化（Proximal Policy Optimization, PPO）算法的一种变体，首次在 DeepSeek 的前期工作《Deep Seek Math: 推动开放语言模型中的数学推理极限》中提出。GRPO 省略了评估模型（critic model），而是使用组得分来估计基线，这与 PPO 相比显著减少了训练资源消耗。\u003c/p\u003e\u003cp data-pid=\"H9ezihFD\"\u003e\u003cb\u003e定义。\u003c/b\u003eGRPO 和 PPO 算法之间最显著的区别在于优势函数的计算方法。从第 2.2.1 节中的公式 1 可以看出，PPO 中优势函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=+A%5E%7B%5Cpi%7D%28s%2Ca%29\" alt=\" A^{\\pi}(s,a)\" eeimg=\"1\"/\u003e 的值是从 Q 值和 V 值之间的差异得出的。\u003c/p\u003e\u003cp data-pid=\"YOBMvsQz\"\u003e\u003cb\u003e目标函数。\u003c/b\u003e具体来说，对于每个问题 \u003cimg src=\"https://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\"/\u003e ，GRPO 从旧策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta_%7B%5Cmathrm%7Bold%7D%7D%7D\" alt=\"\\pi_{\\theta_{\\mathrm{old}}}\" eeimg=\"1\"/\u003e 中采样一组输出 \u003cimg src=\"https://www.zhihu.com/equation?tex=%7Bo_%7B1%7D%2Co_%7B2%7D%2C%5Cldots%2Co_%7BG%7D%7D\" alt=\"{o_{1},o_{2},\\ldots,o_{G}}\" eeimg=\"1\"/\u003e，然后通过最大化以下目标来优化策略模型：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-cd350622e20408894421dbe300ed35d5_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1470\" data-rawheight=\"300\" data-original-token=\"v2-2969a8695b8ddd8a688177afe4d5aadc\" class=\"origin_image zh-lightbox-thumb\" width=\"1470\" data-original=\"https://pic4.zhimg.com/v2-cd350622e20408894421dbe300ed35d5_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"KWfhJrzo\"\u003e 其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/\u003e 是超参数， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7BA%7D_%7Bi%2Ct%7D\" alt=\"\\hat{A}_{i,t}\" eeimg=\"1\"/\u003e 是基于每组内部输出的相对奖励计算的优势，详细内容将在第 5.2 节中介绍。\u003c/p\u003e\u003ch2\u003e三、PoLMs 微调\u003c/h2\u003e\u003cp data-pid=\"VDg_Ocfu\"\u003e微调构成了将预训练大型语言模型（LLMs）适应特定任务的核心，通过有针对性的参数调整来优化其能力。这一过程利用标注数据集或任务特定数据集来优化性能，弥合通用预训练与领域特定需求之间的差距。本章探讨三种主要的微调范式：监督微调（§3.1），使用标注数据集来提高任务特定的准确性；自适应微调（§3.2），通过指令微调和基于提示的方法来定制模型行为；以及强化微调（§3.3），将强化学习整合进来，根据奖励信号迭代地优化输出，通过动态交互促进持续改进。\u003c/p\u003e\u003ch2\u003e3.1 有监督微调\u003c/h2\u003e\u003cp data-pid=\"iEn8Dv9-\"\u003e监督微调(Supervised Fine-Tuning, SFT) 通过利用特定任务的标注数据集，将预训练的大型语言模型(LLMs)适应于特定任务。不同于依赖指令提示的指令微调，SFT直接使用标注数据调整模型参数，生成既精确又具有上下文感知能力的模型，同时保留广泛的泛化能力。SFT弥合了预训练期间编码的广泛语言知识与目标应用的细微需求之间的差距。预训练的LLMs通过接触大量语料库，获得了一般的语言模式，减少了对大量领域特定数据进行微调的依赖。模型选择至关重要：较小的模型如 T5 在资源受限且数据集有限的环境中表现出色，而较大的模型如 GPT-4 则利用其卓越的容量，在复杂且数据丰富的任务中表现出色。\u003c/p\u003e\u003ch2\u003e3.1.1 SFT 数据准备\u003c/h2\u003e\u003cp data-pid=\"GxZ8UWHv\"\u003e构建高质量的SFT数据集是一个多方面的过程，对于微调的成功至关重要。\u003c/p\u003e\u003ch3\u003eSFT 数据集构建\u003c/h3\u003e\u003cp data-pid=\"hqkTeJCw\"\u003eSFT 数据集通常结构化为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D%5C%2C%3D%5C%2C%7B%28I_%7Bk%7D%2CX_%7Bk%7D%29%7D%7Bk%3D1%7D%5E%7BN%7D\" alt=\"\\mathcal{D}\\,=\\,{(I_{k},X_{k})}{k=1}^{N}\" eeimg=\"1\"/\u003e \u003ci\u003e，其中 \u003c/i\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=I_%7Bk%7D\" alt=\"I_{k}\" eeimg=\"1\"/\u003e 是一条指令， \u003cimg src=\"https://www.zhihu.com/equation?tex=X_%7Bk%7D\" alt=\"X_{k}\" eeimg=\"1\"/\u003e 是其对应的实例。这种配对使大语言模型（LLM）能够识别任务特定的模式并生成相关输出。诸如 Self-Instruct 等方法通过合成新的指令-输出对来丰富多样性，并使用如 ROUGE-L 等指标过滤重复项以保持多样性。\u003c/p\u003e\u003ch3\u003eSFT 数据集筛选\u003c/h3\u003e\u003cp data-pid=\"i-zwbA-F\"\u003e筛选确保只有高质量的指令-实例对保留在最终的数据集中。使用筛选函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=r%28%5Ccdot%29\" alt=\"r(\\cdot)\" eeimg=\"1\"/\u003e 来评估每对 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28I_%7Bk%7D%2CX_%7Bk%7D%29\" alt=\"(I_{k},X_{k})\" eeimg=\"1\"/\u003e 的质量，从而得到一个精选子集 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D%5E%7B%5Cprime%7D\" alt=\"\\mathcal{D}^{\\prime}\" eeimg=\"1\"/\u003e ：\u003c/p\u003e\u003cp data-pid=\"vH-cOqmB\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cmathcal%7BD%7D%5E%7B%5Cprime%7D%3D%5C%7B%5C%2C%28I_%7Bk%7D%2CX_%7Bk%7D%29%5Cin%5Cmathcal%7BD%7D%5C%3B%7C%5C%3Br%28I_%7Bk%7D%2CX_%7Bk%7D%29%5Cgeq%5Ctau%5C%7D%2C+\" alt=\" \\mathcal{D}^{\\prime}=\\{\\,(I_{k},X_{k})\\in\\mathcal{D}\\;|\\;r(I_{k},X_{k})\\geq\\tau\\}, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-eef48811f13348251819d9cce69f80c1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1540\" data-rawheight=\"1506\" data-original-token=\"v2-1987110f9bb6064ca9cffb5cb2783ef2\" class=\"origin_image zh-lightbox-thumb\" width=\"1540\" data-original=\"https://pic4.zhimg.com/v2-eef48811f13348251819d9cce69f80c1_r.jpg\"/\u003e\u003cfigcaption\u003e表1：2018年至2025年各组织发布的预训练大语言模型概览。该表详细列出了Meta、DeepSeek、OpenAI及其他机构的关键模型，包括它们的参数规模、训练数据规模（如有报道）、开源状态和发布时间线。开源状态用  ⊙˘⊙˘  表示对研究社区公开的模型，用 ⊛ 表示闭源专有模型\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"-BFzi1y9\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e 是用户定义的质量阈值。例如，指令跟随难度（Instruction Following Difficulty, IFD）度量量化了一条给定的指令如何有效地引导模型生成预期响应。IFD 函数表示为：\u003c/p\u003e\u003cp data-pid=\"WLJbmpnr\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+r_%7B%5Ctheta%7D%28Q%2CA%29%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Clog+P%5Cbig%28w_%7Bi%7D%5E%7BA%7D%5Cmid+Q%2C%5C%3Aw_%7B1%7D%5E%7BA%7D%2C%5C%3A.%5C%3A.%5C%3A%2C%5C%3Aw_%7Bi-1%7D%5E%7BA%7D%3B%5C%3A%5Ctheta%5Cbig%29%7D%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Clog+P%5Cbig%28w_%7Bi%7D%5E%7BA%7D%5Cmid+w_%7B1%7D%5E%7BA%7D%2C%5C%3A.%5C%3A.%5C%3A%2C%5C%3Aw_%7Bi-1%7D%5E%7BA%7D%3B%5C%3A%5Ctheta%5Cbig%29%7D%2C+\" alt=\" r_{\\theta}(Q,A)=\\frac{\\sum_{i=1}^{N}\\log P\\big(w_{i}^{A}\\mid Q,\\:w_{1}^{A},\\:.\\:.\\:,\\:w_{i-1}^{A};\\:\\theta\\big)}{\\sum_{i=1}^{N}\\log P\\big(w_{i}^{A}\\mid w_{1}^{A},\\:.\\:.\\:,\\:w_{i-1}^{A};\\:\\theta\\big)}, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"Ej2HnvGk\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=Q\" alt=\"Q\" eeimg=\"1\"/\u003e 表示指令， \u003cimg src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/\u003e 是预期响应， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e 代表模型的学习参数。该度量通过比较在有无指令的情况下生成响应的可能性，提供了一个归一化的度量，表明指令在促进响应生成方面的有效性。未达到选定IFD阈值的指令-实例对将被排除在外，从而得到一个精炼的数据集 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D%5E%7B%5Cprime%7D\" alt=\"\\mathcal{D}^{\\prime}\" eeimg=\"1\"/\u003e 。\u003c/p\u003e\u003ch3\u003eSFT 数据集评估\u003c/h3\u003e\u003cp data-pid=\"dL7kP01O\"\u003e评估SFT数据集涉及选择一个高质量的子集 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7B%5Cmathrm%7Beval%7D%7D\" alt=\"\\mathcal{D}_{\\mathrm{eval}}\" eeimg=\"1\"/\u003e 作为模型性能的基准。这个子集可以从精选数据集 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D\" alt=\"\\mathcal{D}\" eeimg=\"1\"/\u003e 中抽样，或从独立部分中派生以确保公正性。传统的SFT评估方法，如 FewShot GPT 和微调策略，资源密集型，而指令挖掘提供了一种更高效的替代方案。指令挖掘使用线性质量规则和一组度量来衡量数据集质量，如响应长度和平均奖励模型得分，以评估这些度量与整体数据集质量之间的相关性。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-02efb1925e1f892fffac8983c73127db_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1354\" data-rawheight=\"396\" data-original-token=\"v2-a7d65467dc102d02d3a82d11b2a829e4\" class=\"origin_image zh-lightbox-thumb\" width=\"1354\" data-original=\"https://pic2.zhimg.com/v2-02efb1925e1f892fffac8983c73127db_r.jpg\"/\u003e\u003cfigcaption\u003e图4：有监督微调过程\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e3.1.2 SFT 过程\u003c/h2\u003e\u003cp data-pid=\"dbY4UOCd\"\u003e如图4所示，一旦数据集准备就绪，微调过程便从一个预训练的语言模型开始，该模型通常通过在大规模原始数据集上进行无监督或自监督预训练获得。此预训练阶段的目标是获取适用于各种任务的一般特征表示。随后，在微调阶段，使用特定任务的标注数据调整模型参数，使模型与给定应用的需求对齐。此阶段常用的优化目标函数是交叉熵损失。对于一个具有 \u003cimg src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/\u003e 个样本和 \u003cimg src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/\u003e 个类别的分类任务，它可以表示为：\u003c/p\u003e\u003cp data-pid=\"hxK8m-B5\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+L_%7B%5Cmathrm%7Bfine-time%7D%7D%28%5Ctheta%29%3D-%7B%5Cfrac%7B1%7D%7BN%7D%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Csum_%7Bj%3D1%7D%5E%7BC%7Dy_%7Bi+j%7D%5Clog+P%7B%5Cbig%28%7Dy_%7Bj%7D%5Cmid+x_%7Bi%7D%3B%5Ctheta%7B%5Cbig%29%7D%2C+\" alt=\" L_{\\mathrm{fine-time}}(\\theta)=-{\\frac{1}{N}}\\sum_{i=1}^{N}\\sum_{j=1}^{C}y_{i j}\\log P{\\big(}y_{j}\\mid x_{i};\\theta{\\big)}, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"VUaANcNG\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7Bi+j%7D\" alt=\"y_{i j}\" eeimg=\"1\"/\u003e 是样本 \u003cimg src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/\u003e 在类别 \u003cimg src=\"https://www.zhihu.com/equation?tex=j+\" alt=\"j \" eeimg=\"1\"/\u003e 中的真实标签，而 \u003cimg src=\"https://www.zhihu.com/equation?tex=P%5Cbig%28y_%7Bj%7D%5Cmid+x_%7Bi%7D%3B%5Ctheta%5Cbig%29\" alt=\"P\\big(y_{j}\\mid x_{i};\\theta\\big)\" eeimg=\"1\"/\u003e 表示模型预测样本 \u003cimg src=\"https://www.zhihu.com/equation?tex=+i\" alt=\" i\" eeimg=\"1\"/\u003e 属于类别 \u003cimg src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/\u003e 的概率。最小化这个损失函数促使模型更好地与真实标签对齐，从而提高在目标任务上的性能。\u003c/p\u003e\u003cp data-pid=\"3z1DoTmM\"\u003e一个显著的例子是 BERT 模型，它在广泛的语料库（如Books Corpus和Wikipedia）上进行了广泛的预训练。在微调阶段，这些广泛表示通过使用特定任务的数据（例如，用于情感分析的IMDB数据集）进行细化，使BERT能够专门处理诸如情感分类和问答等任务。\u003c/p\u003e\u003ch2\u003e3.1.3 全参微调\u003c/h2\u003e\u003cp data-pid=\"gZqQpRy6\"\u003e全参数微调指的是调整预训练模型所有参数的过程，与LoRA 或Prefix-tuning 等参数高效方法形成对比，后者仅修改部分参数。全参数微调通常用于需要高精度的任务，例如医疗和法律领域，但其计算开销较大。例如，微调一个650亿参数的模型可能需要超过100 GB的GPU内存，这在资源受限的环境中构成了挑战。为了缓解这些约束，引入了LOMO 等内存优化技术，这些技术减少了梯度计算和优化器状态的内存占用。模型参数根据以下规则更新：\u003c/p\u003e\u003cp data-pid=\"GH7tVzEf\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Ctheta_%7Bt%2B1%7D%3D%5Ctheta_%7Bt%7D-%5Ceta%5Cnabla_%7B%5Ctheta%7DL%28%5Ctheta_%7Bt%7D%29%2C+\" alt=\" \\theta_{t+1}=\\theta_{t}-\\eta\\nabla_{\\theta}L(\\theta_{t}), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"CWhDZwba\"\u003e其中， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%7D\" alt=\"\\theta_{t}\" eeimg=\"1\"/\u003e 表示第 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 次迭代时的模型参数， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/\u003e 是学习率， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctheta%7DL%28%5Ctheta_%7Bt%7D%29\" alt=\"\\nabla_{\\theta}L(\\theta_{t})\" eeimg=\"1\"/\u003e 表示损失函数的梯度。内存优化技术包括混合精度训练和激活检查点，这些技术有助于减少内存需求，使大型模型能够在硬件资源有限的系统上进行微调。\u003c/p\u003e\u003cp data-pid=\"5ZWNcN1V\"\u003e\u003cb\u003e从GPT-3到InstructGPT。\u003c/b\u003e全参数微调的一个显著例子是从 GPT-3 到 InstructGPT 的过渡，其中使用设计用于指令跟随任务的数据集对模型的整个参数集进行了微调。这种方法能够实现最佳性能，但由于需要更新所有参数，因此计算成本较高。\u003c/p\u003e\u003ch2\u003e3.2 自适应微调\u003c/h2\u003e\u003cp data-pid=\"xYkSX2xp\"\u003e自适应微调（Adaptive Fine-tuning）修改了预训练模型的行为，以更好地满足用户特定需求并处理更广泛的任务。这种方法引入了额外的线索来指导模型的输出生成，提供了一个灵活的框架来定制模型的响应。自适应微调中值得注意的方法包括指令微调和基于提示的微调，这两种方法通过引入任务特定的指导，显著增强了大语言模型的适应性。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-9ac67d32bdaab81fde00d9928f4f1655_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1392\" data-rawheight=\"428\" data-original-token=\"v2-46de3ee44e8fc0f2dc3e5ac31acccd55\" class=\"origin_image zh-lightbox-thumb\" width=\"1392\" data-original=\"https://picx.zhimg.com/v2-9ac67d32bdaab81fde00d9928f4f1655_r.jpg\"/\u003e\u003cfigcaption\u003e图5：指令微调的工作流程，展示了大语言模型中指令数据集构建和指令微调的一般管道\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e3.2.1 指令微调\u003c/h2\u003e\u003cp data-pid=\"wmSCQnTS\"\u003e指令微调是一种通过在专门构建的指令数据集上对基础大语言模型（LLM）进行微调来改进其性能的技术。这种方法显著提升了模型在各种任务和领域中的泛化能力，提高了其灵活性和准确性。如图5所示，该过程首先将现有的自然语言处理（NLP）数据集（例如，文本分类、翻译和摘要数据集）转换为包含任务描述、输入示例、预期输出和示例演示的自然语言指令。自动生成更多指令-输出对的技术，如Self-Instruct ，进一步增强了这些数据集的多样性，扩展了模型对更广泛任务的接触。微调过程调整模型的参数以适应这些特定任务的指令，从而生成一个在熟悉和先前未见过的任务中均表现出色的大语言模型。例如，Instruct GPT 和 GPT-4 在广泛的应用中展示了指令跟随能力的显著提升。\u003c/p\u003e\u003cp data-pid=\"4Kpk4CVb\"\u003e指令微调的有效性很大程度上取决于指令数据集的质量和广度。高质量的数据集应涵盖广泛的语言、领域和任务复杂性，以确保模型具有广泛的适用性。此外，指令的清晰性和组织性在使模型能够有效解释和执行任务方面发挥着关键作用。整合示例演示，包括思维链提示（Chain-of-Thought prompting）等技术，可以显著提高需要复杂推理的任务的性能。此外，在微调阶段确保任务分布的平衡是避免过拟合或因任务覆盖不平衡而导致模型性能下降的关键。比例任务采样或加权损失函数等技术有助于解决这些问题，确保每个任务在微调过程中做出公平的贡献。因此，通过精心构建和管理指令数据集，研究人员可以大大增强微调后大语言模型的泛化能力，使其在广泛的任务和领域中表现出色。\u003c/p\u003e\u003ch2\u003e3.2.2 前缀微调\u003c/h2\u003e\u003cp data-pid=\"0TQBbLNa\"\u003e前缀调优(Prefix-tuning)  是一种参数高效的微调方法，涉及在语言模型的每个 Transformer 层中添加一系列可训练的前缀标记（连续向量），同时保持核心模型参数不变。如图 6(a) 所示，这些前缀向量是任务特定的，并充当虚拟标记嵌入。为了优化前缀向量，使用了一种重新参数化技巧，即学习一个小的多层感知器（MLP）函数，将一个较小的矩阵映射到前缀参数，而不是直接优化前缀向量。这种方法已被证明可以稳定训练过程。一旦前缀向量被优化，映射函数将被丢弃，只保留导出的前缀向量以增强任务特定性能。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-16d1007ed7ae20da3ac56f97e3412083_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1480\" data-rawheight=\"290\" data-original-token=\"v2-b911e01a352c530448c156d6ab69f559\" class=\"origin_image zh-lightbox-thumb\" width=\"1480\" data-original=\"https://picx.zhimg.com/v2-16d1007ed7ae20da3ac56f97e3412083_r.jpg\"/\u003e\u003cfigcaption\u003e图 6：前缀调优和提示调优的比较，展示了它们在参数微调方面的不同方法：a) 前缀调优 和 b) 提示调优\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"4ekYjVEw\"\u003e通过在输入序列前附加一个已学习的连续提示并利用分层提示，模型的行为可以被引导到任务特定的输出，而无需对整个模型进行微调。由于只有前缀参数被调整，这导致了一种更参数高效的方法。在此基础上，P-Tuning v2 在 Transformer 架构中引入了分层提示向量，专门用于自然语言理解任务。该方法还利用多任务学习来优化跨任务的共享提示，提高不同参数规模下的模型性能。前缀调优在促进大型语言模型快速高效地适应特定任务方面的潜力显而易见，使其成为需要灵活性和效率的应用中的一个有吸引力的策略。\u003c/p\u003e\u003ch2\u003e3.2.3 提示微调\u003c/h2\u003e\u003cp data-pid=\"JiBGn_p0\"\u003e提示调优（Prompt-Tuning）是一种旨在通过优化输入层的可训练向量而非修改模型内部参数来高效适应大规模语言模型的方法。如图6(b)所示，该技术在离散提示方法的基础上引入了软提示标记，这些标记可以以无限制格式或前缀的形式进行结构化。这些学习到的提示嵌入与输入文本嵌入结合后被模型处理，从而在保持预训练权重不变的情况下引导模型的输出。两种代表性的提示调优实现是 P-tuning 和标准prompt-tuning。\u003c/p\u003e\u003cp data-pid=\"UE2yvbF3\"\u003ePtuning 使用灵活的方法结合上下文、提示和目标标记，使其适用于理解和生成任务。该方法通过双向 LSTM 架构增强软提示表示的学习。相比之下，标准提示调优采用了更简单的设计，其中前缀提示附加到输入中，并且仅在训练过程中根据任务特定的监督更新提示嵌入。\u003c/p\u003e\u003cp data-pid=\"4VGSrmx_\"\u003e研究表明，提示调优在许多任务上可以达到与全参数微调相当的性能，同时需要的可训练参数显著减少。然而，其成功与底层语言模型的容量密切相关，因为提示调优仅修改输入层的一小部分参数。在此基础上，诸如 P-Tuning v2 等新方法已经证明，提示调优策略可以在各种模型大小上有效扩展，处理以前认为需要全微调的复杂任务。这些发现确立了提示调优作为传统微调的高度高效替代方案，提供相当的性能并降低计算和内存成本。\u003c/p\u003e\u003ch2\u003e3.3 强化学习微调\u003c/h2\u003e\u003cp data-pid=\"jQhM86jK\"\u003e强化微调（Reinforcement Fine-Tuning, ReFT）是一种先进的技术，它将强化学习（RL）与监督微调（SFT）相结合，以增强模型解决复杂动态问题的能力。与传统的 SFT 不同，后者通常为每个问题使用单个链式思维（CoT）注释，而 ReFT 使模型能够探索多个有效的推理路径，从而提高其泛化能力和问题解决技能。ReFT 过程从标准的 SFT 阶段开始，在此阶段，模型通过监督注释在标注数据上进行初步训练，以学习基本的任务解决能力。经过这一初始微调后，模型使用强化学习算法（如近端策略优化（Proximal Policy Optimization, PPO)）进行进一步的精炼。在强化阶段，模型为每个问题生成多个 CoT 注释，探索不同的潜在推理路径。这些生成的路径通过将模型预测的答案与真实答案进行比较来评估，正确输出会获得奖励，错误输出则受到惩罚。这一迭代过程促使模型调整其策略，最终改进其推理策略。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-054e44db015da1571624452b5b754f17_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1336\" data-rawheight=\"550\" data-original-token=\"v2-3da9c87d9731a07a3e67fe4eae55c912\" class=\"origin_image zh-lightbox-thumb\" width=\"1336\" data-original=\"https://picx.zhimg.com/v2-054e44db015da1571624452b5b754f17_r.jpg\"/\u003e\u003cfigcaption\u003e图 7：强化微调（ReFT）的过程，展示了迭代的监督微调（SFT）预热阶段，随后在同一数据集上进行强化学习（RL）训练\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"62WcIGHe\"\u003e如图 7 所示，ReFT 过程分为两个阶段。上部表示 SFT 阶段，模型在训练数据上迭代，通过多个周期学习每个问题的正确 CoT 注释。下部引入了 ReFT 阶段：从 SFT 训练的模型开始，模型根据当前策略生成替代的 CoT 注释 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28e%5E%7B%5Cprime%7D%29\" alt=\"(e^{\\prime})\" eeimg=\"1\"/\u003e ，并将其预测的答案 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28y%5E%7B%5Cprime%7D%29\" alt=\"(y^{\\prime})\" eeimg=\"1\"/\u003e 与真实答案 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28y%29\" alt=\"(y)\" eeimg=\"1\"/\u003e 进行比较。正确答案会获得正向奖励，错误答案则受到负向奖励，这促使模型提高性能。这些奖励信号随后用于通过强化学习更新模型的策略，增强其生成准确和多样化的 CoT 注释的能力。\u003c/p\u003e\u003cp data-pid=\"hNutX7HF\"\u003e最近的研究表明，ReFT 显著优于传统的 SFT 方法。此外，集成推理时间策略（如多数投票和重新排序）可以进一步提升性能，使模型在训练后能够优化其输出。值得注意的是，ReFT 在不增加或增强训练数据的情况下实现了这些改进，仅从 SFT 阶段使用的现有数据集中学习。这突显了模型的优越泛化能力，因为它能够更高效和有效地从可用数据中学习。\u003c/p\u003e\u003ch2\u003e4 PoLMs 对齐\u003c/h2\u003e\u003cp data-pid=\"TTSY2dBq\"\u003e在大语言模型中实现对齐涉及引导模型输出以符合人类期望和偏好，特别是在安全关键或面向用户的应用中。本章讨论了实现对齐的三种主要范式：基于人类反馈的强化学习（§4.1），该方法使用人工标注的数据作为奖励信号；基于人工智能反馈的强化学习（§4.2），该方法利用人工智能生成的反馈来解决可扩展性问题；以及直接偏好优化（§4.3），该方法直接从成对的人类偏好数据中学习，而无需显式的奖励模型。每种范式在其追求稳健对齐的过程中都提供了不同的优势、挑战和权衡。这些及相关方法的简要比较总结在表2中。\u003c/p\u003e\u003cp data-pid=\"pG_AS_lW\"\u003e表2：大语言模型对齐方法的比较概述（2022–2024）。该表评估了八项指标下的主要对齐技术：RM1（显式或隐式奖励模型）、RM2（点奖励或偏好概率模型）、RM3（响应级或令牌级奖励）、RM4（正或负奖励模型）、F（反馈类型：人类或AI）、RL1（参考模型或无参考模型的强化学习）、RL2（在线策略或离线策略的强化学习）和O（在线/迭代或离线/非迭代优化）。 \u003c/p\u003e\u003ch2\u003e4.1 人工反馈的强化学习\u003c/h2\u003e\u003cp data-pid=\"iPPk6V9u\"\u003e监督微调(Supervised Fine-Tuning, SFT) 一直作为指导大语言模型(LLMs)遵循人类指令的基础技术。然而，在纯监督场景中，标注数据的多样性和质量可能参差不齐，且监督模型捕捉更细微或适应性更强的人类偏好的能力往往有限。为此，基于强化学习(Reinforcement Learning, RL)的微调方法被提出以解决这些不足。在RL方法中，基于人类反馈的强化学习(Reinforcement Learning from Human Feedback, RLHF) 作为最早且最具影响力的RL后训练对齐方法之一脱颖而出。\u003c/p\u003e\u003cp data-pid=\"LIGs8MNU\"\u003e如图8所示，RLHF首先收集以偏好标签或奖励信号形式的人类反馈，然后利用这些信息训练奖励模型。在该奖励模型的引导下，策略通过迭代调整以更好地匹配人类偏好。与SFT相比，RLHF融入了连续的、偏好驱动的更新，从而实现更强的对齐效果。值得注意的是，现代大语言模型如 GPT-4、Claude 和 Gemini 均受益于这些机制，展示了在指令遵循、事实一致性及用户相关性方面的改进。以下，我们将讨论RLHF的主要组成部分，包括反馈机制、奖励建模及策略学习策略。\u003c/p\u003e\u003ch2\u003e4.1.1 RLHF 的反馈机制\u003c/h2\u003e\u003cp data-pid=\"1kutugwp\"\u003e人类反馈是基于人类反馈的强化学习(RLHF)的核心，它向奖励模型传达用户偏好，并指导策略更新。本小节采用了文献[124]的分类法来对常见的人类反馈形式进行分类。表3展示了这些反馈类型在粒度、参与程度和明确性等维度上的分布。每种反馈模式对模型优化的不同方面都有贡献，提供了不同水平的可解释性、可扩展性和噪声容忍度。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-73c28424e71b432665d134b442cf1681_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"616\" data-original-token=\"v2-23a30adb012e197eeb419d720d39520d\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic2.zhimg.com/v2-73c28424e71b432665d134b442cf1681_r.jpg\"/\u003e\u003cfigcaption\u003e图8：基于人类反馈的强化学习(RLHF)工作流程，概述了大型语言模型与人类偏好对齐的整体训练过程\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-e15a508fc77871fd77ac76241363b9eb_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1530\" data-rawheight=\"484\" data-original-token=\"v2-7265d8d9e6bc70222d682d64deeb18a0\" class=\"origin_image zh-lightbox-thumb\" width=\"1530\" data-original=\"https://pic4.zhimg.com/v2-e15a508fc77871fd77ac76241363b9eb_r.jpg\"/\u003e\u003cfigcaption\u003e表3：大型语言模型后训练方法中的反馈类型分类。该表提供了常见反馈类别的概览及其在六个指标上的定义属性：粒度（范围：场景、段落或步骤）、参与度（参与度：观察、主动或协同生成）、元数（实例数：单个、多个或三元）、抽象层次（目标：特征或实例）、意图（目的：评估、描述或字面意义）和明确性\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Bxa9yXEn\"\u003e\u003cb\u003e主要反馈。\u003c/b\u003e这一类别包括最直接塑造RLHF中奖励模型的反馈类型。例如，批评(Critique)专注于对代理行为的显式人类评估，通常通过二元或多标签注释来减轻噪声。比较(Comparisons)允许评估器比较多个输出或轨迹；虽然更大的选择集可以提供更丰富的信号，但也可能导致因果混淆。时间反馈(Inter-Temporal Feedback)通过在不同时间步长提供判断来细化轨迹评估，而代理奖励(Proxy Rewards)则结合近似奖励函数，引导模型朝向用户定义的目标。社会行为(Social Behavior)利用隐含线索（如面部表情）来使代理目标与用户情感对齐。改进(Improvements)强调实时人类干预以逐步完善策略。最后，自然语言反馈(Natural Language Feedback)利用文本信息传达偏好和改进建议。\u003c/p\u003e\u003cp data-pid=\"mGesh2TI\"\u003e\u003cb\u003e补充反馈。\u003c/b\u003e除了主要反馈之外，还有两类进一步加强奖励建模过程。紧急停止(Emergency stops, e-stops)允许人类在代理行为中进行干预，通过停止其轨迹而不提供替代方案来防止不良行为。这种反馈的特点是隐含参与和单一的防止不良行为的焦点。相比之下，重要性标签(Importance labels)指示特定观察对于实现目标的重要性，提供不直接改变行为的显式反馈。这种反馈因上下文而异，作为补充输入，加强奖励模型的整体学习过程。\u003c/p\u003e\u003cp data-pid=\"t6dOa5cm\"\u003e\u003cb\u003e表示特定反馈。\u003c/b\u003e某些反馈类型主要增强表示学习，而不是直接塑造奖励函数。特征轨迹(Feature Traces)提示人类操作员展示给定特征的单调变化，从而实现特征集的动态扩展。相似性查询(Similarity Queries)比较轨迹的三元组，通过轨迹空间中的成对距离引导表示学习。通过利用这些表示特定的反馈形式，RLHF可以实现对新任务和上下文的更鲁棒泛化。\u003c/p\u003e\u003ch2\u003e4.1.2 RLHF 的奖励模型\u003c/h2\u003e\u003cp data-pid=\"GdgOypj7\"\u003e真正的奖励函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=r%28x%2Cy%29\" alt=\"r(x,y)\" eeimg=\"1\"/\u003e 通常未知，因此需要基于人类提供的偏好构建一个可学习的奖励模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Br%7D_%7B%5Ctheta%7D%28x%2Cy%29\" alt=\"\\boldsymbol{r}_{\\theta}(x,y)\" eeimg=\"1\"/\u003e \u003ci\u003e。该模型预测候选输出 \u003c/i\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\"/\u003e \u003ci\u003e在给定输入 \u003c/i\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/\u003e\u003ci\u003e 下与人类期望的一致程度。为了获得训练数据以训练 \u003c/i\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Br%7D_%7B%5Ctheta%7D%28x%2Cy%29\" alt=\"\\boldsymbol{r}_{\\theta}(x,y)\" eeimg=\"1\"/\u003e ，人类评估者根据输出对的相对适宜性进行比较或标注，模型通常使用这些比较上的交叉熵损失进行训练。为了防止策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e 过度偏离初始模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"/\u003e ，引入了一个由超参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/\u003e 控制的惩罚项到奖励函数中：\u003c/p\u003e\u003cp data-pid=\"t8X6Lcxv\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+r_%7B%5Ctheta%7D%28x%2Cy%29%3Dr%28x%2Cy%29-%5Cbeta%5Clog%5Cfrac%7B%5Cpi%28y%5Cmid+x%29%7D%7B%5Crho%28y%5Cmid+x%29%7D%2C+\" alt=\" r_{\\theta}(x,y)=r(x,y)-\\beta\\log\\frac{\\pi(y\\mid x)}{\\rho(y\\mid x)}, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"pPSYgXSl\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi%28y%5Cmid+x%29\" alt=\"\\pi(y\\mid x)\" eeimg=\"1\"/\u003e 是微调后的策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e 在给定输入 \u003cimg src=\"https://www.zhihu.com/equation?tex=+x+\" alt=\" x \" eeimg=\"1\"/\u003e 时生成输出 \u003cimg src=\"https://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\"/\u003e 的概率，而 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Crho%28y%5Cmid+x%29\" alt=\"\\rho(y\\mid x)\" eeimg=\"1\"/\u003e 是在初始模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"/\u003e 下相应的概率。这一项确保了在 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi+\" alt=\"\\pi \" eeimg=\"1\"/\u003e 适应人类反馈的同时，仍受到 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"/\u003e 中捕获的先验知识的约束。\u003c/p\u003e\u003cp data-pid=\"SLHzqWE7\"\u003e评估奖励函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Br%7D_%7B%5Ctheta%7D%28x%2Cy%29\" alt=\"\\boldsymbol{r}_{\\theta}(x,y)\" eeimg=\"1\"/\u003e 至关重要，因为它直接影响学习效果和策略性能。准确评估此函数有助于识别适合将模型输出与人类偏好对齐的奖励结构。然而，在安全敏感领域，由于在线交互的风险、偏差以及对真实奖励的需求，标准的滚动方法和离策略评估可能不可行。为了解决这些挑战，通常采用两种主要方法：\u003c/p\u003e\u003cp data-pid=\"bX5te-u8\"\u003e\u003cb\u003e距离函数。\u003c/b\u003e最近的研究集中在考虑潜在变换（如潜在塑形）的奖励评估距离函数上。例如，EPIC 测量在各种变换下的奖励函数等价性，而 DARD 通过细化规范化确保评估基于可行的转换。EPIC 类似的距离通过允许规范化、标准化和度量函数的变化来推广 EPIC 的方法论，而 STARC 保留了 EPIC 的理论性质，同时提供了额外的灵活性。\u003c/p\u003e\u003cp data-pid=\"LPM5mKrb\"\u003e\u003cb\u003e可视化和人工检查。\u003c/b\u003e其他方法依赖于解释性和精心策划的数据集来评估学习到的奖励函数的有效性。PRFI 使用预处理步骤简化奖励函数，同时保持等价性，从而增强其透明度。与此同时，CONVEXDA 和 REWARDFUSION 提出了设计用于测试奖励模型对提示语义变化响应一致性的数据集。这些技术共同促进了对奖励函数更可靠的评估，强化了大语言模型与人类偏好的对齐。\u003c/p\u003e\u003ch2\u003e4.1.3 RLHF 的策略学习\u003c/h2\u003e\u003cp data-pid=\"TU3gdbip\"\u003e强化学习中的人类反馈策略学习（Reinforcement Learning with Human Feedback, RLHF），如图9所示，涉及通过在线和离线环境中的真人反馈优化策略。\u003c/p\u003e\u003ch3\u003e在线学习\u003c/h3\u003e\u003cp data-pid=\"V1xaXIPO\"\u003e在在线RLHF中，系统实时收集人类对新生成模型轨迹的偏好。DPS 等算法使用贝叶斯更新来管理对抗过程，而PPS和PEPS 将动态规划和多臂赌博机思想结合以改进策略行为。在LPbRL 中，特征嵌入捕捉奖励结构的变化，PbOP 集成最小二乘估计方法，用于转换动态和偏好信号的估计。最近，PARL 通过将反馈获取视为策略优化的组成部分，提高了数据收集效率。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-b399cb08056e7ad6d4e430332da4b2ae_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1258\" data-rawheight=\"502\" data-original-token=\"v2-46831bedf6c3fb64967024c409ac32c3\" class=\"origin_image zh-lightbox-thumb\" width=\"1258\" data-original=\"https://pic3.zhimg.com/v2-b399cb08056e7ad6d4e430332da4b2ae_r.jpg\"/\u003e\u003cfigcaption\u003e图9：在线与离线RLHF的比较，展示了在线RLHF中策略执行期间的连续反馈收集与离线RLHF中预先收集的轨迹利用的对比\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"jlt6qw-4\"\u003e\u003cb\u003e离线学习。\u003c/b\u003e在离线RLHF中，使用先前收集的带有偏好标签的轨迹来学习或优化策略。例如，文献 [151] 研究了基于成对比较数据的悲观最大似然估计策略学习，并建立了性能界限。FREEHAND 和DCPPO 等扩展方法适用于未知偏好模型，探讨了离线数据覆盖范围与策略泛化之间的相互作用。此外，文献 [154] 解决了成对比较中Boltzmann模型的过拟合问题，而DCPPO 进一步研究了动态离散选择模型，以提高反馈效率。\u003c/p\u003e\u003cp data-pid=\"kiIKQhqe\"\u003e\u003cb\u003e在线与离线学习的融合。\u003c/b\u003e混合方法结合了离线预训练和在线偏好聚合，充分利用已收集的数据，同时仍能纳入实时更新。PFERL 采用两阶段方法以减少人类查询次数，而PERL 探索了乐观最小二乘策略以进行主动探索。Dueling RL 及其扩展（如PRPRL 中的REGIME）通过仔细划分数据获取与反馈收集，减少了人类标注需求，从而优化了样本效率、标注成本和策略性能之间的权衡。\u003c/p\u003e\u003ch2\u003e4.2 AI 反馈的强化学习\u003c/h2\u003e\u003cp data-pid=\"-uAS3nFQ\"\u003e强化学习与人工智能反馈(Reinforcement Learning with AI Feedback, RLAIF)扩展了RLHF范式，通过使用大语言模型(LLMs)生成反馈信号。这种方法可以补充或替代人类反馈，在人类标注稀缺、昂贵或不一致的任务中提供更具可扩展性和成本效益的偏好数据。\u003c/p\u003e\u003ch2\u003e4.2.1 RLAIF vs RLHF\u003c/h2\u003e\u003cp data-pid=\"ESNpmVm_\"\u003e在大规模应用强化学习与人类反馈（RLHF）时，一个主要挑战在于其依赖于人工生成的偏好标签，这需要大量资源来收集、整理和标注数据。数据标注过程既耗时又昂贵，且人工评估者可能会引入不一致性，从而使得在整个模型输出中实现大规模、一致的标注变得复杂。这些限制显著影响了RLHF的可扩展性和效率。为了解决这些挑战，[105] 提出了强化学习与人工智能反馈（RLAIF），该方法结合了人类反馈和人工智能生成的反馈，通过强化学习训练模型。通过利用大语言模型（LLM）作为反馈来源，RLAIF减少了对人工标注者的依赖，提供了一种传统RLHF的可行替代方案。这种方法实现了连续的反馈生成，显著增强了可扩展性，同时保留了人类指导下的模型优化灵活性。\u003c/p\u003e\u003cp data-pid=\"oigynBD0\"\u003e如图10所示，RLHF和RLAIF之间的关键区别在于反馈来源：RLHF依赖于人工生成的偏好，而RLAIF使用人工智能生成的反馈来引导策略更新。实证研究，例如[157]的研究表明，RLAIF可以达到与RLHF相当甚至更优的性能，经由人工评分员评估。值得注意的是，RLAIF不仅超越了传统的监督微调基线，而且在偏好标签器规模与策略模型相同的情况下实现了这一点，突显了该方法的高效性。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-dc917391c84e84a6b8f0ed4b933552a7_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1538\" data-rawheight=\"558\" data-original-token=\"v2-86605a584c365c972435191ccfde3366\" class=\"origin_image zh-lightbox-thumb\" width=\"1538\" data-original=\"https://picx.zhimg.com/v2-dc917391c84e84a6b8f0ed4b933552a7_r.jpg\"/\u003e\u003cfigcaption\u003e图10：RLHF和RLAIF方法的比较，展示了它们在大语言模型偏好对齐方面的不同方法\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e4.2.2 RLAIF 训练流水线\u003c/h2\u003e\u003cp data-pid=\"skd9Qith\"\u003eRLAIF 训练流程遵循几个关键阶段，在这些阶段中，利用 AI 生成的反馈迭代地改进模型的行为。该流程促进了 LLM 输出与人类期望的对齐，并且能够扩展到各种任务，如 [108] 所详述。这些阶段如下：\u003c/p\u003e\u003ch3\u003eAI 反馈收集\u003c/h3\u003e\u003cp data-pid=\"FodHzgAy\"\u003e在这一阶段，AI 系统根据预定义的标准生成反馈，这些标准可能包括特定任务的指标、响应的正确性或模型输出的适当性。与需要解释和手动标注的人类反馈不同，AI 反馈可以在广泛的模型输出中一致生成。这一特性使得 AI 反馈可以持续提供，显著扩展了反馈循环。\u003c/p\u003e\u003ch3\u003e奖励模型训练\u003c/h3\u003e\u003cp data-pid=\"DSNZ5Rgc\"\u003e随后，使用 AI 生成的反馈来训练或优化奖励模型。该模型将输入-输出对映射到相应的奖励，使模型的输出与反馈所指示的期望结果对齐。传统的基于人类反馈的强化学习 (RLHF) 依赖于直接的人类反馈来评估输出，而 RLAIF 则利用 AI 生成的标签，尽管这可能会引入一致性问题和偏见，但在可扩展性和独立于人力资源方面具有优势。\u003c/p\u003e\u003ch3\u003e策略更新\u003c/h3\u003e\u003cp data-pid=\"zfnfo3vh\"\u003e最后阶段涉及根据前一步骤中训练的奖励模型更新模型的策略。使用强化学习算法调整模型的参数，优化策略以在多种任务中最大化累积奖励。这一过程是迭代的，奖励模型指导模型的输出向更高的目标对齐度发展。\u003c/p\u003e\u003cp data-pid=\"g-qTKSL-\"\u003eRLAIF 的主要优势在于其能够在不需持续人类干预的情况下扩展反馈循环。通过用 AI 生成的反馈替代人类反馈，RLAIF 促进了 LLM 在多个任务中的持续改进，缓解了人类标注工作带来的瓶颈。\u003c/p\u003e\u003ch2\u003e4.3 直接偏好优化\u003c/h2\u003e\u003cp data-pid=\"qe3Q5uCc\"\u003e如前所述，基于人类反馈的强化学习(RLHF) 通常包括三个阶段：监督微调(Supervised Fine-Tuning)、奖励建模和强化学习（通常通过近端策略优化(PPO)实现）。尽管其效果显著，RLHF 可能会变得复杂且不稳定，特别是在拟合奖励模型并用于微调大型语言模型的阶段。难点在于创建一个能够准确反映人类偏好的奖励模型，以及在优化这一估计奖励的同时，使语言模型保持接近原始模型的挑战。为了解决这些问题，直接偏好优化(Direct Preference Optimization, DPO) 被引入作为一种更稳定且计算效率更高的替代方案。DPO 通过直接将奖励函数与最优策略联系起来，简化了奖励优化过程。它将奖励最大化问题视为基于人类偏好数据的单阶段策略训练问题，从而避免了奖励模型拟合的复杂性和布拉德利-特里模型(Bradley-Terry model) 的依赖性。\u003c/p\u003e\u003ch2\u003e4.3.1 DPO 基础\u003c/h2\u003e\u003cp data-pid=\"jKdRCsZH\"\u003e强化学习与人类反馈（RLHF）涉及训练一个奖励模型（RM）和通过强化学习微调一个语言模型（LM）。直接偏好优化（DPO）简化了这一过程，通过直接使用人类偏好数据训练LM，隐式地在策略中捕捉奖励模型。\u003c/p\u003e\u003ch3\u003eKL正则化奖励最大化目标\u003c/h3\u003e\u003cp data-pid=\"Bw6r_rNE\"\u003eDPO从已建立的KL正则化奖励最大化框架开始，如下目标函数所示：\u003c/p\u003e\u003cp data-pid=\"RH0C9kx3\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cpi%5E%7B%2A%7D%5C%3B%3D%5C%3B%5Carg%5Coperatorname%2A%7Bmax%7D_%7B%5Cpi%7D%5C%3B%5Cmathbb%7BE%7D_%7Bx%5C%2C%5Csim%5C%2C%5Cmathcal%7BD%7D%2C%5C%2Cy%5C%2C%5Csim%5C%2C%5Cpi%28%5Ccdot%5C%2C%5Cmid%5C%2Cx%29%7D%5CBig%5Br%28x%2Cy%29%5C%3B-%5C%3B%5Cbeta%5C%2C%5Cmathrm%7BKL%7D%5CBig%28%5Cpi%28%5Ccdot%5Cmid+x%29%5C%2C%5Cbig%5C%7C%5C%2C%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28%5Ccdot%5Cmid+x%29%5CBig%29%5CBig%5D%2C+\" alt=\" \\pi^{*}\\;=\\;\\arg\\operatorname*{max}_{\\pi}\\;\\mathbb{E}_{x\\,\\sim\\,\\mathcal{D},\\,y\\,\\sim\\,\\pi(\\cdot\\,\\mid\\,x)}\\Big[r(x,y)\\;-\\;\\beta\\,\\mathrm{KL}\\Big(\\pi(\\cdot\\mid x)\\,\\big\\|\\,\\pi_{\\mathrm{ref}}(\\cdot\\mid x)\\Big)\\Big], \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"A3CCU0Qq\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=r%28x%2Cy%29\" alt=\"r(x,y)\" eeimg=\"1\"/\u003e 表示奖励函数， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta%5C%2C%3E%5C%2C0+\" alt=\"\\beta\\,\u0026gt;\\,0 \" eeimg=\"1\"/\u003e 是一个控制接近参考策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Bref%7D%7D\" alt=\"\\pi_{\\mathrm{ref}}\" eeimg=\"1\"/\u003e 程度的系数， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Coperatorname%7BKL%7D%28%5Ccdot%7C%7C%5Ccdot%29\" alt=\"\\operatorname{KL}(\\cdot||\\cdot)\" eeimg=\"1\"/\u003e 表示Kullback-Leibler散度。这里， \u003cimg src=\"https://www.zhihu.com/equation?tex=x%5Csim%5Cmathcal%7BD%7D\" alt=\"x\\sim\\mathcal{D}\" eeimg=\"1\"/\u003e 表示从数据分布中抽取的输入，\u003cimg src=\"https://www.zhihu.com/equation?tex=y%5Csim%5Cpi%28%5Ccdot%5Cmid+x%29\" alt=\"y\\sim\\pi(\\cdot\\mid x)\" eeimg=\"1\"/\u003e 表示从策略中采样的输出。\u003c/p\u003e\u003ch3\u003e导出最优策略\u003c/h3\u003e\u003cp data-pid=\"gEBaLIxt\"\u003e在适当的假设下，方程 (14) 的解以玻尔兹曼分布的形式给出：\u003c/p\u003e\u003cp data-pid=\"ruKTrbsR\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cpi%5E%7B%2A%7D%28y%5Cmid+x%29%5C%3B%3D%5C%3B%7B%5Cfrac%7B1%7D%7BZ%28x%29%7D%7D%5C%2C%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y%5Cmid+x%29%5Cexp%5CBigl%28%7B%5Cfrac%7B1%7D%7B%5Cbeta%7D%7D%5C%2Cr%28x%2Cy%29%5CBigr%29%2C+\" alt=\" \\pi^{*}(y\\mid x)\\;=\\;{\\frac{1}{Z(x)}}\\,\\pi_{\\mathrm{ref}}(y\\mid x)\\exp\\Bigl({\\frac{1}{\\beta}}\\,r(x,y)\\Bigr), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"Rc9f3n0p\"\u003e其中配分函数\u003c/p\u003e\u003cp data-pid=\"fMOJpZw-\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+Z%28x%29%5C%3B%3D%5C%3B%5Csum_%7By%7D%5C%2C%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y%5Cmid+x%29%5Cexp%5CBigl%28%5Cfrac%7B1%7D%7B%5Cbeta%7D%5C%2Cr%28x%2Cy%29%5CBigr%29+\" alt=\" Z(x)\\;=\\;\\sum_{y}\\,\\pi_{\\mathrm{ref}}(y\\mid x)\\exp\\Bigl(\\frac{1}{\\beta}\\,r(x,y)\\Bigr) \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"fqvTaZBk\"\u003e作为归一化项，确保 \u003cimg src=\"https://www.zhihu.com/equation?tex=y%5Csim%5Cpi%28%5Ccdot%5Cmid+x%29\" alt=\"y\\sim\\pi(\\cdot\\mid x)\" eeimg=\"1\"/\u003e 仍然是一个有效的概率分布（即其概率之和为1）。\u003c/p\u003e\u003ch3\u003e重参数化奖励\u003c/h3\u003e\u003cp data-pid=\"rTE35jaA\"\u003e取方程 (15) 两边的自然对数，可以将奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=r%28x%2Cy%29\" alt=\"r(x,y)\" eeimg=\"1\"/\u003e 与最优策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cpi%5E%7B%2A%7D\" alt=\" \\pi^{*}\" eeimg=\"1\"/\u003e 联系起来。得到：\u003c/p\u003e\u003cp data-pid=\"duKRSJle\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+r%5E%7B%2A%7D%28x%2Cy%29%5C%3B%3D%5C%3B%5Cbeta%5CBigl%5B%5Clog%5Cpi%5E%7B%2A%7D%28y%5Cmid+x%29%5C%3B-%5C%3B%5Clog%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y%5Cmid+x%29%5CBigr%5D%5C%3B%2B%5C%3B%5Cbeta%5C%2C%5Clog+Z%28x%29%2C+\" alt=\" r^{*}(x,y)\\;=\\;\\beta\\Bigl[\\log\\pi^{*}(y\\mid x)\\;-\\;\\log\\pi_{\\mathrm{ref}}(y\\mid x)\\Bigr]\\;+\\;\\beta\\,\\log Z(x), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"Bjuy4kCh\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cbeta%5Clog+Z%28x%29\" alt=\" \\beta\\log Z(x)\" eeimg=\"1\"/\u003e 是一个不影响奖励成对比较的常数。如果已知最优策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi%5E%7B%7D\" alt=\"\\pi^{}\" eeimg=\"1\"/\u003e \u003ci\u003e，则可以确定真实奖励 \u003c/i\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=r%5E%7B%7D%28x%2Cy%29\" alt=\"r^{}(x,y)\" eeimg=\"1\"/\u003e ，最大为这个常数。\u003c/p\u003e\u003ch3\u003e布拉德利-特里偏好\u003c/h3\u003e\u003cp data-pid=\"muZDsYYg\"\u003e根据布拉德利-特里模型，两个输出 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7B1%7D\" alt=\"y_{1}\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7B2%7D\" alt=\"y_{2}\" eeimg=\"1\"/\u003e 之间的人类偏好由它们的奖励值差异决定。偏好 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7B1%7D\" alt=\"y_{1}\" eeimg=\"1\"/\u003e 而非 \u003cimg src=\"https://www.zhihu.com/equation?tex=+y_%7B2%7D\" alt=\" y_{2}\" eeimg=\"1\"/\u003e 的概率为\u003c/p\u003e\u003cp data-pid=\"scsYfqDt\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+p%5E%7B%2A%7D%5Cbig%28y_%7B1%7D%5Csucc+y_%7B2%7D%5Cmid+x%5Cbig%29%5C%3B%3D%5C%3B%5Cfrac%7B%5Cexp%5Cbigl%28r%5E%7B%2A%7D%28x%2Cy_%7B1%7D%29%5Cbigr%29%7D%7B%5Cexp%5Cbigl%28r%5E%7B%2A%7D%28x%2Cy_%7B1%7D%29%5Cbigr%29%5C%3B%2B%5C%3B%5Cexp%5Cbigl%28r%5E%7B%2A%7D%28x%2Cy_%7B2%7D%29%5Cbigr%29%7D.+\" alt=\" p^{*}\\big(y_{1}\\succ y_{2}\\mid x\\big)\\;=\\;\\frac{\\exp\\bigl(r^{*}(x,y_{1})\\bigr)}{\\exp\\bigl(r^{*}(x,y_{1})\\bigr)\\;+\\;\\exp\\bigl(r^{*}(x,y_{2})\\bigr)}. \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"2mxGdujT\"\u003e将方程 (17) 代入方程 (18)，我们得到最终的偏好模型：\u003c/p\u003e\u003cp data-pid=\"WecoMpnA\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+p%5E%7B%2A%7D%5Cbig%28y_%7B1%7D%5Csucc+y_%7B2%7D%5C+%7C%5C+x%5Cbig%29%5C+%3D%5C+%5Cfrac%7B1%7D%7B1%5C+%2B%5C+%5Cexp%5CBigl%28%5Cbeta%5Cleft%5B%5Clog%5Cfrac%7B%5Cpi%5E%7B%2A%7D%28y_%7B2%7D%7Cx%29%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y_%7B2%7D%7Cx%29%7D%5C+-%5C+%5Clog%5Cfrac%7B%5Cpi%5E%7B%2A%7D%28y_%7B1%7D%7Cx%29%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y_%7B1%7D%7Cx%29%7D%5Cright%5D%5CBigr%29%7D.+\" alt=\" p^{*}\\big(y_{1}\\succ y_{2}\\ |\\ x\\big)\\ =\\ \\frac{1}{1\\ +\\ \\exp\\Bigl(\\beta\\left[\\log\\frac{\\pi^{*}(y_{2}|x)}{\\pi_{\\mathrm{ref}}(y_{2}|x)}\\ -\\ \\log\\frac{\\pi^{*}(y_{1}|x)}{\\pi_{\\mathrm{ref}}(y_{1}|x)}\\right]\\Bigr)}. \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"wX5wq5bL\"\u003e该表达式将成对的人类偏好概率与最优策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi%5E%7B%2A%7D\" alt=\"\\pi^{*}\" eeimg=\"1\"/\u003e 和参考策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Bref%7D%7D\" alt=\"\\pi_{\\mathrm{ref}}\" eeimg=\"1\"/\u003e 的比值联系起来。\u003c/p\u003e\u003ch3\u003eDPO的目标\u003c/h3\u003e\u003cp data-pid=\"6v87f4PK\"\u003eDPO通过直接从偏好数据中学习策略，避免了显式的奖励建模。给定一组偏好三元组 \u003cimg src=\"https://www.zhihu.com/equation?tex=%7B%28x%2Cy_%7Bw%7D%2Cy_%7Bl%7D%29%7D\" alt=\"{(x,y_{w},y_{l})}\" eeimg=\"1\"/\u003e ，其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7Bw%7D\" alt=\"y_{w}\" eeimg=\"1\"/\u003e 是首选输出， \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7Bl%7D\" alt=\"y_{l}\" eeimg=\"1\"/\u003e 是次选输出，对于提示 \u003cimg src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/\u003e ，DPO最大化观察到的偏好的似然。形式上，DPO采用以下目标函数：\u003c/p\u003e\u003cp data-pid=\"j-Gv3ABP\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cbegin%7Barray%7D%7Br%7D%7B%5Cmathcal%7BL%7D_%7B%5Cmathrm%7BDPA%7D%7D%5Cbig%28%5Cpi_%7B%5Ctheta%7D%3B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%5Cbig%29%5C+%3D%5C+-%5C%2C%5Cmathbb%7BE%7D_%7B%28x%2Cy_%7Bw%7D%2Cy_%7Bl%7D%29%5Csim%5Cmathcal%7BD%7D%7D%5CBigg%5B%5Clog%5Csigma%5CBig%28%5Cbeta%5Cleft%5B%5Clog%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D%28y_%7Bw%7D%7Cx%29%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y_%7Bw%7D%7Cx%29%7D%5Cright%5D%5C+-%5C+%5Cbeta%5Cleft%5B%5Clog%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D%28y_%7Bl%7D%7Cx%29%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%28y_%7Bl%7D%7Cx%29%7D%5Cright%5D%5CBig%29%5CBigg%5D%2C%7D%5Cend%7Barray%7D+\" alt=\" \\begin{array}{r}{\\mathcal{L}_{\\mathrm{DPA}}\\big(\\pi_{\\theta};\\pi_{\\mathrm{ref}}\\big)\\ =\\ -\\,\\mathbb{E}_{(x,y_{w},y_{l})\\sim\\mathcal{D}}\\Bigg[\\log\\sigma\\Big(\\beta\\left[\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\right]\\ -\\ \\beta\\left[\\log\\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\right]\\Big)\\Bigg],}\\end{array} \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"rSlo772J\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma%28%5Ccdot%29\" alt=\"\\sigma(\\cdot)\" eeimg=\"1\"/\u003e 是sigmoid函数， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta%5Clog%7B%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D%5Cleft%28y%7Cx%5Cright%29%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%5Cleft%28y%7Cx%5Cright%29%7D%7D\" alt=\"\\beta\\log{\\frac{\\pi_{\\theta}\\left(y|x\\right)}{\\pi_{\\mathrm{ref}}\\left(y|x\\right)}}\" eeimg=\"1\"/\u003e 表示 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/\u003e 和参考策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Bref%7D%7D\" alt=\"\\pi_{\\mathrm{ref}}\" eeimg=\"1\"/\u003e 之间的重参数化奖励差异。通过最大化 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7B%5Cmathrm%7BDPA%7D%7D\" alt=\"\\mathcal{L}_{\\mathrm{DPA}}\" eeimg=\"1\"/\u003e \u003ci\u003e，策略 \u003c/i\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/\u003e 在不需单独奖励模型的情况下与人类偏好对齐。由于DPO目标继承了来自RLHF的KL正则化公式，它保留了重要的理论保证——例如，在明确定义的偏好假设下的一致性——同时将训练过程统一为一个阶段。因此，DPO为使语言模型与人类评估对齐提供了更直接的路径，减少了系统复杂性并提高了训练稳定性。\u003c/p\u003e\u003ch2\u003e4.3.2 DPO 的训练细节\u003c/h2\u003e\u003cp data-pid=\"NrtBq77M\"\u003eDPO框架建立在两个核心模型之上：参考策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Bref%7D%7D\" alt=\"\\pi_{\\mathrm{ref}}\" eeimg=\"1\"/\u003e 和目标策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Btar%7D%7D\" alt=\"\\pi_{\\mathrm{tar}}\" eeimg=\"1\"/\u003e 。参考策略通常是一个预先训练并经过监督微调的语言模型，在整个训练过程中保持不变。相比之下，目标策略从 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Bref%7D%7D\" alt=\"\\pi_{\\mathrm{ref}}\" eeimg=\"1\"/\u003e 初始化，并通过基于偏好的反馈进行迭代更新，从而提高与人类判断的一致性。图11展示了这一整体流程。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-a6633798283feb974eb3ec8a5416ef88_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1520\" data-rawheight=\"470\" data-original-token=\"v2-ea2b70ff39f0d8a247b506f27dedd4cb\" class=\"origin_image zh-lightbox-thumb\" width=\"1520\" data-original=\"https://pic1.zhimg.com/v2-a6633798283feb974eb3ec8a5416ef88_r.jpg\"/\u003e\u003cfigcaption\u003e图11：直接偏好优化(DPO)的工作流程，展示了基于人类偏好的大型语言模型输出优化的训练流程\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e数据收集和准备\u003c/h3\u003e\u003cp data-pid=\"CmzsJNTZ\"\u003eDPO依赖于一个精心策划的偏好数据集，该数据集通过从 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Bref%7D%7D\" alt=\"\\pi_{\\mathrm{ref}}\" eeimg=\"1\"/\u003e 中为每个提示 \u003cimg src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/\u003e 抽取多个候选响应而获得。人类标注者随后根据连贯性、相关性和清晰度等标准对这些响应进行比较或排名。由此产生的偏好标签作为优化 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Btar%7D%7D\" alt=\"\\pi_{\\mathrm{tar}}\" eeimg=\"1\"/\u003e 的核心训练信号。\u003c/p\u003e\u003ch3\u003e训练过程\u003c/h3\u003e\u003cp data-pid=\"weu0b5PR\"\u003e目标策略通过一系列旨在最小化损失 \u003cimg src=\"https://www.zhihu.com/equation?tex=L_%7B%5Cmathrm%7BDPCO%7D%7D\" alt=\"L_{\\mathrm{DPCO}}\" eeimg=\"1\"/\u003e 的梯度更新进行精炼。具体步骤如下：1）生成： \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Bref%7D%7D\" alt=\"\\pi_{\\mathrm{ref}}\" eeimg=\"1\"/\u003e 为每个提示 \u003cimg src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/\u003e 生成候选输出。2）标注：人类标注者比较生成的输出，确定它们的相对偏好。3）优化：利用这些成对偏好， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Btar%7D%7D\" alt=\"\\pi_{\\mathrm{tar}}\" eeimg=\"1\"/\u003e 进行迭代更新，以更好地模仿人类偏好的输出。在整个过程中， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Bref%7D%7D\" alt=\"\\pi_{\\mathrm{ref}}\" eeimg=\"1\"/\u003e 保持不变，提供一个稳定的基线，以便衡量改进。\u003c/p\u003e\u003ch3\u003e实践考虑\u003c/h3\u003e\u003cp data-pid=\"lcrzyFHF\"\u003e选择一个稳健的参考策略通常是有效初始化DPO的关键。监督微调(SFT)通常会产生一个表现良好的  \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathrm%7Bref%7D%7D\" alt=\"\\pi_{\\mathrm{ref}}\" eeimg=\"1\"/\u003e  基线，确保后续基于偏好的更新可以集中在精炼而非基本技能的获取上。此外，偏好数据必须足够多样，以捕捉用户期望的变化，从而促进模型的适应性并防止过度拟合到狭义定义的任务中。\u003c/p\u003e\u003ch2\u003e4.3.3 DPO 的变种\u003c/h2\u003e\u003cp data-pid=\"GCPZZC0w\"\u003e多种DPO变体已经出现，以应对特定的对齐挑战并优化文本生成的不同方面。表2概述了这些方法，范围从词元级生成优化到控制冗长性和处理列表或负面偏好。\u003c/p\u003e\u003ch3\u003e优化生成的DPO\u003c/h3\u003e\u003cp data-pid=\"NIcvMJBH\"\u003e词元级和迭代DPO策略有助于更精细地或连续地与人类偏好对齐。将问题重新表述为一个强盗问题，词元级DPO 采用了由 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28S%2CA%2Cf%2Cr%2C%5Crho_%7B0%7D%29\" alt=\"(S,A,f,r,\\rho_{0})\" eeimg=\"1\"/\u003e 定义的马尔可夫决策过程(Markov Decision Process, MDP)。这种方法缓解了诸如对不受欢迎的词元产生过高的KL散度等问题。TDPO 应用了顺序前向KL散度而不是逆向KL散度，从而在文本生成中同时提高了对齐性和多样性保持。迭代DPO 采用多轮方法，通过重复的偏好评估不断优化输出，通常由模型本身执行。成对厌恶优化(Pairwise Cringe Optimization, PCO) 将二元反馈扩展到成对设置，使用软边界来平衡探索和利用。逐步DPO (Step-wise DPO) 将偏好数据集划分为多个部分，并应用迭代更新，每一轮更新后的策略作为下一轮的基线。\u003c/p\u003e\u003ch3\u003e可控且灵活的DPO\u003c/h3\u003e\u003cp data-pid=\"ZdGNe7Pn\"\u003e一些DPO变体旨在管理冗长性并减少对固定参考策略的需求。R-DPO 通过在目标函数中引入正则化项来惩罚输出长度，解决了过于冗长或冗余的响应问题。SimPO 通过规范化响应长度并简化损失函数来处理期望和不期望的输出，消除了对参考策略的需求。RLOO 利用REINFORCE算法而无需训练价值模型，大幅减少了计算开销。它将整个响应视为单个动作，并从稀疏奖励中学习，相比传统的基于PPO的方法简化了实现。\u003c/p\u003e\u003ch3\u003e列表DPO\u003c/h3\u003e\u003cp data-pid=\"DX-9yUBb\"\u003e与将偏好数据限制为成对比较不同，列表DPO方法在一组输出上进行优化。列表偏好优化(Listwise Preference Optimization, LiPO) 直接在候选响应的排名列表上应用排序学习技术，相对于重复的成对比较提高了效率。RRHF 将偏好对齐整合到SFT中，消除了对单独参考模型的需求。PRO 将列表偏好分解为更简单的二元任务，简化了SFT期间的对齐。\u003c/p\u003e\u003ch3\u003e负面DPO\u003c/h3\u003e\u003cp data-pid=\"SQqZmGwD\"\u003e某些任务需要从不期望或有害的输出中学习：否定负例(Negating Negatives, NN) 丢弃积极响应并最大化与较少偏好的输出之间的差异。负面偏好优化(Negative Preference Optimization, NPO) 对负面偏好应用梯度上升，有效减少了有害输出并缓解了灾难性崩溃。\u003c/p\u003e\u003ch2\u003e五、PoLMs 推理\u003c/h2\u003e\u003cp data-pid=\"HNvTA4_6\"\u003e推理是使大语言模型（LLM）能够处理涉及多步骤逻辑、复杂推理和复杂决策任务的核心支柱。本章探讨了两种增强模型推理能力的核心技术：自精炼推理（Self-Refine for Reasoning）（§5.1），该方法指导模型自主检测并修正其推理步骤中的错误；以及强化学习推理（Reinforcement Learning for Reasoning）（§5.2），该方法通过基于奖励的优化来提高模型思维链的一致性和深度。这些方法共同使得模型在长期决策、逻辑证明、数学推理等具有挑战性的任务中能够更加稳健地处理。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-7a99455a0d5c545573051057d8cecfa3_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1550\" data-rawheight=\"608\" data-original-token=\"v2-ac33535b579de1943553768de21512f9\" class=\"origin_image zh-lightbox-thumb\" width=\"1550\" data-original=\"https://pic4.zhimg.com/v2-7a99455a0d5c545573051057d8cecfa3_r.jpg\"/\u003e\u003cfigcaption\u003e图12：自我精炼方法的分类，描绘了增强推理的架构变化大型语言模型\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e5.1 自精炼推理\u003c/h2\u003e\u003cp data-pid=\"_uujOkIO\"\u003e推理仍然是优化大语言模型（LLMs）以应对需要复杂逻辑推理和情境依赖决策任务的核心挑战。在这一背景下，自精炼(self-refine)作为一种强大的机制，能够在文本生成过程中或之后迭代地识别和纠正错误，显著提高推理深度和整体可靠性。如图12所示，自精炼方法可以分为四类：内在自精炼(Intrinsic Self-refine)，依赖于模型内部的推理循环；外在自精炼(External Self-refine)，结合外部反馈资源；微调内在自精炼(Fine-tuned Intrinsic Self-refine)，基于自动生成的校正迭代更新模型的推理过程；以及微调外在自精炼(Fine-tuned External Self-refine)，利用外部信号和微调以更适应性、长期的方式改进推理。表4进一步展示了每种类别如何在各种任务中增强LLM的推理能力。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-9872fb568c5f230656a04f72a5552857_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1532\" data-rawheight=\"1184\" data-original-token=\"v2-3893f991f48b6189c86271c157f988a9\" class=\"origin_image zh-lightbox-thumb\" width=\"1532\" data-original=\"https://pic4.zhimg.com/v2-9872fb568c5f230656a04f72a5552857_r.jpg\"/\u003e\u003cfigcaption\u003e表4：大型语言模型中的自精炼方法概述（2022-2025）。此表总结了突出的自我精炼技术，详细介绍了它们的主要LLM、任务和发布时间表，包括三个指标：ET（外部工具：× 表示使用情况，√表示缺失）、FT（微调：×指示应用程序，√表示非应用程序）和SR（自我精炼类型：IS表示内在自我精炼，ES表示外部自我精）\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e内在自精炼\u003c/h3\u003e\u003cp data-pid=\"TLuKooxh\"\u003e内在自精炼方法侧重于使模型本身能够检测并内部修复错误，而无需借助外部工具。例如，RCI Prompting [190] 只在检测到矛盾或错误时触发校正，避免对轻微不确定性做出过度反应。CAI Revisions [105] 纠正不希望的输出（例如，冒犯性文本），同时教导模型自我调节其响应。同样，Self-Refine [164] 通过从低质量提示过渡到高保真指令来改进中间逻辑，提高一致性。CoVe [169] 通过将多答案问题分解为子任务，并分别验证每个子任务，确保整个推理链的精确性和一致性。弱到强泛化(Weak-to-Strong Generalization, W2SG)方法利用高级算法，使强大的学生模型能够从较弱的教师模型产生的嘈杂演示中有效学习 [191]。该框架已在不同领域看到了几个关键的发展和应用。最近的研究通过各种创新增强了W2SG。例如，集成学习技术已成功应用于提高W2SG方法的鲁棒性和有效性 [192]。[193] 采用弱到强外推法来增强LLM的对齐。\u003c/p\u003e\u003ch3\u003e外在自精炼\u003c/h3\u003e\u003cp data-pid=\"MIkRPJP_\"\u003e这些方法涉及外部反馈源或计算工具来指导和纠正模型的推理。CRITIC [177] 系统地检查逐步输出，提高复杂推理任务的可靠性。Reflexion [172] 和 Self-Debug [173] 分别将生成的答案与参考解决方案或少量示例进行比较，迭代地改进逻辑。像 FLARE [170] 和 Logic-LM [171] 这样的技术通过引用外部文档或符号求解器，从而减少逻辑错误。RARR [165] 和 SelfEvolve [166] 显示，验证中间状态（例如，编译器消息或相关知识来源）是早期修剪错误路径并引导模型向正确解决方案发展的强大方法。文献 [194] 提出了一种基于人类反馈的迭代偏好学习方法，包括在线设置下的直接偏好优化(DPO)算法的迭代版本，以及离线场景下的多步拒绝采样策略。PIT [195] 从人类偏好数据中隐式学习改进目标。\u003c/p\u003e\u003ch3\u003e微调内在自精炼\u003c/h3\u003e\u003cp data-pid=\"yUusaeL_\"\u003e通过专门针对内部修订对基础模型进行微调，这些方法系统地加强了LLM的自我校正循环。Self-Critique [161] 旨在通过自我审查改进总结，而 SelFee [174] 使用迭代反馈循环以确保更高的逻辑一致性。Volcano [180] 通过在LLM架构中微调一个专用校正模块来减少多模态幻觉，RL4F [167] 利用基于强化学习的批评循环，在需要深入推理的基准测试中平均提高了 \u003cimg src=\"https://www.zhihu.com/equation?tex=10%5C%25\" alt=\"10\\%\" eeimg=\"1\"/\u003e 的性能。REFINER [176] 同样专注于中间推理路径，而不改变模型的原始生成过程，表明通过训练模型仔细重新检查其部分输出，可以实现一致的改进。此外，从易到难泛化的概念作为W2SG的一个有前途的变体，已经出现，其中模型最初在易于验证的示例上进行训练，然后处理更复杂的任务 [196]。一种值得注意的实现方法是在人类可验证的示例上训练一个强大的奖励模型，然后引导更强大的模型处理具有挑战性的任务 [197]。此外，W2SG的有效性超越了LLM，在计算机视觉任务中也有成功的应用[198]。\u003c/p\u003e\u003ch3\u003e微调外在自精炼\u003c/h3\u003e\u003cp data-pid=\"4u9k3V29\"\u003e在需要长期改进的情况下，模型参数通过外部反馈机制进行更新。例如，Self-Edit [168] 基于执行结果重新生成代码输出，从而逐步提高正确性。Baldur [163] 通过添加或修改上下文来加强定理证明，而 CodeRL [162] 利用基于测试的批评来验证程序合成任务的功能准确性。这些技术共同表明，将外部资源与有针对性的微调相结合，可以促进模型整体推理性能的可靠、逐步提升。\u003c/p\u003e\u003ch2\u003e5.2 用于推理的强化学习\u003c/h2\u003e\u003cp data-pid=\"9x5GEWfQ\"\u003e在第5.1小节中，我们探讨了自精炼方法，这是一种广泛使用的方法，通过局部调优和优化来改进大语言模型的推理能力。这种技术通常应用于单步任务或输出精炼，例如文本生成和问答，能够提供快速的推理增益。然而，它在处理需要多步逻辑的复杂长期推理任务时显得力不从心。OpenAI发布的o1系列[41]突显了强化学习（Reinforcement Learning, RL）作为一种强大的替代方案，通过基于奖励的反馈来训练大语言模型进行高级推理，优化内部的长链因果关系（Chain of Thought, CoT）。这在数学证明和战略规划等复杂任务中显著提升了性能。o1的成功激发了对大规模强化学习的研究，诸如QwQ-32B-Preview [199]在数学和编程方面表现出色，而DeepSeekR1 [28]则与o1的能力相当。本小节将考察强化学习在增强推理能力中的作用，重点关注领先的开源模型DeepSeek-R1和DeepSeek-R1-Zero。\u003c/p\u003e\u003ch2\u003e5.2.1 将推理表述为MDP\u003c/h2\u003e\u003cp data-pid=\"IByaJnaA\"\u003e在大语言模型（LLMs）中的推理可以被优雅地建模为一个顺序决策过程，在该过程中，模型针对输入查询 \u003cimg src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/\u003e 迭代地构建一系列中间步骤 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_%7B1%7D%2Ca_%7B2%7D%2C%5Ccdots%2Ca_%7BT%7D\" alt=\"a_{1},a_{2},\\cdots,a_{T}\" eeimg=\"1\"/\u003e ，以优化到达正确最终答案的可能性。这种概念化将推理转化为一个适合强化学习（RL）的结构化框架，特别是通过马尔可夫决策过程（MDP）的视角，记为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BM%7D%3D%28%5Cmathcal%7BS%7D%2C%5Cmathcal%7BA%7D%2CP%2CR%2C%5Cgamma%29\" alt=\"\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,\\gamma)\" eeimg=\"1\"/\u003e 。MDP封装了状态、动作、转换、奖励和时间折现之间的动态相互作用，为训练LLMs处理复杂推理任务提供了坚实的数学基础。通过将推理视为一系列有意的选择，这种方法使模型能够系统地探索并精炼其逻辑路径，类似于游戏或机器人领域中的决策制定，但适应了语言和概念推理的独特挑战。最终目标是推导出一个最优策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi%5E%7B%2A%7D%28a_%7Bt%7D%7Cs_%7Bt%7D%29\" alt=\"\\pi^{*}(a_{t}|s_{t})\" eeimg=\"1\"/\u003e ，以最大化预期累积奖励，表示为\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Br%7D%7BJ%28%5Ctheta%29%3D%5Cmathbb%7BE%7D_%7B%5Cpi_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Cgamma%5E%7Bt%7D%5Cbar%7BR%7D%28s_%7Bt%7D%2C%5Cbar%7Ba_%7Bt%7D%7D%29%5Cright%5D%7D%5Cend%7Barray%7D\" alt=\"\\begin{array}{r}{J(\\theta)=\\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{t=1}^{T}\\gamma^{t}\\bar{R}(s_{t},\\bar{a_{t}})\\right]}\\end{array}\" eeimg=\"1\"/\u003e，利用RL技术如近端策略优化（Proximal Policy Optimization, PPO）或优势行动者-评论家（Advantage Actor-Critic, A2C）来根据环境反馈迭代地增强推理能力。\u003c/p\u003e\u003ch3\u003e状态空间\u003c/h3\u003e\u003cp data-pid=\"peQKynXX\"\u003e状态空间 \u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e 构成了这个MDP的骨干，每个状态 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt%7D%5Cin%5Cmathcal%7BS%7D\" alt=\"s_{t}\\in\\mathcal{S}\" eeimg=\"1\"/\u003e 代表了在时间步$t$处的当前推理轨迹，这是一个由语言和结构元素组成的丰富复合体，对推理过程至关重要。具体而言， \u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt%7D\" alt=\"s_{t}\" eeimg=\"1\"/\u003e 包括初始查询 \u003cimg src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/\u003e 、先前的推理步骤序列 \u003cimg src=\"https://www.zhihu.com/equation?tex=%7Ba_%7B1%7D%2C%5Cdotsc%2Ca_%7Bt-1%7D%7D\" alt=\"{a_{1},\\dotsc,a_{t-1}}\" eeimg=\"1\"/\u003e 以及编码逻辑依赖和中间结论的内部记忆表示，例如部分解决方案或推断的关系。随着推理的展开，这种状态会动态演变，通过整合生成步骤中明确表述的路径和从上下文中提炼的潜在知识，反映了思维的进展。例如，在数学证明中， \u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt%7D\" alt=\"s_{t}\" eeimg=\"1\"/\u003e 可能包括问题陈述、先前推导的方程和适用定理的记忆，使模型能够在步骤之间保持连贯性。这种多方面的状态表示确保了LLM能够自适应地跟踪其推理上下文，这是处理需要持续逻辑连续性的任务（如多步骤问题解决或文本生成中的叙事连贯性）的前提。\u003c/p\u003e\u003ch3\u003e动作空间\u003c/h3\u003e\u003cp data-pid=\"ARn07Qsk\"\u003e动作空间 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BA%7D\" alt=\"\\mathcal{A}\" eeimg=\"1\"/\u003e 定义了每个步骤中可能的决策范围，其中动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_%7Bt%7D%5Cin%5Cmathcal%7BA%7D\" alt=\"a_{t}\\in\\mathcal{A}\" eeimg=\"1\"/\u003e 对应于选择下一个推理步骤，为推进推理过程提供了灵活的工具包。这些动作可能包括生成自然语言中的一个词或短语来表达推理段落、应用预定义的逻辑或数学变换（如代数简化）、从知识库中选择相关定理或规则以扩展推理链，或在达到结论性答案时停止过程。动作空间的性质因任务而异：在形式证明中选择有限逻辑规则集时可能是离散的，而在开放性推理场景中产生自由形式文本时则是连续的，反映了LLM的生成灵活性。这种双重性使模型能够在结构化领域（如符号逻辑）和非结构化领域（如常识推理）之间导航，同时适应任务需求，保持向解决方案的连贯轨迹。\u003c/p\u003e\u003ch3\u003e转移函数\u003c/h3\u003e\u003cp data-pid=\"Q6VnGLnW\"\u003e转移动力学由函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=P%28s_%7Bt%2B1%7D%7Cs_%7Bt%7D%2Ca_%7Bt%7D%29\" alt=\"P(s_{t+1}|s_{t},a_{t})\" eeimg=\"1\"/\u003e 封装，决定了每次动作后状态如何演变，界定了推理轨迹在MDP框架内的进展。与传统RL环境中由于外部变量（如环境噪声）引起的随机性不同，LLMs中的推理转移主要是确定性的，由模型的自回归输出或结构化推理规则驱动，例如在证明中应用演绎步骤。然而，不确定性源于模型固有的局限性——如不完美的知识、模糊的中间状态或文本生成中的概率采样——引入了RL必须应对的变化。对于自回归LLMs，转移遵循可预测的序列生成过程，但错误累积或解释分歧的可能性需要稳健的设计以确保可靠性。这种确定性但不确定的动力学强调了需要适应性策略，以在从精确数学推导到细致入微的叙事构造的各种情境中稳定推理。\u003c/p\u003e\u003ch3\u003e奖励函数\u003c/h3\u003e\u003cp data-pid=\"orAvHPfu\"\u003e奖励函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28s_%7Bt%7D%2Ca_%7Bt%7D%29\" alt=\"R(s_{t},a_{t})\" eeimg=\"1\"/\u003e 作为MDP的评估核心，提供了对每个推理步骤质量的关键反馈，以指导模型的学习过程。与具有显式奖励（如游戏中得分）的传统RL任务不同，推理奖励必须精心设计以平衡稀疏性和密集性，反映任务的复杂性和目标。稀疏奖励，如仅在达到正确最终答案时分配值，提供简单性但可能在多步骤情景中延迟学习；而密集奖励，如评估逐步正确性、逻辑有效性或与人类偏好的一致性，则提供详细的指导，如§5.2.2所述。这种灵活性使奖励函数能够适应多样化的推理需求——无论是奖励证明中有效推理规则的应用还是叙事段落的一致性——确保模型接收到有意义的信号，以在其即时和扩展推理范围内精炼其策略。\u003c/p\u003e\u003ch3\u003e折扣因子\u003c/h3\u003e\u003cp data-pid=\"N6Qk0XiV\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/\u003e ：一个标量 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cgamma%5Cin%5B0%2C1%5D\" alt=\"\\gamma\\in[0,1]\" eeimg=\"1\"/\u003e ，决定了即时奖励和未来奖励之间的权衡。较高的γ鼓励多步骤推理优化，促进深度推理链而非短期启发式。基于这种MDP公式，目标是学习一个最优推理策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi%5E%7B%2A%7D%28a_%7Bt%7D%7Cs_%7Bt%7D%29\" alt=\"\\pi^{*}(a_{t}|s_{t})\" eeimg=\"1\"/\u003e ，以最大化预期累积奖励：\u003c/p\u003e\u003cp data-pid=\"qZuDJpcu\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+J%28%5Ctheta%29%3D%5Cmathbb%7BE%7D_%7B%5Cpi_%7B%5Ctheta%7D%7D%5Cleft%5B%5Csum_%7B%7Bt%3D1%7D%7D%5E%7BT%7D%5Cgamma%5E%7Bt%7DR%28s_%7Bt%7D%2Ca_%7Bt%7D%29%5Cright%5D.+\" alt=\" J(\\theta)=\\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{{t=1}}^{T}\\gamma^{t}R(s_{t},a_{t})\\right]. \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"YIVqr268\"\u003e这一框架使得可以应用强化学习技术，如近端策略优化（PPO）或优势行动者-评论家（A2C），通过根据推理环境的反馈迭代调整策略 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/\u003e 来精炼LLM的推理能力。\u003c/p\u003e\u003ch2\u003e5.2.2 推理奖励设计\u003c/h2\u003e\u003cp data-pid=\"AGu3krBv\"\u003e与具有明确奖励（如游戏得分）的传统强化学习任务不同，大型语言模型（LLM）中的推理需要结构化的奖励设计，以反映正确性、效率和信息量。常见的方法包括：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"wXbDkG1B\"\u003e二元正确性奖励，对于正确的最终答案分配 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_%7BT%7D%3D1\" alt=\"r_{T}=1\" eeimg=\"1\"/\u003e ，否则分配 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_%7BT%7D%3D0\" alt=\"r_{T}=0\" eeimg=\"1\"/\u003e，这种方法简单但因稀疏反馈而引入高方差；\u003c/li\u003e\u003cli data-pid=\"KZOqB8Yk\"\u003e分步准确度奖励，基于推理规则的有效性或中间步骤的一致性等指标提供增量反馈，以引导多步推理；\u003c/li\u003e\u003cli data-pid=\"Nd551F_W\"\u003e自一致性奖励，测量多个推理路径的稳定性，并对一致性强的情况分配更高的奖励，以增强鲁棒性；\u003c/li\u003e\u003cli data-pid=\"BMmVQNCr\"\u003e基于偏好的奖励，从基于人类反馈的强化学习（RLHF）或基于人工智能反馈的强化学习（RLAIF）中得出，其中模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_%7B%5Cphi%7D%28s_%7Bt%7D%2Ca_%7Bt%7D%29\" alt=\"r_{\\phi}(s_{t},a_{t})\" eeimg=\"1\"/\u003e 经过人类或人工智能反馈训练，评估推理质量，为复杂任务提供细致的指导。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e5.2.3 基础模型上的大规模 RL\u003c/h2\u003e\u003cp data-pid=\"5RBby3ZN\"\u003e大规模强化学习作为提升大型语言模型（LLM）推理能力的一种变革性后训练范式，已经崭露头角。这一方法将重点从传统的监督微调（SFT）转向动态、自我演化的优化策略。通过利用广泛的计算框架和基于奖励的迭代反馈，该方法能够直接精炼基础模型，无需预先标注的数据集，从而实现复杂推理技能的自主发展。通过整合大规模强化学习，LLMs可以解决复杂的多步推理任务（例如数学问题求解、逻辑演绎和战略规划），而传统SFT由于依赖静态、人工策划的数据往往难以胜任这些任务[45]。DeepSeek-R1 模型是这一范式的典型代表，它采用了先进的强化学习技术，在优化资源效率的同时实现了顶级的推理性能，如图13所示。本小节概述了支撑 DeepSeek-R1 成功的关键方法，包括新颖的优化算法、自适应探索和轨迹管理，这些共同重新定义了强化学习驱动的LLM推理潜力。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-fc9433bcf7d5ca1d6e74d9f6ef31a5fe_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1384\" data-rawheight=\"950\" data-original-token=\"v2-6a1a47e1e4f1e616a78706f8a100efb5\" class=\"origin_image zh-lightbox-thumb\" width=\"1384\" data-original=\"https://pic1.zhimg.com/v2-fc9433bcf7d5ca1d6e74d9f6ef31a5fe_r.jpg\"/\u003e\u003cfigcaption\u003e图13：DeepSeek-R1中推理强化学习的工作流程，说明了优化大型语言模型中推理能力的过程\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e组相对策略优化\u003c/h3\u003e\u003cp data-pid=\"QSAdgFe0\"\u003eDeepSeek-R1-Zero 模型利用了一种复杂的近端策略优化（PPO）变体，称为组相对策略优化（GRPO），以减轻传统强化学习训练中对大规模计算和资源的需求。与依赖于广泛批评网络的标准PPO不同，GRPO采用基于组的基线估计来简化优化过程，显著减少训练开销，同时保持策略更新的稳健性。这种效率使得在资源受限系统上部署大规模强化学习成为可能，促进跨长时间段的推理策略迭代优化。通过在可控计算范围内优化策略，GRPO使 DeepSeek-R1-Zero 成为增强推理能力的可扩展解决方案，如图13所示，使其成为当代强化学习驱动推理研究的基石。\u003c/p\u003e\u003ch3\u003eDeepSeek-R1-Zero\u003c/h3\u003e\u003cp data-pid=\"cfoQM-Uc\"\u003eDeepSeek-R1-Zero 展现了大规模强化学习在不依赖传统SFT作为初始步骤的情况下提升LLM推理能力的巨大潜力，而是采用纯强化学习驱动的自我演化范式。这种方法使模型能够通过奖励反馈迭代地优化其内部链式思维（CoT），从而自主发展复杂的推理技能，避免了SFT通常需要的预标注数据集。结果是在复杂的多步推理任务（例如数学问题求解和逻辑推导）中表现出显著的性能提升，展示了强化学习从基础模型中解锁高级推理能力的能力。作为最强大的开源推理模型之一，DeepSeek-R1-Zero 的成功证明了冷启动强化学习策略的可行性，提供了一种资源高效的替代传统训练管道的方法，同时达到了与顶级基准相当的水平。\u003c/p\u003e\u003ch3\u003e分步奖励建模\u003c/h3\u003e\u003cp data-pid=\"HBX6EmRQ\"\u003e为了引导轨迹 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau+%3D+%28s_1%2C+a_1%2C+%5Cldots%2C+s_T%2C+a_T%29\" alt=\"\\tau = (s_1, a_1, \\ldots, s_T, a_T)\" eeimg=\"1\"/\u003e 上的推理，DeepSeek-R1 采用了一个分步奖励模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=f_%5Ctheta\" alt=\"f_\\theta\" eeimg=\"1\"/\u003e ，在每个时间步提供详细的反馈，定义为 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_t+%3D+f_%5Ctheta%28s_t%2C+%5Cbar%7Ba%7Dt+%5Cmid+%5Cmathcal%7BD%7D%7B%5Ctext%7Breasoning%7D%7D%29\" alt=\"r_t = f_\\theta(s_t, \\bar{a}t \\mid \\mathcal{D}{\\text{reasoning}})\" eeimg=\"1\"/\u003e ，其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7B%5Ctext%7Breasoning%7D%7D\" alt=\"\\mathcal{D}_{\\text{reasoning}}\" eeimg=\"1\"/\u003e 包含带有步骤级正确性标签的人工标注CoT序列。这种密集的奖励结构与稀疏的序列末尾奖励形成对比，通过提供即时、可操作的关于单个推理步骤质量的见解，使模型能够精确地调整其策略。通过利用专家策划的数据，奖励模型确保反馈与人类推理标准一致，促进在扩展推理链中的连贯性和准确性，这是处理需要长时间逻辑合成的任务的关键特征。\u003c/p\u003e\u003ch3\u003e自适应探索\u003c/h3\u003e\u003cp data-pid=\"gHdLNqE5\"\u003eDeepSeek-R1 通过集成自适应探索机制来增强策略优化，其目标函数如下：\u003c/p\u003e\u003cp data-pid=\"XAqLyIPF\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cbegin%7Barray%7D%7Br+l%7D+%26%7B%5Cmathcal%7BL%7D_%7B%5Ctext%7BPPO%2B%7D%7D+%3D+%5Cmathbb%7BE%7D_%5Ctau+%5Cleft%5B%5Coperatorname%2A%7Bmin%7D%5Cleft%28%5Cfrac%7B%5Cpi_%5Cphi%28a+%5Cmid+s%29%7D%7B%5Cpi_%7B%5Ctext%7Bold%7D%7D%28a+%5Cmid+s%29%7D+A_t%2C+%5Coperatorname%7Bclip%7D%5Cleft%28%5Cfrac%7B%5Cpi_%5Cphi%28a+%5Cmid+s%29%7D%7B%5Cpi_%7B%5Ctext%7Bold%7D%7D%28a+%5Cmid+s%29%7D%2C+1+-+%5Cepsilon%2C+1+%2B+%5Cepsilon%5Cright%29+A_t%5Cright%29%5Cright%5D%7D%5C%5C++%26%7B%5Cqquad%5Cqquad+%2B+%5Cleft.%5Clambda_t+%5Cmathcal%7BH%7D%28%5Cpi_%5Cphi%28%5Ccdot+%5Cmid+s%29%29%2C%5Cright.%7D+%5Cend%7Barray%7D+\" alt=\" \\begin{array}{r l} \u0026amp;{\\mathcal{L}_{\\text{PPO+}} = \\mathbb{E}_\\tau \\left[\\operatorname*{min}\\left(\\frac{\\pi_\\phi(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)} A_t, \\operatorname{clip}\\left(\\frac{\\pi_\\phi(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)}, 1 - \\epsilon, 1 + \\epsilon\\right) A_t\\right)\\right]}\\\\  \u0026amp;{\\qquad\\qquad + \\left.\\lambda_t \\mathcal{H}(\\pi_\\phi(\\cdot \\mid s)),\\right.} \\end{array} \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"sTtmfssS\"\u003e其中熵项 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BH%7D\" alt=\"\\mathcal{H}\" eeimg=\"1\"/\u003e 由一个自适应系数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Clambda_t+%3D+%5Calpha+%5Ccdot+%5Cexp%28-%5Cbeta+%5Ccdot+%5Cmathrm%7BVar%7D%28R%28%5Ctau_%7B1%3At%7D%29%29%29\" alt=\"\\lambda_t = \\alpha \\cdot \\exp(-\\beta \\cdot \\mathrm{Var}(R(\\tau_{1:t})))\" eeimg=\"1\"/\u003e 调制，根据轨迹上的奖励方差动态调整。这种方法平衡了探索和利用，鼓励模型在训练早期探索多样化的推理路径，随着方差减小逐渐收敛到最优策略，从而在推理优化中提高鲁棒性和效率。\u003c/p\u003e\u003ch3\u003e轨迹剪枝\u003c/h3\u003e\u003cp data-pid=\"XWdlpYoo\"\u003e为了在推理过程中优化计算效率，DeepSeek-R1 实现了一个双注意力批评器 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%5Cpsi%28s_t%29%5E-+%3D+%5Cmathrm%7BLocalAttn%7D%28s_t%29+%2B+%5Cmathrm%7BGlobalAttn%7D%28s_%7B1%3At%7D%29\" alt=\"V_\\psi(s_t)^- = \\mathrm{LocalAttn}(s_t) + \\mathrm{GlobalAttn}(s_{1:t})\" eeimg=\"1\"/\u003e ，评估每个状态的局部步骤评估和全局轨迹上下文。当 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%5Cpsi%28s_t%29+%3C+%5Cgamma+%5Ccdot+%5Coperatorname%2A%7Bmax%7D%7Bk+%5Cleq+t%7D+V%5Cpsi%28%5Cbar%7Bs%7D_k%29\" alt=\"V_\\psi(s_t) \u0026lt; \\gamma \\cdot \\operatorname*{max}{k \\leq t} V\\psi(\\bar{s}_k)\" eeimg=\"1\"/\u003e 时，剪枝低价值的推理路径，集中资源于有前景的轨迹。这一机制减少了无效探索，加速了收敛，并确保模型优先考虑高质量的推理序列，从而在其复杂推理任务中表现出色。\u003c/p\u003e\u003ch2\u003e5.2.4 冷启动推理强化学习\u003c/h2\u003e\u003cp data-pid=\"BX6FzoUL\"\u003eDeepSeek-R1-Zero 进一步推进了强化学习(RL)的应用，通过采用冷启动方法，放弃了监督微调(SFT)，完全依赖于从未经训练的基础模型进行大规模RL。这种自我进化策略通过迭代反馈来优化推理，生成强大的链式思维(CoT)序列，而无需依赖预标注数据。通过直接在推理任务上进行训练，DeepSeek-R1-Zero 展示了RL的灵活性，其性能可与使用SFT初始化的模型（如其DeepSeek-R1对等模型）相媲美甚至超越。这种方法不仅减少了对大量标注数据集的依赖，还展示了RL自主开发复杂推理能力的潜力，为未来的大型语言模型(LLM)发展提供了一个可扩展的范式。总体而言，RL为增强推理提供了一个有前景的框架，有效的奖励设计、策略优化（例如GRPO）和探索策略仍然是关键。未来的研究可以探索结合模仿学习或自监督目标的混合方法，以进一步完善这些能力，巩固RL在推进LLM推理中的作用。\u003c/p\u003e\u003ch2\u003e六、PoLMs 效率\u003c/h2\u003e\u003cp data-pid=\"8M22GaAx\"\u003e基于前几章讨论的后训练优化技术，后训练效率特别针对大型语言模型（LLMs）在初始预训练后的操作性能。主要目标是优化关键部署指标（例如处理速度、内存使用和资源消耗），从而使 LLMs 更适用于实际应用。实现后训练效率的方法主要分为三类：模型压缩(§6.1)，通过剪枝和量化等技术减少整体计算足迹；参数高效微调(§6.2)，仅更新模型的一部分参数或采用专用模块，从而最小化再训练成本并加速对新任务的适应；以及知识蒸馏(§6.3)，将较大预训练模型的知识转移到较小的模型上，使较小模型能够在减少资源需求的情况下实现相当的性能。\u003c/p\u003e\u003ch2\u003e6.1 模型压缩\u003c/h2\u003e\u003cp data-pid=\"0NfGqVgo\"\u003e模型压缩包括一系列旨在减少大语言模型（LLMs）的大小和计算需求的技术，其中包括训练后量化、参数剪枝和低秩近似。\u003c/p\u003e\u003ch2\u003e6.1.1 后训练量化\u003c/h2\u003e\u003cp data-pid=\"SQKpZybR\"\u003e对于大语言模型（LLMs）而言，一种关键的压缩方法是量化，它将高精度数据类型 \u003cimg src=\"https://www.zhihu.com/equation?tex=X%5E%7BH%7D\" alt=\"X^{H}\" eeimg=\"1\"/\u003e （30位浮点数）转换为低精度格式 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathit%7B%5Cdot%7BX%7D%7D%5E%7BL%7D\" alt=\"\\mathit{\\dot{X}}^{L}\" eeimg=\"1\"/\u003e （8位整数）[201]。这种转换可以表示为：\u003c/p\u003e\u003cp data-pid=\"1RWWsIxS\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+X%5E%7BL%7D%3D%5Coperatorname%7BRound%7D%5Cleft%28%5Cfrac%7B%5Cmathrm%7Babsmax%7D%28X%5E%7BL%7D%29%7D%7B%5Cmathrm%7Babsmax%7D%28X%5E%7BH%7D%29%7D+X%5E%7BH%7D%5Cright%29%3D%5Coperatorname%7BRound%7D%28%5Cmathcal%7BK%7D+%5Ccdot+X%5E%7BH%7D%29%2C+\" alt=\" X^{L}=\\operatorname{Round}\\left(\\frac{\\mathrm{absmax}(X^{L})}{\\mathrm{absmax}(X^{H})} X^{H}\\right)=\\operatorname{Round}(\\mathcal{K} \\cdot X^{H}), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"XsezxSKO\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BK%7D\" alt=\"\\mathcal{K}\" eeimg=\"1\"/\u003e 表示量化常数，absmax 指的是元素的最大绝对值。函数 Round 将浮点数转换为整数。大语言模型的量化包括训练后量化（PTQ）和量化感知训练（QAT）。PTQ 使模型权重和激活在预训练后进行调整，使用一个小的校准数据集来优化计算效率和性能，如图14所示。此外，表5列出了几种主要的大语言模型量化方法的性能指标。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-e448a0dd9cddf1ca3052c9c18ee33327_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1120\" data-rawheight=\"308\" data-original-token=\"v2-2c47a21304ba3acb0976d4c0aab15b68\" class=\"origin_image zh-lightbox-thumb\" width=\"1120\" data-original=\"https://pic4.zhimg.com/v2-e448a0dd9cddf1ca3052c9c18ee33327_r.jpg\"/\u003e\u003cfigcaption\u003e图14：大语言模型训练后量化技术的示意图\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-1af51a8a02e6dc968b20ddd35b1845d0_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1540\" data-rawheight=\"1268\" data-original-token=\"v2-d23a583f57a653154247fe00f2877d8d\" class=\"origin_image zh-lightbox-thumb\" width=\"1540\" data-original=\"https://pic1.zhimg.com/v2-1af51a8a02e6dc968b20ddd35b1845d0_r.jpg\"/\u003e\u003cfigcaption\u003e表5：大型语言模型量化方法概述（2021-2025）。此表总结了代表性的量化技术，详细介绍了它们的主要LLM、位宽、困惑度差异、加速和三个指标的发布时间线：位宽（权重、激活和KV缓存的位）、困惑度差（Wikitext-2和C4数据集的性能变化）和加速（相对于基线模型的计算速度提高）\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"wdhLV5Zs\"\u003e\u003cb\u003e仅权重量化（WOQ）\u003c/b\u003e。WOQ 专注于压缩模型权重以提高效率。GPTQ [230] 使用逐层量化和最优大脑量化（Optimal Brain Quantization, OBQ），将权重减少到3或4位，以降低内存使用和处理时间。为了进一步提高效率，QuIP [203] 引入了不一致性处理，实现2位量化，提供更紧凑的表示。同样，AWQ [204] 和 OWQ [205] 通过保持对特别敏感权重的高精度，来最小化推理过程中的潜在精度损失。最后，SpQR [201] 结合稀疏量化和解码，实现在保持模型响应性的前提下高效地逐令牌推理。\u003c/p\u003e\u003cp data-pid=\"nvPWlZec\"\u003e\u003cb\u003e权重-激活共量化（WAQ）\u003c/b\u003e。WAQ 将权重和激活集成在一起以提高效率。LLM.int8() [214] 通过精确存储解决激活异常值问题，并量化为8位，同时保持性能。Smooth Quant [218] 实现每通道缩放，将量化难题从激活转移到权重，实现无损结果。此外， \u003cimg src=\"https://www.zhihu.com/equation?tex=%7B%5Cmathrm%7BOS%2B%7D%7D\" alt=\"{\\mathrm{OS+}}\" eeimg=\"1\"/\u003e [219] 通过通道级移位和缩放减轻异常值的影响，从而提高效率。OmniQuant [220] 将量化难题从激活转移到权重，并微调极端值的裁剪阈值。为了进一步提高效率，RPTQ [231] 对相似通道进行分组，确保量化参数的一致性。\u003c/p\u003e\u003cp data-pid=\"xh6FJdlo\"\u003e\u003cb\u003e键值缓存量化（KVQ）\u003c/b\u003e。键值缓存量化解决了大语言模型中输入令牌数量增加带来的内存优化挑战。KVQuant [224] 引入了针对长上下文长度高效推理的定制方法，保持性能的同时最小化损失。KIVI [228] 通过应用不同的量化策略来优化键和值缓存的内存节省，实现2位量化而无需微调。WKVQuant [225] 进一步改进了这一点，采用了二维量化策略和跨块正则化，实现了与权重-激活量化相当的内存效率，且性能几乎相同。\u003c/p\u003e\u003ch2\u003e6.1.2 参数剪枝\u003c/h2\u003e\u003cp data-pid=\"jryd7GE_\"\u003e参数剪枝[232]是提高大语言模型（LLMs）效率的关键技术，通过最小化模型大小和复杂度而不牺牲精度。如图15所示，剪枝可以分为非结构化剪枝和结构化剪枝。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-8eeef22f639463ce063977f05d4425bc_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1166\" data-rawheight=\"486\" data-original-token=\"v2-2d86358b4b109136f8154ece4ad9521c\" class=\"origin_image zh-lightbox-thumb\" width=\"1166\" data-original=\"https://pica.zhimg.com/v2-8eeef22f639463ce063977f05d4425bc_r.jpg\"/\u003e\u003cfigcaption\u003e图16：参数高效微调（PEFT）的说明，说明了大型语言模型中资源高效适应的方法\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e非结构化剪枝\u003c/h3\u003e\u003cp data-pid=\"6ek8oIHX\"\u003e非结构化剪枝通过消除不重要的权重来增强大语言模型的稀疏性。已知的方法SparseGPT[230]通过一次剪枝实现了高达 \u003cimg src=\"https://www.zhihu.com/equation?tex=60%5C%25\" alt=\"60\\%\" eeimg=\"1\"/\u003e 的稀疏性，同时保持最小的损失。Wanda[233]基于权重大小和激活值进行剪枝，无需重新训练。与此同时，SAMSP[234]利用Hessian矩阵的敏感性进行动态调整稀疏性，旨在最小化误差。DSnoT[235]通过使用迭代剪枝周期提高了性能。最后，Flash-LLM[236]从全局内存中检索稀疏权重，并在芯片上的缓冲区中重建为密集形式，以促进高效计算。\u003c/p\u003e\u003ch3\u003e结构化剪枝\u003c/h3\u003e\u003cp data-pid=\"SFQmDrYp\"\u003e这种方法侧重于修剪大语言模型中的整个参数组，以提高硬件效率并简化结构。例如，LLM-runer[237]评估了LLaMA[65]的重要性，并使用LoRA[92]在剪枝后恢复精度。FLAP[238]使用结构化指标优化压缩，无需微调。此外，SliceGPT[239]使用PCA进行剪枝，同时保持效率。Sheared LLaMA[240]通过基于正则化的剪枝优化模型形状。LoRAPrune[241]通过基于LoRA重要性的迭代结构化剪枝提高效率。此外，Deja Vu[242]通过预测关键注意力头和MLP参数，使用上下文稀疏性减少延迟，同时保持精度。\u003c/p\u003e\u003ch3\u003e低秩近似\u003c/h3\u003e\u003cp data-pid=\"FRByckBT\"\u003e低秩近似通过将权重矩阵 \u003cimg src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/\u003e 近似为较小的矩阵 \u003cimg src=\"https://www.zhihu.com/equation?tex=U\" alt=\"U\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/\u003e ，即 \u003cimg src=\"https://www.zhihu.com/equation?tex=%7BW%7D%5Capprox%7BU%7D%7B%5Cdot%7BV%7D%7D%5E%7B%5Ctop%7D\" alt=\"{W}\\approx{U}{\\dot{V}}^{\\top}\" eeimg=\"1\"/\u003e ，从而压缩大语言模型。这种方法不仅减少了参数数量，还提高了操作效率。例如，TensorGPT[243]使用张量列车分解（Tensor-Train Decomposition, TTD）开发了更高效的嵌入格式。LoSparse[244]将低秩近似与剪枝结合，特别针对压缩连贯神经元组件。FWSVD[245]实现了一种加权SVD方法，而ASVD[246]提供了一种无训练的SVD替代方案，两者均针对训练后的效率。最后，SVD-LLM[247]通过建立奇异值与压缩损失之间的直接关系进一步改进了压缩效果。\u003c/p\u003e\u003ch2\u003e6.2 参数高效微调\u003c/h2\u003e\u003cp data-pid=\"GS9NHKks\"\u003e参数高效微调(PEFT)的过程包括冻结完整的LLM主干，同时仅修改少量新增的参数。如图16所示，PEFT方法分为四类：加法PEFT、选择性PEFT、重参数化PEFT和混合PEFT。\u003c/p\u003e\u003ch2\u003e6.2.1 加性 PEFT\u003c/h2\u003e\u003cp data-pid=\"H-RNyarC\"\u003e加法式参数高效微调（Additive PEFT）在大型语言模型（LLM）中引入新的可训练模块，而不改变原有参数，从而实现在保留基础模型知识的同时进行特定任务的调整，这对于微调来说非常高效。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-ade733f4374d87c3b22ed4c8c9de0711_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1176\" data-rawheight=\"470\" data-original-token=\"v2-b8dda301a3813fa0d084a35c2c9dfbae\" class=\"origin_image zh-lightbox-thumb\" width=\"1176\" data-original=\"https://picx.zhimg.com/v2-ade733f4374d87c3b22ed4c8c9de0711_r.jpg\"/\u003e\u003cfigcaption\u003e图16：参数高效微调（PEFT）的插图，展示了大型语言模型中资源高效的适应方法。\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"O6wjTYtG\"\u003e\u003cb\u003e适配器（Adapters）。\u003c/b\u003e适配器在变压器块内集成紧凑层，定义为：\u003c/p\u003e\u003cp data-pid=\"txtNigwd\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cmathrm%7BAdaptor%7D%28x%29%3DW_%7B%5Cmathrm%7Bup%7D%7D%5Csigma%28W_%7B%5Cmathrm%7Bdown%7D%7Dx%29%2Bx%2C+\" alt=\" \\mathrm{Adaptor}(x)=W_{\\mathrm{up}}\\sigma(W_{\\mathrm{down}}x)+x, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"VSPCuCET\"\u003e其中， \u003cimg src=\"https://www.zhihu.com/equation?tex=W_%7B%5Cmathrm%7Bdown%7D%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Br+%5Ctimes+d%7D\" alt=\"W_{\\mathrm{down}} \\in \\mathbb{R}^{r \\times d}\" eeimg=\"1\"/\u003e 是下投影矩阵， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/\u003e 是非线性激活函数， \u003cimg src=\"https://www.zhihu.com/equation?tex=W_%7B%5Cmathrm%7Bup%7D%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd+%5Ctimes+r%7D\" alt=\"W_{\\mathrm{up}} \\in \\mathbb{R}^{d \\times r}\" eeimg=\"1\"/\u003e 是上投影矩阵。这里， \u003cimg src=\"https://www.zhihu.com/equation?tex=d\" alt=\"d\" eeimg=\"1\"/\u003e 是隐藏层维度， \u003cimg src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/\u003e 是瓶颈维度，通过减少复杂度同时保持性能。基于这种结构，Serial Adapter [248] 在每个变压器块中引入了两个模块。Adapter Fusion [249] 通过在 Add \u0026amp; Norm 之后放置适配器来提高效率。Parallel Adapter (PA) [250] 并行运行适配器和子层，而 CoDA [251] 通过并行运行适配器和子层来优化性能。与 Adapter Fusion 不同，MerA [252] 使用最优传输技术统一适配器的权重和激活。\u003c/p\u003e\u003cp data-pid=\"vJaz3jDu\"\u003e软提示（Soft Prompt）。软提示通过在输入序列中添加可调向量而不是优化离散令牌来增强模型性能 [253]。这种方法的形式化表示为：\u003c/p\u003e\u003cp data-pid=\"qUwOKQSn\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+X%5E%7B%28l%29%7D+%3D+%5Bs_%7B1%7D%5E%7B%28l%29%7D%2C+%5Cldots%2C+s_%7BN_%7BS%7D%7D%5E%7B%28l%29%7D%2C+x_%7B1%7D%5E%7B%28l%29%7D%2C+%5Cldots%2C+x_%7BN_%7BX%7D%7D%5E%7B%28l%29%7D%5D%2C+\" alt=\" X^{(l)} = [s_{1}^{(l)}, \\ldots, s_{N_{S}}^{(l)}, x_{1}^{(l)}, \\ldots, x_{N_{X}}^{(l)}], \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"e20nxYos\"\u003e其中， \u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bi%7D%5E%7B%28l%29%7D\" alt=\"s_{i}^{(l)}\" eeimg=\"1\"/\u003e 表示软提示令牌， \u003cimg src=\"https://www.zhihu.com/equation?tex=x_%7Bi%7D%5E%7B%28l%29%7D\" alt=\"x_{i}^{(l)}\" eeimg=\"1\"/\u003e 表示原始输入令牌。 \u003cimg src=\"https://www.zhihu.com/equation?tex=N_%7BS%7D\" alt=\"N_{S}\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=N_%7BX%7D\" alt=\"N_{X}\" eeimg=\"1\"/\u003e 分别是软提示和原始输入令牌的数量。Prefix Tuning [254] 在变压器层之间引入可学习向量，并通过重新参数化和 P-Tuning v2 [99] 及 APT [255] 进行优化。与此同时，Prompt Tuning [44] 专注于初始嵌入层，以低计算成本优化大型模型。Xprompt [256] 和 IDPG [257] 简化了提示生成和插入。SPoT [258] 和 PTP [259] 解决了稳定性和收敛速度问题，而 DePT [260] 和 SMoP [261] 通过优化提示结构减少了计算需求。\u003c/p\u003e\u003cp data-pid=\"WKZlpi87\"\u003e其他加法式方法。除了早期的技术，诸如 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathrm%7B%28LA%29%5E%7B3%7D%7D\" alt=\"\\mathrm{(LA)^{3}}\" eeimg=\"1\"/\u003e [262] 和 SSF [263] 等方法通过引入最小但强大的模型参数调整来关注训练后的效率。自注意力和前馈网络（FFN）操作在数学上定义为：\u003c/p\u003e\u003cp data-pid=\"sFL0Oo-A\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+S+A%28x%29+%3D+%5Coperatorname%7BSoftmax%7D%5Cleft%28%5Cfrac%7BQ+%5Ccdot+%28l_%7Bk%7D+%5Codot+K%29%5E%7BT%7D%7D%7B%5Csqrt%7Bd_%7Bhead%7D%7D%7D%5Cright%29+%5Ccdot+%28l_%7Bv%7D+%5Codot+V%29%2C+\" alt=\" S A(x) = \\operatorname{Softmax}\\left(\\frac{Q \\cdot (l_{k} \\odot K)^{T}}{\\sqrt{d_{head}}}\\right) \\cdot (l_{v} \\odot V), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"3uHyKh0O\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+F+F+N_%7B%5Ctext%7Btransformer%7D%7D%28x%29+%3D+W_%7Bup%7D+%5Ccdot+%28l_%7Bff%7D+%5Codot+%5Csigma%28W_%7Bdown%7Dx%29%29%2C+\" alt=\" F F N_{\\text{transformer}}(x) = W_{up} \\cdot (l_{ff} \\odot \\sigma(W_{down}x)), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"kufBYoZC\"\u003e其中， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Codot\" alt=\"\\odot\" eeimg=\"1\"/\u003e 表示哈达玛积，尺度向量 \u003cimg src=\"https://www.zhihu.com/equation?tex=l_%7Bk%7D+%E5%92%8C+l_%7Bv%7D\" alt=\"l_{k} 和 l_{v}\" eeimg=\"1\"/\u003e 可以平滑地融入  \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7BQ%7D\" alt=\"A_{Q}\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7BW%7D\" alt=\"A_{W}\" eeimg=\"1\"/\u003e 的权重矩阵中。此外，IPA [264] 使像 GPT-4 这样的大型语言模型与用户特定需求对齐。此外，它不需要对底层模型进行更改，因此在微调过程中保持了效率。\u003c/p\u003e\u003ch2\u003e6.2.2 选择式 PEFT\u003c/h2\u003e\u003cp data-pid=\"NDQqTnwH\"\u003e选择性参数高效微调（Selective PEFT）通过仅对参数的一个子集进行微调来提高效率，如图16(b)所示。这涉及到将一个二元掩码 \u003cimg src=\"https://www.zhihu.com/equation?tex=M%5C%2C%5Cstackrel%7B.%7D%7B%3D%7D%5C%2C%7Bm_%7B1%7D%2Cm_%7B2%7D%2C%5Cldots%2Cm_%7Bn%7D%7D\" alt=\"M\\,\\stackrel{.}{=}\\,{m_{1},m_{2},\\ldots,m_{n}}\" eeimg=\"1\"/\u003e 应用到参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5C%2C%3D%5C%2C%7B%5Ctheta_%7B1%7D%2C%5Ctheta_%7B2%7D%2C%5Cldots%2C%5Ctheta_%7Bn%7D%7D\" alt=\"\\theta\\,=\\,{\\theta_{1},\\theta_{2},\\ldots,\\theta_{n}}\" eeimg=\"1\"/\u003e 上，其中每个 \u003cimg src=\"https://www.zhihu.com/equation?tex=m_%7Bi%7D\" alt=\"m_{i}\" eeimg=\"1\"/\u003e 表示 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi%7D\" alt=\"\\theta_{i}\" eeimg=\"1\"/\u003e 是否被选中进行微调。更新后的参数集表示为：\u003c/p\u003e\u003cp data-pid=\"xCv9qktF\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Ctheta_%7Bi%7D%5E%7B%5Cprime%7D%3D%5Ctheta_%7Bi%7D-%5Ceta%5Ccdot+m_%7Bi%7D%5Ccdot%5Cfrac%7B%5Cpartial%5Cmathcal%7BL%7D%7D%7B%5Cpartial%5Ctheta_%7Bi%7D%7D%2C+\" alt=\" \\theta_{i}^{\\prime}=\\theta_{i}-\\eta\\cdot m_{i}\\cdot\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{i}}, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"XtHdd2IZ\"\u003e其中 $\\eta$ 是学习率， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5Cmathcal%7BL%7D%7D%7B%5Cpartial%5Ctheta_%7Bi%7D%7D\" alt=\"\\frac{\\partial\\mathcal{L}}{\\partial\\theta_{i}}\" eeimg=\"1\"/\u003e 是损失函数的梯度。只有被选中的参数（即 \u003cimg src=\"https://www.zhihu.com/equation?tex=m_%7Bi%7D%5C%2C%3D%5C%2C1\" alt=\"m_{i}\\,=\\,1\" eeimg=\"1\"/\u003e ）会被更新，从而在保持效果的同时减少计算成本。早期的方法包括差分剪枝（Diff pruning）[265]，该方法使用可微的 \u003cimg src=\"https://www.zhihu.com/equation?tex=L_%7B0%7D\" alt=\"L_{0}\" eeimg=\"1\"/\u003e -范数对可学习的二元掩码进行正则化；以及 FishMask [266]，该方法基于 Fisher 信息选择参数以提高相关性。LT-SFT [267] 应用了彩票假设（Lottery Ticket Hypothesis）来识别重要的参数。SAM [268] 使用二阶近似进行选择，而 Child-tuning [269] 在子网络中动态选择参数。此外，FAR [270] 和 BitFit [271] 进一步展示了选择性参数高效微调（Selective PEFT），通过专注于优化特定的参数组。\u003c/p\u003e\u003ch2\u003e6.2.3 重参数 PEFT\u003c/h2\u003e\u003cp data-pid=\"F13A5FtU\"\u003e重新参数化的PEFT（Parameter-Efficient Fine-Tuning）主要采用低秩参数化来提高效率，如图16(c)所示。LoRA（低秩适应）[92]引入了两个可训练矩阵， \u003cimg src=\"https://www.zhihu.com/equation?tex=W_%7B%5Cmathrm%7Bup%7D%7D+%5Cin+%5Cdot%7B%5Cmathbb%7BR%7D%7D%5E%7Bd+%5Ctimes+r%7D\" alt=\"W_{\\mathrm{up}} \\in \\dot{\\mathbb{R}}^{d \\times r}\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=W_%7B%5Cmathrm%7Bdown%7D%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Br+%5Ctimes+k%7D\" alt=\"W_{\\mathrm{down}} \\in \\mathbb{R}^{r \\times k}\" eeimg=\"1\"/\u003e ，修改输出为：\u003c/p\u003e\u003cp data-pid=\"fWLk69Vd\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+h_%7B%5Cmathrm%7Bout%7D%7D+%3D+W_%7B0%7D+h_%7B%5Cmathrm%7Bin%7D%7D+%2B+%5Calpha+%5Cbig%28W_%7B%5Cmathrm%7Bup%7D%7D+W_%7B%5Cmathrm%7Bdown%7D%7D+h_%7B%5Cmathrm%7Bin%7D%7D%5Cbig%29%2C+\" alt=\" h_{\\mathrm{out}} = W_{0} h_{\\mathrm{in}} + \\alpha \\big(W_{\\mathrm{up}} W_{\\mathrm{down}} h_{\\mathrm{in}}\\big), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"to-zfWPb\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/\u003e 是一个缩放因子。这种方法允许在保持核心知识的同时高效地适应新任务。基于LoRA，Intrinsic SAID [272] 最小化微调参数空间，进一步减少计算需求。动态变体包括DyLoRA [273] 和AdaLoRA [274]，根据特定任务的需求动态调整秩，而AdaLoRA还结合了基于SVD的剪枝以提高效率。SoRA [275] 通过去除正交性约束简化过程，Laplace-LoRA [276] 应用贝叶斯校准进行微调。Compacter [277] 和VeRA [278] 进一步减少了参数复杂度。此外，DoRA [279] 在方向分量中优化更新，HiRA [280] 使用Hadamard积进行高秩更新，从而提高了效率和性能。为了处理多个任务和不断变化的领域，Terra [281] 集成了一个时变矩阵，ToRA [282] 利用Tucker分解进一步改进LoRA结构。除了结构设计，PiSSA [283] 和LoRA-GA [284] 使用SVD和梯度对齐优化LoRA的初始化。同时， \u003cimg src=\"https://www.zhihu.com/equation?tex=LoRA+%5E%2B\" alt=\"LoRA ^+\" eeimg=\"1\"/\u003e [285]、LoRA-Pro [286] 和CopRA [287] 进一步细化了梯度更新策略。此外，ComLoRA [288] 采用竞争学习选择表现最佳的LoRA组件。\u003c/p\u003e\u003ch2\u003e6.2.4 混合 PEFT\u003c/h2\u003e\u003cp data-pid=\"eQCozGJG\"\u003e混合 PEFT 方法通过整合或优化各种微调策略来提高训练后效率。一种突出的技术是 UniPELT [289]，它在变压器块中集成了 LoRA、前缀调优和适配器。该方法通过由前馈网络 (FFNs) 管理的门控机制动态激活组件，这些 FFNs 生成标量 \u003cimg src=\"https://www.zhihu.com/equation?tex=G%5Cin+%7B%5B0%2C1%5D%7D\" alt=\"G\\in {[0,1]}\" eeimg=\"1\"/\u003e ，最终优化参数利用。另一种创新方法是 MAM Adapter [250]，它通过在自注意力层中战略性地定位前缀调优，并在前馈层中使用缩放并行适配器来改进这一技术。此外，基于 NAS 的方法如 NOAH [290] 和 AUTOPEFT [291] 通过识别特定任务的最佳 PEFT 配置来提高训练后效率。HeadMap [292] 使用贪婪方法识别一系列在某些任务中起关键作用的注意力头（即知识电路），并通过将这些注意力头的输出映射回 LLM 的残差流中，高效地提升模型性能。最后，LLM-Adapters [293] 提供了一个框架，用于在 LLM 中集成各种 PEFT 技术，确保最有效的模块放置以维持不同模型规模下的效率。\u003c/p\u003e\u003ch2\u003e6.3 知识蒸馏\u003c/h2\u003e\u003cp data-pid=\"RqAu_mYQ\"\u003e知识蒸馏(KD)是大型语言模型(LLM)后训练优化中的关键技术，能够将大型预训练教师模型的知识转移到紧凑的学生模型中，从而提高效率而不牺牲性能。最初在模型压缩背景下提出，KD因其能够将复杂知识提炼到资源高效的架构中而受到广泛关注，使得其能够在边缘设备和嵌入式系统等受限制环境中部署。通过利用教师模型的细腻输出分布——比传统的硬标签更丰富——KD使学生不仅能够复制类别预测，还能复制类间关系和教师表示中固有的细微模式。这一过程通常涉及优化一个复合损失函数，该函数平衡监督学习目标与蒸馏特定目标，显著减少计算和内存需求，同时保持泛化能力。\u003c/p\u003e\u003cp data-pid=\"crJ20zfP\"\u003eKD的基本机制在于最小化一个混合损失，该损失集成了传统分类损失与蒸馏项。形式上，给定教师模型的软输出概率 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bp%7D_%7B%5Cmathbf%7Bt%7D%7D\" alt=\"\\mathbf{p}_{\\mathbf{t}}\" eeimg=\"1\"/\u003e \u003ci\u003e和学生模型的预测\u003c/i\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bp_%7Bs%7D%7D\" alt=\"\\mathbf{p_{s}}\" eeimg=\"1\"/\u003e ，以及真实标签 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D\" alt=\"\\mathbf{y}\" eeimg=\"1\"/\u003e 和学生输出 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By_%7Bs%7D%7D\" alt=\"\\mathbf{y_{s}}\" eeimg=\"1\"/\u003e ，KD损失表示为：\u003c/p\u003e\u003cp data-pid=\"tAhKeW-G\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cmathcal%7BL%7D_%7BK+D%7D%3D%5Calpha%5Cmathcal%7BL%7D_%7BC+E%7D%28%5Cmathbf%7By%7D%2C%5Cmathbf%7By_%7Bs%7D%7D%29%2B%281-%5Calpha%29%5Cmathcal%7BL%7D_%7BK+L%7D%28%5Cmathbf%7Bp_%7Bt%7D%7D%2C%5Cmathbf%7Bp_%7Bs%7D%7D%29%2C+\" alt=\" \\mathcal{L}_{K D}=\\alpha\\mathcal{L}_{C E}(\\mathbf{y},\\mathbf{y_{s}})+(1-\\alpha)\\mathcal{L}_{K L}(\\mathbf{p_{t}},\\mathbf{p_{s}}), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"NT2UteRb\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7BC+E%7D\" alt=\"\\mathcal{L}_{C E}\" eeimg=\"1\"/\u003e \u003ci\u003e表示捕捉与真实标签对齐的交叉熵损失，\u003c/i\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7BK+L%7D\" alt=\"\\mathcal{L}_{K L}\" eeimg=\"1\"/\u003e 表示测量教师和学生分布之间差异的Kullback-Leibler散度[294]， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Calpha%5C%2C%5Cin%5C%2C%5B0%2C1%5D\" alt=\"\\alpha\\,\\in\\,[0,1]\" eeimg=\"1\"/\u003e 是一个调节目标的超参数。软目标 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bp_%7Bt%7D%7D\" alt=\"\\mathbf{p_{t}}\" eeimg=\"1\"/\u003e 通常由温度参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/\u003e 调整（即 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bp_%7Bt%7D%7D%3D%5Cmathrm%7Bsoftmax%7D%28%5Cmathbf%7Bz_%7Bt%7D%7D%2FT%29\" alt=\"\\mathbf{p_{t}}=\\mathrm{softmax}(\\mathbf{z_{t}}/T)\" eeimg=\"1\"/\u003e ，其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bz_%7Bt%7D%7D\" alt=\"\\mathbf{z_{t}}\" eeimg=\"1\"/\u003e 是教师的logits），编码了更丰富的概率信息，使学生能够模拟教师的决策细微之处，而不仅仅是标签准确性。\u003c/p\u003e\u003cp data-pid=\"-h5kEk_E\"\u003eKD广泛用于资源受限环境下的模型压缩和迁移学习，其中预训练的教师指导特定任务的学生。其有效性取决于教师容量、学生架构和蒸馏损失设计等因素。最近的进展将KD扩展到输出蒸馏之外，使得在后训练优化中实现更高效和适应性强的LLM。根据对教师模型内部参数和中间表示的访问程度，KD方法大致可以分为黑盒KD和白盒KD。\u003c/p\u003e\u003cp data-pid=\"ZHmOY3Em\"\u003e表6：大型语言模型(2020–2025年)知识蒸馏方法总结。此表概述了关键的蒸馏技术，详细列出了它们的技能、教师和学生模型、目标及发布时间线，并按黑盒KD（访问限于教师输出，通常来自闭源LLM）和白盒KD（访问教师参数或分布，通常来自开源LLM）分类。指标包括IF（指令跟随）、CoT（链式思维）、ICL（上下文学习）、SFT（监督微调）、D\u0026amp;S（差异与相似性）、RL（强化学习）、TP（思考模式）、NLU（自然语言理解）和NLG（自然语言生成）。 \u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-d2660b8df818045f45d80bad74c13cc7_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"1046\" data-original-token=\"v2-82e325b3e5490784916532e910523a13\" class=\"origin_image zh-lightbox-thumb\" width=\"1556\" data-original=\"https://picx.zhimg.com/v2-d2660b8df818045f45d80bad74c13cc7_r.jpg\"/\u003e\u003cfigcaption\u003e表6：大型语言模型的知识提炼方法总结（2020-2025）。本表概述了关键的蒸馏技术，详细介绍了它们的技能、教师和学生模型、目标和发布时间表，分为黑盒KD（仅限于教师输出，通常来自闭源LLM）和白盒KD（通常来自开源LLM，访问教师参数或分布版）。指标包括IF（指令遵循）、CoT（思维链）、ICL（情境学习\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"l8oITcks\"\u003e如表6所示，知识蒸馏方法大致可以分为两类：黑盒KD和白盒KD。我们提供了各种大型语言模型(LLMs)中的知识蒸馏技术的系统总结，以及它们相应的技能、教师模型和学生模型。\u003c/p\u003e\u003ch3\u003e黑盒KD\u003c/h3\u003e\u003cp data-pid=\"lP6BQLyf\"\u003e黑盒KD是指学生模型仅从教师的输出logits学习，而无法访问其内部表示或架构细节的情况。这种方法最初由Hinton[321]提出，符合经典的KD范式，由于其灵活性而被广泛采用。黑盒KD的一个关键优势是将教师模型视为不透明函数，即使教师是一个专有或预训练模型且访问受限，也能实现知识转移。实际上，大型教师LLM（如ChatGPT和GPT-4[9]）通常用于生成高质量输出。与此同时，较小的语言模型(SLM)，包括GPT-2[14]、T5[322]、Flan-T5[323]和CodeT5[324]，作为学生模型。这些SLM经过优化以提高效率，同时保持强大的泛化能力，使其适合在资源受限环境中部署。\u003c/p\u003e\u003ch3\u003e白盒KD\u003c/h3\u003e\u003cp data-pid=\"VTNDfeWY\"\u003e白盒KD通过利用教师模型的内部表示，扩展了传统的蒸馏范式。当教师模型的架构已知且可访问时，这种方法是有益的，允许进行更丰富的监督形式。与将教师视为不透明函数的黑盒KD不同，白盒KD允许学生模型不仅从教师的输出logits学习，还可以从中间激活、隐藏层甚至注意权重[325]学习。\u003c/p\u003e\u003ch3\u003eDeepSeek-R1：推理模式的直接蒸馏\u003c/h3\u003e\u003cp data-pid=\"tKK8lj1S\"\u003eDeepSeek-R1展示了KD通过将大规模模型中的复杂推理模式蒸馏到紧凑架构中，显著增强了小型LLM的推理能力，而无需承担直接在这些模型上进行强化学习(RL)的计算负担。这种方法称为直接蒸馏，利用由大型教师模型生成的大约80万样本的精心策划数据集，其中包括从DeepSeek-V3派生的20万非推理实例和由DeepSeek-R1-Stage1检查点生成的60万推理实例。这些样本构成了应用于开源基础模型（如Qwen和LLaMA的小型变体）的SFT的基础，使学生模型能够继承通常为其较大对应物保留的复杂推理能力。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-2e653b30dedafecf4438d507fa99fb5f_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"574\" data-original-token=\"v2-c31e164e8a5bbaaa8382e34d65cfec44\" class=\"origin_image zh-lightbox-thumb\" width=\"1308\" data-original=\"https://pic2.zhimg.com/v2-2e653b30dedafecf4438d507fa99fb5f_r.jpg\"/\u003e\u003cfigcaption\u003e图17：DeepSeek-R1中的知识蒸馏工作流程，展示了从大型模型到紧凑模型的推理模式转移过程\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"pyayFKoV\"\u003eDeepSeek-R1中的直接蒸馏过程如图17所示，展开在一个结构化的管道中。最初，教师模型——在大量数据集上预训练——生成一个包含推理和非推理输出的多样化语料库，捕捉逻辑模式和事实知识的谱系。非推理数据（约20万样本）提供了一般知识的基线，而推理数据（约60万样本）封装了多步推理链，通过教师的高级功能进行了细化。然后，在SFT阶段使用该数据集，学生模型在此过程中被训练以使其输出分布与教师对齐，使用推理数据直接微调较小模型以蒸馏出紧凑的推理模型。与直接应用于小模型的传统RL不同，后者可能因容量有限而导致次优推理，DeepSeek-R1的直接蒸馏通过转移预先优化的推理行为，克服了这些限制，实现了更高的性能并减少了资源需求。\u003c/p\u003e\u003cp data-pid=\"v0GPNobJ\"\u003eDeepSeek-R1的KD方法的一个显著特点是强调在不同模型规模下保持推理完整性。通过集成从DeepSeek-R1-Stage1——通过大规模RL精炼的检查点——提取的推理轨迹，学生模型不仅复制了事实准确性，还模拟了复杂的推理过程，例如数学问题解决或逻辑演绎所需的推理过程。这种有针对性的转移与传统的KD形成对比，后者通常优先考虑分类任务，突显了DeepSeek-R1在面向推理的蒸馏方面的创新。此外，该方法减少了对学生进行大量RL迭代的需求，利用教师预先计算的推理输出简化训练，从而提高效率和可扩展性。这一方法论将DeepSeek-R1定位为将高级推理蒸馏到紧凑LLM的典范，为未来的后训练优化工作提供了蓝图。\u003c/p\u003e\u003ch2\u003e七、PoLMs 集成和适配\u003c/h2\u003e\u003cp data-pid=\"WXbvNtNI\"\u003e集成和适应技术对于增强大语言模型（LLMs）在各种实际应用中的灵活性和有效性至关重要。这些方法使 LLMs 能够无缝处理异构数据类型，适应特定领域，并利用多种架构优势，从而应对复杂、多方面的挑战。本章详细介绍了三种主要策略：多模态集成 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28%5CS7.1%29\" alt=\"(\\S7.1)\" eeimg=\"1\"/\u003e ，使模型能够处理文本、图像和音频等不同的数据模态；领域适应 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28%5CS7.2%29\" alt=\"(\\S7.2)\" eeimg=\"1\"/\u003e ，对模型进行优化以适用于特定行业或用例；以及模型合并 (§7.3)，将不同模型的能力融合在一起以优化整体性能。总体而言，这些方法增强了 LLMs 的适应性、效率和鲁棒性，拓宽了它们在不同任务和场景中的应用范围。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-c9e5f589ee83240fbcd71bc552eb4b84_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"956\" data-rawheight=\"476\" data-original-token=\"v2-c00d0f9a409587b7922b8fdce81c322f\" class=\"origin_image zh-lightbox-thumb\" width=\"956\" data-original=\"https://pic3.zhimg.com/v2-c9e5f589ee83240fbcd71bc552eb4b84_r.jpg\"/\u003e\u003cfigcaption\u003e图18：多模态集成中模态连接方法的分类，包括基于投影、基于查询和基于融合的方法\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e7.1 多模态集成\u003c/h2\u003e\u003cp data-pid=\"w2-_8T0g\"\u003e在前几章阐述的后训练优化策略基础上，本节探讨旨在增强大型语言模型（LLMs）和大型多模态模型（LMMs）有效处理多模态数据的高级方法。虽然监督微调提高了LLMs在特定任务上下文中的能力，但其在充分利用多模态能力方面的局限性需要更复杂的后训练方法。这些技术通过将不同数据类型整合到统一框架中，使LMMs能够应对复杂的跨模态任务（例如，从视觉输入生成网页代码[326]、解释复杂的文化符号如表情包[327]以及在不依赖光学字符识别的情况下进行数学推理[50]）。通常，LMMs包括一个模态编码器、预训练的LLM主干和一个模态连接器[328]，如图18所示。这种架构构成了后训练方法的基础，这些方法通过优化每个组件，促进稳健的多模态集成和性能提升。\u003c/p\u003e\u003ch2\u003e7.1.1 模态连接\u003c/h2\u003e\u003cp data-pid=\"Pvn-9qFi\"\u003e模态连接方法在将多模态数据综合成一个连贯的表示框架中起着关键作用，可以分为三种主要策略：基于投影的方法、基于查询的方法和基于融合的方法[328]，如图19所示。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-1265b69b411b4ecc5aeaa9e7deddecc4_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"528\" data-original-token=\"v2-0ac2579a83e112ad93c443314fdb1cee\" class=\"origin_image zh-lightbox-thumb\" width=\"1272\" data-original=\"https://pica.zhimg.com/v2-1265b69b411b4ecc5aeaa9e7deddecc4_r.jpg\"/\u003e\u003cfigcaption\u003e图19：多模态集成中模态连接方法的分类，包括基于投影、基于查询和基于融合的方法\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e基于投影的模态连接\u003c/h3\u003e\u003cp data-pid=\"AHpsqAiB\"\u003e基于投影的方法将不同的模态输入转换为统一的文本嵌入空间，使它们的特征与大语言模型（LLMs）的语言维度对齐，实现无缝集成。LLaMA-Adapter[329]通过引入图像编码器，将LLMs扩展到多模态系统中，实现了图像条件下的指令跟踪。其后续版本LLaMA-Adapter V2[330]通过将视觉标签嵌入到早期LLM层中，进一步改进了这一过程，促进了视觉知识的更好吸收。FROMAGe[331]通过对冻结的LLM和视觉编码器框架中的输入和输出层进行微调，实现了跨模态交互，而LLaVA-1.5[332]则利用双线性多层感知机（MLP）增强了多模态处理的鲁棒性。最近的发展，如Shikra[333]，通过整合空间坐标来增强自然语言对话，而VILA[334]优化了视觉-语言预训练，以实现卓越的零样本能力。DetGPT[335]通过将推理驱动的对象检测与自然语言交互相结合，进一步推进了这一范式，利用投影技术促进了有效的多模态通信。SOLO[336]使用单一的Transformer架构进行统一和端到端的视觉-语言建模，接受原始图像块（以像素形式）和文本作为输入，而无需使用单独的预训练视觉编码器。MiniGPT-4[326]通过单个投影层将冻结的视觉编码器与Vicuna对齐，实现了类似GPT-4的能力，并采用了两阶段训练过程。Idefics[337]凭借自回归设计和多阶段预训练，在高效推理方面表现出色。LaVIT[338]通过离散视觉分词器统一了视觉和语言，实现了无缝生成。DeepSeek-VL2[339]通过动态平铺和多头潜在注意力增强了高分辨率图像理解。最后，Qwen2.5-VL[340]通过重新设计的视觉Transformer，提高了多模态任务的性能，在感知和视频理解方面表现出色。\u003c/p\u003e\u003ch3\u003e基于查询的模态连接\u003c/h3\u003e\u003cp data-pid=\"g-tj0uTA\"\u003e基于查询的方法通过使用可学习的查询令牌从不同模态中提取结构化信息，弥合文本和非文本数据之间的差距，从而增强多模态集成。BLIP-2[52]率先采用了查询Transformer，实现了文本和视觉输入的有效集成。Video-LLaMA[341]通过组合视觉编码器将这一技术扩展到视频理解，而Instruct BLIP[342]通过改进查询机制确保了对指令的精确遵循。X-LLM[343]通过专用接口对齐多模态输入，而随后的创新如mPLUG-Owl[344]和Qwen-VL[345]优化了Q-Former架构以提高计算效率。LION[346]进一步展示了基于查询方法的有效性，通过提升视觉知识集成，强调了其在各种任务中增强LMM性能的实用性。Qwen-VL[345]是一系列大规模视觉-语言模型，基于Qwen-7B构建，集成了视觉接收器、位置感知适配器和三阶段训练管道，实现了多语言、细粒度的视觉-语言理解。Lyrics[347]是一个细粒度的视觉-语言预训练和指令微调框架，通过视觉细化器（图像标注、对象检测和语义分割）和多尺度查询Transformer（MQ-Former），增强了大型视觉-语言模型（LVLMs）的功能。\u003c/p\u003e\u003ch3\u003e基于融合的模态连接\u003c/h3\u003e\u003cp data-pid=\"Bsrgaw4b\"\u003e基于融合的技术通过直接将多模态特征嵌入到LLM架构中，加深了跨模态交互，促进了推理层面更丰富的集成。Flamingo[51]通过交叉注意力层在标记预测过程中融合视觉特征，实现了动态多模态处理。Open Flamingo[348]在此基础上，允许冻结的LLMs关注视觉编码器的输出，增强了灵活性。Otter[349]引入了指令调优，以改善多模态指令的遵循，而CogVLM[350]通过在Transformer层中集成视觉专家模块，实现了无缝特征合成。Obelics[351]利用交错的图像-文本训练数据，突显了基于融合的方法在实现连贯多模态性能方面的稳健性。InternVL[352]是一个大规模的视觉-语言基础模型，将视觉编码器扩展到60亿参数，并通过语言中间件（QLLaMA）逐步与LLMs对齐。Llama 3[25]是Meta开发的一系列多语言、工具使用的基础模型，参数规模达到4050亿，具有128K标记上下文窗口，通过改进的数据质量、更大规模的训练和结构化的后训练策略进行了优化。\u003c/p\u003e\u003cp data-pid=\"jLQcUF6e\"\u003e表7：2022-2025年各模态编码器和大规模多模态模型概览。该表总结了关键的多模态模型，详细列出了它们的编码器类别、大小、输入投影器、LLM骨干和发布时间线，涵盖了视觉、音频和其他模态。指标包括C-a（交叉注意力）、Q-F（Q-Former）、MQ-F（多查询Q-Former）和LP（线性投影器），代表输入投影机制。 \u003c/p\u003e\u003ch2\u003e7.1.2 模态编码器\u003c/h2\u003e\u003cp data-pid=\"m8QI1IXx\"\u003e模态编码器将原始多模态输入压缩成紧凑且语义丰富的表示，使各种任务和模态的处理更加高效。这些组件对于将异构数据转换为与大语言模型（LLM）主干兼容的格式至关重要，支持从视觉推理到音频理解的各种应用。表7提供了在视觉、音频及其他模态中广泛使用的编码器的全面总结，详细描述了它们的特点及其对多模态集成的贡献。\u003c/p\u003e\u003ch3\u003e视觉编码器\u003c/h3\u003e\u003cp data-pid=\"7gw8RH8G\"\u003e视觉编码器是多模态学习的基础，有助于在大规模多模态模型（LMMs）中解释和生成视觉数据。CLIP [372] 通过对比学习建立了图像-文本的联合表示，增强了跨模态对齐。EVA [373] 优化了视觉注意力机制，提高了效率；而 ImageBind [374] 创建了一个统一的嵌入空间，跨越多个模态，提升了零样本识别能力。SigLIP [375] 引入了一种配对的Sigmoid损失来优化图像-文本预训练，DINOv2 [376] 则利用无监督学习从多样化的来源中提取鲁棒的视觉特征。LLaVA [53] 采用自指导策略将图像转换为文本描述，使用先进的大语言模型生成新的数据集。Video-ChatGPT [354] 支持大规模指令数据集下的对话视频理解，BT-Adapter [355] 通过高效的时序建模优化了视频理解。VideoChat [353] 专注于时空推理，利用专门的数据集和模型，如 CoDi-2 [369] 和 Mipha [358]，在多模态处理中实现了效率提升。VL-Mamba [357] 和 Cobra [359] 引入了状态空间模型以优化推理，SPHINX-Tiny [356] 则强调数据多样性和训练效率。\u003c/p\u003e\u003ch3\u003e音频编码器\u003c/h3\u003e\u003cp data-pid=\"9a_GPf9E\"\u003e音频编码器增强了大规模多模态模型（LMMs）处理和解释听觉输入的能力，扩展了其多模态范围。SpeechGPT [364] 将大规模语音数据集与卷积和变换器架构 [377] 结合，实现了强大的指令跟随能力。AudioPaLM [363] 结合了文本和语音处理，使用通用语音模型（USM）编码器 [378]，在零样本语言翻译等任务中表现出色。WavCaps [379] 使用 CNN14 [380] 和 HTSAT [381] 来缓解音频-语言数据的稀缺问题，利用先进的大语言模型改进数据集质量和增强学习效果，突显了音频模态在多模态系统中的关键作用。\u003c/p\u003e\u003ch3\u003e其他编码器\u003c/h3\u003e\u003cp data-pid=\"atJ6KdKr\"\u003e除了视觉和音频，其他模态的编码器，如3D理解和多模态融合，也是全面的大规模多模态模型（LMMs）的重要组成部分。NEXT-GPT [368] 促进了文本、图像、视频和音频之间的跨模态内容生成，通过最小的参数调整推进了类人AI能力的发展。ImageBind-LLM [366] 对齐了视觉和语言嵌入，以提高跨模态的指令跟随能力。LL3DA [370] 处理点云数据，用于3D推理和规划，引入了新的空间理解方法。X-LLM [343] 使用 Q-Former [52] 处理图像和视频输入，使用 C-Former [343] 处理语音，将音频特征压缩成词级别的嵌入，以提高多模态学习的效率。\u003c/p\u003e\u003ch2\u003e7.2 领域适配\u003c/h2\u003e\u003cp data-pid=\"-fiB7d0k\"\u003e领域适应(DA)是优化大型语言模型(LLMs)以在特定领域内表现出色的关键后训练策略，确保其在目标应用中的有效性。基于迁移学习[382, 383]的原则，DA通过适应函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=F_%7B%5Cmathrm%7Badapt%7D%7D\" alt=\"F_{\\mathrm{adapt}}\" eeimg=\"1\"/\u003e 将初始模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=M_%7B%5Cmathrm%7Bsource%7D%7D\" alt=\"M_{\\mathrm{source}}\" eeimg=\"1\"/\u003e 转换为领域特定模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=M_%7B%5Cmathrm%7Btarget%7D%7D\" alt=\"M_{\\mathrm{target}}\" eeimg=\"1\"/\u003e ，如图所示：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-0df84ae57edc9fa6ce0e8fa0ff85f70a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"658\" data-rawheight=\"220\" data-original-token=\"v2-cbd56abb787df51ca437c3a039d28885\" class=\"origin_image zh-lightbox-thumb\" width=\"658\" data-original=\"https://pica.zhimg.com/v2-0df84ae57edc9fa6ce0e8fa0ff85f70a_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"kvfMzy5D\"\u003e这一过程使 \u003cimg src=\"https://www.zhihu.com/equation?tex=M_%7B%5Cmathrm%7Btarget%7D%7D\" alt=\"M_{\\mathrm{target}}\" eeimg=\"1\"/\u003e 能够应对指定领域的独特需求和复杂性，从而优化其性能和相关性。通过提高LLMs在编程[384, 385]和数学推理[386]等领域的熟练度，DA不仅提升了领域特定能力，还提高了计算效率，缓解了通用模型在处理领域特定术语和推理范式时的局限性。此外，DA显著减少了从头开始训练领域特定模型通常所需的大量标注数据集和计算资源[387]，使其成为后训练方法的核心。\u003c/p\u003e\u003ch2\u003e7.2.1 知识编辑\u003c/h2\u003e\u003cp data-pid=\"fcQSCvxK\"\u003e知识编辑代表了一种复杂的后训练方法，旨在修改大语言模型（LLMs）以满足特定领域的需求，同时不损害其基础能力。该技术促进了目标参数调整，保留了模型的现有性能，同时整合了新的或更新的领域知识[388]。通过实现对不断变化的知识景观的快速适应，知识编辑成为了后训练管道中不可或缺的组成部分。表8展示了主要方法的概述（例如，包括外部知识利用、集成和内在编辑）。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-15d866a2eba8c5f92d3a01aac9e1870f_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1536\" data-rawheight=\"800\" data-original-token=\"v2-a612b4676be14b484c79cfe6d83f8de1\" class=\"origin_image zh-lightbox-thumb\" width=\"1536\" data-original=\"https://pic2.zhimg.com/v2-15d866a2eba8c5f92d3a01aac9e1870f_r.jpg\"/\u003e\u003cfigcaption\u003e表8：在LLMs中进行知识编辑的代表性方法的比较分析。编辑区域指定了模型中要修改的组件；编辑器参数数表示编辑过程中需要更新的参数。$L$ 表示受修改影响的层数，$d_{h}$ 表示变压器架构中隐藏层的维度，$d_{m}$ 指的是上投影和下投影阶段之间的中间维度，$N$ 表示每层中需要更新的神经元总数。\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"MTaOXJ4T\"\u003e\u003cb\u003e知识编辑的形式定义。\u003c/b\u003e考虑一个由 $\\theta$ 参数化的原始LLM，预先在数据集 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7B%5Cmathrm%7Bold%7D%7D\" alt=\"\\mathcal{D}_{\\mathrm{old}}\" eeimg=\"1\"/\u003e\u003ci\u003e 上训练。令 \u003c/i\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7B%5Cmathrm%7Bnew%7D%7D\" alt=\"\\mathcal{D}_{\\mathrm{new}}\" eeimg=\"1\"/\u003e 表示包含新或更新的信息 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CDelta+K\" alt=\"\\Delta K\" eeimg=\"1\"/\u003e 的数据集。知识编辑的目标是通过应用调整 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CDelta%5Ctheta\" alt=\"\\Delta\\theta\" eeimg=\"1\"/\u003e 来推导出修订后的参数集 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%5Cprime%7D\" alt=\"\\theta^{\\prime}\" eeimg=\"1\"/\u003e ，有效地同化 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CDelta+K\" alt=\"\\Delta K\" eeimg=\"1\"/\u003e 同时最小化对 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7B%5Cmathrm%7Bold%7D%7D\" alt=\"\\mathcal{D}_{\\mathrm{old}}\" eeimg=\"1\"/\u003e 的退化。正式地，这被表述为一个约束优化问题，其中更新后的参数定义为：\u003c/p\u003e\u003cp data-pid=\"Lt6N4zZ-\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Ctheta%5E%7B%5Cprime%7D%3D%5Ctheta%2B%5CDelta%5Ctheta%2C%5Cmathrm%7Bwhere%5C%3B%5Cmathcal%7BL%7D%5Cleft%28%5Ctheta%5E%7B%5Cprime%7D%3B%5Cmathcal%7BD%7D_%7B%5Cmathrm%7Bnew%7D%7D%5Cright%29%5C%3B%5Crightarrow%5C%3B%5Coperatorname%2A%7Bmin%7D%2C%7D+\" alt=\" \\theta^{\\prime}=\\theta+\\Delta\\theta,\\mathrm{where\\;\\mathcal{L}\\left(\\theta^{\\prime};\\mathcal{D}_{\\mathrm{new}}\\right)\\;\\rightarrow\\;\\operatorname*{min},} \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"IA8l53ze\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D\" alt=\"\\mathcal{L}\" eeimg=\"1\"/\u003e 表示评估模型在 \u003cimg src=\"https://www.zhihu.com/equation?tex=%7B%5Cmathcal%7BD%7D%7D_%7B%5Cmathrm%7Bnew%7D%7D\" alt=\"{\\mathcal{D}}_{\\mathrm{new}}\" eeimg=\"1\"/\u003e 上质量的损失函数（例如，交叉熵）。为了保护在原始数据集上的性能，施加了一个约束：\u003c/p\u003e\u003cp data-pid=\"RwWA3yFS\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Cmathcal%7BL%7D%5Cbig%28%5Ctheta%5E%7B%5Cprime%7D%3B%5Cmathcal%7BD%7D_%7B%5Cmathrm%7Bold%7D%7D%5Cbig%29%5C+%5Cleq%5C+%5Cmathcal%7BL%7D%5Cbig%28%5Ctheta%3B%5Cmathcal%7BD%7D_%7B%5Cmathrm%7Bold%7D%7D%5Cbig%29%5C+%2B%5C+%5Cepsilon%2C+\" alt=\" \\mathcal{L}\\big(\\theta^{\\prime};\\mathcal{D}_{\\mathrm{old}}\\big)\\ \\leq\\ \\mathcal{L}\\big(\\theta;\\mathcal{D}_{\\mathrm{old}}\\big)\\ +\\ \\epsilon, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"_Sl1AvP1\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/\u003e 是一个小的正常数，限制在 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7B%5Cmathrm%7Bold%7D%7D\" alt=\"\\mathcal{D}_{\\mathrm{old}}\" eeimg=\"1\"/\u003e 上的性能损失。这种公式确保了 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%5Cprime%7D\" alt=\"\\theta^{\\prime}\" eeimg=\"1\"/\u003e 同化了 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CDelta+K\" alt=\"\\Delta K\" eeimg=\"1\"/\u003e 同时保留了模型的先前知识库。实际上， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CDelta%5Ctheta\" alt=\"\\Delta\\theta\" eeimg=\"1\"/\u003e 可能被限制在特定的架构组件（例如，注意力层 (Attn) 或前馈网络 (FFN)），减少了计算开销并避免了全面重新训练，从而保留核心功能。\u003c/p\u003e\u003cp data-pid=\"spA4PXzh\"\u003e知识识别。知识编辑的初始阶段集中在检测并将新信息融入模型。PokeMQA [390] 使用可编程范围检测器和知识提示来解析查询，高效检索相关事实。相反，SERAC [389] 集成了反事实模型和分类器来确定新知识源的适用性，提供了一种最小侵入的方法，无需广泛的结构修改即可保留基础模型的完整性。[406] 分析了为什么LLM知识更新会创建混乱的连锁反应。现实世界中的编辑通常源自新兴事件，这些事件涵盖了新事实与过去事实之间的逻辑联系。基于这一观察，EvEdit [407] 提出了一种基于事件的知识编辑方法，用于确定知识锚点和知识更新边界。\u003c/p\u003e\u003cp data-pid=\"IPfhzqx5\"\u003e知识关联。在识别之后，这一阶段将新获取的信息与模型现有的知识框架关联起来。Transformer-Patcher [392] 适应变压器架构以整合更新的事实，而CaliNET [391] 重新校准参数以与事实内容对齐。诸如Eva-KELLM [395]、MELO [396] 和REMEDI [393] 等方法精炼特定行为以进行精确更新，GRACE [394] 在知识插入后增强预测准确性，确保与先前表示的无缝集成。\u003c/p\u003e\u003cp data-pid=\"7DKgplv5\"\u003e内在知识编辑。最后阶段将关联的事实嵌入模型的内部结构，确保全面同化。虽然传统的微调可能资源密集，但先进的技术减轻了这一负担。约束微调 [397] 和元学习 [399] 最小化了知识损失和过拟合风险。可编辑训练 [398] 和知识编辑器 [399] 能够迅速调整参数，同时最小化性能影响，而SLAG [400]、MEND [401] 和MALMEN [402] 解决编辑冲突并支持大规模更新，同时保持基础能力并纳入新的领域见解。LLM Surgery [403] 通过应用逆梯度移除过时数据、梯度下降整合新事实，并引入KL散度项以保留现有知识，实现了显著的计算效率。KNE [404] 引入了一种知识神经元集合方法，该方法仅定位和更新与新插入事实强烈相关的神经元，实现更准确的编辑同时保留无关知识。OVERTONE [405] 通过引入一种逐词平滑技术解决知识编辑中的异构词过拟合问题，自适应地细化训练目标，从而保留预训练知识并提高模型对新插入事实的推理能力。这些有针对性的技术确保了模型在整合新获取信息的同时保留其基础能力。\u003c/p\u003e\u003ch2\u003e7.2.2 检索增强生成\u003c/h2\u003e\u003cp data-pid=\"bM3RgTrE\"\u003e检索增强生成(Retrieval-Augmented Generation, RAG)将传统信息检索与现代大语言模型(LLMs)结合，以提高生成输出的相关性和事实准确性[48, 408, 409]。通过动态地从外部源检索相关信息并将其嵌入生成过程，RAG解决了LLMs在特定领域知识上的不足，并减少了生成幻觉内容的倾向。这种方法在需要精确、最新信息的领域特别有效，例如问答系统[48]、科学研究[410]和医疗保健[411]，这些领域能够处理复杂的查询和知识密集型任务。此外，RAG减少了对话系统中误导性响应的频率，提高了基于知识的自然语言生成的保真度[411, 412]。\u003c/p\u003e\u003cp data-pid=\"_4sAfZH7\"\u003e本小节重点介绍基于训练的RAG方法[413]，认识到无训练的RAG方法[414, 415, 416]可能因缺乏任务特定优化而影响知识利用效率。三种主要的训练策略——独立训练(Independent Training)、顺序训练(Sequential Training)和联合训练(Joint Training)——增强了模型的适应性和集成能力，如图20所示。\u003c/p\u003e\u003ch3\u003e独立训练\u003c/h3\u003e\u003cp data-pid=\"HX6Y2rag\"\u003e该策略将检索器和生成器作为独立模块进行训练，使根据任务需求灵活使用稀疏或密集检索器成为可能。例如，DPR[417]使用双BERT网络分别对查询和段落进行编码，通过对比学习优化检索，而不涉及生成器的交互。同样，[418]提出了Reward-RAG，利用奖励模型根据GPT反馈仅微调检索器，而不改变生成器。\u003c/p\u003e\u003ch3\u003e顺序训练\u003c/h3\u003e\u003cp data-pid=\"meUfmW6O\"\u003e顺序训练通过一次优化一个模块来提高效率，促进检索器和生成器之间的协同作用。它包括检索器优先方法[419, 420, 421, 422, 423]，\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-6efa8ee9e815075832fb9368457d6c71_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1250\" data-rawheight=\"1130\" data-original-token=\"v2-b0e98c528474626210b19a85c67e6caf\" class=\"origin_image zh-lightbox-thumb\" width=\"1250\" data-original=\"https://pic4.zhimg.com/v2-6efa8ee9e815075832fb9368457d6c71_r.jpg\"/\u003e\u003cfigcaption\u003e图20: 检索增强生成(RAG)训练方法的分类，包括独立训练、顺序训练和联合训练策略\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"8KXy9Upd\"\u003e如RETRO[424]，它首先预训练一个基于BERT的检索器，然后训练一个编码器-解码器以无缝集成检索到的内容，从而提高性能。另一种方法是语言模型优先方法[425, 426, 427]，如RA-DIT[428]，首先微调语言模型以有效利用检索到的知识，然后再优化检索器以实现更好的对齐和连贯性[419, 425]。\u003c/p\u003e\u003ch3\u003e联合训练\u003c/h3\u003e\u003cp data-pid=\"-dIoLfFa\"\u003e联合训练在一个端到端框架中同步优化检索器和生成器。RAG[48]通过最小化负对数似然来共同训练这两个组件，而REALM[429]则通过最大内积搜索(MIPS)[430]提高检索精度。这些方法适应特定任务的需求，最大化外部知识的好处，同时最小化生成错误。\u003c/p\u003e\u003ch2\u003e7.3 模型合并\u003c/h2\u003e\u003cp data-pid=\"jkKgm_H2\"\u003e模型合并已成为提高大语言模型（LLM）在训练和推理阶段性能和效率的重要后训练策略[431, 432]。这种方法将专门化的模型整合到一个统一的架构中，避免了大量重新训练的需求，并解决了大型模型尺寸和计算需求带来的挑战。与在混合数据集上进行训练不同，模型合并将单任务模型整合为一个多任务能力的协调实体，提供了一种资源高效的多任务学习范式。通过简化训练管道并促进在各种应用中具有强大泛化能力的多功能模型的开发，该技术优化了LLM在不同场景中的部署。\u003c/p\u003e\u003cp data-pid=\"H1IEHxnh\"\u003e给定一组候选模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BM%7D%3D%7BM_%7B1%7D%2CM_%7B2%7D%2C.%5C%2C.%5C%2C.%5C%2C%2CM_%7Bn%7D%7D\" alt=\"\\bar{M}={M_{1},M_{2},.\\,.\\,.\\,,M_{n}}\" eeimg=\"1\"/\u003e ，目标是设计一个合并函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=F_%7B%5Cmathrm%7Bmerge%7D%7D\" alt=\"F_{\\mathrm{merge}}\" eeimg=\"1\"/\u003e ，生成一个统一模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=M%5E%7B%5Cprime%7D\" alt=\"M^{\\prime}\" eeimg=\"1\"/\u003e ，可能以基础模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=M_%7B1%7D\" alt=\"M_{1}\" eeimg=\"1\"/\u003e 为锚点，如图所示：  \u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-5115ddfc0d360376cc0c3c5e8913adf5_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"684\" data-rawheight=\"250\" data-original-token=\"v2-04a53661d6c2d5dd261f111c5fcd471b\" class=\"origin_image zh-lightbox-thumb\" width=\"684\" data-original=\"https://pic2.zhimg.com/v2-5115ddfc0d360376cc0c3c5e8913adf5_r.jpg\"/\u003e\u003c/figure\u003e\u003ch2\u003e7.3.1 分层模型合并\u003c/h2\u003e\u003cp data-pid=\"f4n7cbbI\"\u003e模型融合技术被系统地划分为三个层次——权重级、输出级和模型级融合——如图21所示。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-a2d20a3f8f8952c28ce0886901d44e99_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1256\" data-rawheight=\"556\" data-original-token=\"v2-3735082299b174fbe8d8de0727a478af\" class=\"origin_image zh-lightbox-thumb\" width=\"1256\" data-original=\"https://pic2.zhimg.com/v2-a2d20a3f8f8952c28ce0886901d44e99_r.jpg\"/\u003e\u003cfigcaption\u003e图21: 模型融合技术的分类，展示了包括权重级、输出级和模型级方法在内的层次结构，适用于大型语言模型\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e权重级模型融合\u003c/h3\u003e\u003cp data-pid=\"uIULhS5p\"\u003e权重级融合直接操作参数空间，对于具有相似架构或在相关任务上训练的模型特别有效。形式上，给定参数集 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7B1%7D%2C%5Ctheta_%7B2%7D%2C%5Cldots%2C%5Ctheta_%7Bn%7D%5Cin%5Cmathbb%7BR%7D%5E%7Bd%7D\" alt=\"\\theta_{1},\\theta_{2},\\ldots,\\theta_{n}\\in\\mathbb{R}^{d}\" eeimg=\"1\"/\u003e ，线性融合方案将这些参数集聚合为统一的集合 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%5Cprime%7D\" alt=\"\\theta^{\\prime}\" eeimg=\"1\"/\u003e ，表示为：\u003c/p\u003e\u003cp data-pid=\"WqP37GnV\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Ctheta%5E%7B%5Cprime%7D%5C%3B%3D%5C%3B%5Calpha_%7B1%7D%5C%2C%5Ctheta_%7B1%7D%5C%3B%2B%5C%3B%5Calpha_%7B2%7D%5C%2C%5Ctheta_%7B2%7D%5C%3B%2B%5C%3B.%5C%3B.%5C%3B.%5C%3B%2B%5C%3B%5Calpha_%7Bn%7D%5C%2C%5Ctheta_%7Bn%7D%2C%5C%3B%5C%3B%5C%3B%5C%3B%7B%5Cmathrm%7Bsubject%7D%7D%5C%3B%7B%5Cmathrm%7Bto%7D%7D%5C%3B%5C%3B%5C%3B%5C%3B%5Calpha_%7Bk%7D%5C%3B%5Cgeq%5C%3B0%2C%5C%3B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%5Calpha_%7Bk%7D%5C%3B%3D%5C%3B1.+\" alt=\" \\theta^{\\prime}\\;=\\;\\alpha_{1}\\,\\theta_{1}\\;+\\;\\alpha_{2}\\,\\theta_{2}\\;+\\;.\\;.\\;.\\;+\\;\\alpha_{n}\\,\\theta_{n},\\;\\;\\;\\;{\\mathrm{subject}}\\;{\\mathrm{to}}\\;\\;\\;\\;\\alpha_{k}\\;\\geq\\;0,\\;\\sum_{k=1}^{n}\\alpha_{k}\\;=\\;1. \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"jeE8XwQs\"\u003eModel Soup [433, 434] 通过线性组合在不同任务上微调的模型权重，生成一个单一且高效的模型，体现了这一点。Task Arithmetic (TA) [435] 通过参数的算术运算扩展了这种灵活性，提高了性能适应性。为了缓解对齐问题，TIESmerging [436] 确保参数的一致性，而 DARE [437] 通过概率调整参数增量来最小化干扰，优化融合过程的连贯性和效率。\u003c/p\u003e\u003ch3\u003e输出级模型融合\u003c/h3\u003e\u003cp data-pid=\"NP8ZH2Kc\"\u003e当模型在架构或初始化上存在差异，使得权重级方法不切实际时，输出级融合变得有利。这种方法聚合输出分布而非内部参数，表示为：\u003c/p\u003e\u003cp data-pid=\"M5QylMKJ\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+y%5E%7B%5Cprime%7D%5C%3B%3D%5C%3B%5Calpha%5C%2Cy_%7B1%7D%5C%3B%2B%5C%3B%5Cleft%281-%5Calpha%5Cright%29y_%7B2%7D%2C%5Cquad%5Calpha%5Cin%5B0%2C1%5D%2C+\" alt=\" y^{\\prime}\\;=\\;\\alpha\\,y_{1}\\;+\\;\\left(1-\\alpha\\right)y_{2},\\quad\\alpha\\in[0,1], \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"m-9Z7E14\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7B1%7D\" alt=\"y_{1}\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=y_%7B2%7D\" alt=\"y_{2}\" eeimg=\"1\"/\u003e 分别代表来自模型 \u003cimg src=\"https://www.zhihu.com/equation?tex=+M_%7B1%7D\" alt=\" M_{1}\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=M_%7B2%7D\" alt=\"M_{2}\" eeimg=\"1\"/\u003e 的概率分布。类似于集成策略，此方法将模型预测综合为统一输出。LLMBlender [438] 通过生成独立输出并使用排名和生成过程进行融合来实现这一点，而 FuseLLM [439] 将组合输出概率蒸馏到单个网络中以保持分布的保真度。FuseChat [440] 通过将多个大型语言模型的知识转移到一个整合的目标模型中，结合了权重级和输出级融合，增强了跨模型的协同效应。\u003c/p\u003e\u003ch3\u003e模型级模型融合\u003c/h3\u003e\u003cp data-pid=\"fnc34neG\"\u003e模型级融合通过路由机制整合子模型或层，通常在混合专家（MoE）框架内实现，表示为：\u003c/p\u003e\u003cp data-pid=\"tiaoJh0K\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+M%5E%7B%5Cprime%7D%5C%3B%3D%5C%3B%5Cmathrm%7BMerge%7D%5Cbig%28M_%7B1%7D%2C%5C%2CM_%7B2%7D%5Cbig%29%2C+\" alt=\" M^{\\prime}\\;=\\;\\mathrm{Merge}\\big(M_{1},\\,M_{2}\\big), \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"iFLxhdR_\"\u003e其中 Merge 表示硬路由或软路由函数。Switch Transformer [54] 通过离散门控选择性激活专家层，减少了计算负载，但可能因刚性路由而导致性能折衷。SoftMoE [441] 和 SMEAR [442] 利用连续门控促进专家之间的平滑过渡，增强组件集成和模型一致性。\u003c/p\u003e\u003ch2\u003e7.3.2 预合并方法\u003c/h2\u003e\u003cp data-pid=\"HtyUw5cv\"\u003e预合并方法通过优化独立模型的权重空间、架构一致性及参数对齐，为模型合并建立兼容基础，从而减少后续融合阶段中的冲突和干扰。这些技术提高了合并过程的有效性，确保最终统一的模型保留其组成部分的优势，同时减轻潜在的性能下降。\u003c/p\u003e\u003cp data-pid=\"1oE6632L\"\u003e线性化微调。这种方法在预训练模型的切线空间内精炼模型，避免使用原始的非线性参数空间以实现权重解耦，从而减少合并过程中的干扰。例如，部分适配器的线性化（如TAFT [443]）或注意力层 [444] 将权重更新对齐到不相交的输入区域，保持合并模型中的独立功能 [445]。通过将更新限制在线性框架内，该方法促进了不同模型之间的无缝集成。\u003c/p\u003e\u003cp data-pid=\"lnTobFMN\"\u003e架构转换。这种策略将具有不同架构的异构模型转换为适合直接参数合并的同质形式。方法包括知识蒸馏，如FuseChat [440] 所示，以及身份层插入，如CLAFusion [446]。GAN Cocktail [447] 初始化目标模型以吸收来自不同架构的输出，实现有效桥接结构差异的统一合并过程。\u003c/p\u003e\u003cp data-pid=\"C0oWec4X\"\u003e权重对齐。此方法通过置换将模型对齐到共享的权重盆地，利用线性模式连通性（LMC）属性增强兼容性。技术包括最优传输（OTFusion [448]）、启发式匹配（Git re-basin [449]）和基于学习的对齐（DeepAlign [450]）。REPAIR [451] 减轻了缺乏归一化层的模型中的对齐失败，确保在融合前实现稳健的参数收敛。\u003c/p\u003e\u003ch2\u003e7.3.3 合并中方法\u003c/h2\u003e\u003cp data-pid=\"asizt-jq\"\u003e在合并过程中，动态优化参数融合策略的方法旨在解决任务冲突，减轻干扰，并提升合并后模型的性能和泛化能力。这些方法应对实时整合不同模型的挑战，增强统一架构的适应性和鲁棒性。\u003c/p\u003e\u003ch3\u003e基本合并\u003c/h3\u003e\u003cp data-pid=\"v4G5FtJU\"\u003e该方法利用简单的参数平均或任务向量算术，定义任务向量 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau_%7Bt%7D\" alt=\"\\tau_{t}\" eeimg=\"1\"/\u003e 为第 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 个任务微调后的参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7B%5CTheta%7D%5E%7B%28t%29%7D\" alt=\"\\bar{\\Theta}^{(t)}\" eeimg=\"1\"/\u003e 与初始预训练参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CTheta%5E%7B%280%29%7D\" alt=\"\\Theta^{(0)}\" eeimg=\"1\"/\u003e 之间的偏差：\u003c/p\u003e\u003cp data-pid=\"hs3zEgbC\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Ctau_%7Bt%7D%3D%5CTheta%5E%7B%28t%29%7D-%5CTheta%5E%7B%280%29%7D%2C+\" alt=\" \\tau_{t}=\\Theta^{(t)}-\\Theta^{(0)}, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"m32DGUtJ\"\u003e并通过以下公式促进多任务学习：\u003c/p\u003e\u003cp data-pid=\"rWNvUmdf\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5CTheta%5E%7B%28%5Cmathrm%7Bmerge%7D%29%7D%3D%5CTheta%5E%7B%280%29%7D%2B%5Clambda%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Ctau_%7Bt%7D+\" alt=\" \\Theta^{(\\mathrm{merge})}=\\Theta^{(0)}+\\lambda\\sum_{t=1}^{T}\\tau_{t} \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"HyZLxjKU\"\u003e虽然计算效率高且概念简洁，但这种方法通常会遇到未缓解的参数交互导致的任务干扰，限制了其在需要复杂任务协调场景中的实用性。\u003c/p\u003e\u003ch3\u003e加权合并\u003c/h3\u003e\u003cp data-pid=\"vkTGqMlh\"\u003e该策略根据各个模型的重要性动态分配合并系数，调整贡献以优化融合结果。MetaGPT [452] 通过归一化每个任务向量的平方 L2 范数来计算最优权重：\u003c/p\u003e\u003cp data-pid=\"4IJcFuxL\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+%5Clambda_%7Bt%7D%5E%7B%2A%7D%3D%5Cfrac%7B%5CVert%5Ctau_%7Bt%7D%5CVert%5E%7B2%7D%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BT%7D%5CVert%5Ctau_%7Bk%7D%5CVert%5E%7B2%7D%7D%2C+\" alt=\" \\lambda_{t}^{*}=\\frac{\\Vert\\tau_{t}\\Vert^{2}}{\\sum_{k=1}^{T}\\Vert\\tau_{k}\\Vert^{2}}, \" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"YNlhRblA\"\u003e从而赋予具有较大参数变化的任务更大的影响，如较高的 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5ClVert%5Ctau_%7Bt%7D%5CrVert%5E%7B2%7D\" alt=\"\\lVert\\tau_{t}\\rVert^{2}\" eeimg=\"1\"/\u003e 所示。SLERP [432] 使用球面插值确保平滑的参数过渡，保持模型连续性，而 Layer-wise AdaMerging [453] 通过逐层优化系数来改进这一过程，提高合并架构中任务特定的精度。\u003c/p\u003e\u003ch3\u003e子空间合并\u003c/h3\u003e\u003cp data-pid=\"sw2ezpoX\"\u003e该方法将模型参数投影到稀疏子空间，以最小化干扰并保持计算效率，解决参数贡献的重叠问题。TIESMerging [436] 保留按大小排序的前 20% 的参数，解决符号冲突以保持一致性，DARE [437] 缩放稀疏权重以减少冗余，而 Concrete [454] 利用双层优化创建自适应掩码，确保任务间干扰减少的模型组件细致集成。\u003c/p\u003e\u003ch3\u003e基于路由的合并\u003c/h3\u003e\u003cp data-pid=\"eizYAFwI\"\u003e该技术根据输入特定属性动态融合模型，实现上下文响应的集成过程。SMEAR [442] 计算样本依赖的专家权重以优先考虑相关特征，Weight-Ensembling MoE [455] 采用输入驱动的线性层路由进行选择性激活，而 Twin-Merging [456] 融合任务共享和任务私有知识，构建一个灵活的合并框架，适应多样化的输入需求并增强多任务鲁棒性。\u003c/p\u003e\u003ch3\u003e后校准\u003c/h3\u003e\u003cp data-pid=\"uHUm62PS\"\u003e该技术通过将统一模型的隐藏表示与独立组成部分的隐藏表示对齐，纠正合并后的表示偏差，减轻性能下降。Representation Surgery [319] 通过改进表示一致性，增强了合并模型的鲁棒性和准确性。\u003c/p\u003e\u003ch2\u003e八、数据集\u003c/h2\u003e\u003cp data-pid=\"BWjQZxVW\"\u003e训练后技术精心设计以优化大语言模型（LLM）对特定领域或任务的适应性，利用数据集作为这一优化过程的基石。对先前研究[457, 82]的深入审查表明，数据的质量、多样性和相关性深刻影响模型的有效性，通常决定着训练后工作的成败。为了阐明数据集在这一背景下的关键作用，我们对用于训练后阶段的数据集进行了全面回顾和深入分析，并根据其收集方法将其归类为三种主要类型：人工标注数据、蒸馏数据和合成数据。这些类别反映了不同的数据管理策略，模型可以采用单一方法或结合多种类型的混合方法，以平衡可扩展性、成本和性能。表9提供了这些数据集类型的详细概述，包括它们的来源、大小、语言、任务和训练后阶段（例如SFT和RLHF），我们在后续部分中探讨了它们在提升LLM能力方面的贡献和挑战。\u003c/p\u003e\u003ch2\u003e8.1 人工标注的数据集\u003c/h2\u003e\u003cp data-pid=\"Cvt-7QhF\"\u003e人类标注的数据集以其卓越的准确性和上下文保真度而著称，这些属性源自标注者的任务复杂性的细致理解及其能够做出精确、上下文敏感的调整的能力。这些数据集是优化指令微调的基础，通过提供高质量、专家策划的训练信号，显著提升了大型语言模型在多种任务中的性能。在这个类别中，Flan [17]、P3（公共提示池）[459]、Sup-Natinst（超级自然指令）[462] 和 Dolly-15K [468] 等突出资源被广泛应用于大型语言模型的后训练阶段，每个资源都通过人类专业知识为模型能力的优化贡献了独特的优势。\u003c/p\u003e\u003ch3\u003e用于 SFT 的人类标注数据\u003c/h3\u003e\u003cp data-pid=\"A1-WZN7J\"\u003e在 SFT 阶段，人类标注的数据集发挥着不可或缺的作用，这一点从 Flan、Sup-Natinst 和 Dolly-15K 的贡献中得到了体现，这些数据集提供了精心设计的提示-响应对和任务特定的指令，以提升大型语言模型在各种自然语言处理基准测试中的效能。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Lnastpo_\"\u003e\u003cb\u003eFlan\u003c/b\u003e. Flan 数据集 [17] 是一个基础资源，最初涵盖了 62 个广受认可的自然语言处理基准测试——如 HellaSwag [482]、MRPC [483] 和 ANLI [484]——通过其 180 万个示例促进英语中的稳健多任务学习。最近，FlanV2 [34] 作为其前身的高级迭代版本，通过整合 Flan [17]、P3 [459]、Sup-Natinst [462] 和大量其他数据集，形成了一个综合全面的语料库，从而增强了其在多样化语言和任务领域中的 SFT 实用性。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"BVhztTV8\"\u003e\u003cb\u003eSup-Natinst\u003c/b\u003e. 超级自然指令（Sup-Natinst）[462] 提供了涵盖 55 种语言的 76 种任务类型的广泛多样资源，成为多语言大型语言模型后训练的多功能资源。每种任务都精心配有一个包含明确任务定义的指令——概述了从输入文本到期望输出的映射关系——以及一组示例，展示了正确和错误的响应，为指导模型进行精确的任务执行并增强跨语言适应性提供了强大的框架。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"dD1m_iJJ\"\u003e\u003cb\u003eDolly-15K\u003c/b\u003e. 由 Databricks 员工开发的 Dolly-15K [468] 是一个精选的包含 15,000 个高质量、人类生成的提示-响应对的语料库，专门设计用于大型语言模型的指令微调。该数据集涵盖了广泛的主题和场景——包括头脑风暴、内容生成、信息提取、开放式问答和总结——反映了丰富的任务类型多样性，使模型能够灵活适应各种教学情境，并提高上下文相关性。\u003cbr/\u003e \u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"rc8osmPN\"\u003e人类标注数据集在 SFT 中的强大作用源于其对任务和场景的广泛覆盖，这一特点在上述语料库中得到了充分体现。此外，Open Assistant [466] 提供了一个来自全球众包努力的多语言对话语料库，免费用于推进研究，而 OpenOrca [472] 在 FlanV2 [34] 的基础上增加了数百万个 GPT-3.5 和 GPT-4 的完成结果，构成了一个动态扩展的资源，用于微调和任务对齐。然而，尽管它们对模型泛化做出了重要贡献，确保一致的标注质量和多样性仍然是一个挑战，需要严格的质量控制以最大化其影响。\u003c/p\u003e\u003ch3\u003e用于 RLHF 的人类标注数据\u003c/h3\u003e\u003cp data-pid=\"eeiSZ_fK\"\u003e对于 RLHF，人类标注的数据集如 P3、其多语言扩展 xP3 [463] 和 SHP [460] 提供了关键的人类标注评估，优化了大型语言模型与用户偏好的对齐，提供了奖励建模的细致反馈机制。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"G7-LS3Z_\"\u003e\u003cb\u003eP3：\u003c/b\u003e P3 数据集 [459] 是一个精心策划的指令调优资源，汇集了来自 Hugging Face Hub 的 2300 万个多任务提示，每个提示都配有手动编写的指令，涵盖了多样化的自然语言处理任务，为 RLHF 提供了丰富的基础，以增强大型语言模型在不同应用中的适应性和精确性。\u003c/li\u003e\u003cli data-pid=\"GDBShIhK\"\u003e\u003cb\u003exP3：\u003c/b\u003exP3（跨语言公共提示池）[463] 将 P3 扩展到多语言框架中，涵盖了 46 种语言和 16 个自然语言处理任务的提示和监督数据，旨在支持像 BLOOMZ 和 mT0 这样的模型的多任务提示微调。其内容整合了英语 P3 数据集、四个新的英语任务（如翻译、程序合成）和 30 个多语言自然语言处理数据集，为跨语言 RLHF 优化提供了全面的资源。\u003c/li\u003e\u003cli data-pid=\"GYIoqiiv\"\u003e\u003cb\u003eSHP\u003c/b\u003e：SHP [460] 包含了 349,000 个人类偏好注释，涉及 18 个主题领域的提问和指令的响应评估，用于训练 RLHF 奖励模型并评估自然语言生成（NLG）的质量，其独特之处在于完全依赖于人类编写的注释，使其区别于 HH-RLHF 等混合数据集。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"SrTjD320\"\u003e这些数据集通过提供多样化的、人类标注的评估，增强了 RLHF，精炼了模型与用户偏好的对齐。OpenAI 总结 [458] 和 WebGPT [461] 提供了结构化的、基于比较的反馈和李克特量表评分，有助于更紧密地将模型输出与人类期望对齐。HH-RLHF [104] 进一步强化了这一框架，包括了对有用性和无害性的评估，为旨在确保安全和道德响应的模型奠定了坚实的基础。同时，StackExchange [473] 贡献了特定领域的用户生成内容，丰富了训练数据，特别有利于需要技术领域专业知识的模型。然而，这些数据集面临诸如可扩展性、人类标注中的潜在偏差以及在其特定领域之外的有限适用性等挑战。因此，尽管它们非常有价值，但可能需要补充更广泛的数据集，以实现跨多样化现实任务的全面模型对齐。\u003c/p\u003e\u003ch2\u003e8.2 蒸馏数据集\u003c/h2\u003e\u003cp data-pid=\"2wUJPSJc\"\u003e蒸馏数据（distilled dataset）源自将庞大的原始数据集提炼成紧凑、优化的子集的复杂过程，这些子集保留了对大语言模型训练至关重要的信息，同时在保持性能的前提下提高了训练效率并减少了计算需求。该方法生成的数据集通常在效能上能匹敌甚至超越未经过精炼的原始数据集，加速模型收敛并减少资源消耗，特别是在强化学习人类反馈（RLHF）阶段。关键示例包括 ShareGPT [469] 和 HC3（人类-ChatGPT 对比语料库）[467]，这些数据集通过将真实世界互动和比较见解提炼成有效的训练信号，成为广泛采用的大语言模型微调资源。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"rOCGB1sa\"\u003e\u003cb\u003eShareGPT：\u003c/b\u003eShareGPT [469] 作为一个动态数据收集平台，通过其 API 收集了约 90,000 次来自用户与 ChatGPT 或 GPT-4 的真实交互对话。该数据集包含了真实的人类指令和查询及其对应的 AI 回答，将自然对话模式浓缩成一个集中资源，使 RLHF 能够以高相关性和高质量改进大语言模型的对话流畅性和上下文响应能力。\u003c/li\u003e\u003cli data-pid=\"90KxdiSc\"\u003e\u003cb\u003eHC3\u003c/b\u003e： HC3 数据集 [467] 是专门设计用于对比 ChatGPT 生成的 AI 回答与人类撰写的答案，包含跨开放性话题、金融、医学、法律和心理学等领域的 161,000 个问题-回答对。这个精炼的语料库有助于分析回答特征和质量的差异，使研究人员能够在 RLHF 过程中提升大语言模型输出的真实性和领域特定准确性，同时突出人类与 AI 生成内容之间的区别。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e8.3 合成数据集\u003c/h2\u003e\u003cp data-pid=\"pASqncCR\"\u003e合成数据（Synthetic Datasets）在大型语言模型（LLM）后训练的SFT阶段中构成了一种变革性的资产，通过AI模型生成，提供成本效益高、可扩展且保护隐私的人类标注数据集替代方案。通过自动化创建指令-响应对和对话，合成数据能够生成广泛的训练语料库，增强模型的适应性，Self-Instruct-52K [86]、Vicuna [465] 和Baize [478] 是广泛用于提升LLM指令跟随和对话生成能力的主要示例。\u003c/p\u003e\u003ch3\u003e基于自我指令方法的数据集\u003c/h3\u003e\u003cp data-pid=\"FgWfnfxc\"\u003e使用自我指令方法的合成数据集从少量手工制作的种子示例开始，利用LLM生成大量指令跟随数据，增强模型对多样化指令的响应能力，例如Self-Instruct-52K、Alpaca和Magpie系列，这些数据集共同通过可扩展的自动化推进指令调优。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"xyZRFKWH\"\u003e\u003cb\u003eSelf-Instruct-52K: \u003c/b\u003eSelf-Instruct-52K [86] 为指令跟随模型建立了基础基准，使用多种提示模板从手工制作的种子生成52,000个示例，指导LLM，从而提高其解释和执行特定任务指令的精确性和一致性。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"-UZ1TJQn\"\u003e\u003cb\u003eAlpaca: \u003c/b\u003eAlpaca [464] 和Alpaca-GPT4 [18] 分别使用GPT-3和GPT-4将初始175个种子对扩展为52,000个高质量的指令-响应对，提高指令跟随能力，而InstInWild [477] 将这种方法应用于多语言环境，生成英语和中文数据集，以增强跨语言适应性。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"gYxQL4n6\"\u003e\u003cb\u003eMagpie 数据集: \u003c/b\u003eMagpie数据集 [481] 利用对齐的LLM从预定义的模板生成指令-响应对，产生专门的系列，如Magpie Reasoning V2（强调链式思维推理）、Magpie Llama-3和Qwen-2系列（针对流行模型定制）、Magpie Gemma-2（适用于Gemma架构）以及结合偏好优化信号的变体如Magpie-Air-DPO，这些数据集共同增强了SFT和指令调优在对话和推理任务中的应用。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"l-HqPMTG\"\u003e除了这些数据集之外，如Unnatural Instructions [97]（240,000个示例）、Evol-Instruct [470]（通过迭代复杂度增强生成70,000至143,000个改进条目）和Belle [471]（从ChatGPT生成500,000至1,100,000个中文对话）显著扩大了指令生成规模，尽管在质量保证、复杂度校准和偏见缓解方面仍存在挑战，需要持续改进以确保在复杂应用中的可靠性。\u003c/p\u003e\u003ch3\u003e基于自我对话方法的数据集\u003c/h3\u003e\u003cp data-pid=\"M1Oookrm\"\u003e自我对话数据集采用一种技术，使模型内部或与同伴模拟多轮对话，增强对话生成能力并解决现有语料库的不足，Baize、UltraChat和OpenHermes通过自动交互策略展示了这一方法。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"HZu2xxpH\"\u003e\u003cb\u003eBaize\u003c/b\u003e: Baize [478] 利用ChatGPT的自我对话技术生成653,000个多轮对话，整合来自Quora、Stack Overflow和Alpaca的种子数据，丰富指令跟随质量，从而优化LLM的对话连贯性和任务遵循性，用于SFT。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"6YlICBiq\"\u003e\u003cb\u003eUltraChat\u003c/b\u003e: UltraChat [476] 使用多个ChatGPT API生成超过1200万条高质量的对话记录，涵盖各种主题，克服了多轮数据集中常见的低质量和不准确注释问题，为对话增强提供了强大的SFT资源。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"FwWn6PgO\"\u003e\u003cb\u003eOpenHermes: \u003c/b\u003eOpenHermes由Teknium开发，包括OpenHermes-1 [474]（243,000条记录）及其扩展后续版本OpenHermes-2.5 [475]（100万条记录），提供高质量的SFT数据集，增加了数量和多样性，涵盖广泛的主题和任务类型，增强对话和指令跟随能力。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"WwbYguP1\"\u003e这些自我对话数据集使模型通过自我互动构建多轮对话，如Baize使用具有多样种子的ChatGPT和UltraChat通过API驱动的广泛对话，显著提高了对话质量和填补了训练数据可用性的关键空白。\u003c/p\u003e\u003ch3\u003e基于真实用户交互的数据集\u003c/h3\u003e\u003cp data-pid=\"rSL_vs_X\"\u003e基于真实用户交互的数据集利用与LLM的真实对话交流，捕捉多样且真实的输入，增强模型处理现实场景的能力，Vicuna、WildChat和GenQA是这一方法的关键示例。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"q45L8zc1\"\u003e\u003cb\u003eVicuna: \u003c/b\u003eVicuna [465] 在ShareGPT的公共API上约70,000次用户共享的对话上进行微调，通过将HTML转换为Markdown、过滤低质量样本和分割长对话以适应模型上下文长度，确保高质量的SFT数据，用于现实交互建模。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"CU5L11xL\"\u003e\u003cb\u003eWildChat: \u003c/b\u003eWildChat [479] 包含100万次真实世界用户与ChatGPT的多语言和多种提示类型的交互，包括独特的交换如模糊请求和代码切换，既作为SFT资源又作为分析用户行为的工具。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"qoQc7rWC\"\u003e\u003cb\u003eGenQA: \u003c/b\u003eGenQA [480] 提供一个超过1000万个清理和过滤的指令样本的庞大SFT数据集，完全由LLM生成，无需人工输入或复杂管道，补充现有语料库，通过快速生成合成数据来解决覆盖缺口。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"QYkZNGF3\"\u003e合成数据在成本、可扩展性和隐私方面的优势受到与人类标注数据相比深度和真实性潜在不足的限制，存在偏差传播和过度简化的风险。依赖AI生成的内容可能延续模型固有的错误，强调了整合合成数据和人类生成数据以增强LLM鲁棒性和在不同情境下适用性的必要性。\u003c/p\u003e\u003ch2\u003e九、应用\u003c/h2\u003e\u003cp data-pid=\"lymPClFu\"\u003e尽管预训练赋予了大型语言模型（LLMs）强大的基础能力，但在特定领域部署时，这些模型仍经常面临持续的限制，包括上下文长度受限、倾向产生幻觉、推理能力欠佳以及固有的偏见。这些不足在实际应用中显得尤为重要，因为精确性、可靠性和伦理一致性是至关重要的。这些挑战引发了根本性的疑问：（1）如何系统地提升 LLM 的性能以满足特定领域的需求？（2）哪些策略可以有效缓解应用环境中固有的实际障碍？后训练作为关键解决方案，通过优化 LLM 对特定领域术语和推理模式的识别，增强其适应性，同时保留其广泛的综合能力。本章阐述了后训练 LLM 在专业、技术和互动领域的变革性应用，阐明了定制化后训练方法如何应对这些挑战并提升模型在不同背景下的实用性。\u003c/p\u003e\u003ch2\u003e9.1 专业领域\u003c/h2\u003e\u003ch3\u003e法律助理\u003c/h3\u003e\u003cp data-pid=\"KY0yuVJB\"\u003e法律领域是利用后训练赋予大语言模型（LLMs）专门知识的一个引人注目的领域，使它们能够驾驭复杂的法律知识并应对法学中固有的多方面挑战。越来越多的研究[485, 486, 487]探讨了这一领域中LLMs的应用，涵盖了法律问答[488, 489]、判决预测[490, 491]、文档摘要[492, 493]以及检索增强和司法推理等更广泛的任务[494, 495, 496]。以LawGPT[497]和Lawyer-LLaMA[498]为代表的后训练法律助手展示了显著的熟练度，不仅在各种法律事务中提供可靠的指导，还在专业资格考试中取得成功，这证明了其先进的解释和分析能力。多语言支持，如LexiLaw[499]和SAUL[500]等模型，扩展了这种实用性的语言范围，包括英语和中文，从而扩大了可访问性。这些进展的核心是在精选的法律语料库上进行后训练，例如ChatLaw[501]，它将广泛的法律文本整合到对话数据集中，使模型能够精炼其推理能力和术语识别能力。\u003c/p\u003e\u003ch3\u003e医疗保健和医学\u003c/h3\u003e\u003cp data-pid=\"Ux8XWzYb\"\u003e后训练显著提升了LLMs在医疗保健和医学应用中的性能，通过特定领域的数据精确地满足临床和学术需求。在临床环境中，LLMs促进了药物发现[502]、药物协同预测[503]和催化剂设计[504]、诊断支持、病历生成和患者互动等任务；在学术界，它们在医疗报告合成[505]和问答[506]中表现出色，这得益于定制后训练带来的性能提升。例如，基于50万条医疗咨询记录训练的ChatMed[507]展示了增强的诊断和咨询服务准确性；而PULSE[508]则通过400万条涵盖中文医疗和通用领域的指令进行了微调，展示了卓越的多任务能力。这些模型通过利用后训练适应嵌入细致的医学知识，超越了通用模型的表现，突显了定制数据集在实现实际效用中的不可或缺性。这些进展不仅提高了特定任务的结果，还为将LLMs集成到医疗工作流程中铺平了道路，在这些工作中，精度和上下文相关性是不可妥协的，突显了后训练对现实世界医疗应用的变革影响。\u003c/p\u003e\u003ch3\u003e金融和经济学\u003c/h3\u003e\u003cp data-pid=\"jM8BMS90\"\u003e在金融和经济学领域，LLMs在情感分析[509]、信息提取[510]和问答[511]等任务中展现出巨大的潜力，后训练通过特定领域的改进增强了其有效性。尽管通用LLMs提供了坚实的基础，但像FinGPT[512]和DISC-FinLLM[513]这样的专用模型在经过金融语料库的后训练后表现出显著的改进，擅长需要细致理解市场动态和术语的任务。同样，XuanYuan[514]利用广泛的金融数据集和先进的后训练技术，提高了经济建模和预测的准确性，超过了未调优的基准。这些发展展示了后训练在适应LLMs以满足金融应用复杂需求中的关键作用，其中精确解释定量数据和定性见解至关重要，确保模型提供可靠且符合行业标准和期望的领域信息输出。\u003c/p\u003e\u003ch3\u003e移动代理\u003c/h3\u003e\u003cp data-pid=\"0OFozTW6\"\u003e大型多模态模型（LMMs）的发展推动了一个新兴的代理研究领域，专注于基于LMM的图形用户界面（GUI）代理[515]。该领域旨在开发能够在多种GUI环境中执行任务的AI助手，涵盖网络界面[516, 517, 518, 519, 520]、个人计算平台[521, 522, 523, 524, 525]和移动设备[526, 527, 528, 529, 530]。在移动环境中，一个研究方向通过工具集成[526]和额外的探索阶段[527, 528]来增强单个代理的感知和推理能力。最近的进展通过使用多代理系统进行决策和反思[531, 529]，显示出巨大的潜力，从而提高了任务效率。特别是，Mobile Agent-E[532]引入了代理之间的分层结构，促进了稳健的长期规划，并提高了低级动作的精度。这些发展突显了多模态后训练策略在培养适应性强、高效的复杂移动环境代理中的变革作用。\u003c/p\u003e\u003ch2\u003e9.2 技术和逻辑推理\u003c/h2\u003e\u003ch3\u003e数学推理\u003c/h3\u003e\u003cp data-pid=\"mw23Ywpe\"\u003e大语言模型（LLM）在数学推理方面展现出显著潜力，涵盖代数操作、微积分和统计分析。通过后训练，这些模型能够弥合计算能力和人类水平之间的差距。GPT-4 [9] 在标准化数学测试中取得了高分，这归功于其多样化的预训练语料库，但后训练进一步提升了这一能力。例如，Deep Seek Math [64] 利用专门的数学数据集和技术，如监督微调（Supervised Fine-Tuning, SFT）和组相对策略优化（Group Relative Policy Optimization, GRPO）[64]，以提高其推理精度，通过结构化的思维链（Chain of Thought, CoT）解决复杂问题。OpenAI 的 o1 [41] 通过强化学习（Reinforcement Learning, RL）进一步推进了这一领域，逐步优化推理策略，以在多步骤推导和证明中实现卓越性能。这种通过后训练的持续改进不仅提高了准确性，还使 LLM 的输出与严格的数学逻辑对齐，使其成为教育和研究领域中不可或缺的工具，特别是在需要高级推理的场景中。\u003c/p\u003e\u003ch3\u003e代码生成\u003c/h3\u003e\u003cp data-pid=\"Vxg5bVTL\"\u003e后训练彻底改变了代码生成，使 LLM 能够在自动化编码、调试和文档生成方面表现出色，从而变革了软件开发工作流程。Codex [533] 基于庞大的多样化代码库进行训练，支持 GitHub Copilot * ，提供实时编码辅助，准确率极高。专门的模型如 Code Llama [384] 进一步提升了这一能力，通过针对编程特定数据集的后训练，帮助开发者跨语言和框架进行开发。OpenAI 的 o1 [41] 将其数学推理能力扩展到代码生成，生成高质量且上下文感知的代码片段，与人类输出相当。当前的研究重点是增强个性化、深化上下文理解，并嵌入伦理保障措施，以减轻代码滥用等风险，确保 LLM 在技术领域内最大化生产力的同时，遵循负责任的开发原则。\u003c/p\u003e\u003ch2\u003e9.3 理解和交互\u003c/h2\u003e\u003ch3\u003e推荐系统\u003c/h3\u003e\u003cp data-pid=\"tJ6OBZay\"\u003e大型语言模型（LLMs）作为变革者出现在推荐系统中，通过分析用户交互、产品描述和评论，以前所未有的细致程度提供个性化建议 [534, 535, 536]。后训练增强了它们整合情感分析的能力，使内容和情感细微差别的理解更加细腻，这一点在 GPT4 [9] 和专门系统如 LLaRA [537] 和 AgentRec [538] 中得到了证明。亚马逊和淘宝等电子商务巨头利用这些能力处理评论情感、搜索查询和购买历史，优化客户偏好模型并高精度地预测兴趣 [535]。除了对项目进行排名，后训练的 LLM 还参与对话推荐、规划和内容生成，通过提供动态、上下文敏感的互动来适应不断变化的偏好，从而提升用户体验，这证明了后训练在连接数据分析与实际应用中的作用。\u003c/p\u003e\u003ch3\u003e语音对话\u003c/h3\u003e\u003cp data-pid=\"pXeXx3Zz\"\u003e后训练的 LLM 重新定义了语音处理，将识别、合成和翻译推进到自然度和准确性的新高度 [539]。这些模型处理诸如文本转语音 [540]、文本转音频生成 [541] 和语音识别 [542] 等任务，支持了亚马逊的 Alexa、苹果的 Siri 和阿里巴巴的天猫精灵等普遍工具。Whisper [543] 以其高保真转录展示了这一进步，而 GPT-4o [78] 引入了实时语音交互，无缝融合多模态输入。未来的发展方向包括多语言翻译和个人化语音合成，其中后训练使 LLM 能够打破语言障碍并根据个人用户档案定制响应，增强全球背景下人机交互的可访问性和参与度。\u003c/p\u003e\u003ch3\u003e视频理解\u003c/h3\u003e\u003cp data-pid=\"qwKWypIl\"\u003eLLM 在视频理解领域的扩展标志着一个重要的前沿，后训练使像 Video-LLaMA [341] 这样的模型能够执行字幕生成、总结和内容分析，简化多媒体创作和理解。Sora [544] 进一步革新了这一领域，通过文本提示生成复杂视频，降低了技术门槛并促进了创新故事讲述，使内容生产更加民主化。这些进展利用后训练使 LLM 适应视觉-时间数据，提高了其解释深度和在教育到娱乐等各种应用中的实用性。然而，它们也带来了计算可扩展性、隐私保护和伦理治理方面的挑战，尤其是在生成内容的滥用问题上。随着后训练方法的演变，解决这些问题将是确保视频相关应用可持续、负责任部署的关键，平衡创新与社会考虑。\u003c/p\u003e\u003ch2\u003e十、开放问题和未来方向\u003c/h2\u003e\u003cp data-pid=\"1QaarjYQ\"\u003e在本节中，我们批判性地评估了大型语言模型（LLMs）后训练方法中存在的未解决挑战和未来的发展轨迹，将分析置于由OpenAI的o1 [41] 和DeepSeek-R1 [28] 的发布所带来的变革性进展的背景下。这些模型通过大规模强化学习（RL）重新定义了推理基准，但它们的出现也加剧了对后训练技术中持续存在的限制进行解决的紧迫性。以下小节详细阐述了六个关键的开放问题，每个问题都突显了其对领域进展的重要性和迫切需要解决的问题，并提出了推动未来研究和确保LLM在各种应用中负责任发展的可行策略。\u003c/p\u003e\u003ch3\u003e推理能力超越大规模RL\u003c/h3\u003e\u003cp data-pid=\"NO0nn778\"\u003eo1和DeepSeek-R1的引入标志着LLM推理能力的范式转变，利用了如RLHF和Group Relative Policy Optimization (GRPO)等广泛的RL框架，在多步问题解决（如数学证明和逻辑推导）中实现了前所未有的准确性。然而，依赖二元奖励信号和大量人工反馈暴露了一个关键限制：它们无法在复杂的开放式任务中有效泛化，例如科学假设生成或动态环境中的战略决策。这一差距亟待解决，因为对LLM在现实世界情境中模拟人类推理的需求日益增长，其重要性在于解锁其作为自主智能代理的潜力，超越当前的基准。当前的RL方法在奖励稀疏性和缺乏适应任务复杂性的能力方面存在困难，需要创新框架。可行的解决方案包括开发多目标RL系统，整合自监督一致性检查（例如，验证推理步骤之间的逻辑连贯性）和领域特定先验知识，如数学公理或科学原理，以在无需详尽的人工注释的情况下指导推理 [545, 546]。这些进展可以减少对昂贵反馈循环的依赖，增强可扩展性，并使LLM能够应对未知的推理领域，这一前景由DeepSeek-R1的冷启动RL创新得以实现。\u003c/p\u003e\u003ch3\u003e下一代LLM的后训练可扩展性\u003c/h3\u003e\u003cp data-pid=\"b9QwXfK8\"\u003e随着LLM规模和复杂性的增加，以下一代模型的参数密集型架构为例，后训练的可扩展性成为了一个严峻而紧迫的挑战。基于RL的方法的资源密集性，如DeepSeek-R1的冷启动方法需要大量的计算基础设施，这限制了其对资金充足的实体的可访问性，并引发了显著的可持续性问题，特别是在多模态应用（如视频分析）和实时系统（如对话代理）中。这个问题至关重要，因为它威胁到资源丰富和资源受限的研究社区之间的差距扩大，阻碍了LLM发展的公平进步。虽然参数高效微调（PEFT）[92] 减轻了一些开销，但其性能在大规模数据集上往往会下降，突显了对可扩展替代方案的需求。可行的未来方向 [547, 548, 549] 包括设计轻量级RL算法——可能通过调整GRPO以减少内存占用——联邦后训练框架，将计算负载分布在去中心化的网络中，以及先进的蒸馏技术，保留推理和适应性的同时最小化资源需求。如果这些解决方案得以实现，将有助于民主化后训练，与领域内对可持续和包容性创新的迫切需求相一致。\u003c/p\u003e\u003ch3\u003e基于RL的模型的伦理对齐和偏见缓解\u003c/h3\u003e\u003cp data-pid=\"Wr9-vksd\"\u003e通过RL进行后训练，如o1的谨慎对齐策略所展示的，放大了伦理风险，可能会强化嵌入在训练数据集（如HH-RLHF [104] 或合成语料库）中的偏见。鉴于LLM在敏感领域（如医疗诊断和司法决策）中的部署，这是一个极其紧迫的挑战。伦理对齐的动态变化——在一种文化背景下被认为是公平的，在另一种文化背景下可能构成偏见——是实现普遍可信的LLM的重大障碍，这使得确保公平和安全的AI系统变得至关重要。当前的方法存在过度审查的风险，损害实用性（例如，抑制创造性输出），或纠正不足，延续有害的偏见（例如，种族或性别差异）。解决这一问题需要开发公平意识的RL目标，纳入多利益相关者的偏好模型（例如，聚合多样化的人类判断）和对抗性去偏技术，以在训练过程中中和数据集偏见。这些方法的可行性 [550] 得到了最近解释工具和多目标优化进展的支持，能够在伦理稳健性和实际功能之间实现平衡，这是由o1在现实世界部署挑战中强调的必要性。\u003c/p\u003e\u003ch3\u003e无缝多模态集成以实现整体推理\u003c/h3\u003e\u003cp data-pid=\"5-e-tgZK\"\u003e向多模态LLM的轨迹，预示着o1的推理增强和GPT-4o的综合能力 [78]，突显了对后训练方法的迫切需求，这些方法能够无缝集成文本、图像、音频和其他数据类型，以实现整体推理——这对于实时视频分析、增强现实和跨模态科学研究等应用至关重要。当前的方法在实现稳健的跨模态对齐方面失败，主要是由于数据异质性和全面多模态训练语料库的稀缺，限制了LLM在不同输入之间协同推理的能力。这一挑战的重要性在于其潜在的变革应用，但在没有可扩展框架的情况下，其解决仍然难以捉摸。DeepSeek-R1的冷启动RL提供了一个有希望的起点，表明统一的模态编码器（例如，能够将异构数据编码到共享的潜在空间中）和动态RL策略，能够自适应地加权模态贡献，可以弥合这一差距。未来的研究应优先创建多模态基准和合成数据集，建立在Magpie [481] 等努力的基础上，以推动进展，鉴于最近在多模态预训练和RL优化方面的进展，这是一个可行的努力。\u003c/p\u003e\u003ch3\u003e上下文适应的信任框架\u003c/h3\u003e\u003cp data-pid=\"br-VYSiu\"\u003e后训练LLM的信任度越来越被视为一个动态的、上下文依赖的属性，而不是静态的质量，这一点在o1在教育等敏感领域谨慎的输出与其在创意任务中更自由的响应之间的对比中得到了证明。这种变异性——在安全要求（例如，避免教育设置中的错误信息）可能与实用需求（例如，促进写作中的创造力）冲突的情况下——是一个紧迫的挑战，鉴于其对用户信任和LLM在各种现实场景中适用性的关键重要性。当前的后训练方法往往过于重视安全，导致实用性的折衷，从而降低实际价值，或者未能适应特定上下文的需求，削弱可靠性。解决这一问题需要上下文敏感的RL模型，能够动态调整安全与实用性的权衡，利用实时用户反馈和可解释的安全指标（例如，生成输出的透明度分数）来确保适应性。这种方法的可行性 [551] 得到了自适应学习系统和实时监控进展的支持，提供了一条在信任度与功能性之间取得平衡的路径，这是随着o1等LLM扩展到高风险应用而迫切需要的。\u003c/p\u003e\u003ch3\u003e后训练创新的可访问性和民主化\u003c/h3\u003e\u003cp data-pid=\"PHiyPg6-\"\u003e先进后训练方法的计算强度，以DeepSeek-R1的RL驱动方法为代表，将其应用限制在资源丰富的实体中，成为一个阻碍可访问性的紧迫障碍，抑制了小型研究社区和行业部门内的创新（即，对于促进AI领域的公平进步而言，这是一个极其重要的问题）。这种排他性不仅限制了贡献的多样性，还阻碍了领域应对全球挑战的合作能力。民主化这些创新需要开发高效的开源工具和框架，降低进入门槛而不牺牲质量，这一目标通过可扩展的PEFT适应RL [92]、协作平台共享后训练模型（例如，Hugging Face枢纽）和类似于Magpie [481]的简化合成数据生成管道得以实现。未来的工作应专注于优化这些解决方案，以实现广泛采用，确保后训练的变革潜力——由o1和DeepSeek-R1所体现——超越精英机构，丰富更广泛的AI生态系统。\u003c/p\u003e\u003ch3\u003e创造性智能与系统2思维\u003c/h3\u003e\u003cp data-pid=\"iPKnpTwK\"\u003e将创造性智能融入系统2推理代表了LLM演进的一个新兴前沿，如 [552] 所强调的。尽管像OpenAI的o1和DeepSeek的R1这样的推理LLM在刻意、逐步的逻辑分析中表现出色——模仿系统2思维——它们在创造性智能方面的能力仍处于探索阶段，创造性智能涉及生成新想法、综合不同概念和灵活应对非结构化问题。这一差距至关重要，因为创造性智能支撑了艺术创作、科学发现和战略创新等领域中的人类问题解决，这些领域中仅靠僵化的逻辑框架是不够的。这一挑战的紧迫性在于其潜力，可以将LLM从分析工具提升为自主创造代理，这是向通用人工智能（AGI）迈出的变革性一步。下面，我们概述了这一开放问题，并提出了未来的方向，借鉴了调查的见解。\u003c/p\u003e\u003ch2\u003e十一、总结\u003c/h2\u003e\u003cp data-pid=\"v_TaxJTx\"\u003e本文首次全面综述了后训练语言模型(Post-training Language Models, PoLMs)，系统地追溯了从2018年ChatGPT的对齐起源到2025年DeepSeek-R1的推理里程碑的轨迹，并肯定了它们在推理精度、领域适应性和伦理完整性方面的变革性影响。我们评估了一系列广泛的技术（即微调(Fine-tuning)、对齐(Alignment)、推理(Reasoning)、效率(Efficiency)和集成与适应(Integration and Adaptation)），综合了这些技术在专业、技术和交互领域（从法律分析到多模态理解）的贡献。我们的分析强调，PoLMs显著提升了大语言模型(LLMs)的能力，从最初的对齐创新演变为复杂的推理框架；然而，它也揭示了持续存在的挑战，包括偏见持续存在、计算可扩展性和情境变量的伦理对齐问题。这些发现被纳入一个新颖的分类体系中，强调了将推理进展与效率和伦理要求相结合的综合性方法的必要性。我们得出结论，实现LLMs作为可靠、负责任工具在各种应用中的潜力，需要持续的跨学科合作、严格的方法论评估以及适应性和可扩展框架的开发。作为此类研究的开创性综述，本工作整合了近年来PoLMs的进步，奠定了坚实的知识基础，激励未来的研究培养能够灵活结合精度、伦理稳健性和多功能性的LLMs，以满足科学和社会背景不断变化的需求。\u003c/p\u003e","is_labeled":false,"visited_count":3857,"thumbnails":["https://picx.zhimg.com/50/v2-3bfa63c21d615293d5631638c00e3aa3_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-9b8d13fb7ee1b423ec2cca058f30df23_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-60cbb1fa9f5953ebcbd9b255686be9cc_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-15bfb065081cc9326e2584b2ff3ec8a0_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-7ae6ca4f90930ca5a2fb2ad681c25ce9_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-a42863bdc3fbc6ff838cfa1e2253fe7c_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-f7139b5e9a50f1cf501f8c4f34f7ec09_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-1537fdebc6fb5fcc7aa4e53c18a24202_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-62a4e456381ae75142d4d9440295225a_720w.jpg?source=b6762063"],"favorite_count":498,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 49358536242}","attached_info":"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","action_card":false},{"id":"104_1750898493.962","type":"feed","offset":104,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898493,"updated_time":1750898493,"target":{"id":"19867553313","type":"answer","url":"https://api.zhihu.com/answers/19867553313","author":{"id":"93c3eca9095ed402341706055ef6ba1d","url":"https://api.zhihu.com/people/93c3eca9095ed402341706055ef6ba1d","user_type":"people","url_token":"acelorahelwein","name":"专业施针容嬷嬷","headline":"Viewer's discretion advised.","avatar_url":"https://picx.zhimg.com/50/v2-6e0f5f22aaff7f3cd1242a934f66ea12_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":15971,"is_following":false,"is_followed":false},"created_time":1730393459,"updated_time":1749665950,"voteup_count":24988,"thanks_count":2515,"comment_count":530,"is_copyable":false,"question":{"id":"45441140","type":"question","url":"https://api.zhihu.com/questions/45441140","author":{"id":"8b437c88e5e1bd4905fa604b8eea236a","url":"https://api.zhihu.com/people/8b437c88e5e1bd4905fa604b8eea236a","user_type":"people","url_token":"huo-da-31-7","name":"豁达","headline":"","avatar_url":"https://pic1.zhimg.com/50/a579ac6880358d16a6ad0737cd9f697e_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":5,"is_following":false,"is_followed":false},"title":"为什么有钱人的孩子和穷人（小康？）的孩子能一眼就分辨出来呢？","created":1462355393,"answer_count":0,"follower_count":0,"comment_count":27,"bound_topic_ids":[3007,5386,45058,74118,158937],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://picx.zhimg.com/50/v2-6ef510d52cc6072ccf4514b34325a800_720w.jpg?source=b6762063","excerpt":"马克吐温的《百万英镑》中，富豪兄弟俩为了打赌，将一张百万英镑支票借给了因意外流落英国街头的股票交易员亨利，其中一个赌除非他把支票兑现，否则他一定会再次流落街头变成流浪汉；另一个则大胆地赌亨利不需要兑现支票，也能过上舒适的生活。后者赌赢了，亨利真的在没有兑现支票的情况下白吃白喝白住了一个月，锦衣玉食。 且超乎两兄弟预料的是：亨利还在一分钱都没花的情况下以“百万富翁”的身份技术入股一个矿产交易所，赚…","excerpt_new":"马克吐温的《百万英镑》中，富豪兄弟俩为了打赌，将一张百万英镑支票借给了因意外流落英国街头的股票交易员亨利，其中一个赌除非他把支票兑现，否则他一定会再次流落街头变成流浪汉；另一个则大胆地赌亨利不需要兑现支票，也能过上舒适的生活。后者赌赢了，亨利真的在没有兑现支票的情况下白吃白喝白住了一个月，锦衣玉食。 且超乎两兄弟预料的是：亨利还在一分钱都没花的情况下以“百万富翁”的身份技术入股一个矿产交易所，赚…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"0Ff5eLeC\"\u003e马克吐温的《百万英镑》中，富豪兄弟俩为了打赌，将一张百万英镑支票借给了因意外流落英国街头的股票交易员亨利，其中一个赌除非他把支票兑现，否则他一定会再次流落街头变成流浪汉；另一个则大胆地赌亨利不需要兑现支票，也能过上舒适的生活。后者赌赢了，亨利真的在没有兑现支票的情况下白吃白喝白住了一个月，锦衣玉食。\u003c/p\u003e\u003cp data-pid=\"StB8DIuo\"\u003e且超乎两兄弟预料的是：亨利还在一分钱都没花的情况下以“百万富翁”的身份技术入股一个矿产交易所，赚到了一笔钱。\u003c/p\u003e\u003cp data-pid=\"7gUH9QZR\"\u003e奉俊昊编导的电影《寄生虫》中，金家兄妹高考艺考双双落榜，他们分别给自己编造的人设是：韩国排名最高的大学之一的延世大学在读高材生，和出身书香门第毕业于芝加哥大学艺术系的海归精英。\u003c/p\u003e\u003cp data-pid=\"PXaDyZRc\"\u003e哥哥凭着朋友的引荐，拿着一张伪造的延世大学在读证书，妹妹从网上搜索了一些美术学相关的资料，兄妹二人就这样骗过了科技行业高层富人一家，混进豪宅给真正的少爷小姐们做家庭教师，还成功给自己待业的老父亲老母亲也编出了高端人设，赶走了原本的司机和管家，让父母也混进来工作。\u003c/p\u003e\u003cp data-pid=\"UwpZEIpK\"\u003e然而现实又是怎样的呢？\u003c/p\u003e\u003cp data-pid=\"xqzX5Q25\"\u003e答案是，现实可比艺术创作癫多了。\u003c/p\u003e\u003cp data-pid=\"a4FV_NJN\"\u003e癫到马克吐温和奉俊昊要是知道了，恐怕都要大呼：我都不敢这么写/这么拍啊！！！\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"nE-Mn6vc\"\u003e前两年中央美院的一位毕业生拍摄了一支纪录片，记录自己假扮成有钱人，在北京一分钱没花生活了一个月，而且好吃好喝住得也不错——乍一听，似乎是二十一世纪版本《百万英镑》，不同于马克吐温的文学创作的是，这是一部现实的纪录片。\u003c/p\u003e\u003cp data-pid=\"ly_V2vfD\"\u003e创作者名叫邹雅琦，她在一次逛商场的时候留意到奢侈品商店里的店员会为客户免费提供高档甜品酒水。于是她产生了一个大胆的想法——假扮名媛，用这些免费的资源生活一个月，并拍摄成纪录片。\u003c/p\u003e\u003cp data-pid=\"Y16m6EBL\"\u003e说干就干！邹雅琦开始着手调查那些有钱人常常光顾的场所，比如机场贵宾室，五星级大酒店，奢侈品牌专卖店等，她埋伏在在这些场所里悄悄观察有钱人的言行举止，记录他们的穿搭。\u003c/p\u003e\u003cp data-pid=\"Ciuvgi95\"\u003e之后邹雅琦在酒吧打工了一段时间，赚来的几百块钱工资就成为了这支纪录片的制片经费：\u003c/p\u003e\u003cp data-pid=\"riyZixhL\"\u003e最大的几百块钱的支出，是夜市和地摊上买来的几件假货：大牌款式的伪劣衣服鞋子假皮草；\u003c/p\u003e\u003cp data-pid=\"2mrEixIp\"\u003e超大号的假爱马仕包；\u003c/p\u003e\u003cp data-pid=\"D6yd5PIO\"\u003e还有一百多块，买了一张短途的经济舱机票；\u003c/p\u003e\u003cp data-pid=\"Ba83jGRS\"\u003e不到一百块，买了一些临期甩卖的化妆品；\u003c/p\u003e\u003cp data-pid=\"pDv_p_cC\"\u003e剩下几十块钱，在路边买了几件假珠宝，都是能闪瞎眼的款式，大水钻，大金戒指，大金链子。\u003c/p\u003e\u003cp data-pid=\"uQsrVoOf\"\u003e【不是影视剧和坊间流言里那些美化过的什么“贵族”，“名媛”，“有气质的有钱人”，“低调但有格调”的扮相，没有什么“看不出牌子的老钱穿搭”，而是刻板印象中的“暴发户”扮相，大家可以去看原片，皮草袄子紫色丝绒，超大爱马仕，黑色恨天高配白丝袜，走哪都翘兰花指。】\u003c/p\u003e\u003ch2\u003e然后，邹雅琦就穿着这一身假大牌，戴着假珠宝，拎着假爱马仕，在北京一分钱没花，白吃白喝白住，过了一个月。\u003c/h2\u003e\u003cp data-pid=\"8Xb6HGQH\"\u003e首先，邹雅琦拿着机票来到了机场，办理值机手续，拿到经济舱登机牌，过了安检之后，直奔贵宾休息室。前台没有检查她的登机牌和会员，于是她在贵宾室里生活了好几天，每天三餐都有热菜热饭自助餐供应，还有吃不完的饮料零食甜品水果，wifi不断，还有洗手间可以洗漱，到处都是可以给手机充电的插座，宽敞到足够躺着睡觉的沙发卡座，毯子枕头毛巾等也是一应俱全。\u003cb\u003e后来邹雅琦查了一下，这家航司其实有规定贵宾休息室一次最多可以待多少个小时，然而别说几个小时了，邹雅琦在贵宾休息室吃喝拉撒几天下来都没人查过她的登机牌，也没人查会员。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Ij18XFbr\"\u003e后来邹雅琦在采访中提到，由于她那些天一直在机场，\u003cb\u003e工作人员大概是把她当成了一个经常飞来飞去的富婆，有一天她走出贵宾休息室在机场里溜达的时候，另一家航司的贵宾休息室工作人员居然还拉住她说：“我们航司的贵宾室可比XX航司舒服多了”，主动把她请进了这家航司的贵宾休息室\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"SVeQW-rY\"\u003e但是这样每天吃了睡睡了吃，拍摄素材高度重复，邹雅琦于是决定进城寻找新的素材。离开机场前，她吃着贵宾休息室里免费的零食饮料觉着还挺好吃，想带走一些，便走进了一家奢侈品牌免税店问柜姐，能不能给个袋子，下次飞一定来消费。柜姐听到这里，免费给了她一个超大号纸袋。后来她一查才发现\u003cb\u003e其实这纸袋都是得花钱买的，平时卖二十块一个\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"LjYPslVU\"\u003e进城后邹雅琦找了个五星级酒店，但她\u003cb\u003e没有开房，还是一分钱没花，继续白吃白住\u003c/b\u003e：每天三餐到饭点就混进酒店自助餐厅里吃他们供应的餐点，吃完还有小零食饮料水果甜品等；混进酒店的健身房设施洗漱，洗完还能蒸桑拿，没事还能去里面健身锻炼游泳。晚上她就睡酒店大堂，wifi不断，24小时开着空调，冬暖夏凉，沙发宽敞，没有人赶她走。\u003c/p\u003e\u003cp data-pid=\"kwJCCdUa\"\u003e这家酒店是邹雅琦前期调研后做出的选择，因为这家酒店的自助餐厅和健身房并不查房卡，只要求填房号和姓名，邹雅琦每次就根据其他客人的房号，自己编一个，然后随便填个小说人物的名字，其中大部分出自《三国演义》，比如刘备，张飞，关羽等，后来邹雅琦发现，其实不填也完全ok，跟在其他客人后面混进去就行。\u003c/p\u003e\u003cp data-pid=\"Q253v65K\"\u003e邹雅琦住在酒店大堂那几天原本还有些担心，一个年轻女孩子天天蹲酒店大堂，会不会被当成特殊工作者赶出去，那样纪录片拍摄就得中断了。\u003c/p\u003e\u003cp data-pid=\"dVlX_CAk\"\u003e还记得《百万英镑》中的那个西装店老板吗？当衣衫褴褛的亨利走进西装店，老板原本冷眼相待，将店员拨给亨利，店员又将小学徒拨给亨利，看到两位富豪借给亨利的百万英镑支票之后，西装店老板一边鞍前马后地伺候亨利，一边教育店员和小学徒：“人家随身带一百万现金的有钱人不穿得低调一点那不是等着被零元购吗？你们这群没见识的傻瓜，居然把一位有个性的富豪当成了流浪汉！”\u003c/p\u003e\u003cp data-pid=\"cqTnF7JA\"\u003e\u003cb\u003e现实也是如此，自有大儒为一身大logo的邹雅琦辩经。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"-KKkSbAc\"\u003e某天晚上，邹雅琦正要睡下，偶然间她听到工作人员窃窃私语：“她在这好几天了，到底是来干啥的啊？”，“该不会是做特殊职业的吧？你们看她，每天晚上都守在这大堂里！”，“可是你看她那一身，我瞅挺贵的，会不会是富家千金离家出走，体验生活来了？”。邹雅琦的心都悬到了嗓子眼里，想着如果他们要找她查房卡，或许可以拿“等约会对象”当借口。\u003cb\u003e没想到经理笃定道：“你们几个懂啥？看到她手上那颗鸽子蛋【十几块的地摊儿货】没，还有这貂，这爱马仕【假货】，肯定是豪门正室！依我看呐，她每天晚上守在这大堂里，肯定是来捉奸的！估计她老公这会儿就在楼上跟小三开房呢！”\u003c/b\u003e，“我去！豪门还真是drama多！”，“这可怎么是好？你说她会不会在我们这闹起来啊？”，“所以啊，你们几个可万万不能得罪她，千万别把她给惹了，她要是一冲动，直接杀到上楼抓小三，那可不好收拾了！懂吗？”。\u003c/p\u003e\u003cp data-pid=\"tmnvOctY\"\u003e于是邹雅琦彻底放下了心，高枕无忧，有一次用酒店健身房的时候她随手填了个动漫角色的名字被员工留意到了，她随口说自己今天退房，临走前补个妆，工作人员就没拦着也没再追问。这些天里邹雅琦也没闲着，每天吃饱喝足就出门逛逛，专门混进奢侈品店和美术馆的活动四处溜达，试穿各种大牌衣服，最重要的是蹭免费的食物饮料。有时候，碰到那些受邀来参加活动或消费的有钱人聊上了，还能搭上他们的豪车车接车送。吃腻了就去海底捞假装等朋友，喝免费的酸梅汤吃免费的银耳粥水果小零食，还有免费的小玩具可以拿【拍完纪录片之后她去那家海底捞给工作人员送了饮料】。\u003c/p\u003e\u003cp data-pid=\"1b1jMPFH\"\u003e然后最讽刺的一幕来了。\u003c/p\u003e\u003cp data-pid=\"LMf110Gg\"\u003e最后一天，邹雅琦穿着那身已经沾了黄油食物渣的假皮草，混进了中国嘉德——中国最大的拍卖行，这里多说一点，\u003cb\u003e如今想在拍卖行消费，根据相关法律规定，都是实名制的，规定注册账户和登记都需要查身份证地址证明，尤其国内拍卖行还要缴押金【不信的不妨上网查查看，\u003c/b\u003e\u003cb\u003e嘉德官网都有明文规定，而且有的比较重要的场子可能还需要银行等资金证明】\u003c/b\u003e，这些邹雅琦一个都没有，她过去也没有在中国嘉德消费过，对于员工们来说完全就是一张生面孔。\u003c/p\u003e\u003cp data-pid=\"n67LPslh\"\u003e\u003cb\u003e拍卖行员工们瞧邹雅琦一身穿金戴银的打扮，就忙不迭地把她请进去了\u003c/b\u003e，并且争先恐后地服务她，端茶送水。那一场拍卖是珠宝专场，在场的工作人员都是嘉德的珠宝专家【这里的“专家”不是形容词，而是职称，他们每天光验货估值鉴定都得过手几十甚至上百件珠宝】，最值钱的几件大货工作人员全都拿给她请她试戴，其中压轴的拍品是一个估价一千多万的翡翠镯子【查了下，镯子当晚以一千二百万rmb落槌】，工作人员献宝似的拿给她，一个劲地求她戴。\u003c/p\u003e\u003cp data-pid=\"X71Q_JLG\"\u003e盛情难却，邹雅琦戴上了这个价值连城的镯子。邹雅琦费了老大劲才把手套进镯子里，一边吐槽：“这镯子对于我来说尺寸还是小了。”嘉德拍卖行的珠宝专家却在一边拼命拍马屁：\u003cb\u003e“您瞧这翡翠，您戴着可真好看！这镯子跟您手上那枚钻戒，还有您这珍珠项链【都是路边几十块买来的】，真配啊！”\u003c/b\u003e邹雅琦听到这离谱的扯淡差点笑场，险些演不下去了。她并不关注也不消费奢侈品和珠宝，这一趟也不是来举牌买东西的，而是来吃好吃的，看到一边自助餐区端上了香槟鹅肝巧克力甜品等，她便过去连吃带拿，一整盘鹅肝吃完之后她还有点不好意思，没想到工作人员见她吃得欢，急忙催促后厨：来了个大客户，快快快，赶紧上菜！！！【此处强烈推荐原片，台上的拍卖师喊得热火朝天，她坐在台下从包里掏出一颗酒店里顺的粽子扒开大口吃】\u003c/p\u003e\u003cp data-pid=\"DPGhDnaU\"\u003e好不容易演完了，另一名工作人员，一位年轻的保安又来献殷勤，一口一个“小姐姐”喊得十分热切，一会儿问她想不想喝杯咖啡，一会儿又问她买完珠宝想不想喝杯奶茶，求着她加微信。邹雅琦随口说自己已婚，亮出手上的“鸽子蛋”【十几块的地摊货】，想借此拒绝，谁知保安还是不依不饶，求着她加了微信。其他来参加拍卖的藏家看到她一身大牌也拿着图录走过来跟她聊天搭话，讨论起拍品，临走前她还搭一个藏家的顺风车回了学校宿舍。\u003c/p\u003e\u003cp data-pid=\"rpSw-Nfs\"\u003e引用几条邹雅琦后续在采访中和微博上的回应：\u003c/p\u003e\u003cp data-pid=\"VGfntlV6\"\u003e针对“创作者本来就是名媛，只是作秀”的争议，邹雅琦回应，自己是无产阶级，原生家庭就是小城市里小康水平的四口之家，甚至不支持她在美术学院念书，更希望她考个公务员或是去当个老师，找个稳定工作。\u003c/p\u003e\u003cp data-pid=\"UT_6iTxV\"\u003e且在她另一件作品《重金求母》相关的访谈中，她提到自己在上大一那年爷爷过世之后，已经跟原生家庭基本断联，大学几年里生活费都是自己兼职打工赚来的钱。从小到大一件奢侈品都没有买过，更没有在这些有钱人成堆的场所里消费过，大学几年吃住都是食堂和宿舍，用度靠pdd和咸鱼解决。\u003c/p\u003e\u003cp data-pid=\"gnwr772b\"\u003e知道哪里可以蹭吃蹭住和装得像是因为前期做了很多准备工作，包括但不限于：1.在北京的各种奢侈品门店附近观察进门消费的有钱人都穿什么，然后找类似款式的假货；2.观察身边家里有钱的同学朋友怎样谈吐举止，并且多次采访她们，了解有钱人的生活方式和消费习惯；3.四处踩点考察，观察哪些机场贵宾室不检查登机牌和会员卡，哪些酒店的自助餐厅和健身房不需要房卡。\u003c/p\u003e\u003cp data-pid=\"LnN78Ozg\"\u003e\u003cb\u003e邹雅琦表示，自己的本意在于揭露信息差，记录下这些对于有钱人来说可以免费取得，无限量供应，却过剩，时常被他们随意浪费掉的资源，并且希望这些资源可以被分配给真正需要的人，而不是挑拨阶级矛盾。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"dZp2Au20\"\u003e除此之外，邹雅琦表示，自己拍摄这部纪录片的另一个原因也是为了实验：一个人，是否可以不花钱活下去？而且是吃得不错，住得不错地活下去？\u003c/p\u003e\u003cp data-pid=\"rBJ5rELB\"\u003e因为在北京念书的这几年，邹雅琪切身感受到大城市的物价房价等带给人的生存压力，然而她所学的专业不仅是美术类，而且还是美术中最最难找工作最最不赚钱的创作专业，所以大四一年她都在思考，如果搞艺术养不活自己，那自己是不是就得继续打工才能活下去？毕竟当今社会，想讨一口热饭吃都不容易，在这种环境下还想搞艺术，简直是奢侈。\u003c/p\u003e\u003cp data-pid=\"v53U4hRz\"\u003e尤其，考虑到性别问题，她还不能住收容所。\u003c/p\u003e\u003cp data-pid=\"jO0Si8-7\"\u003e大学几年，邹雅琦一边上学，一边打工赚取生活费和学费，时常陷入生存危机的焦虑，或许是压力到达了最大阈值，她产生了一个听起来有些不现实的想法：有没有不花钱也可以活下去的方法？\u003c/p\u003e\u003cp data-pid=\"H2M_pfD4\"\u003e逛街时，她留意到很多奢侈品商店会为客户提供免费的甜品和饮料，还有很多五星级酒店的大堂宽敞舒服，但是似乎永远空荡荡。可是这些免费的甜品酒水和不收费的休息区，对于买得起奢侈品，住得起五星级酒店的人来说，是他们完全消费得起的，而且他们不见得会吃那些店员端上来的甜品酒水，也不见得会使用酒店免费的休息区，这样一来甜品酒水也就被浪费掉了，这些免费的赠品于有钱人而言，属于过剩资源，但其实完全可以分配给真正需要这些食物的人。\u003c/p\u003e\u003cp data-pid=\"CrWesoR3\"\u003e邹雅琦逐渐意识到：其实对于有钱人来说，很多资源他们根本不用花钱就可以轻松获得，这些资源甚至可能是直接送上门的。\u003c/p\u003e\u003cp data-pid=\"cdvhEtQX\"\u003e于是她的想法愈发大胆：不仅要用免费资源生存一个月，而且，还要吃得香，住得好。\u003c/p\u003e\u003cp data-pid=\"vfR_LQ6z\"\u003e起初，邹雅琦自己也觉得这个想法听起来不切实际——白吃白喝白住，还要吃得香住得好，这听起来是真刑，她做好了随时停止拍摄的计划和打算，毕竟在北京的四年给她的感觉是，仿佛连呼吸空气都要收费...\u003c/p\u003e\u003cp data-pid=\"h4OhVn25\"\u003e\u003cb\u003e——但这样看来也不尽然，有很多资源对于普通人来说需要用劳动换取，但是对于另一些群体来说其实唾手可得，只是被他们随意地浪费掉了而已。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"NmjwTsRl\"\u003eps.\u003c/p\u003e\u003cp data-pid=\"buRzD2ay\"\u003e我看到有人在评论区讨论“她自己就是有钱人所以穿什么都显得贵”，额，其实，你们不妨可以去看看原片，假皮草裹得人跟熊一样，衣服也是姹紫嫣红，假戒指假项链都是俗不可耐的款式。这部纪录片是她的毕设作品，她后来还把这一身行头摆在毕设展厅里自己的展位上。\u003c/p\u003e\u003cp data-pid=\"sABxveWK\"\u003e然而就是这样一身假货，不仅骗过了贵宾室前台，酒店大堂经理，奢侈品店柜姐，还骗过了人家中国嘉德拍卖行的珠宝专家。\u003c/p\u003e\u003cp data-pid=\"XDTHqXuC\"\u003e我也不知道各位有什么火眼金睛，能精准分辨出这些天天伺候大款的人都没分辨出的假名媛，假爱马仕...\u003c/p\u003e\u003cp data-pid=\"UCAPmTge\"\u003e这是邹雅琦在中国嘉德试戴珠宝的照片，摄影师是当时那位追着她求包养的保安。\u003c/p\u003e\u003cp data-pid=\"NKP-Jt7S\"\u003e如果我不告诉你什么是真，什么是假，你会觉得下图什么是真，什么是假？\u003c/p\u003e\u003cp data-pid=\"tPVQh8pG\"\u003e衣服？\u003c/p\u003e\u003cp data-pid=\"_0LNHTX5\"\u003e项链？\u003c/p\u003e\u003cp data-pid=\"wxPggvf-\"\u003e左手那个戒指？\u003c/p\u003e\u003cp data-pid=\"JPoLW1sr\"\u003e右手那个戒指？...\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-ebc783008c8d35f1894a93c59f79d2e7_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1893\" data-rawheight=\"1183\" data-original-token=\"v2-fe322defcbcfe43f07479b9a9bd2761e\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-e46526853be5b23924a42f6fabe54897_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1893\" data-original=\"https://pic4.zhimg.com/v2-ebc783008c8d35f1894a93c59f79d2e7_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"jKP7FY8h\"\u003e顺带说一句，拍卖行的拍品都是不保真的【保真的那种拍卖行会写进图录拍品介绍强调是真货有证书等】，别说中国第一的嘉德，就是全球最大的，什么苏记，佳记，也是不保真的。\u003c/p\u003e\u003cp data-pid=\"Yl77Cmeh\"\u003e而且，邹雅琦的纪录片火了之后，还有媒体去过嘉德，想要采访那位拍马屁的专家和求包养的保安，但是没有人敢站出来承认。\u003c/p\u003e\u003cp data-pid=\"Rc9n0jVz\"\u003e这是邹雅琦免费住五星级酒店大堂期间的截图。\u003c/p\u003e\u003cp data-pid=\"qstIqVKS\"\u003e如果我不告诉你她的真实身份是《瞬间所有制》纪录片创作者，这是在一家五星级酒店，你会觉得她是谁？这是在哪？\u003c/p\u003e\u003cp data-pid=\"2H-w7fR_\"\u003e在五星级酒店消费的名媛？\u003c/p\u003e\u003cp data-pid=\"Thv3JVn3\"\u003e出来体验生活的千金？\u003c/p\u003e\u003cp data-pid=\"yjcSMkay\"\u003e酒店工作人员猜测中“来抓小三的阔太太”？\u003c/p\u003e\u003cp data-pid=\"Ps3wwHvQ\"\u003e逛街累了走进一家咖啡厅躺着看看杂志的路人？\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-ae931459e9023ea829e88ac01499fec2_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2438\" data-rawheight=\"1369\" data-original-token=\"v2-eef2360adaa141f1009798ab2db99fdf\" data-default-watermark-src=\"https://pica.zhimg.com/v2-a9e33e04d83a8f22ecf90acc97cec4ca_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"2438\" data-original=\"https://pica.zhimg.com/v2-ae931459e9023ea829e88ac01499fec2_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"yaNFz5Cj\"\u003e吐槽一句，很多大牌的设计我一个学美术，做艺术品行业的人看来...也好看不到哪里去。很多什么什么印花刺绣，我姥姥都不会穿；而且很多大牌质量也好不到哪里去，前几年某大牌热销的鞋被人吐槽质量差到穿几次就破，穿几个月还没坏的反而是假货。\u003c/p\u003e\u003cp data-pid=\"3me1TfUP\"\u003e至于评论区里其他用户提到的天天跟你强调“奢侈品跟假货有什么什么品质区别，你用了才知道”，“几百块的假货一眼假”的那些视频，我也看过一些，除了一些不吃不喝也要把工资贡献给商场，坚定不移的消费主义信徒之外，基本上都是卖高仿的，人家当然要跟你强调他们卖一两千照着真品一寸一寸打出来的高仿跟几十上百的地摊儿货有什么区别。\u003c/p\u003e\u003cp data-pid=\"iYSkyUfd\"\u003e看来他们这思想工作是做得挺到位，嗯。\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"_q0XQ5RY\"\u003e再更。\u003c/p\u003e\u003cp data-pid=\"jTYka4tP\"\u003e如果你觉得邹雅琦这纪录片是假的，或是她本来就是假装成假名媛实际去消费的真名媛——啊对对对！\u003c/p\u003e\u003ch2\u003e但是美剧《制造安娜》和韩剧《安娜》中假名媛的原型，Anna Sorokin的故事可是真的，这会儿她还在笼子里蹲着呢。\u003c/h2\u003e\u003cp data-pid=\"2_FVy-cT\"\u003e假如马克吐温穿越到21世纪，知道了Anna Sorokin，一定会反思：我的文学创作似乎还是太保守了。\u003c/p\u003e\u003cp data-pid=\"cs-HtD6L\"\u003e论假扮名媛，Anna Sorokin比邹雅琦狠多了，艺高人胆大。\u003c/p\u003e\u003cp data-pid=\"jlUTl4Zn\"\u003e毕竟，邹雅琦穿假货装名媛白吃白喝，是为了拍拍摄一部纪录片当作毕设作品；\u003c/p\u003e\u003cp data-pid=\"fk-STuBP\"\u003e马克吐温的《百万英镑》中，有一位濒临破产的矿业代理，他求助于持有百万英镑支票的亨利，亨利不费吹灰之力，只是向那些富豪们动动嘴皮就卖出了囤积的矿产，亨利不仅盘活了矿厂，还给自己赚来了二十万英镑的中介费。\u003c/p\u003e\u003cp data-pid=\"Mo71UQjI\"\u003e但是事实证明，人的想象力还是有限的，即使是马克吐温这样的文豪也并不例外。\u003c/p\u003e\u003ch2\u003eAnna Sorokin假扮名媛全靠演技，差点在纽约曼哈顿骗到了两千二百万美金的贷款，以及一栋楼。\u003c/h2\u003e\u003ch2\u003e\u003cb\u003e或许，她给自己取的假名字——“安娜德尔维/Anna Delvey”——更加广为人知。\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"1VctzDj6\"\u003e她给自己立下的人设包括但不限于：\u003c/p\u003e\u003cp data-pid=\"ih9lkTKo\"\u003e\u003cb\u003e俄罗斯克里姆林宫高层官员家的名媛官二代；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"On8qKYlX\"\u003e\u003cb\u003e坐拥俄罗斯大量油田的石油行业寡头家的千金；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"zBhjE9ao\"\u003e\u003cb\u003e西欧某国皇亲国戚，神秘贵族后代；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"JMtgaceq\"\u003e\u003cb\u003e德国老钱世家的继承人；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"wUq9wEjR\"\u003e\u003cb\u003e在英国牛剑/美国常春藤大学取得了一个MBA学位，会说七国语言；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"SVKQjknb\"\u003e\u003cb\u003e在瑞士银行拥有六千万欧元的信托基金；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"qWXsWLIk\"\u003e\u003cb\u003e拥有一个慈善基金会和一家美术馆，同时更是致力于投资人工智能研究和开发的新生代企业家创业家领军人物，股神巴菲特等华尔街巨头，对冲基金，顶级投行，以及曼哈顿各种顶级律所的座上宾；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"tlnd9AA1\"\u003e\u003cb\u003e家住德国大庄园，兴趣爱好是收藏艺术品，家中藏有某印象派大师的代表作，价值四千二百万美金...\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"5bP8wqho\"\u003e\u003cb\u003e你说这么个人设，该得多贵气才撑得起来？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"4THwN7mP\"\u003e\u003cb\u003e实际上Anna Sorokin也就是来自德国小镇上蓝领阶级家庭的女孩，出生在莫斯科郊区，父亲是卡车司机，母亲是家庭主妇，高中文化水平\u003c/b\u003e【我们这一代人没接受过高等教育都算另类了】，平时在二三线时装杂志打打零工，工资连房租都不够付，每天的爱好就是玩手机，看ins等社交媒体和各种时尚杂志上的时尚博主千金小姐们穿金戴银，英语德语都说不利索。\u003c/p\u003e\u003cp data-pid=\"9fafV-3_\"\u003e\u003cb\u003e实打实的草根。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"d-GE_lfJ\"\u003e这就是Anna Sorokin，如果我不告诉你她的案底，你会想到她扮假名媛扮得这么成功吗？\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-08a18305c3ac0e5ea65b9d09739c5eaf_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"1728\" data-original-token=\"v2-97b65d3d163e8bc454c40d2812281313\" data-default-watermark-src=\"https://pic1.zhimg.com/v2-9e47785804d04e439934f546581974ca_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1440\" data-original=\"https://picx.zhimg.com/v2-08a18305c3ac0e5ea65b9d09739c5eaf_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"gb2xq_9_\"\u003e你可以说：\u003c/p\u003e\u003cp data-pid=\"VD45jeVz\"\u003e“有钱人都是这样的，自信，反而不喜欢花枝招展，医美保养！”\u003c/p\u003e\u003cp data-pid=\"TYqd50gn\"\u003e也可以说：\u003c/p\u003e\u003cp data-pid=\"zAoKIdtz\"\u003e“难怪露馅，草根阶层就是保养得差，皮肤不好！”\u003c/p\u003e\u003cp data-pid=\"APeIBp6r\"\u003e反正都说得通。\u003c/p\u003e\u003cp data-pid=\"RBmw0Uv8\"\u003e而且，Anna Sorokin可\u003cb\u003e不仅骗倒了服务人员\u003c/b\u003e，在纽约各种五星级大酒店白住，在米其林三星吃霸王餐；她\u003cb\u003e还骗倒了一大票子真少爷真名媛\u003c/b\u003e，让他们请客吃大餐，住豪宅，送奢侈品，在世界各地逛博览会艺术节，出席各种酒会晚宴，搭私人飞机，出入各种社会名流常常光顾的顶级私人会所俱乐部；\u003c/p\u003e\u003cp data-pid=\"Fj2XC1sA\"\u003e\u003cb\u003e她甚至骗倒了少爷千金们的父母长辈，老爷夫人。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"4GHrlHin\"\u003e她骗倒的“上流人”包括但不限于：\u003c/p\u003e\u003cp data-pid=\"QmjLf4JE\"\u003e\u003cb\u003e华尔街的投资银行，信贷公司，对冲基金，资产管理公司的银行家们；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Lw-dN_0B\"\u003e\u003cb\u003e亲手把现实中的“华尔街之狼”送进监狱的大律师Joel Cohen，以及他的朋友，一家国际律师事务所的投资法律顾问；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"57cON2JG\"\u003e\u003cb\u003e设计出包括纽约世界贸易中心车站，雅典奥林匹克体育场，等多个顶级建筑物的建筑设计师；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"A_-hFSnz\"\u003e\u003cb\u003e蓝筹艺术品收藏家【拥有两家美术馆的京圈二代】。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"sf2eEaAA\"\u003e《百万英镑》的改编电影中，亨利在身无分文地意外登陆英国后来到美国大使馆求助，他的要求并不高，只想快点回到美国，或是找一份工作在英国攒点路费回去，前台办事员一边吃着零食一边漫不经心地说：“您的要求得等起码一个月！”\u003c/p\u003e\u003cp data-pid=\"JQYuPQ0i\"\u003e在富豪两兄弟借给他百万英镑支票后，亨利再一次来到美国大使馆，驻英国大使亲自接待：“我滴个姥爷哟，您怎么招呼都不打一声就揣着一百万英镑来英国体验生活了！快快快，什么古巴雪茄都给拿过来，还有多拿点现金，姥爷在英国没零钱多不方便！哦对了，再介绍一些英国上流人，什么贵族富豪还有国会议员，都喊过来跟姥爷作陪，姥爷在英国无亲无故没人唠嗑解闷儿可不行！”\u003c/p\u003e\u003cp data-pid=\"zTB75Fvx\"\u003e这是Anna Sorokin假扮名媛的时期跟纽约上东区某真千金的合照。她当初跟其他千金小姐大少爷们一样，被Anna Sorokin骗得团团转，并且\u003cb\u003e真千金这样形容他们的圈子：“Anna Delvey”【Anna Sorokin的假名】在我们的圈子里是食物链顶尖的大姐大，所以她有权利搞霸凌搞鄙视链，见谁ins粉丝少啊，穿的是过季货或者不够有品位啦，住的公寓不如她下榻的酒店豪华啦，或者豪车不如她坐的私人飞机高档...那都是没有资格跟她成为好姐妹站c位的，会被她讽刺，挖苦，毒舌一条龙...所以大家都在努力讨好她。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"CvtI3jIA\"\u003e他们的聚会中，很常见的就是到了该买单的时候，大家争先恐后地拿出信用卡，放在桌上打混，然后Anna Sorokin闭上眼睛，从中盲选出一张。\u003c/p\u003e\u003cp data-pid=\"K4jD7rKW\"\u003e下图右边是无产阶级小镇女孩，卡车司机的女儿Anna Sorokin，左边是那个出身豪门，却被前者骗得怂兮兮不敢喘大气的真千金。\u003c/p\u003e\u003cp data-pid=\"ZsAjVbKU\"\u003e\u003cb\u003e如果我不告诉你，你觉得她们分别是谁？\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-b275ff6bd1135a5f6392b8403e11fed5_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"976\" data-rawheight=\"549\" data-original-token=\"v2-ba999160d9f29365978711dc9d2a8311\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-680fda7ce02127d280f25d3164804400_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"976\" data-original=\"https://pic4.zhimg.com/v2-b275ff6bd1135a5f6392b8403e11fed5_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"-e_AEM30\"\u003e《百万英镑》中，有一位专门研究贵族世家家谱的史学家对亨利说：“我认识您的老父亲，尊敬的老亚当斯先生！”亨利听到后忍俊不禁——他其实是孤儿，自己都不知道自己的父母身在何方。\u003c/p\u003e\u003cp data-pid=\"YPnfr495\"\u003e改编电影中，西装店老板忙不迭地伺候亨利，不停地询问他：我来看看有什么适合您穿的？您至少需要三十套西装，我知道您这样尊贵的绅士都喜欢骑自行车，欣赏歌剧，跳舞，当然还有皇室成员都喜欢的马术！在造船厂上班的亨利无奈道：额，这些我都没有尝试过，我有一艘小船，休息日会划着它在海面上晒太阳...西装店老板急忙接话恭维：划船！多么高雅的志趣呀！看来，划船正在上流社会兴起，很快就会取代马术成为新的皇室项目了！\u003c/p\u003e\u003cp data-pid=\"HIzbrp6j\"\u003e老公爵开了个玩笑，指使佣人藏起了支票。在支票不翼而飞的事情传开后，酒店老板把亨利赶出了套房，西装店老板收回了为亨利做的那些好衣服，股民们纷纷抛售亨利的股票。他们是这样说的：“他连酒店/西装的钱都没有付，怎么可能是富豪！”在老公爵意识到玩笑过火归还了支票后，这群人的态度又是180度大反转，将亨利迎接回套房，送上香槟，西装...\u003c/p\u003e\u003ch2\u003e\u003cb\u003e很多人都习惯根据结果反向推理过程，根据自己的所见去合理化那些不合理的现象。现实中，Anna Sorokin行骗曝光后，她所接触过的那些“上流人”里，也不乏事后诸葛亮。\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"6IxqQYCx\"\u003e\u003cb\u003e其中也包括上面这位被Anna Sorokin骗得五体投地，甚至因为被她鄙视而惶恐不已的真千金，Anna Sorokin行骗曝光后，她又赶紧补充：“我从一开始就知道她根本不是什么‘俄罗斯寡头家的名媛’！我从不相信她在瑞士银行有什么六千万欧元的信托基金，在德国有大豪斯大庄园！”\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"ERgrvteE\"\u003e还包括上述的蓝筹艺术品收藏家——\u003cb\u003e一位来自我国的京圈少爷，事后有人就Anna Sorokin行骗骗走他不少钱一事采访他，他连忙表示：“我打从一开始，就觉得她老奇怪了，而且她不漂亮，横看竖看都不像什么‘德国老钱世家千金’！”\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"QG1mHOED\"\u003e话是这么说，但是，当初这位少爷通过一名艺术品交易商，认识了Anna Sorokin，听闻她是一名来自德国老钱家族的女继承人且对艺术品感兴趣，\u003cb\u003e这位“打从一开始就觉得她老奇怪了，长得不漂亮，根本不像千金”的少\u003c/b\u003e\u003cb\u003e爷果断为她定下了往返意大利的头等舱机票，豪华酒店，以及威尼斯双年展艺术博览会的门票。\u003c/b\u003e事后Anna Sorokin全然没有给钱的意思，他也一点都不着急：“她可是德国老钱世家的继承人！机票酒店于她而言微不足道，区区小事，何足挂齿。”\u003c/p\u003e\u003cp data-pid=\"zzc61FoK\"\u003e被Anna Sorokin骗倒的一位姥爷还曾经拿她当别人家的孩子教育自家孩子：你说你，成天除了买买买还知道个锤子，书也读不好打工也不行，就不能学学人家“Anna Delvey”，人家爸爸是石油大王，妈妈是皇亲国戚，就这，人家还发奋读书，在纽约只身创业打拼，同样的年纪，你找我要零花钱，人家是我的大客户！！！\u003c/p\u003e\u003cp data-pid=\"humE_xu0\"\u003e奉俊昊的《寄生虫》里，穷困的金家寄生于富裕的朴家，他们行骗的流程是这样的：\u003c/p\u003e\u003cp data-pid=\"gdM5zi0_\"\u003e就读于名牌大学并准备海外留学的朋友向富家太太引荐了哥哥，妹妹在网吧里用简单的修图软件给哥哥伪造了延世大学的在读证明，哥哥凭借这两点骗过了富家太太，成为了千金的家教，取得富家太太的信任之后又把金家妹妹推荐给富家太太；\u003c/p\u003e\u003cp data-pid=\"vZ3M0vT9\"\u003e金家妹妹在取得了富家太太的信任后成为了富家小少爷的美术老师，之后又通过富家太太，向富家老爷推荐了金家爸爸去当司机；\u003c/p\u003e\u003cp data-pid=\"d3SMwsHD\"\u003e金家爸爸在取得了富家老爷的信任之后，金家伪造出了一张高档家政公司的名片，通过富家老爷转交给了富家太太，金家妹妹捏着鼻子假扮成客服接听了富家太太的电话，将金家妈妈介绍给富家太太，成为了金家的管家；\u003c/p\u003e\u003cp data-pid=\"enmD28G9\"\u003e至此，金家一家四口寄生到了富人一家身上，每天各自下班之后，回到他们住的地下室里，痛快地吃着烤肉喝啤酒。\u003c/p\u003e\u003cp data-pid=\"kCJSTaqN\"\u003e\u003cb\u003e而现实中，Anna Sorokin单枪匹马就骗到了上百万rmb的现金，还在没有收入和身份证明的情况下骗到了银行贷款和信用卡，最后还差点在一家纽约证交所上市的投资集团骗到两千二百万美金的贷款，和一栋位于纽约曼哈顿，被登记在美国国家历史遗迹名录列表里的大楼。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"LIRUMA7S\"\u003e这一切直到Anna Sorokin由于高额诈骗被起诉才曝光，她身边那些走得近的真名媛，真少爷们都对此感到难以置信——她是怎么混进这个圈子里，还骗走这么多钱，还差点骗来了一栋楼的？\u003c/p\u003e\u003cp data-pid=\"Pd4jTDDo\"\u003e《名利场》的记者Jessica Pressler经过长达几个月的调查，采访，信息搜集以及整理总结之后大致梳理出了Anna Sorokin行骗的流程：\u003c/p\u003e\u003cp data-pid=\"K6AEN_Gm\"\u003e首先，创作出“Anna Delvey”的人设，怎么“上流”怎么来，有钱人常见的各种关键词，什么神秘的俄罗斯寡头，低调的德国老钱世家，西欧皇亲国戚，信托基金，慈善基金，艺术品收藏，投资人——都往身上套；\u003c/p\u003e\u003cp data-pid=\"Q91N2lCW\"\u003e其次，准备好一些假证件，拿来向银行申请小额度的贷款，不需要太多，\u003cb\u003e够置办一身昂贵的行头，在高档餐厅酒店下馆子就行\u003c/b\u003e；\u003c/p\u003e\u003cp data-pid=\"SpE0cNTS\"\u003e之后，\u003cb\u003e雇佣一家\u003c/b\u003e\u003cb\u003e公共关系公司PR Agency，通过他们拿到“社会名流”的联系方式\u003c/b\u003e，把自己想要认识的那些人，什么真千金名媛，真少爷，华尔街银行家，律政精英，奢侈品牌设计师，找个由头邀请来参加自己组的局，各种酒会晚宴聚会等，\u003cb\u003e当这些人齐聚一堂，看到来其他宾客也都是“上流人”，就会自动认为这个能把纽约“上流社会”聚在一起组局的“Anna Delvey”肯定也是个上流人\u003c/b\u003e；\u003c/p\u003e\u003cp data-pid=\"kVaAdb45\"\u003e然后趁热打铁，向上述这些人介绍自己，搬出各种人设；\u003c/p\u003e\u003cp data-pid=\"iNSyiTgL\"\u003e最后，\u003cb\u003e通过这些有钱人的引荐，找来一群商业精英和行内专家作为商业和技术顾问，制作出ADF私人会所的商业企划书；同时准备一些伪证，假装自己在德国的资产；再加上有钱人所引荐的银行家和律师出面协助申请并担保，向投资集团或银行申请大额贷款，然后启动商业计划\u003c/b\u003e【最后因为出生证造假被发现功亏一篑】。\u003c/p\u003e\u003cp data-pid=\"Bhblfxfu\"\u003e一位被Anna Sorokin忽悠的华尔街银行家后来在采访中这样说：\u003c/p\u003e\u003cp data-pid=\"TmtDy0oz\"\u003e“For an equity partner, that\u0026#39;s money in perpetuity. Should the vetting be more thorough? Of course. But sometimes, a big fish swims by, you don\u0026#39;t waste time deciding if it\u0026#39;s a marl or a carp. You just cast a net and reel it in.”\u003c/p\u003e\u003cp data-pid=\"W-DodLEW\"\u003e结合当时的情况和Anna Sorokin行骗的流程，简单来说，那群有钱人和精英就是这样被她骗倒的：\u003c/p\u003e\u003cp data-pid=\"eL_woPQK\"\u003eAnna Sorokin请那些她在组局时认识的商业顾问，技术顾问和设计师组成了一支梦之队，设计出了宏伟的ADF俱乐部商业企划，金额涉及几千万美金，并且估算出了每年超过两亿美金的净利润，在此基础上设计出全球业务拓展计划。同时，摆出人设忽悠，并且拉出各种有钱人为自己的商业计划背书。另一方面，持续拓展“上流社会”的人脉，邀请他们提前申请加入“ADF俱乐部”，作为潜在业务。\u003c/p\u003e\u003cp data-pid=\"L5fGgv5I\"\u003e对于她想拉入局的那些合伙人，投资人来说，这就是商机，巨大的商机。审核工作和调查应不应该更加彻底，更加谨慎？当然应该。但是如今眼前这个“Anna Delvey”和她的梦之队，以及那些有钱人源源不断上车，画出的饼是如此之大，现在抓住这一单，未来就能稳赚十几年甚至几十年的钱，那后半辈子就直接财务自由啦！\u003c/p\u003e\u003cp data-pid=\"2Te2xGMs\"\u003e所以这些银行家选择了抓紧时间抓住“Anna Delvey”这一单生意。其余的，可以之后再说！否则错过这个村没了这个店，还得在华尔街揣着速效救心丸干到油尽灯枯才能退休。\u003c/p\u003e\u003cp data-pid=\"G6bn5mkh\"\u003e一旦有谁质疑这件事不合理——比如，这个“Anna Delvey”到底谁呀？谁认识她背后的“皇亲国戚老妈”，或是“石油寡头老爸”？谁在她在德国的豪宅庄园串过门儿？谁见过她在瑞士银行的信托基金和存款证明？从而放慢贷款流程——\u003c/p\u003e\u003cp data-pid=\"OElWxr-o\"\u003e那就搬出“Anna Delvey”的身边那些“上流人”圈子——瞧瞧，上东区这些喊着金汤匙出生的少爷小姐，还有他们身经百战的父母，那些姥爷太太，都急着巴结这个“Anna Delvey”，全纽约的“上流社会”都要阿谀奉承的人，肯定来头不小，最起码也是个有钱人中的有钱人！\u003c/p\u003e\u003cp data-pid=\"r0rA2_6Q\"\u003e再说了，梦之队这些个顶级设计师，顶级建筑师，还有商业精英都技术入股了，他们的眼光能有错吗？申请加入这个“ADF俱乐部”的人，什么千金，少爷，姥爷，夫人，华尔街投行银行家，硅谷高层，蓝筹艺术家，都已经排起长队啦，等“ADF俱乐部”开张剪彩，这些人的消费能力用不了几天就能回本，赚大钱！\u003c/p\u003e\u003cp data-pid=\"_9zDynC1\"\u003e错过这一单大几千万美金的生意你来负责？你拿什么负责？你那点工资，还是你哪颗腰子？\u003c/p\u003e\u003cp data-pid=\"3SG8v1GF\"\u003e\u003cb\u003e其中有个有趣的细节：Anna Sorokin是如何差点骗到纽约曼哈顿一栋楼的？\u003c/b\u003e答案是：她差点拿到了一笔高达两千二百万美金的贷款，计划竞标这栋楼，建设自己构想的ADF俱乐部——可是\u003cb\u003e那笔贷款又是哪里来的？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"4fcjQODE\"\u003e是这样的，Anna Sorokin通过一位大律师，Joel Cohen【也就是前文提到的那位亲手把“华尔街之狼”送进监狱的律师】，认识了一位纽约金融圈法律圈都声名显赫的国际律师事务所投资法律顾问Andy Lance。起初Andy也有点疑虑：这个“Anna Delvey”真的是什么“德国老钱家族，欧洲皇亲国戚”之后吗？\u003c/p\u003e\u003cp data-pid=\"B5dYJnzQ\"\u003e\u003cb\u003eAnna Sorokin于是用Word伪造出了一份瑞士银行信托基金的资产证明和存款证明，之后花五美金买了个瑞士手机号【电信诈骗常用的手机软件，app store就有】，在手机里下载了一个整蛊恶搞用的变声软件，假装成一名瑞士银行家，致电Andy，证明“Anna Delvey”真的有六千万欧元的信托基金。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"MzqhcSrq\"\u003eAndy于是乎相信了“Anna Delvey”的人设，可是他是谁？他是从金融危机中生存下来化险为夷的顶级投资法律顾问，通讯录里有成千上万来自世界各地有钱人的联系方式，想求他帮自己处理投资事宜管理资产的人排成队华尔街都站不下，他凭什么为“ADF俱乐部”干活呀？\u003c/p\u003e\u003cp data-pid=\"UHwXlnwS\"\u003e于是乎，\u003cb\u003eAnna Sorokin邀请Andy参加了自己的聚会，眼见整个纽约的“上流社会”齐聚一堂，争相向“Anna Delvey”推销自家业务，ADF私人会所的“梦之队”里集齐了建筑，餐饮，奢侈品，时尚，艺术设计等行业的巨头为项目保驾护航\u003c/b\u003e，他便笃定——这个ADF项目，就是下一个Soho House俱乐部【一家汇集了全球时尚媒体行业精英的俱乐部】啊？哦不！Soho算啥？！这ADF是全球豪门少爷千金姥爷夫人的俱乐部啊！\u003c/p\u003e\u003cp data-pid=\"nmDrVSJF\"\u003e于是，他忙不迭加入了“Anna Delvey”的“梦之队”，为ADF私人会所商业计划申请贷款作为启动资金。\u003c/p\u003e\u003cp data-pid=\"LdxI1ltW\"\u003e名镇纽约的大律师费用可不便宜，更不要说几千万的贷款申请，背后还需要一支由初级律师，律师助理，等等工作人员组成的团队，齐心协力，每小时要一千美金起步。Anna Sorokin当然也不是空手来的，真要花钱，她随时都可以骗出来一笔，\u003cb\u003e但是Anna Sorokin请来Andy，和一支精兵悍将组成的团队，一分钱都没有花\u003c/b\u003e——\u003c/p\u003e\u003cp data-pid=\"PX-M8tgo\"\u003e这家跨国律师事务所，一听说“Anna Delvey”是一名“德国老钱世家继承人”，看到梦之队作出的商业企划中每年净利润超过两亿美金的大饼，再加上Anna Sorokin用word和手机app轻松伪造出的资产证明，态度瞬间卑微——还管什么繁文缛节，先把大客户稳住再说！赶紧把贷款申下来，申不下贷款咱全律所上下提头来见！\u003c/p\u003e\u003cp data-pid=\"YFfcs7_q\"\u003e再加上，某天Anna Sorokin随口提到，希望ADF俱乐部建成后，提拔Andy进入ADF俱乐部董事会，Andy想都没想便一口答应：好好好！！！在这华尔街我还不知道得熬多少年才能退休，去您的俱乐部当董事那每年分红都够我每天躺沙滩上晒太阳了！就这点工时费，算啥？！\u003c/p\u003e\u003cp data-pid=\"k6ujZFfS\"\u003eAndy为ADF私人会所相中了两家公司：一个是一家银行，另一个是一家投资集团。\u003c/p\u003e\u003cp data-pid=\"lqjUUsHv\"\u003e银行比较谨慎，业务人员表示必须先看看“Anna Delvey”在德国的资产证明和在瑞士的信托基金再向她放贷——这些Anna Sorokin早都捏造出一箩筐了，但是业务人员经过一系列评估，认为“Anna Delvey”完全没有相关的行业经验，没有经营过酒店，服务行业，不可能在纽约曼哈顿经营起一家六层楼高，包含了餐饮酒店奢侈品零售艺术品展览等业务在内的俱乐部；再说谁知道这二十出头的“德国老钱世家继承人”会不会就是一时半会脑子发热想创业，到头来却只是千金出来体验生活，三分钟热度搞了个过家家？随后拒绝了她的贷款申请。\u003c/p\u003e\u003cp data-pid=\"WU9TOi5U\"\u003e而\u003cb\u003e投资集团堪称艺高人胆大，见这位威名赫赫的投资法律顾问都亲自上阵了，听闻“Anna Delvey”是“德国老钱世家继承人”，什么“尽职调查”，“资产证明”，“业绩记录”，“相关行业经验”那都是小事，不要在意这些细节！投资集团表示只需要“Anna Delvey”拿出十万美金作为押金就可以向她贷款两千二百万美金。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"DlhePNHa\"\u003e弄个十万美金对于此时行骗手段已经炉火纯青的Anna Sorokin来说就跟回新手村打个野一样简单，但是Anna Sorokin甚至还没有出手，那家原先拒绝向她贷款的银行却主动申请出战——这家银行当初审理“Anna Delvey”贷款申请的业务人员，听闻投资集团已经把放贷提上了日程，两眼一黑，以为自己错过了一个亿。更糟的是，要是让上司同行知道自己亲手给竞争对手送温暖，弄丢这么大一单生意和这么大一个客户，那自己在华尔街的金融之旅也就结束啦！于是他连忙找上负责“Anna Delvey”的ADF俱乐部贷款事宜的那位投资法律顾问Andy，滑跪认错，并请求他说服“Anna Delvey”在自家银行开户。\u003c/p\u003e\u003cp data-pid=\"hBfPZlZs\"\u003e于是\u003cb\u003eAnna Sorokin又“勉为其难”地，在三无的情况下在这家银行开户，办理了一张二十万美金透支的信用卡，拿出其中十万美金的额度缴了投资集团的押金\u003c/b\u003e。投资集团收到钱后马不停蹄，旋即开启了两千二百万美金的放贷流程。至此，“ADF俱乐部”从小镇女孩Anna Sorokin的诈骗行为，升级成了一个开始奔腾的宏伟商业计划。\u003c/p\u003e\u003cp data-pid=\"bwPoDrL4\"\u003e直到投资集团的一名业务员偶然间发现“Anna Delvey”的护照模糊难辨，好奇之下打开后台资料一看，发现“Anna Delvey”这个身份和ADF俱乐部项目的账户背后，完全没有可以担保风险的资产证明。紧急叫停了放贷，并派出调查人员赴德国和瑞士查证“Anna Delvey”的资产和信托基金。于是乎，“Anna Delvey”因资金链断裂，终于被多家银行，酒店，航空公司，奢侈品牌以拖欠贷款为由联合送上了法庭，开庭后荣登纽约时报头条。\u003c/p\u003e\u003cp data-pid=\"6yGVBMsB\"\u003e讽刺的是，Anna Sorokin忽悠的那群千金，少爷，夫人，姥爷，还有各行各业的顶尖人才，看到新闻后，有相当一部分人感到难以置信，甚至表示：“啊这，这是个误会吧？她爹是俄罗斯寡头/石油大王，她娘是德国老钱世家/欧洲皇亲国戚，她有六千万欧元的信托基金/坐拥慈善基金会和美术馆/AI科研团队，怎么可能是诈骗犯呢？”\u003c/p\u003e\u003cp data-pid=\"wUhx8TzZ\"\u003e就连Jessica Pressler，将“Anna Delvey”的故事送上纽约客头条，之后又跟Anna Sorokin一起把版权卖给Netflix，改编成美剧，韩剧，最后让还蹲在铁栅栏后面戴着电子脚镣的Anna Sorokin摇身一变，成为了如今ins上坐拥百万粉丝，举办轰轰烈烈的展会和时装秀的网红的那位精英记者，在彻头彻尾调查了Anna Sorokin，访问了她出生的莫斯科市郊，成长的德国小镇，以及她朴实的蓝领父亲和家庭主妇母亲之后，发现她真的只是，草根出身，没有接受过高等教育的小镇女孩之后，也只能说出这四个字——\u003c/p\u003e\u003ch2\u003e难以置信！\u003c/h2\u003e\u003cp data-pid=\"DBmfK7nf\"\u003e不过，如今的Anna Sorokin，可不仅仅是个网红。\u003c/p\u003e\u003cp data-pid=\"8mOpBnO4\"\u003e她不仅成为了话题中心，德国小镇的红人，更成为了许多人心目中的英雄：一个出生在莫斯科城郊，成长于德国小镇，父亲是朴实的蓝领工人，母亲是家庭主妇，高中文化水平，为数不多的社会经验只有在三线杂志打零工，二十出头身无分文的年轻草根，居然把华尔街投行，对冲基金，国际律师事务所，顶级设计事务所，等等行业的精英大腕，还有那些含着金汤匙出生，成长于曼哈顿上东区的纽约“上流社会”少爷千金们，通通骗上了她的贼船！也就最后功亏一篑。\u003cb\u003e假如行骗要分段位，那这简直是贼船界的泰坦尼克！\u003c/b\u003e\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"12Y18GX9\"\u003e艺术文学创作的灵感来源于现实，但是艺术文学的创作夸张到一定程度的时候就会被人质疑：这不合理。\u003c/p\u003e\u003cp data-pid=\"3MLcVRQg\"\u003e然而现实颠起来比艺术光怪陆离多了。\u003c/p\u003e\u003cp data-pid=\"3deTDka1\"\u003e邹雅琦在拍摄纪录片期间，曾经屡次担心过被人拆穿，被赶出去，被迫中断拍摄；Anna Sorokin后来在法庭上为自己辩驳也表示：“很多东西我根本没有伸手找任何人所要，他们听说我是德国富豪家的千金小姐之后就主动给予了我顶级礼遇。”\u003c/p\u003e\u003ch2\u003e有句老生常谈的俗话说：“世界就是个巨大的草台班子。”\u003c/h2\u003e\u003ch2\u003e这句话我不评价。\u003c/h2\u003e\u003ch2\u003e但是，如果真是如此，那看来有钱人的世界也并不例外——同样是个草台班子。\u003c/h2\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":1491634,"thumbnails":["https://pica.zhimg.com/50/v2-6ef510d52cc6072ccf4514b34325a800_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-3d9b09b631a0d9590a4415a0c9af748d_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-f320f43ae8c6f456346cbd16429cb921_720w.jpg?source=b6762063"],"favorite_count":19520,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 19867553313}","attached_info":"Cp8ICNu3uMHCytKhqwEQBBoJNjk3MDg5MDkxIPPqjrkGKJzDATCSBEBoSkgKH1RTX1NPVVJDRV9aUkVDQUxMX0lURU1DRl9VUFZPVEUSH2RvY190eXBlOiBBbnN3ZXIKaWQ6IDcyMTYzNTcxNgoYACAAOgBaCDEwMzg4OTEwYiA1OTAxOTkzODk3YjJiNjcxNTM0YjQxZGZlMTcwMzk0N3ILMTk4Njc1NTMzMTOKAQg0NTQ0MTE0MKoBCXJlY29tbWVuZMIBIDkzYzNlY2E5MDk1ZWQ0MDIzNDE3MDYwNTVlZjZiYTFk8gEKCAwSBk5vcm1hbPIBKAgKEiRlZDgzOTVmNC0yYjgyLTQ5MGQtYjA0MS01OGNmMWE3ZDlkMDTyAQYICxICMTiCAgCIAtOTu836MpICIDkzYzNlY2E5MDk1ZWQ0MDIzNDE3MDYwNTVlZjZiYTFkmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFkFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhtJbnRlcmFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhZSZXZpc2l0VmFsdWVXZWlnaHRSdWxlygIYUGVyaW9kSW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIUQ29udGVudEFnZVdlaWdodFJ1bGXKAhNUaGVtZVdha2VVcFJld2VpZ2h02gIfVFNfU09VUkNFX1pSRUNBTExfSVRFTUNGX1VQVk9URegCA/oCC05PUk1BTF9GTE9XigMgMDFjZjgwOTBjNTc5NDg5OWE0ZWQwOTQwZjdmOTMxMmOaAw0KAnYyEAAaBW90aGVyqAOyhVvYAwDqAxt0ZXh0QWxsU2l0ZUhxQWN0aW9uSXRlbUNGVjL6A9sBEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAMQ5Q4YnwkiI3YyLWZlMzIyZGVmY2JjZmU0M2YwNzQ3OWI5YTliZDI3NjFlOi0IAxCGExjZCiIjdjItZWVmMjM2MGFkYWExNDFmMTAwOTc5OGFiMmRiOTlmZGY6LQgFEKALGMANIiN2Mi05N2I2NWQzZDE2M2U4YmM0NTRjNDBkMjgxMjI4MTMxMzotCAQQ0AcYpQQiI3YyLWJhOTk5MTYwZDlmMjkzNjU5Nzg3MTFkYzlkMmE4MzExgAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEBm1hbnVhbMIEAjMwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAAAAqtXBP4EFAAAAAAAAAACJBQCofMr1mtI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRKQBgCgBmyoBgCSAiYKCTY5NzA4OTA5MRILMTk4Njc1NTMzMTMYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"105_1750898493.244","type":"feed","offset":105,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898493,"updated_time":1750898493,"target":{"id":"2647172454","type":"answer","url":"https://api.zhihu.com/answers/2647172454","author":{"id":"eee1a40ac26662c60156610d39972c46","url":"https://api.zhihu.com/people/eee1a40ac26662c60156610d39972c46","user_type":"people","url_token":"present-is-gift","name":"俗不可耐","headline":"敬祖法天","avatar_url":"https://picx.zhimg.com/50/v2-aea1197f8d8c736701c8853a1f4cb9fa_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"阿斯麦 员工"}],"followers_count":340218,"is_following":false,"is_followed":false},"created_time":1661531963,"updated_time":1661887815,"voteup_count":9602,"thanks_count":1894,"comment_count":1266,"is_copyable":true,"question":{"id":"397556649","type":"question","url":"https://api.zhihu.com/questions/397556649","author":{"id":"a338ce5cba7e5fb3c0385d4ec20cdaf3","url":"https://api.zhihu.com/people/a338ce5cba7e5fb3c0385d4ec20cdaf3","user_type":"people","url_token":"ping-guo-79-24","name":"苹果","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"电视剧《天道》想要表达是什么？","created":1590503402,"answer_count":0,"follower_count":0,"comment_count":27,"bound_topic_ids":[2102,2557,8412,599614,1988517],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"我在广东的时候最让我吃惊的就是广东人那种骨子里想要当老板的劲头。 什么学历低，什么出身差，什么长得丑，什么没有启动资金，人家统统不在乎。只要有机会他们就跃跃欲试要自己搞个名堂，当老板。 许多年下来，好几个朋友真的自己当上了老板。最不济也开了个饭店。还有中专毕业的把公司开到上亿销售额的。 我算是近距离跟这些人有过接触，而且是他们没发达的时候跟他们有过接触。总之，这些人吧，比学识，比思想，比眼光，比文…","excerpt_new":"我在广东的时候最让我吃惊的就是广东人那种骨子里想要当老板的劲头。 什么学历低，什么出身差，什么长得丑，什么没有启动资金，人家统统不在乎。只要有机会他们就跃跃欲试要自己搞个名堂，当老板。 许多年下来，好几个朋友真的自己当上了老板。最不济也开了个饭店。还有中专毕业的把公司开到上亿销售额的。 我算是近距离跟这些人有过接触，而且是他们没发达的时候跟他们有过接触。总之，这些人吧，比学识，比思想，比眼光，比文…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"qpw1TiEx\"\u003e我在广东的时候最让我吃惊的就是广东人那种骨子里想要当老板的劲头。\u003c/p\u003e\u003cp data-pid=\"6-w90s51\"\u003e什么学历低，什么出身差，什么长得丑，什么没有启动资金，人家统统不在乎。只要有机会他们就跃跃欲试要自己搞个名堂，当老板。\u003c/p\u003e\u003cp data-pid=\"cWvL2IWf\"\u003e许多年下来，好几个朋友真的自己当上了老板。最不济也开了个饭店。还有中专毕业的把公司开到上亿销售额的。\u003c/p\u003e\u003cp data-pid=\"-hYySfWQ\"\u003e我算是近距离跟这些人有过接触，而且是他们没发达的时候跟他们有过接触。总之，这些人吧，比学识，比思想，比眼光，比文化，非但不算上流，其实中等水平都未必达到。\u003c/p\u003e\u003cp data-pid=\"flO-OAck\"\u003e别说去谈谈尼采，叔本华，就是聊聊金庸古龙都聊不下去，当然聊聊古惑仔倒是可以。\u003c/p\u003e\u003cp data-pid=\"zbN6CeGQ\"\u003e所以我看《天道》真的觉得它是全程装13，创业这事儿不确定因素太多了！绝不是哪个幕后大佬可以算无遗策的。我认识的这帮人，都是试了很多次最后阴差阳错地走到后来他们的道路上的。他们比普通人了不起的地方在于从来都不预定自己一定能成功，一开始就本着得之我幸，失之再来的心态。最后总算找到了机会。\u003c/p\u003e\u003cp data-pid=\"yz6X42UY\"\u003e而创业最麻烦的说白了就是需求端，怎么找到市场需求，怎么让别人相信你的产品，这个东西才是最最最最最麻烦的。为了解决需求端，求爷爷告奶奶，挖空心思，拉关系，甚至撬别人的客户……\u003c/p\u003e\u003cp data-pid=\"LfZeJKlW\"\u003e而《天道》里面恰恰是创业最难的这个环节，略略带过，而在一些无关紧要的破事儿上扯来扯去。\u003c/p\u003e\u003cp data-pid=\"eT-uIWyl\"\u003e如果用真实的创业来看，假设这是一个饭店。丁元英对格律诗饭店最大的贡献是他知道米其林菜品还认识圈子里的大佬，能把有钱人拉来这个饭店。说白了靠的是他的关系，跟“天道”，“文化属性”没啥关系！\u003c/p\u003e\u003cp data-pid=\"CeqYZGEQ\"\u003e因此《天道》跟《三国演义》类似，真正的带兵打仗根本没有武将单挑，阴谋诡计的作用也远没那么大。带过兵的人知道，练兵比指挥更重要。战场上处理一堆乱七八糟破事儿的临场能力，比纸上谈兵更重要。不断在真实战场积累经验比研究天道更重要。\u003c/p\u003e\u003cp data-pid=\"PijivKJW\"\u003e霍去病懂什么“天道”？他就是懂得大漠里哪里能找到水，刮什么风匈奴会往哪里跑，马粪的颜色代表匈奴人几天前来过这里……\u003c/p\u003e\u003cp data-pid=\"K0IUiLf1\"\u003e就是这一堆一堆无法尽说的琐碎信息，才成就了一代名将！\u003c/p\u003e\u003cp data-pid=\"cfAorJsO\"\u003e诸葛亮在历史上刚出山也没打过几个漂亮仗，北伐是一次比一次纯熟，也是到了最后才算是一个优秀的将领。\u003c/p\u003e\u003cp data-pid=\"yANFlrHp\"\u003e躲在屋子里，能掐会算，然后运筹帷幄。这是文人的想象，所谓“天道”其实就类似于武侠小说里的内功。现实世界里分明是五大三粗的人更能打，但发明了内功这个概念，白面书生也可以战胜彪形大汉，还能以一敌百。\u003c/p\u003e\u003cp data-pid=\"tpYeo-hD\"\u003e可惜，这都是书生的意淫。图个乐子尚可，真的想要能打，没有任何捷径，就是要汗流浃背的撸铁，压腿；规律的作息；把搏击动作练到不用脑子就能精准反应。至于武侠小说里那种打打坐，不流汗，不变壮，甚至把蛋蛋切了就能天下无敌，你猜泰森能答应吗？\u003c/p\u003e\u003cp data-pid=\"8MJqKG13\"\u003e同理所谓“天道”也就是文人的意淫，文人觉得自己懂点儿玄妙哲学就能天下无敌。其实想多了，陶碧华怎么把老干妈做大的？陶碧华懂多少天道？懂哪门子文化属性？还有我那些朋友，老庄都是赚了钱去养生课学的，做生意的时候一堆破事，既要当销售，又要当质检，还要找货源……，哪儿有漏洞哪儿要自己顶上去，哪里有什么时间坐而论道，把把小迷妹？\u003c/p\u003e\u003cp data-pid=\"nQQKVEXC\"\u003e到华强北看看，呜呜泱泱一个又一个电子元器件的小摊子。去广州北京路看看，那七八个楼层到处都是的玉石小店。这些在密不透风憋屈的大楼里，捧着盒饭，堆着笑，跟你讨价还价的小老板。他们才是真实的在做生意，而且总会有那么几个阴差阳错结识了贵人，找到了机会，做大做强，变得你都认不出来。\u003c/p\u003e\u003cp data-pid=\"7WQZCPpb\"\u003e大道理如果不能让你更踏实，更愿意顾全琐碎，那么这个大道理不如不听。而“天道”对琐碎是全面鄙视的！就像练拳击不强调训练而反复告诉你独孤九剑的剑意，也就是要找人破绽。结果就是上了拳击台，你的看到对手到处都是破绽，但是把手举起来的瞬间，就被人一拳撂倒！再不然就是你锤人家十拳，人家觉得像蚊子挠痒痒，笑嘻嘻地让你再来几下！\u003c/p\u003e\u003cp data-pid=\"jvyATolI\"\u003e总之，《天道》是一部浪漫主义的作品。所谓浪漫主义，更多的是写意。你是不能把它当做指导和现实来看的。至于觉得理解了丁元英的“天道”就能如何如何的人更是想多了。扎扎实实把两万个单词背熟，熟到听到就知道什么意思，比提高什么“文化属性”都管用。\u003c/p\u003e\u003cp data-pid=\"Zi8Saff1\"\u003e花里胡哨的东西，其实是最远天道的。真正的天道在琐碎里，在烦恼中，在无聊时……\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"mBLVspDP\"\u003e我其实回答了问题，不过可能有的人觉得我答非所问，那么我就再总结一下。\u003c/p\u003e\u003cp data-pid=\"AxLoC3tg\"\u003e《天道》想要表达的是，抬头看天，不要只顾眼前的苟且。\u003c/p\u003e\u003cp data-pid=\"XuvqgeML\"\u003e但我觉得，苟且就是天，现在这个时代，咨询太发达！不同于90年代，在一个人人刷个手机都能看到“天“的时代，少看天发呆反而更重要。\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"CvbBN1hr\"\u003e武侠小说很好看，独孤九剑，九阴真经，降龙十八掌都有他的寓意，但是武侠小说不能当搏斗教材。\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"996hlpQr\"\u003e评论区真精彩！我再举个例子，丁元英曾经在饭桌上告诉人要买那只股票，一年后卖掉……，这可能就是很多人心里对天道了解后的赚钱形象吧。但是对不起，假的！但凡真的接触过金融操盘手，都知道对市场的敬畏！我小学中学的学霸就在华尔街投行做，投资从头到尾是一个技术活儿。大量的报告，大量大数据，大堆的模型，然后最多给出一个收益的概率。没有谁敢拍着胸脯说未来一年会有几个点儿的收益，除非他是佩洛西的老公。\u003c/p\u003e\u003cp data-pid=\"RswovDwL\"\u003e很多人说得很对，《天道》不是在讲商战，商战是一个壳子，其实它是在论道。但是我骂的就是它论的这个道！\u003c/p\u003e\u003cp data-pid=\"jMPsSk2I\"\u003e它这个道说白了是邪魔外道！它的逻辑就是一旦开悟人就能有“神通”。这个“神通”就是丁元英算无遗策的本领，这完全是对天道的误读。天道无常，你到哪里去算无遗策？\u003c/p\u003e\u003cp data-pid=\"6mLzhEW2\"\u003e悟道后，非但不会觉得自己会“掌握规律”，能“算无遗策”，能操纵人心，反而会更加谦卑。\u003c/p\u003e\u003cp data-pid=\"2HnovdxD\"\u003e老子，庄子，六祖慧能，没有一个升官发财，建立不世功勋的。开悟是一种心灵状态，是一种看待事务的态度，没有神通！没有神通！没有神通！\u003c/p\u003e\u003cp data-pid=\"6n6WrZWH\"\u003e这部剧之所以是邪魔外道，就是因为它所描述的悟道后的神通。这个“神通”是专业知识，是时运，是家庭背景，跟“悟道”没有关系。\u003c/p\u003e\u003cp data-pid=\"kOIYpX_Y\"\u003e读老庄，读坛经，读传习录，这些最多帮你解决精神上的问题，让你无论逆顺都有一个好的心态，让你在任何情况下都能把自己手上的牌打到最好，不受情绪干扰，让你不会执着于不值得的事浪费生命。但是除了这些，要做成具体的事，这些大道理没什么太大作用非但没什么作用，更可能因为少了那份功利心而做不成事。你看历史上，老庄也好，六祖也罢，有几个家财万贯，封官拜爵的。即便是备受吹捧的王阳明，对明帝国的影响力跟后来的张居正完全不在一个层次。缔造中国两千年制度的商鞅，根本就跟悟道扯不上关系。\u003c/p\u003e\u003cp data-pid=\"OIIGBv0g\"\u003e说了那么多，总结起来就是，《天道》里引用的大段大段的禅宗或道家的东西都是在谈修心。跟神通没有关系。但是《天道》却特别喜欢卖弄神通。给人感觉好像“悟道”了就能有神通，就能轻易发财一样，这就是误导。谁要是按照这个路子去“赚钱”，肯定是事与愿违。而如果要《悟道》，直接读老庄，坛经，金刚经不好吗？看它作甚？\u003c/p\u003e\u003cp data-pid=\"9h_m8f0I\"\u003e《天道》吸引很多人是因为丁元英的神通，但是基督教里耶稣反复强调不能因神迹而信仰，佛法反复强调佛门修行不是为了神通，道家从头到尾都把求名逐利的手段视为扰乱真元，甚至王阳明的心学也从未说过致良知就能操纵人心建功立业。说白了《天道》里引用的各家思想都是给人心安的，心安了就能不被成败得失所困扰。但耶稣也好，佛祖也罢，阳明也好，老庄也罢，包括禅宗的慧能，从来没有人会说，心安了你就能随随便便从股市里捞钱，你就能随随便便操纵人心，就能算无遗策，就在朝局里长青不倒，能把世间不确定性都变成确定。做到这个的是神仙，而这些哲学思想让人获得的心安，不是成神！\u003c/p\u003e\u003cp data-pid=\"NA5xt4HZ\"\u003e丁元英不是靠什么“天道”赚的钱，他靠得是专业知识！投行里的操盘手有一个是一个都是名校毕业生中的成绩卓越者，若非如此谁敢把成千上万的钱给你一个操作？凭你会聊老庄禅吗？\u003c/p\u003e\u003cp data-pid=\"TT1r6DR-\"\u003e名校毕业什么意思？思考一下自己能不能考上清华北大。你们觉得去理解丁元英那些“大道理”就能考上清华北大吗？现代金融不是玄学！一大堆的理论，一大堆的编程，一大堆的数据分析，一大堆的考试！美国特许金融分析师的资格认证CFA，全世界都能报考，拿到了就是金领，年薪百万分分钟的事，一年千万人民币的收入都不稀奇。这玩意儿跟丁元英嘴里的“天道”有几毛钱关系？我把话放在这儿，就这个CFA考试，一年两次，不限专业年龄。真想有丁元英来钱的“神通”，你们最该研究这个！而不是他嘴里的“天道”。\u003c/p\u003e\u003cp data-pid=\"_r_OsW1Q\"\u003e故事的真相很简单，丁元英通过了某个极为苛刻的考试，拿到了资格认证，做了操盘手。运气好（再好的眼光也有风险）赚了一笔小钱。在当操盘手的时候因为自己喜爱音乐，接触过许多音响上的国际级别的大佬，顺带着学了几招。并且因为共同的爱好跟一下有钱爱高级音响设备的大佬有了联系。然后回到中国三线小城市，降维打击了这帮人。\u003c/p\u003e\u003cp data-pid=\"LYiU9XqX\"\u003e其实复盘一下，论道不过是丁元英的一个爱好。就像乔布斯喜欢灵修一样。如果丁元英喜欢的是讨论康德，这部剧换个名字叫《星空》也没有半毛钱关系。喜欢叔本华这部剧叫《钟摆》也毫无违和感，喜欢尼采给个名字叫《扎拉图斯特》也讲得通。因为哲学都能自圆其说，哪一套哲学都足以解释世俗里所有的现象。但追本溯源，丁元英的那“神通”并不来源于他的“天道”就像乔布斯的成就并非源自他的灵修还有那套吃素能治百病的奇葩逻辑。乔布斯英年早逝其实就是因为他那套神秘主义。而丁元英之所以处理不好家庭关系，之所以没有朋友，孤独就是因为他这套奇奇怪怪的“文化属性”理论。而很多人看了这个剧买椟还珠，幻想着理解了丁元英那套蹩脚的二流哲学思考就能年薪百万，纵横股市，脱贫致富。甚至还觉得不懂这些，早晚就是“风口上的猪”，要摔下来，何其荒谬？\u003c/p\u003e\u003cp data-pid=\"RQgEdeLH\"\u003e实际上，刘冰跟丁元英最本质的区别不在于什么“天道”，而是一个连大学都考不上，另一个轻轻松松北大清华。智力上的差距，靠论个道就能弥补了？\u003c/p\u003e\u003cp data-pid=\"llTpoCQS\"\u003e所以我说这部剧是在装13，避重就轻，装神弄鬼，充满了“皇帝都用金锄头”式的想象，属于是跪着往上看到了鼻孔就觉得鼻孔大是主人高高在上的原因。上层精英有牛逼得让人五体投地的地方，但绝大多数都是专业上的。至于专业以外，其实跟普通人差不多，很多方面甚至不如普通人。去听丁元英讲讲金融理论，出国留学，资格证考试那是绝对不会错，但是听他给你讲讲天道，量子力学……听听就好！\u003c/p\u003e\u003cp data-pid=\"qMZTZhe9\"\u003e上层精英里奇奇怪怪想法的人多了去了，特殊癖好的人也多了去了，只是你要脑袋清醒，时刻明白一个核心问题，他到底靠得什么有了今天的成就。而非因为收入，地位，阶层的差距就全方位仰视。跪着看人是永远看不明白的，更毋宁说取而代之。\u003c/p\u003e\u003cp data-pid=\"RuilHkMI\"\u003e刘邦，朱元璋，教员，都是不信邪的，都是不会被任何蹩脚理论唬住的。我提到的广东人，不少人之所以最后成功其实也是如此。专注于赚钱，只对赚钱感兴趣，跟人打交道永远去思考他的钱怎么挣来的，我能不能用同样的办法赚钱？电路板原理看不懂就刷板，没有工厂就贴牌，反正不惜一切代价把钱赚来，把客户抢来，再一点一点改进技术，扩大规模，建厂。看着他们从一个销售员慢慢做到上亿资产，公司上市，就会发现，书生造反三年不成这句话真的有道理。\u003c/p\u003e\u003cp data-pid=\"mYG0oY2c\"\u003e所以我《天道》里那个刘冰反而是现实中真的有可能把事业做大的人。当然剧里他的心态崩了，这点是硬伤。但是他那套做派乃至猥琐，才是许多下层突破阶层壁垒所必须，尤其是第一桶金。至于空降的丁元英，看看自己高考考了几分吧，这样的人，这样的路线，是你学的来的吗？\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"_ANDUQPm\"\u003e我再附一段庄子对得道的人的描述，大家可以拿这个对比一下丁元英。\u003c/p\u003e\u003cblockquote data-pid=\"_7PcWRE5\"\u003e鲁哀公问于仲尼曰：\u0026#34;卫有恶人焉，曰哀骀它。丈夫与之处者，思而不能去也；妇人见之，请于父母曰：\u0026#39;与为人妻，宁为夫子妾\u0026#39;者，数十而未止也。未尝有闻其唱者也，常和人而已矣。无君人之位以济乎人之死，无聚禄以望人之腹，又以恶骇天下，和而不唱，知不出乎四域，且而雌雄合乎前，是必有异乎人者也。寡人召而观之，果以恶骇天下。与寡人处，不至以月数，而寡人有意乎其为人也；不至乎期年，而寡人信之。国无宰，而寡人传国焉。闷然而后应，氾而若辞。寡人丑乎，卒授之国。无几何也，去寡人而行。寡人恤焉若有亡也，若无与乐是国也。是何人者也！\u0026#34;\u003c/blockquote\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"RgfIu55M\"\u003e译文：\u003c/p\u003e\u003cblockquote data-pid=\"qJH6L98G\"\u003e鲁哀公姬将问孔子：“听说卫国有个面貌十分丑陋的人，名叫哀骀它。男人跟他相处，常常想念他而舍不得离去。女人见到他便向父母提出请求，说‘与其做别人的妻子，不如做哀骀它先生的妾，’这样的人已经十多个了，而且还在增多。从不曾听说哀骀它倡导什么，只是常常附和别人罢了。他没有居于统治者的地位而拯救他人于临近败亡的境地，他没有聚敛大量的财物而使他人吃饱肚子。他面貌丑陋使天下人吃惊，又总是附和他人而从没首倡什么，他的才智也超不出他所生活的四境，不过接触过他的人无论是男是女都乐于亲近他。这样的人一定有什么不同于常人的地方。我把他召来看了看，果真相貌丑陋足以惊骇天下人。跟我相处不到一个月，我便对他的为人有了了解；不到一年时间，我就十分信任他。国家没有主持政务的官员，我便把国事委托给他。他神情淡漠地回答，漫不经心又好像在加以推辞。我深感羞愧，终于把国事交给了他。没过多久，他就离开我走掉了，我内心忧虑像丢失了什么，好像整个国家没有谁可以跟我一道共欢乐似的。这究竟是什么样的人呢？”\u003c/blockquote\u003e\u003cp data-pid=\"FDy8x3Km\"\u003e按照道家的看法，一个人不必给人钱财却能够让所有人都喜欢；不用讲大道理却让所有人跟随；没看出什么大刀阔斧却能让治理好国家；这就是真的得天道者。\u003c/p\u003e\u003cp data-pid=\"E5QpR-e7\"\u003e反观丁元英，是不是说教太多，手段太狠，拧拧巴巴，故弄玄虚？\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":775784,"favorite_count":8487,"answer_type":"normal","reaction_instruction":{"REACTION_CONTENT_SEGMENT_LIKE":"HIDE"},"is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 2647172454}","attached_info":"CusGCNu3uMHCytKhqwEQBBoJNTExMzkwNDkxILvuo5gGKIJLMPIJQGlKQQosVFNfU09VUkNFX1RXT1RPV0VSX1NIT1JUSU5URVJFU1RfUkVDQUxMX1RFWFQSATAYACAAOgp7InJhdyI6IiJ9Wgg1MDYyNTM5N2IgNTkwMTk5Mzg5N2IyYjY3MTUzNGI0MWRmZTE3MDM5NDdyCjI2NDcxNzI0NTSKAQkzOTc1NTY2NDmqAQlyZWNvbW1lbmTCASBlZWUxYTQwYWMyNjY2MmM2MDE1NjYxMGQzOTk3MmM0NvIBCggMEgZOb3JtYWzyASgIChIkZDdjMjQxYzUtMTc0Zi00ZjcyLTkzMDktNWNiNDY0ZDQ3OGRl8gEGCAsSAjE4ggIAiALTk7vN+jKSAiBlZWUxYTQwYWMyNjY2MmM2MDE1NjYxMGQzOTk3MmM0NpoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhZBY3Rpb25TaG9ySW50ZXJlc3RSdWxlygIbSW50ZXJhY3Rpb25TaG9ySW50ZXJlc3RSdWxlygIWUmV2aXNpdFZhbHVlV2VpZ2h0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxlygIXVGVzdGVkQW5kV29ya1dlaWdodFJ1bGXaAixUU19TT1VSQ0VfVFdPVE9XRVJfU0hPUlRJTlRFUkVTVF9SRUNBTExfVEVYVOgCA/oCC05PUk1BTF9GTE9XigMgMDFjZjgwOTBjNTc5NDg5OWE0ZWQwOTQwZjdmOTMxMmOaAw0KAnYyEAAaBW90aGVyqAPorC/YAwDqAxpmZWVkX2F0dG1fdHdvdG93ZXJfdjJfdGV4dPoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQGbWFudWFswgQDMTYwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAADAWn3AP4EFAAAAAAAAAACJBQCofMr1mtI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRKQBgCgBm2oBgCSAiUKCTUxMTM5MDQ5MRIKMjY0NzE3MjQ1NBgEIgpJTUFHRV9URVhU","action_card":false},{"id":"106_1750898493.204","type":"feed","offset":106,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750898493,"updated_time":1750898493,"target":{"id":"1921223205709914570","type":"article","url":"https://api.zhihu.com/articles/1921223205709914570","author":{"id":"754e8bbc60c7694d231db1c09486ce5c","url":"https://api.zhihu.com/people/754e8bbc60c7694d231db1c09486ce5c","user_type":"people","url_token":"li-ying-38-87","name":"自在小颖","headline":"让花成花，让树成树，让自己成为更好的自己","avatar_url":"https://picx.zhimg.com/50/v2-6c5ea3fcbc4ae65260937a6666140bfd_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":1,"is_following":false,"is_followed":false},"title":"我发现，原来我们都“误解”了工作与生活，《斯坦福人生设计课》：你真正需要的不是一份工作，而是一种能让你持续“活下去”的生活模式。","image_url":"https://picx.zhimg.com/v2-f90ee729597b349759cd303939715828.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1750835364,"updated":1750835364,"voteup_count":0,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"命运的画笔，握在自己手中 你好呀，我是自在小颖 今天读到了颠覆认知的一段话，很炸裂，分享给你： 赚钱的方式有 1 万种，而以前的我只知道找个公司去打工。 于是世界上最让人痛苦的两件事儿就成了工作和没工作。 但是我们不该在恐惧中度过一生。 仔细想想，你真正需要的是一份工作吗？ 工作只是谋生的手段，你需要的只是一种活下去的模式。 你们有没有被震撼到，我有，我深深的震撼 我仔细查找了这个观点的出处， 这是《斯坦福…","excerpt_new":"命运的画笔，握在自己手中 你好呀，我是自在小颖 今天读到了颠覆认知的一段话，很炸裂，分享给你： 赚钱的方式有 1 万种，而以前的我只知道找个公司去打工。 于是世界上最让人痛苦的两件事儿就成了工作和没工作。 但是我们不该在恐惧中度过一生。 仔细想想，你真正需要的是一份工作吗？ 工作只是谋生的手段，你需要的只是一种活下去的模式。 你们有没有被震撼到，我有，我深深的震撼 我仔细查找了这个观点的出处， 这是《斯坦福…","preview_type":"default","preview_text":"","content":"\u003cp\u003e\u003c/p\u003e\u003cp data-pid=\"CXmqD7cM\"\u003e命运的画笔，握在自己手中\u003c/p\u003e\u003cp data-pid=\"cxXiVKzz\"\u003e你好呀，我是自在小颖\u003c/p\u003e\u003cp data-pid=\"N72AQZNK\"\u003e今天读到了颠覆认知的一段话，很炸裂，分享给你：\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"iSig2zD6\"\u003e\u003cb\u003e赚钱的方式有 1 万种，而以前的我只知道找个公司去打工。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"U6AC7PJO\"\u003e\u003cb\u003e于是世界上最让人痛苦的两件事儿就成了工作和没工作。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"R50O5Ygh\"\u003e\u003cb\u003e但是我们不该在恐惧中度过一生。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"sFRq4m2n\"\u003e\u003cb\u003e仔细想想，你真正需要的是一份工作吗？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"xcapDdXL\"\u003e\u003cb\u003e工作只是谋生的手段，你需要的只是一种活下去的模式。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"xz2Opr-U\"\u003e你们有没有被震撼到，我有，我深深的震撼\u003c/p\u003e\u003cp data-pid=\"VT5VG0hy\"\u003e我仔细查找了这个观点的出处，\u003c/p\u003e\u003cp data-pid=\"O38cT5Kv\"\u003e这是《斯坦福人生设计课》提出的：\u003c/p\u003e\u003cp data-pid=\"E3vHNwtZ\"\u003e\u003cb\u003e你真正需要的不是一份工作，而是一种能让你持续“活下去”的生活模式。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Ysmz9Wyl\"\u003e有种被击中感，于是我发起了几个灵魂拷问：\u003c/p\u003e\u003cp data-pid=\"_rLDgZ-o\"\u003e\u003cb\u003e“我讨厌上班，但不知道还能做什么。”  \u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"PyaN_OQT\"\u003e\u003cb\u003e“每天忙到崩溃，却感觉生活毫无意义。”  \u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"kuKICaay\"\u003e\u003cb\u003e“如果财务自由了，我第一件事就是辞职！”\u003c/b\u003e \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"iFzp-nzV\"\u003e这些问题你是不是也很熟悉？是不是也是你心之所想？\u003c/p\u003e\u003cp data-pid=\"2rS_FL8Z\"\u003e那么另一个问题出现了：\u003c/p\u003e\u003cp data-pid=\"f9KQy4Bg\"\u003e我们似乎默认了一个逻辑：\u003c/p\u003e\u003cp data-pid=\"xjp0eaBt\"\u003e\u003cb\u003e工作是为了赚钱，赚钱是为了活下去，而活下去的方式只有拼命工作。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"azzKjgw_\"\u003e看到这里你是不是觉得有点拧巴，我也这样觉得\u003c/p\u003e\u003cp data-pid=\"K8PfoIat\"\u003e因为道理都懂，但却不知道该怎么做\u003c/p\u003e\u003cp data-pid=\"jtDbJBJi\"\u003e于是我又去梳理了《斯坦福人生设计课》的脉络\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-037075d8f851262fbd53decc7493ded1_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"800\" data-original-token=\"v2-706627c2d470130e33ff71ae84959359\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://picx.zhimg.com/v2-037075d8f851262fbd53decc7493ded1_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"E1GqrfQV\"\u003e\u003cb\u003e1. 工作≠生活，但大多数人被困住了\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"FS1u3lmb\"\u003e小 A，30 岁，某互联网公司运营，月薪 2 万，但每天加班到 10 点，周末随时待命。她说：“我赚得不少，但根本没时间花钱，更别提享受生活。”  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"isg3ooJj\"\u003e小 B，35 岁，体制内员工，工作稳定但极度枯燥，每天重复同样的流程。他想辞职创业，又怕失败，只能日复一日地熬着。  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"yrGDZhX6\"\u003e他们的困境很典型：\u003c/p\u003e\u003cp data-pid=\"TBbbof_9\"\u003e\u003cb\u003e工作占据了全部精力，但生活却像被掏空。 我们默认“工作=生存”，却很少思考：有没有另一种方式，既能养活自己，又能活得有温度？\u003c/b\u003e \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"FAXM9qDw\"\u003e\u003cb\u003e2. 斯坦福的答案：设计你的“生活模式”\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"mIRprkRW\"\u003e《斯坦福人生设计课》的核心方法是：\u003cb\u003e像设计师一样思考人生，而不是像机器一样执行程序\u003c/b\u003e。 具体怎么做？  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"u-xFFZmV\"\u003e\u003cb\u003e（1）先问自己：你真正需要的是什么？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"lbTYelaC\"\u003e书里提到一个工具：“健康/工作/娱乐/爱”仪表盘。\u003c/p\u003e\u003cp data-pid=\"LIyHX3gS\"\u003e试着给你的生活四个维度打分（1-10 分）：  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"CZokSiLS\"\u003e- 健康（身体、心理状态）  \u003c/p\u003e\u003cp data-pid=\"dP4BCYk5\"\u003e- 工作（收入、成就感）  \u003c/p\u003e\u003cp data-pid=\"S5tSuSS2\"\u003e- 娱乐（兴趣、放松）  \u003c/p\u003e\u003cp data-pid=\"JfGze081\"\u003e- 爱（家人、朋友、伴侣）  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"QQ3dlEi1\"\u003e很多人发现，“工作”分数高，但其他三项极低——这意味着你的生活模式是失衡的。  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"KSR2TCF3\"\u003e\u003cb\u003e（2）设计你的“奥德赛计划”\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"L_LtkSY0\"\u003e不要只想着“换工作”，而是设计三种可能的未来生活：  \u003c/p\u003e\u003cp data-pid=\"YAoN1qvI\"\u003e1. 现在生活的优化版（比如减少加班，增加副业）  \u003c/p\u003e\u003cp data-pid=\"EtYNACV0\"\u003e2. 完全不同的生活方式（比如自由职业、数字游民）  \u003c/p\u003e\u003cp data-pid=\"9EM74j8y\"\u003e3. 不考虑钱的理想版本（比如开咖啡馆、做独立艺术家）  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"tggbiV-8\"\u003e案例：一位厌倦了 996 的程序员，尝试用业余时间做知识付费，半年后副业收入超过主业，最终辞职，过上了边旅行边工作的生活。  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"tmc-lUZm\"\u003e\u003cb\u003e（3）用“原型”低成本试错\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"kDpha2U1\"\u003e很多人不敢改变，是因为怕“一步错，步步错”。但斯坦福的方法建议：\u003cb\u003e先做最小可行性实验\u003c/b\u003e。 \u003c/p\u003e\u003cp data-pid=\"2hvIYdgZ\"\u003e- 想开咖啡馆？先去兼职体验一周。  \u003c/p\u003e\u003cp data-pid=\"lw0GeeYl\"\u003e- 想自由职业？先接单试试能否养活自己。  \u003c/p\u003e\u003cp data-pid=\"fBE7aFg7\"\u003e- 想换行业？先找业内人士聊聊真实情况。  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"FdF6P0Hx\"\u003e案例：一位教师想转行做心理咨询师，但没有经验。她先考了证书，然后在公益平台提供低价咨询，积累案例，最终成功转型。  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-2f01dc6a8aa90058ff530c9a44237f77_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"800\" data-original-token=\"v2-9d3efebc8a9d0d078bb51fb0d258620d\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic4.zhimg.com/v2-2f01dc6a8aa90058ff530c9a44237f77_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"gHt4PH9A\"\u003e\u003cb\u003e3. 关键思维：从“工作生存”到“生活模式”\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"B1XCpmgY\"\u003e\u003cb\u003e（1）工作只是工具，不是目的\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"RgnDJH0A\"\u003e\u003cb\u003e如果你只是为了钱而忍受一份工作，那它就是在消耗你的生命。真正的“活下去”，是找到一种能让你持续感到充实、有成长、有幸福感的生活模式。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Hn8BWAEu\"\u003e（\u003cb\u003e2）收入可以多元化\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"9zjT10wS\"\u003e除了工资，你还可以有：  \u003c/p\u003e\u003cp data-pid=\"emrb7Hyz\"\u003e- 副业（写作、咨询、电商）  \u003c/p\u003e\u003cp data-pid=\"BB9hqBIF\"\u003e- 投资（理财、房租、版权收入）  \u003c/p\u003e\u003cp data-pid=\"og4gUfy_\"\u003e- 低成本的兴趣变现（比如摄影、手作）  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"g1E_tN-0\"\u003e案例：一位宝妈喜欢烘焙，最初只是朋友圈接单，后来开了工作室，现在月入 3 万，时间自由还能陪孩子。  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"E1xc91wM\"\u003e\u003cb\u003e（3）生活是可以“自定义”的\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"N-Ehiv60\"\u003e有人适合朝九晚五，有人适合自由职业，有人喜欢都市拼搏，有人向往田园生活。\u003c/p\u003e\u003cp data-pid=\"S91mAL35\"\u003e\u003cb\u003e生活没有标准答案，只有适不适合\u003c/b\u003e。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"vwwwov6I\"\u003e\u003cb\u003e4. 现在，你可以做什么？\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"0jJdLCB5\"\u003e1. 记录一周的时间分配：看看你的时间都花在哪里，是否值得。  \u003c/p\u003e\u003cp data-pid=\"0_JEzfmS\"\u003e2. 尝试一个小副业：哪怕赚 100 块，也能验证可行性。  \u003c/p\u003e\u003cp data-pid=\"k-dxKAtd\"\u003e3. 和不同行业的人聊天：打破信息差，发现新可能。  \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-f4ee2d189b7f64977439c37226246c74_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"533\" data-original-token=\"v2-a02119f3a1c633ae5f45b11057fd3a80\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic1.zhimg.com/v2-f4ee2d189b7f64977439c37226246c74_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"zJAVealY\"\u003e\u003cb\u003e写在最后：\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"56CqCF6i\"\u003e梳理完，细细品读，的确如此\u003c/p\u003e\u003cp data-pid=\"vzVKhwE1\"\u003e我们总说“为了生活而工作”，但往往“工作成了生活本身”。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"GOppCKip\"\u003e《斯坦福人生设计课》的智慧在于：\u003c/p\u003e\u003cp data-pid=\"58G6qQlF\"\u003e\u003cb\u003e你的人生不是一条固定轨道，而是一片可探索的旷野。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"sbOtixHD\"\u003e\u003cb\u003e真正的生存，不是忍受一份工作，而是设计一种能让你持续热爱的生活模式。  \u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"HZv7Mwr_\"\u003e但切记：想法一定要付诸行动，否则一切为零\u003c/p\u003e\u003cp data-pid=\"mjErGevv\"\u003e我们和成功者的差距或许真的只在于“执行力”\u003c/p\u003e\u003cp data-pid=\"VsrM1Hhn\"\u003e\u003cb\u003e先起飞在调整航向，在不断地尝试中寻找属于自己的轨道\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"UgwgE9Iz\"\u003e我在不断地探寻，希望你也不要泄气，\u003c/p\u003e\u003cp data-pid=\"GVbW2hR3\"\u003e加油努力，相信我们的人生有无限可能，相信我们都能找到独属于我们自己的航向。\u003c/p\u003e\u003cp data-pid=\"hjFCp3dg\"\u003e所以，你准备好重新设计你的生活了吗？\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"r6YKg2M9\"\u003e\u003cb\u003e如果你喜欢小编的文章，记得点赞收藏，关注自在小颖呦\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"x2A8TDLH\"\u003e\u003cb\u003e让我们一起加油，成为更好的自己。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Jmgdb2dG\"\u003e//往期回顾//\u003c/p\u003e\u003cp data-pid=\"xM8KDBTA\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4Mzg0MjY1NA%3D%3D%26mid%3D2247483852%26idx%3D1%26sn%3Dfa101471ec493959d3023b0ebdbc2f66%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e生活将我推入谷底：从技术总工到厕所保洁员，一个中年人的职场“滑铁卢”与重生\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"IixwwrK5\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4Mzg0MjY1NA%3D%3D%26mid%3D2247483814%26idx%3D1%26sn%3D49dca40bf03476898e2499ac47ec28cd%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e人民日报带你“入局”：别让天赋沉睡！三步激活搞钱内核\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"3l4nVVyw\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4Mzg0MjY1NA%3D%3D%26mid%3D2247483787%26idx%3D1%26sn%3De495cfd58c4224949bba09cafb8b4f28%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e别再喊赚钱难！99%的人都输在没搞懂：你不是没能力，而是根本没“入局”\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"qKNtaIcN\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4Mzg0MjY1NA%3D%3D%26mid%3D2247483781%26idx%3D1%26sn%3D4653caeccc16670163084cdbe23e8e8c%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e改命的唯一办法：大胆在你的生命中引入变量\u003c/a\u003e\u003c/p\u003e","is_labeled":false,"visited_count":5,"thumbnails":["https://picx.zhimg.com/v2-f90ee729597b349759cd303939715828.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-538e0c13054cd44b7e1554aa0e643c3a_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-8d2a039e9d0451c63d351aa17d8d20e4_720w.jpg?source=b6762063"],"favorite_count":0,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1921223205709914570}","attached_info":"CuQHCNu3uMHCytKhqwEQBxoJMjU5NTM5MzM2IKTB7sIGKAAwAEBqSiQKGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDESATAYACAAOgBiIDU5MDE5OTM4OTdiMmI2NzE1MzRiNDFkZmUxNzAzOTQ3chMxOTIxMjIzMjA1NzA5OTE0NTcwggFfaHR0cHM6Ly9waWN4LnpoaW1nLmNvbS92Mi1mOTBlZTcyOTU5N2IzNDk3NTljZDMwMzkzOTcxNTgyOC5qcGc/c291cmNlPTdlN2VmNmUyJm5lZWRCYWNrZ3JvdW5kPTGqAQlyZWNvbW1lbmTCASA3NTRlOGJiYzYwYzc2OTRkMjMxZGIxYzA5NDg2Y2U1Y/IBCggMEgZOb3JtYWzyASgIChIkNGYyMWVjYjctMDlkMS00ZDFjLWJhMTctNGM3NjE2Y2QzN2Q18gEGCAsSAjE4ggIAiALTk7vN+jKSAiA3NTRlOGJiYzYwYzc2OTRkMjMxZGIxYzA5NDg2Y2U1Y5oCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhhQZXJpb2RJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhhDb250ZW50V2FybVVwQnJlYWtJblJ1bGXaAhlUU19TT1VSQ0VfV0FSTV9VUF9OT1JNQUwx6AID+gILTk9STUFMX0ZMT1eKAyAwMWNmODA5MGM1Nzk0ODk5YTRlZDA5NDBmN2Y5MzEyY5oDDQoCdjIQABoFb3RoZXKoAwXYAwDqAx90ZXh0XzEyaG91cl91bmlmaW5zaGVkX3JlY2FsbGVy+gPbARIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgEEKAGGKAGIiN2Mi03MDY2MjdjMmQ0NzAxMzBlMzNmZjcxYWU4NDk1OTM1OTotCAQQoAYYoAYiI3YyLTlkM2VmZWJjOGE5ZDBkMDc4YmI1MWZiMGQyNTg2MjBkOi0IBBCgBhiVBCIjdjItYTAyMTE5ZjNhMWM2MzNhZTVmNDViMTEwNTdmZDNhODA6LQgEEKAGGJUEIiN2Mi1mOTBlZTcyOTU5N2IzNDk3NTljZDMwMzkzOTcxNTgyOIAEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAIBvznj+BBQAAAAAAAAAAiQUAqHzK9ZrSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUSkAYAoAZuqAYBkgIuCgkyNTk1MzkzMzYSEzE5MjEyMjMyMDU3MDk5MTQ1NzAYByIKSU1BR0VfVEVYVA==","action_card":false},{"id":"107_1750898493.617","type":"feed","offset":107,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898493,"updated_time":1750898493,"target":{"id":"1921152116954735662","type":"answer","url":"https://api.zhihu.com/answers/1921152116954735662","author":{"id":"980932ff959d4f1ed591bffa58be74fb","url":"https://api.zhihu.com/people/980932ff959d4f1ed591bffa58be74fb","user_type":"people","url_token":"luna-1-27","name":"LUNA","headline":"","avatar_url":"https://pica.zhimg.com/50/v2-40b38aa0bdefbd3acfd829ebb7ddbd9a_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":1268,"is_following":false,"is_followed":false},"created_time":1750818357,"updated_time":1750818357,"voteup_count":3,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"558598514","type":"question","url":"https://api.zhihu.com/questions/558598514","author":{"id":"b9840c983f4f7e7aedb10e6b0ad126fc","url":"https://api.zhihu.com/people/b9840c983f4f7e7aedb10e6b0ad126fc","user_type":"people","url_token":"qiu-sheng-23-64","name":"秋生","headline":"秋天见。","avatar_url":"https://pica.zhimg.com/50/v2-30fd59dc505e7275f2d955c2169a6195_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":278,"is_following":false,"is_followed":false},"title":"外贸人需要准备的浏览器插件有什么？","created":1665457033,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[101,12359,21336],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://pic1.zhimg.com/50/v2-e7f516dade959b0b9ce3cd28530cd7c6_720w.jpg?source=b6762063","excerpt":"做外贸的朋友都知道，现在市场竞争太激烈了！想要把生意做好，工作效率得高，还得能及时掌握市场动态。其实咱们常用的谷歌浏览器里，藏着好多超好用的插件，能帮我们大忙！今天就来分享 3 个超实用的外贸插件，用好了能让你工作更轻松，业绩也蹭蹭往上涨！ 一、客户联系信息获取神器 - Hunter（暂时只支持 Chrome）   进入Hunter后，点击Discover，就会看到搜索框，输入你想查询的产品/行业，就可以得到公司列表了，这里可以看到我…","excerpt_new":"做外贸的朋友都知道，现在市场竞争太激烈了！想要把生意做好，工作效率得高，还得能及时掌握市场动态。其实咱们常用的谷歌浏览器里，藏着好多超好用的插件，能帮我们大忙！今天就来分享 3 个超实用的外贸插件，用好了能让你工作更轻松，业绩也蹭蹭往上涨！ 一、客户联系信息获取神器 - Hunter（暂时只支持 Chrome）   进入Hunter后，点击Discover，就会看到搜索框，输入你想查询的产品/行业，就可以得到公司列表了，这里可以看到我…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-458d444e5821cfba1ef75f1a27d4534e_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1108\" data-rawheight=\"465\" data-original-token=\"v2-a808a7d9a981bf52005709b1e6e8547f\" class=\"origin_image zh-lightbox-thumb\" width=\"1108\" data-original=\"https://pic1.zhimg.com/v2-458d444e5821cfba1ef75f1a27d4534e_r.jpg\"/\u003e\u003cfigcaption\u003e以上是我的公众号，欢迎关注外贸Aimee，了解更多资讯~\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Bp6WbwWI\"\u003e做外贸的朋友都知道，现在市场竞争太激烈了！想要把生意做好，工作效率得高，还得能及时掌握市场动态。其实咱们常用的谷歌浏览器里，藏着好多超好用的插件，能帮我们大忙！今天就来分享 3 个超实用的外贸插件，用好了能让你工作更轻松，业绩也蹭蹭往上涨！\u003c/p\u003e\u003cp data-pid=\"4tJ5K0Jm\"\u003e\u003cb\u003e一、客户联系信息获取神器 - Hunter（暂时只支持 Chrome）\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-a08970c25aa3f6b31a54b18059baedcf_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"510\" data-original-token=\"v2-354bbefb976551bc6d0cc7216796a856\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-a08970c25aa3f6b31a54b18059baedcf_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"exnIfkPd\"\u003e进入Hunter后，点击Discover，就会看到搜索框，输入你想查询的产品/行业，就可以得到公司列表了，这里可以看到我利用行业词直接匹配到了百万家大致的公司\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-50ee68c077887c68bc2f4a5221be2cb5_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"512\" data-original-token=\"v2-88254bd509c8fdac2525712c6018ef3b\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-50ee68c077887c68bc2f4a5221be2cb5_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-4eb35f51803520d6400ff11a6772d8dd_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"667\" data-original-token=\"v2-cbc6a00a13fc6682c5584ba7645a3ffd\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-4eb35f51803520d6400ff11a6772d8dd_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"g3UIAFr2\"\u003e点击Find email addresses获取该公司的邮箱，并且可以查看公司的简介，紧接着是该公司的所有邮箱列表，名字以及职位，还有该联系人的领英账号，点击LinkedIn图标就能跳转到联系人的LinkedIn账号\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-f94a40bef7acecbc7e865959df0d7b2c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"83\" data-original-token=\"v2-f94a40bef7acecbc7e865959df0d7b2c\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-f94a40bef7acecbc7e865959df0d7b2c_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-3431b5b63b1242bed15cae681b391987_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1220\" data-original-token=\"v2-0b881f85cb2c9916a31e9bd55880e8a0\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-3431b5b63b1242bed15cae681b391987_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-58141f28f03a1374969dc3d893b3a9f9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"811\" data-original-token=\"v2-abdc54f89fa7707a77fc4d7c304f2e3f\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-58141f28f03a1374969dc3d893b3a9f9_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"HqRCPK0l\"\u003e还有其他很多功能这里就不一一介绍啦，有需要的话自己去摸索试试吧~\u003c/p\u003e\u003cp data-pid=\"QjPO6hPJ\"\u003e\u003cb\u003e二、网站流量分析大师 - Similarweb（同时支持 Chrome \u0026amp; Firefox）\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"sw2es9WV\"\u003eSimilarweb 堪称强大的网站流量分析工具，通过它可以深入了解竞争对手网站的整体流量状况。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-166f9bd3aeaf3c762f51f9b90d37d5ab_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"564\" data-original-token=\"v2-0018ada8b5b0bec2638064750eafb04b\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-166f9bd3aeaf3c762f51f9b90d37d5ab_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"AIhx4oN0\"\u003e查看竞争对手总体流量和排名：能分析竞争对手网站的全球排名、所在国家排名、所在目录排名，以及每月大概流量、网站跳出率、停留时间、浏览页面数量等。这些数据能帮助我们快速掌握一个公司网站的基本情况。\u003c/p\u003e\u003cp data-pid=\"wKlTMsZJ\"\u003e使用方法也一样，在浏览网站的时候，点击SimilarWeb插件，就能看到该网站的浏览数据情况，包括\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Nr3CwIDt\"\u003eBounce Rate 跳出率\u003c/li\u003e\u003cli data-pid=\"26TTMsRL\"\u003ePage per visit 页面访问时长\u003c/li\u003e\u003cli data-pid=\"nDmDJvMf\"\u003eMonthly Visits 每月访客\u003c/li\u003e\u003cli data-pid=\"actAGxN3\"\u003eAvg. Visit Duration 平均停留时间\u003c/li\u003e\u003c/ul\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-aa9646798a6dab49a2faec43b19918b6_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1026\" data-rawheight=\"1530\" data-original-token=\"v2-b960b26396bf1d06eb4167ebc6055c1c\" class=\"origin_image zh-lightbox-thumb\" width=\"1026\" data-original=\"https://pic3.zhimg.com/v2-aa9646798a6dab49a2faec43b19918b6_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"nkFtSlvY\"\u003e查看竞争对手流量市场：分析竞争对手的前五大流量国家，大致推测出对方的主要市场，再结合海关数据进一步确认，从而精准把握市场动态。\u003c/p\u003e\u003cp data-pid=\"XoyiX5WF\"\u003e往下拉，可以看到用户主要来源哪些国家（这个数据可以帮助你决策）比如，你想开拓哪个国家的市场，选择哪个平台入驻适合。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-75642a9c452ca0d75e74931312384795_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"884\" data-rawheight=\"1256\" data-original-token=\"v2-207461eaa188359627a01a3e1b66566c\" class=\"origin_image zh-lightbox-thumb\" width=\"884\" data-original=\"https://pic4.zhimg.com/v2-75642a9c452ca0d75e74931312384795_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"3PMbERjC\"\u003e查看流量来源Traffic Sources，这个数据板块也非常重要！分析竞争对手流量渠道：全面了解 Direct 流量、Referral 流量、Search 流量来源、Social 流量、Email 流量、Display 流量等，有助于制定更具针对性的营销策略。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-d5c983b7eda33d340280052a20216e8a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"914\" data-rawheight=\"836\" data-original-token=\"v2-28413ab4fa626c0082589f2a9a708499\" class=\"origin_image zh-lightbox-thumb\" width=\"914\" data-original=\"https://pic1.zhimg.com/v2-d5c983b7eda33d340280052a20216e8a_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"8MiZrNBB\"\u003e\u003cb\u003e三、WhatsApp 沟通助力神器 - WhatsApp助手（支持 Chrome 及多种主流浏览器）\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"HQup8MbT\"\u003e在客户管理方面，WhatsApp 助手的批量添加功能极为高效。传统手动添加需将客户手机号逐一保存至通讯录再操作，耗时费力。而使用该插件，只需导入客户手机号表格，即可一键完成添加。例如外贸展会后，能快速将大量潜在客户导入 WhatsApp，率先开启沟通。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"1olCSOW0\"\u003e\u003cb\u003e六大社媒找电话号码\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"tTJRv2XB\"\u003e第一步，打开Whatsapp 助手（需要在科学上网的状态下），  点击选择找联系方式，设定好查询内容，如下图☟☟☟  \u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-bc1922f632bd325ff6af47b438dac2e6_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"958\" data-rawheight=\"877\" data-original-token=\"v2-a69ba34d9957009a81bc4fb38f06bf62\" class=\"origin_image zh-lightbox-thumb\" width=\"958\" data-original=\"https://pic3.zhimg.com/v2-bc1922f632bd325ff6af47b438dac2e6_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"PlaW6AGm\"\u003e点击查询，查询完成后可导出\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-13ff3da1263d203bb4bc195c66ea6f9e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"830\" data-original-token=\"v2-67c51562e69e4f79d43d02169c5e413a\" class=\"origin_image zh-lightbox-thumb\" width=\"939\" data-original=\"https://pic1.zhimg.com/v2-13ff3da1263d203bb4bc195c66ea6f9e_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"2HJLfnMK\"\u003e\u003cb\u003eWhatsApp群组功能\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"gUJuZsNF\"\u003e点击找群组输入关键词，以led为例，点击开始查询就会出现数据，可手动选择添加也可一键添加群组\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-77961dc179d99537325fce86edebf57b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"839\" data-original-token=\"v2-adb79562719bd18e5b601a1d1775016a\" class=\"origin_image zh-lightbox-thumb\" width=\"697\" data-original=\"https://pic2.zhimg.com/v2-77961dc179d99537325fce86edebf57b_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"oz-N_x6T\"\u003e\u003cb\u003eWhatsapp号码验证\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Vmouxp0Z\"\u003e第一步，将之前社媒搜索到的电话号码导入\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-1cae4e72e1a2884ffd666e95368a17cb_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"946\" data-rawheight=\"881\" data-original-token=\"v2-99a4e737e4ca439643c0ac5d2021d068\" class=\"origin_image zh-lightbox-thumb\" width=\"946\" data-original=\"https://picx.zhimg.com/v2-1cae4e72e1a2884ffd666e95368a17cb_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-18bd6e15b853f9f053c90c8431c6b620_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"780\" data-original-token=\"v2-47ad9f50df79e79649103ff5bf924c12\" class=\"origin_image zh-lightbox-thumb\" width=\"851\" data-original=\"https://pica.zhimg.com/v2-18bd6e15b853f9f053c90c8431c6b620_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"b6GHM7Rj\"\u003e第二步，点击开始验证，验证完之后可导出有效号码\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-48a7af25a1464f8d5a35704c09f910fa_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"941\" data-rawheight=\"838\" data-original-token=\"v2-4ed9db961739d78a9e5adbfe072e8952\" class=\"origin_image zh-lightbox-thumb\" width=\"941\" data-original=\"https://pic3.zhimg.com/v2-48a7af25a1464f8d5a35704c09f910fa_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"6G4XyKow\"\u003e\u003cb\u003eWhatsapp一键群发\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"q2Cc-8IT\"\u003e第一步，添加刚刚验证完的有效号码，如下图所示\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-9eb0539550a1ac90adf6361d1697feea_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"914\" data-rawheight=\"841\" data-original-token=\"v2-a3bf5282866fd49e0e5b77a0a8a8cd2b\" class=\"origin_image zh-lightbox-thumb\" width=\"914\" data-original=\"https://pic1.zhimg.com/v2-9eb0539550a1ac90adf6361d1697feea_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"J-EpO4Ui\"\u003e第二步，选择验证完成的有效号码，编辑要发送的信息，即可一键群发\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-90b28b5cdd1c2fefcdc6fc1899d1e2a3_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"843\" data-rawheight=\"750\" data-original-token=\"v2-137f22995f8ab147dcb9dc365a9bcd75\" class=\"origin_image zh-lightbox-thumb\" width=\"843\" data-original=\"https://pic4.zhimg.com/v2-90b28b5cdd1c2fefcdc6fc1899d1e2a3_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"WTNOWMpD\"\u003e\u003cb\u003e批量群发群组成员\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"n3lrAIXD\"\u003e第一步，选择你要群发的群组进行批量提取群成员\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-31020385fb1abf33ad469f71d189e67b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"954\" data-rawheight=\"838\" data-original-token=\"v2-1b8f18144ea2dcf45ce5b7a5f799e34f\" class=\"origin_image zh-lightbox-thumb\" width=\"954\" data-original=\"https://picx.zhimg.com/v2-31020385fb1abf33ad469f71d189e67b_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"3oHlHlBa\"\u003e最后选择一键群发还是导出\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-6d9231afa0f7a214bd3d6ce7efce6424_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"885\" data-rawheight=\"798\" data-original-token=\"v2-5c71fb4b0f70d754b92bd71f856b8ea4\" class=\"origin_image zh-lightbox-thumb\" width=\"885\" data-original=\"https://pic1.zhimg.com/v2-6d9231afa0f7a214bd3d6ce7efce6424_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"2B4ioTFW\"\u003e\u003cb\u003e注意\u003c/b\u003e：WhatsApp群发信息有封号风险，所以前期不建议群发大量信息，以养号为主，手动添加一些联系人发送信息\u003c/p\u003e\u003cp data-pid=\"3loSV1Zk\"\u003e想了解更多WhatsAPP助手怎么使用可添加客服咨询，输入（topeasy666）添加客服\u003c/p\u003e\u003cp data-pid=\"FbNYwjIf\"\u003e\u003cb\u003e\u003ci\u003e免费获客平台推荐\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"lS0Nke0H\"\u003e\u003ca href=\"https://link.zhihu.com/?target=http%3A//t.smartsousou.com/w/4EAE42\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e图灵搜谷歌地图一键搜索上千客户信息\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"CvvXaysJ\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//hg.smtso.com/%3Fi%3D4EAE42\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e33国海关数据免费在线试用查询\u003c/a\u003e（限时优惠）\u003cbr/\u003e \u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":27,"thumbnails":["https://picx.zhimg.com/50/v2-e7f516dade959b0b9ce3cd28530cd7c6_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-27f3b42c03681db84dd0c641b1a16aaa_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-23cae6b18d4f6a1b62567e137d8d62ca_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-280ce7d8ea28d740895766f2c3bff9d2_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-6b64059794a7cc814a610787b0be4c2c_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-2c408cfbb66c5dc8bbf2e6f9a00e97e3_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-063b5a0f6aeaeb60d2f3f9d072571aee_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-06e3ae1d482349616501d9bac9a442dc_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-f2c9dd7e5f02962df6b6ff6b3bf214a9_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-6b9291a187ab6826c48987b6a2e59ca5_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-ebcc82a9fcf86b219a49d8c0280f2864_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-a3b0ee8492d3cd96c6708163a0b3152d_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-2b33177e382f5bdeae4abc5746ee46e7_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-967e2e93ae910de81c42988639c8bdb8_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-3923de456e6231d85c4b7386238931b3_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-79126c1761274b14ba3bbd998d878f1a_720w.jpg?source=b6762063"],"favorite_count":2,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921152116954735662}","attached_info":"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","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=107\u0026desktop=true\u0026end_offset=111\u0026page_number=19\u0026session_token=5901993897b2b671534b41dfe1703947","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=107\u0026desktop=true\u0026end_offset=111\u0026page_number=19\u0026session_token=5901993897b2b671534b41dfe1703947","totals":0},"fresh_text":"推荐已更新"}
