{"data":[{"id":"150_1750899227.468","type":"feed","offset":150,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899227,"updated_time":1750899227,"target":{"id":"3612960178","type":"answer","url":"https://api.zhihu.com/answers/3612960178","author":{"id":"b86cd67c7c80f1dd3dfb44e74daa5d51","url":"https://api.zhihu.com/people/b86cd67c7c80f1dd3dfb44e74daa5d51","user_type":"people","url_token":"chuan-shi-wen-xin","name":"Safetyobserver","headline":"安全科学、风险评估与可靠性相关知识分享","avatar_url":"https://pica.zhimg.com/50/v2-224e993c18218c75359c74db7d6d6c49_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":9590,"is_following":false,"is_followed":false},"created_time":1725207623,"updated_time":1725208686,"voteup_count":1428,"thanks_count":238,"comment_count":31,"is_copyable":false,"question":{"id":"33971854","type":"question","url":"https://api.zhihu.com/questions/33971854","author":{"id":"11bf5fca8273980c6164f7d1aa81c8a6","url":"https://api.zhihu.com/people/11bf5fca8273980c6164f7d1aa81c8a6","user_type":"people","url_token":"cui-xiao-mao-63","name":"崔小毛","headline":"大学生","avatar_url":"https://picx.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":0,"is_following":false,"is_followed":false},"title":"为什么毕业三四年后，同学间的差距会越拉越大？","created":1438599458,"answer_count":0,"follower_count":0,"comment_count":239,"bound_topic_ids":[1537,1576,2566,3479,6603],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"以前读《俗世奇人》讲钱权才貌，才是通用的。 那时候不太相信。现在想想，看来大部分同学读书的时候连读书人的本事和社会人的本事都分不清，怎么能明白很多社会现实呢？ （1）读书的时候容易理想化，把成绩当一切。学校又学不到真本事，也没有想着在学校去争取可能会拿到的一些资源。读书的时候，评价体系比较单一。谁成绩好，谁考的大学好，谁是专业前几名，谁能在拿各种奖项荣誉，谁能保研，谁能发好论文，这都是学校层面的优…","excerpt_new":"以前读《俗世奇人》讲钱权才貌，才是通用的。 那时候不太相信。现在想想，看来大部分同学读书的时候连读书人的本事和社会人的本事都分不清，怎么能明白很多社会现实呢？ （1）读书的时候容易理想化，把成绩当一切。学校又学不到真本事，也没有想着在学校去争取可能会拿到的一些资源。读书的时候，评价体系比较单一。谁成绩好，谁考的大学好，谁是专业前几名，谁能在拿各种奖项荣誉，谁能保研，谁能发好论文，这都是学校层面的优…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"5zJ3Y7xa\"\u003e以前读《俗世奇人》讲钱权才貌，才是通用的。\u003c/p\u003e\u003cp data-pid=\"Cqme-ni9\"\u003e那时候不太相信。现在想想，看来大部分同学读书的时候连读书人的本事和社会人的本事都分不清，怎么能明白很多社会现实呢？\u003c/p\u003e\u003cp data-pid=\"0jnjqndE\"\u003e\u003cb\u003e（1）读书的时候容易理想化，把成绩当一切。学校又学不到真本事，也没有想着在学校去争取可能会拿到的一些资源。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"0BXAM-Jc\"\u003e读书的时候，评价体系比较单一。谁成绩好，谁考的大学好，谁是专业前几名，谁能在拿各种奖项荣誉，谁能保研，谁能发好论文，这都是学校层面的优秀。\u003c/p\u003e\u003cp data-pid=\"HdHOyfP5\"\u003e普通家庭的学生，学校层面的优秀如果能帮助你在下一个发展阶段获得入场券，那这的确是非常有用的，否则，对你来讲是浪费你的时间。\u003c/p\u003e\u003cp data-pid=\"b6SkONtT\"\u003e比如，把绩点刷的很高，也参加了一些竞赛，如果拿不到进入下一个人生阶段的入场券（如保研到很好的学校不错的课题组，获得很好的工作机会等等），这些努力意义不大。\u003c/p\u003e\u003cp data-pid=\"EemlGZaF\"\u003e有些人早早就明白了这个问题，准备考研的，把某些专业知识学扎实的，锤炼某一项技能并提高到一定段位的，琢磨以后要干啥的，做各种赚钱的尝试的等等。\u003c/p\u003e\u003cp data-pid=\"rcI5hkGl\"\u003e总而言之，大家都想着具体以后要干点啥，这种努力往往才是有效的努力。\u003c/p\u003e\u003cp data-pid=\"6PFqa6KD\"\u003e而大多数后知后觉。举个简单的例子，当初我们本科，很多专业第一，专业第二的，拿不到国奖，也拿不到励志，只能拿学业奖学金。因为国奖全学院一起评，除非你非常优秀，不然动辄人家各种什么比赛的绩点就分就加上去了。励志奖学金得需要贫困证明，中等成绩，有贫困证明的，就可以拿到这个励志奖学金。而用心学习的你，只能拿钱最少的学业奖学金。\u003c/p\u003e\u003cp data-pid=\"vdZVFp1y\"\u003e明明你的家庭也不是很富裕，但是你碍于脸面不去争取励志奖学金。就算学习很好，也拿不到那么多钱。你说你干家教吧，你上多少节课才能多拿到那5000啊，能拿励志的一般还会拿到国家助学金，奖奖不兼得，奖助可兼得。并且，毕业的时候还会有就业补助，你无形中你就错失了多少。\u003c/p\u003e\u003cp data-pid=\"-p2PVUWQ\"\u003e对于普通家庭的学生来说，你没有去争取一些你本来可能会拿到的资源。你自己想要通过努力去弥补，无形中你就会比别人付出更多的努力，浪费了很多时间。\u003c/p\u003e\u003cp data-pid=\"-bypGdMX\"\u003e读了硕士也是，别人老早就想着选那个导师，能发文章的，项目资源多的等等，有些早就琢磨国奖需要发几篇文章，那些毕业的师兄师姐找的工作好，能不能想办法让他推荐一下工作，还有为了后面的工作，赶紧实习一下，等等等等。\u003c/p\u003e\u003cp data-pid=\"7lbaBt3j\"\u003e你这边还在打鸡血，还在幻想毕业了能够如何如何，还想着做学术发文章能够怎么怎么样，还在为了别人的事情操心，还没有为了将来积累必备的技能。\u003c/p\u003e\u003cp data-pid=\"BWweWyUL\"\u003e因为你的好心、善良、幼稚和天真，你白帮了别人多少忙，你无形中你浪费了多少时间，人家一句谢谢就一笔带过，你也不会去说什么，哑巴亏吃了别人还说你是煞笔。\u003c/p\u003e\u003cp data-pid=\"f8wBViE4\"\u003e\u003cb\u003e（2）普通家庭，如果家里没有什么积蓄，父母亲戚给不了你什么有用的帮助，没有办法给你提供一些门路。那么读书，工作至上，赚钱至上，想办法在读书期间积累一技之长，想办法利用网络将自己某一项技能放大做副业，赚到钱，才是正经事。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"_esJY4bt\"\u003e很多普通家庭的孩子，家里提供不了什么帮助。\u003c/p\u003e\u003cp data-pid=\"6jPdg5vO\"\u003e房子首付，彩礼等比较关键的财务支持提供不了，那你就得比别人晚好几年结婚，晚好几年生孩子，感情具有一定的不确定性，并且潜在的恋爱花销也相对较大，攒钱更加困难，时间更少。最重要的是，你比别人基本上晚了一代人。\u003c/p\u003e\u003cp data-pid=\"TIjYSncG\"\u003e你就看吧，有钱不代表说一定要在大城市买房买车。只要是一个小县城能够力所能及给你提供帮助，那你就会轻松很多。\u003c/p\u003e\u003cp data-pid=\"pngcciQA\"\u003e如果你没有这些前置条件，你不琢磨赚钱，那才是“不务正业”。所以一定要琢磨赚钱，起码维持生活，能够存下一定的积蓄。不至于有点事身上拿不出钱，从别人借钱。\u003c/p\u003e\u003cp data-pid=\"NizbnB38\"\u003e并且，在人生的发展过程中，一定要注意，自己做什么，那些尝试能够做到小有所成，变现了，一定要对身边人保密。\u003c/p\u003e\u003cp data-pid=\"J-HGxQzv\"\u003e以前我对“有些身边人生怕你过得比他好”这句话嗤之以鼻。现在结合一些人的经历来看，的确是这样。\u003c/p\u003e\u003cp data-pid=\"t3gcY3-B\"\u003e人红是非多。你赚到钱了，你比别人厉害。借钱的，各种跟你攀关系，问门路的，嫉妒你既能工作也能干副业的人，只会多，不会少。低调行事，才能不给自己带来额外的麻烦。\u003c/p\u003e\u003cp data-pid=\"4qq6TkjI\"\u003e财不露白，不然这些钱你守不住。\u003c/p\u003e\u003cp data-pid=\"aY2Fz6KH\"\u003e再说工作，不要认为不读研学历不够发展受限，不要认为实习可耻。\u003c/p\u003e\u003cp data-pid=\"pLymun10\"\u003e有些专业的本科生找不到啥好工作，所以必须考研。如果你能找到不错的工作，那么真的没必要读研。早毕业早工作早占坑早赚钱。只要工作不错，读不读研又能怎么样？\u003c/p\u003e\u003cp data-pid=\"O-z-Fw6X\"\u003e而且，以前的研究生是两年，现在为了缓解就业压力，学制增加了成了3年，学费也贵了。你能两年完成学业，达到毕业条件，完成学位论文的工作量，那为啥不能实习，不能提前准备考公考编，不能去提前积累一些必备的技能？很多人找工作那都是琢磨了很久的，做了很多尝试，还要问导师问师兄师姐各种逛招聘会，研三了，保证毕业的前提下，没有啥比你就业重要。\u003c/p\u003e\u003cp data-pid=\"XJrwI-2p\"\u003e\u003cb\u003e（3）结交真朋友，维持小范围内的长久关系。不要耗费大量的时间、精力和金钱去维持表面关系。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"akRlqQqX\"\u003e你是否发现，高中和大学玩的一好二好，恨不得天天在一起呆着的兄弟姐妹，毕业了不在一个城市，慢慢就不联系了。\u003c/p\u003e\u003cp data-pid=\"av9hqBKW\"\u003e而且，互联网时代，大家交流的便利多了，但是大家的联系却少了。\u003c/p\u003e\u003cp data-pid=\"NFp8Aqmc\"\u003e你应该能够分清楚谁真心跟你维持长久的关系吧？\u003c/p\u003e\u003cp data-pid=\"g-X8ZYoz\"\u003e有太多男生，交了很多酒肉朋友。吃饭喝酒吹牛b的时候舍不得散场，交流下来恨不得立谈生死。结果到真正办事，需要帮助的时候，喊不来一个人。\u003c/p\u003e\u003cp data-pid=\"aEGPJ3Bk\"\u003e这点男生往往没女生看的清楚。很多女生的社交圈很宅，她就只花钱维持那么点朋友。\u003c/p\u003e\u003cp data-pid=\"ULT48t20\"\u003e你应该清楚的知道，如果你帮助了一个女生，她都不舍得给你花钱，请你吃顿饭啊，喝杯奶茶啊，发个红包之类的，那绝对是没把你当朋友。\u003c/p\u003e\u003cp data-pid=\"AOiv2QW1\"\u003e你这边傻了吧唧，跟谁都能称兄道弟，恨不得四海之内皆兄弟，天下谁人不识君，结果请客你掏钱，唱歌你掏钱，买点水还得你掏钱。一次两次也就罢了，有些学生比较幼稚，次次就是这样，他也不好意思拒绝，怕影响了大家之间的关系。\u003c/p\u003e\u003cp data-pid=\"ia7PUKtA\"\u003e你有没有想过别人把不把你当真朋友？\u003c/p\u003e\u003cp data-pid=\"6FhS-wN7\"\u003e可是，人就是贱啊，别人越不把你当回事，越想着舔别人，在乎别人对你的看法。更傻b的是还想挽回你们之间的关系。\u003c/p\u003e\u003cp data-pid=\"D2bzd7Cp\"\u003e钱搭进去了不知道多少，不长记性。\u003c/p\u003e\u003cp data-pid=\"G4rpvSPU\"\u003e还有一些，各种机缘巧合之下，认识一些有钱的有资源的混得好的大哥大姐，那都不知道姓啥了。说我认识谁谁怎么怎么样，怎么牛b，怎么有排面。\u003c/p\u003e\u003cp data-pid=\"lWJnYF_D\"\u003e牛b人家咋不愿意帮你呢？\u003c/p\u003e\u003cp data-pid=\"wdW9xhZO\"\u003e分不清“里外人”，慢慢把真心朋友搞没了，一堆无法提供给你任何帮助的酒肉朋友多了。尽管他们躺在你的微信列表里，你都不会联系他们。但是你总觉得，认识他们你就很牛b，与有荣焉。\u003c/p\u003e\u003cp data-pid=\"tLpXMOxn\"\u003e这是最煞笔的。\u003c/p\u003e\u003cp data-pid=\"lA9WuZaZ\"\u003e我记得知乎里看到有句话说的很好，“一些期末考试都藏题的同学，你竟然指望毕业涉及到利益的时候他能够帮你”。\u003c/p\u003e\u003cp data-pid=\"EWo07wmk\"\u003e进了社会更是如此，一天上一当，当当不重样。这一课一课，不都是你认为想好好处的人给你上的嘛？\u003c/p\u003e\u003cp data-pid=\"Qu_1uKDS\"\u003e不是一个圈子的人，别硬融。很有道理。\u003c/p\u003e\u003cp data-pid=\"RAvNj_FG\"\u003e现在的社会情况，你帮了一个人，给他提供了一些对他此刻办某件事重要的资源和便利，他连给你花点钱吃个饭之类的都不愿意，这样的朋友也没必要结交了。\u003c/p\u003e\u003cp data-pid=\"3qJ0IU2B\"\u003e\u003cb\u003e（4）别“眼光太高”，别“低端用高配”。门当户对真的有一定道理。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"7FiU2kCI\"\u003e这个社会的门当户对，跟传统社会的门当户对，本质上一点也没变，就是两个人的“钱权才貌”在一定程度下匹配。\u003c/p\u003e\u003cp data-pid=\"PMWD0YYm\"\u003e大家闺秀和书生私奔，书生长得丑点他还愿意嘛？别以为古代美女都是煞笔，也别觉得这些大家闺秀都是久在闺阁不晓人事。骗骗哥们罢了。\u003c/p\u003e\u003cp data-pid=\"ogGeHCKW\"\u003e一些人举例子，谁谁谁，找的老婆很好，家庭条件好，岳父有能力。给他工作弄得怎么怎么好，事业上提拔他，家庭里照顾他，小日子过得很舒服。家里不也农村出身，什么之类的。\u003c/p\u003e\u003cp data-pid=\"g7CjuwB9\"\u003e你咋不看人家长得怎么样，学历如何，能力如何？撇开这些不谈你觉得现实嘛？\u003c/p\u003e\u003cp data-pid=\"OYkIqJQB\"\u003e刘光耀那top2毕业的，不照样被豪门千金耍的团团转。\u003c/p\u003e\u003cp data-pid=\"UAFK9N0l\"\u003e没有在一定程度上，门当户对。女生也不通情达理，一定会出问题，只是时间早晚罢了。\u003c/p\u003e\u003cp data-pid=\"c8Nk30Wt\"\u003e更不用说，现在的情况是，你没钱你真谈不到对象啊。\u003c/p\u003e\u003cp data-pid=\"ZrD4ePnB\"\u003e大学里大家还是相对单纯的，本科生硕士生谈恋爱还比较容易，博士群体不太容易，因为期望值会更高。\u003c/p\u003e\u003cp data-pid=\"IG5rCuQm\"\u003e所以大家趁着读书的时候，感觉谈得来的赶紧谈吧，别指望毕业了有钱了就能找的好的。首先你不一定能有钱，其次，等你有钱了，有但是不多的情况下，你这个年龄匹配的，也不一定好找。\u003c/p\u003e\u003cp data-pid=\"HuFHWESu\"\u003e\u003cb\u003e（5）本职工作挣得多，壁垒强，那么一定要把业务能力拉满，趁着红利期赶紧赚钱攒钱。本职工作挣得勉强温饱，高不成低不就，本职工作挣得多但是不稳定容易被裁员，或者本职工作晋升无望，混吃等死等等。一定要想办法开展副业。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"3rADIsQu\"\u003e当年我们在校广播站编辑部这一批人。我是真没想到有人写小说一年能赚大几十万的，而且某些作品是小说细分领域的小爆款。\u003c/p\u003e\u003cp data-pid=\"b2KsVSVe\"\u003e也是玩的好，所以人家才跟你说这些事。先搞飞卢，后搞橙光，再写番茄。起点这种正经的人家不碰，琢磨自己擅长写的内容，那就是快餐爽文。\u003c/p\u003e\u003cp data-pid=\"pJ_SoobQ\"\u003e中石油某公司就业的他，现在副业比主业赚的多的多。我也是读研究生时候受他还有另一个搞数学竞赛的哥们的影响，才开始做知乎的。\u003c/p\u003e\u003cp data-pid=\"FcOwW4CZ\"\u003e我这个搞数学竞赛的哥们也是神人，考研985。自己当过家教，也开过辅导班，专门教高中数学和考研数学，自己做公众号和抖音，赚的盆满钵满。巅峰时期，一个3千左右的课，一年买了上千份。\u003c/p\u003e\u003cp data-pid=\"i1XHu7vU\"\u003e那时候我想着做点什么，就选了一个自己擅长的，开始慢慢写。\u003c/p\u003e\u003cp data-pid=\"IdQMgBSf\"\u003e后来才知道，考研按照学科做这个赛道，还是充满了活力，能动专业的，专门讲考研的，一年整个30多万很轻松。交通运筹学的，一年都是50万起。金融，计算机，控制，自动化，化工等专业那直接赚飞了。\u003c/p\u003e\u003cp data-pid=\"Q-IRB9Ix\"\u003e没办法，我这个专业的赛道我精力有限，也摆不下面子，赚不了那么多。\u003c/p\u003e\u003cp data-pid=\"6Ehr06ru\"\u003e后来对象偶尔看抖音，才知道，很多top985的学生都开抖音搞直播带货，还有一些海外名校的，哈佛、剑桥、普林斯顿和MIT，人家照样露脸开讲，没啥不好意思的。\u003c/p\u003e\u003cp data-pid=\"kr2_qpWz\"\u003e挣钱嘛，不丢人，自己找好自己的节奏就可以了。\u003c/p\u003e\u003cp data-pid=\"4WGRxAH_\"\u003e有些高校的创业现在怎么玩，趁着学校现在好申高新企业。把专利都放在公司，刷流水，申资质，拿下高新企业后转头就卖出去。顺带着做商赛，拿奖金，两头赚。\u003c/p\u003e\u003cp data-pid=\"bhJfHbdb\"\u003e你思路不打开，很多事情你玩不明白的。\u003c/p\u003e\u003cp data-pid=\"15M2e61o\"\u003e想好做什么就好好做，很多短期很难见成效，做着做着才有手感，才知道怎么做。\u003c/p\u003e\u003cp data-pid=\"Zcz7Sfeh\"\u003e\u003cb\u003e（6）拓宽人生的边界，找到自己真正感兴趣的事。多感受，多体验，多想想能不能在这个领域赚到钱，能不能在这个领域做到专业。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"rtH2DpWI\"\u003e我一个表侄念高一，读书一般般，但是打篮球打的很好，身高也够，1米9几，臂展也长，力量也够。\u003c/p\u003e\u003cp data-pid=\"n7BkGlKX\"\u003e他说想专门打篮球，他爸不同意，他爷爷也不同意。觉得他很不务正业，这个高中的时候，正是冲刺的时候，能不能考上好大学就看这三年。\u003c/p\u003e\u003cp data-pid=\"QYdJKx1V\"\u003e没办法，打篮球成为一个职业，在这个家庭的眼中几乎是不可能的。反正家里高中印象中身体素质比较好的，不怎么爱学习的，都是高中练习三年，准备考飞行员。理由是，飞行员待遇好，伙食好，转业后补助多，下来了分配的工作好。\u003c/p\u003e\u003cp data-pid=\"3j9ORfLu\"\u003e具体反正我也不知道啥情况。\u003c/p\u003e\u003cp data-pid=\"mDYGDLgv\"\u003e但是看他在县里的体育馆，跟一些20来岁30来岁打球挺厉害的人打，也是比较突出的。\u003c/p\u003e\u003cp data-pid=\"7UfPOGCc\"\u003e这种情况我个人觉得可以走一下特长。但是家长不同意。这也没办法，我也不能说。\u003c/p\u003e\u003cp data-pid=\"hpkFDe6g\"\u003e如果他打的不好也就算了，他打的真的挺好，身体素质也好，可以尝试一下走这条路。\u003c/p\u003e\u003cp data-pid=\"3eBPgy2b\"\u003e我想起那个时候，我有个表哥，专科学历，毕业了想自己去广东租个房干物流，但是家里人觉得太累了也不稳定，非要托人给他找工作，觉得别人有权有势的还开公司，肯定给帮忙找的工作好。\u003c/p\u003e\u003cp data-pid=\"qmhJXsMg\"\u003e那肯定的，找的一个大国企下面的子公司，一般人进不去，但是基本上去的都是有关系的，只是想找个地上个班。但是工资也是真的低，18年还是19年的时候，一个月2000来块钱，租个房完事了，吃饭都不够。现在听说一个月也才4000多，有时候加班5000多。值得说的就是公积金高一点，但是也并没有啥用。三班倒，直接能把人干废了。\u003c/p\u003e\u003cp data-pid=\"9FCl5QJD\"\u003e现在家里人想给他换个工作，自己都不愿意换了。\u003c/p\u003e\u003cp data-pid=\"FbeEPYNJ\"\u003e我有时候真的觉得，找点自己想干的事，看看有没有额外的发展。\u003c/p\u003e\u003cp data-pid=\"8rKTPz1_\"\u003e有时候都是大家看起来不稳定，没前途，浪费时间，甚至抛头露面比较让人没面子的事情，后来慢慢做出了名堂。\u003c/p\u003e\u003cp data-pid=\"LRT3GAFH\"\u003e一切新兴事物刚开始的时候是非共识的，也是小趋势，但是你能洞察这里面伴随的商机，愿意短期做出成绩，相信慢慢会得到应有的回报。\u003c/p\u003e\u003cp data-pid=\"JkUYJQh7\"\u003e有些事啊，都是干着干着就会了，就有想法了，就专业了。\u003c/p\u003e\u003cp data-pid=\"oAcjw2z1\"\u003e\u003cb\u003e（7）尽人事，听天命。有时候运气来了挡都挡不住。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"gvEknFu1\"\u003e有时候真的人生有一些运气成分在里面，但是还是要努力争取一下。\u003c/p\u003e\u003cp data-pid=\"t_UrAZnA\"\u003e最开始我关注到北航这个院校的时候，有个湖南二本学院的女生联系我，说想考北航，她的情况我看了一下，六级没过，数学一般，没有什么奖项荣誉，复习的晚，没学过可靠性，基本上不存在什么考上的可能。\u003c/p\u003e\u003cp data-pid=\"WEOM6RRs\"\u003e但是她跟我说，她要试一下，因为这个就业好，其他的985虽然可以，但是不一定有这个就业好，而且容易换专业。\u003c/p\u003e\u003cp data-pid=\"16Py9UT3\"\u003e然后她考了，那年分数很低，倒数第二名还是第一名上岸了。\u003c/p\u003e\u003cp data-pid=\"NR3rdf7X\"\u003e这很难评。\u003c/p\u003e\u003cp data-pid=\"d2BI8ovI\"\u003e她要是不去争取，她的成绩肯定考不上，才300出头，很多事情没办法，运气来了挡都挡不住。\u003c/p\u003e\u003cp data-pid=\"cxrzKfOO\"\u003e又比如，前几年一个师兄考选调生，本来没考上，结果扩招一个名额，顺位排名在他前面的都放弃了，他顺利替补进去了。\u003c/p\u003e\u003cp data-pid=\"y99dWYEb\"\u003e所以有时候，就是那么一丝幸运，这没法说。\u003c/p\u003e\u003cp data-pid=\"AQD4kTF4\"\u003e现在比较卷，再有这么好的运气也难。\u003c/p\u003e\u003cp data-pid=\"kMh3WzTf\"\u003e所以普通人就要想办法，给自己找那么一丝可能性。\u003c/p\u003e\u003cp data-pid=\"BI0dMsxa\"\u003e你不想办法，肯定不会有机会。\u003c/p\u003e\u003cp data-pid=\"JidiuhmW\"\u003e很多事情，就是做着做着就有思路了，格局就打开了。\u003c/p\u003e\u003cp data-pid=\"nlso_NMP\"\u003e像我们看一些高校的副教授教授，真的凭借工资是发不了财的，那肯定要做项目。\u003c/p\u003e\u003cp data-pid=\"7zKeVVlL\"\u003e你觉得难，不想去做，那就是没可能。你想方设法的找个点切入，可能慢慢就能打开局面。\u003c/p\u003e\u003cp data-pid=\"nof-lFTY\"\u003e但是前提你必须要投入很大的精力。\u003c/p\u003e\u003cp data-pid=\"Fra9unae\"\u003e\u003cb\u003e（8）若不撇开终是苦，各自捺住即人生。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"aITA_857\"\u003e很多规则是给非既得利益者制定的，有人吃到了规则的红利，就有人受规则所害。\u003c/p\u003e\u003cp data-pid=\"EUY0Tz--\"\u003e换句话说，一种形式的公平，其实也就是另一种形式的不公平。\u003c/p\u003e\u003cp data-pid=\"20WkOyeX\"\u003e在你的小圈子里，这是没办法的事。\u003c/p\u003e\u003cp data-pid=\"ETo67H1q\"\u003e那对你来说，所有别人对你的看法也好，对你的鼓励，表扬也好，贬低也罢。都是无所谓的事情，这对你没有什么影响。\u003c/p\u003e\u003cp data-pid=\"tueiuZCs\"\u003e一个规则有约束力，必须有激励在里面。大多数小圈子，还谈不到非激励因素。\u003c/p\u003e\u003cp data-pid=\"ryKEQCKF\"\u003e因此没有激励和奖惩措施的规则，遵不遵守其实无所谓，因为不会对人造成伤害。\u003c/p\u003e\u003cp data-pid=\"0K0Ynl-f\"\u003e那么什么情况下会造成影响呢？就是你个人的看法。\u003c/p\u003e\u003cp data-pid=\"JNKzGFRw\"\u003e你是一个好学生，那么对你施加的规则只能越来越多，你承受的必然越多。这就是精神压力。\u003c/p\u003e\u003cp data-pid=\"S5HqnK95\"\u003e这个事分配给我，不公平，我不干又怎么样？会扣我钱嘛？会得罪谁嘛？得罪了他又怎么样？不得罪他他就不会做这种不公平的分配嘛？所以没必要，敢于拒绝，敢于冷场，甚至敢于翻脸。\u003c/p\u003e\u003cp data-pid=\"8rQU-2Bd\"\u003e说白了就是\u003c/p\u003e\u003cp data-pid=\"WO4AOhT5\"\u003e你能干活，你就有干不完到活。\u003c/p\u003e\u003cp data-pid=\"CIGDuhLa\"\u003e你肯吃苦，你就有吃不完到苦。\u003c/p\u003e\u003cp data-pid=\"l-ul6C4d\"\u003e放下，不要在乎别人的看法，找到让自己舒服的生活方式就好了。\u003c/p\u003e\u003cp data-pid=\"zdoL4_fx\"\u003e\u003cb\u003e（9）找到适合自己生活的地方，把buff拉满。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"3bKYgfuL\"\u003e曾经我一个本科室友说，说在吉林四平，你一个月挣1万，你就想螃蟹一样你横着走，就是这么有排面。（别曲解哈，就是说这个挣得多。）\u003c/p\u003e\u003cp data-pid=\"ozLr0AzQ\"\u003e你应该找到适合你工作生活的地方。\u003c/p\u003e\u003cp data-pid=\"25eXLUZ7\"\u003e我有个高中同学，男生，硕士学历，父亲是乡长，因为飞来横祸，身体不太好了，现在任县里某个局的副局长。家里不是很有钱，但是也有一定积蓄。县里有2套房子，1套自己住，还有1套是县城周边的三层楼拆迁时候分的，另外补贴了小200万。小县城挺强了。\u003c/p\u003e\u003cp data-pid=\"FkJO5byq\"\u003e虽说现在是闲职，但是关系还在。他父亲想让他去市里或者县里当公务员，毕竟在家也能帮趁着点。\u003c/p\u003e\u003cp data-pid=\"1kzZg-gJ\"\u003e但是他不愿意，他想闯。去了郑州一个研究所。经常外派出差，工资一个月到手才8000多，累的都不想花钱，就租个房子吃个饭，用他自己的话来说，是自己给自己找罪受。\u003c/p\u003e\u003cp data-pid=\"bzwvKm30\"\u003e如果他在市里在县里，过得绝对不会是这样。这个五一想开了，给我打了电话，说不倔强了，辞职了。认清了自己的能力。\u003c/p\u003e\u003cp data-pid=\"bf7nC-Pp\"\u003e还有一个高中同学，女生，硕士学历。在广东一个民办专科当老师，一个月到手1万3还是1万5。假期四处旅游，要么就是各种看演唱会。生活的别提多滋润。\u003c/p\u003e\u003cp data-pid=\"EwuFgOcb\"\u003e本科及以上的同学就不说了，那更是各有各的滋润。\u003c/p\u003e\u003cp data-pid=\"hidBFnJh\"\u003e你应该有找到，适合你生活的城市，把buff叠满。\u003c/p\u003e\u003cp data-pid=\"31KfgMA_\"\u003e陆陆续续说了这么多，比较零散。\u003c/p\u003e\u003cp data-pid=\"OLEUzXSI\"\u003e大家读来一乐吧。\u003c/p\u003e\u003cp data-pid=\"ldBIHk4q\"\u003e多谢读完。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":74995,"favorite_count":2975,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 3612960178}","attached_info":"CqUGCNei8J626sH0oQEQBBoJNjg2OTcxNzk5IMeo0rYGKJQLMB9AlgFKMAobVFNfU09VUkNFX0JBU0lDX0lORk9fUkVDQUxMEgEwGAAgADoKeyJyYXciOiIifVoHNTc5NTM2MWIgYTg0NWI1N2NlYWQzNDA3YjljOGE5OGNhMzIyYzQ3MWJyCjM2MTI5NjAxNziKAQgzMzk3MTg1NKoBCXJlY29tbWVuZMIBIGI4NmNkNjdjN2M4MGYxZGQzZGZiNDRlNzRkYWE1ZDUx8gEKCAwSBk5vcm1hbPIBKAgKEiQyNWNkZWU2YS02MjczLTRiNTktYjBjYi1iMGViYzFlZjM5YzfyAQYICxICMjaCAgCIAtTy5836MpICIGI4NmNkNjdjN2M4MGYxZGQzZGZiNDRlNzRkYWE1ZDUxmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFkFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhtJbnRlcmFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhZSZXZpc2l0VmFsdWVXZWlnaHRSdWxlygIYUGVyaW9kSW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIUQ29udGVudEFnZVdlaWdodFJ1bGXaAhtUU19TT1VSQ0VfQkFTSUNfSU5GT19SRUNBTEzoAgT6AgtOT1JNQUxfRkxPV4oDIDAxNjM0YjVlYWNiYTQzN2Q4MDFmZjZkYTZlMTdlMGY2mgMNCgJ2MhAAGgVvdGhlcqgD88kE2AMA6gMRYmFzaWNfaW5mb19yZWNhbGz6Ax8SDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFgAQAiAQAkgQGTm9ybWFsmgQBNKAEAKgEALAEALoEBm1hbnVhbMIEAzE2MMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAgLbHwD+BBQAAAAAAAAAAiQVYG9GNkIXSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUakAYAoAaXAagGAZICJQoJNjg2OTcxNzk5EgozNjEyOTYwMTc4GAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"151_1750899227.262","type":"feed","offset":151,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899227,"updated_time":1750899227,"target":{"id":"1918876698696479659","type":"answer","url":"https://api.zhihu.com/answers/1918876698696479659","author":{"id":"758c48f1e72f9bef2fc4676bb42e367f","url":"https://api.zhihu.com/people/758c48f1e72f9bef2fc4676bb42e367f","user_type":"people","url_token":"wzwyz","name":"Manziel","headline":"网络/计算/体系架构","avatar_url":"https://pic1.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":133,"is_following":false,"is_followed":false},"created_time":1750275855,"updated_time":1750275855,"voteup_count":4,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"20691338","type":"question","url":"https://api.zhihu.com/questions/20691338","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pica.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"机器学习该怎么入门？","created":1357304890,"answer_count":0,"follower_count":0,"comment_count":17,"bound_topic_ids":[3084,4253,187240],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://pic1.zhimg.com/50/v2-c3420acb804fef80565bcbb95b825fb6_720w.jpg?source=b6762063","excerpt":"背景这篇文章记录了一些机器学习入门的探索过程。从一个简单的曲线拟合问题入手，用简单的例子讲解机器学习的基本原理，用类比和对比的方式展示ML中一些操作的目的和效果，用于理清机器学习中的一些基本概念。 曲线拟合 我们先从一个简单的问题入手，二次函数的拟合问题；我现在有一个二次函数在一个范围内的值的采样如下图所示，我应该怎么通过这些采样点来求解这个二次函数呢？   对于这个问题来讲其实分成两种情况，第一种情况…","excerpt_new":"背景这篇文章记录了一些机器学习入门的探索过程。从一个简单的曲线拟合问题入手，用简单的例子讲解机器学习的基本原理，用类比和对比的方式展示ML中一些操作的目的和效果，用于理清机器学习中的一些基本概念。 曲线拟合 我们先从一个简单的问题入手，二次函数的拟合问题；我现在有一个二次函数在一个范围内的值的采样如下图所示，我应该怎么通过这些采样点来求解这个二次函数呢？   对于这个问题来讲其实分成两种情况，第一种情况…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003ch2\u003e背景\u003c/h2\u003e\u003cp data-pid=\"d4HROv_m\"\u003e这篇文章记录了一些机器学习入门的探索过程。从一个简单的曲线拟合问题入手，用简单的例子讲解机器学习的基本原理，用类比和对比的方式展示ML中一些操作的目的和效果，用于理清机器学习中的一些基本概念。\u003c/p\u003e\u003ch2\u003e曲线拟合\u003cbr/\u003e\u003c/h2\u003e\u003cp data-pid=\"tCE4Gez4\"\u003e我们先从一个简单的问题入手，二次函数的拟合问题；我现在有一个二次函数在一个范围内的值的采样如下图所示，我应该怎么通过这些采样点来求解这个二次函数呢？\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-5d28509f0e6db51aa3cc055a6bd2f5c8_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1420\" data-rawheight=\"946\" data-original-token=\"v2-70a9ef2f311e996db534c10a05c29195\" class=\"origin_image zh-lightbox-thumb\" width=\"1420\" data-original=\"https://pic1.zhimg.com/v2-5d28509f0e6db51aa3cc055a6bd2f5c8_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"wt3Uf2H7\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e对于这个问题来讲其实分成两种情况，第一种情况是这个采样是没有误差的，那这个就简单了，我们可以随机选择三个点就可以通过解析式的方式直接解出这个二次函数。\u003cbr/\u003e这里主要看第二种情况也就是采样存在误差的情况。再存在误差的时候我们需要通过求解最小二乘法的方式来求出二次函数的三个系数，计算过程如下：\u003cbr/\u003e首先我们计算得到这个采样的最小二乘算式：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-50c1df2919796e0fc66fea9bbca280a2_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"274\" data-rawheight=\"86\" data-original-token=\"v2-f72c926a4e151f40624601541a5f50a1\" class=\"content_image\" width=\"274\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"F0fi0T3n\"\u003e\u003cbr/\u003e我们将它展开：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-d80dd5eed26e3cbf9369bedbb1f49daa_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"449\" data-rawheight=\"57\" data-original-token=\"v2-d80dd5eed26e3cbf9369bedbb1f49daa\" class=\"origin_image zh-lightbox-thumb\" width=\"449\" data-original=\"https://pica.zhimg.com/v2-d80dd5eed26e3cbf9369bedbb1f49daa_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"X7_yHKdJ\"\u003e\u003cbr/\u003e求最小值等同于对每个系数求偏导等于零，即：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-f8bfb1d9242792093d55eac4e2b09416_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"207\" data-rawheight=\"190\" data-original-token=\"v2-06c981834977f39c912861bba696f5f9\" class=\"content_image\" width=\"207\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"Vcr6tvOD\"\u003e\u003cbr/\u003e\u003cbr/\u003e所以我们就可以得到三个方程，并且解出三个未知数了：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-a9d2643477b7e9d3f04544f78c3e67f9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"434\" data-rawheight=\"119\" data-original-token=\"v2-5478e5b8f21bebf25cd1bea785d0946d\" class=\"origin_image zh-lightbox-thumb\" width=\"434\" data-original=\"https://pic4.zhimg.com/v2-a9d2643477b7e9d3f04544f78c3e67f9_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"D_4erITl\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e在我这个例子里面，这个函数的系数是(a=1, b=2, c= 2)，经过计算我们拟合出来的结果如下：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003ey = 0.9999x² + 2.0053x + 1.9546\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"toQp-CBT\"\u003e这是用最小二乘法，通过采样的结果去拟合函数、求出函数的系数的方法。\u003cbr/\u003e上面的模型我们是知道这个函数的解析式的（他是形如ax^2 + bx + c的），我只需求各项的系数；但是如果不知道这个函数长什么样子呢，例如下面这个曲线，应该如何去拟合这个曲线呢？\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-8417fb5c444d0a55222010e541e3244c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2874\" data-rawheight=\"1962\" data-original-token=\"v2-30db71fea34b7cc569f3d036ff8cab92\" class=\"origin_image zh-lightbox-thumb\" width=\"2874\" data-original=\"https://pica.zhimg.com/v2-8417fb5c444d0a55222010e541e3244c_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"syCxlblq\"\u003e\u003cbr/\u003e\u003cbr/\u003e这就是机器学习要做的事情，\u003cb\u003e即在不知道一个复杂函数的具体解析式、也无法对他建模的情况下，使用很多简单函数把这个复杂函数给拟合出来。从最早的手写数字模式识别，到现在的大语言模型，机器学习做的都是这一件事情。\u003c/b\u003e接下来看看机器学习是如何做到这一点的，以及他背后的理论支撑。\u003c/p\u003e\u003ch2\u003e复杂曲线拟合\u003c/h2\u003e\u003cp data-pid=\"ObBbvEi3\"\u003e\u003cbr/\u003e我们首先定义一个函数，假设这就是我们无法建模，但还是想要去拟合的这个复杂函数。为了计算的速度，这里用一个三次函数：\u003cbr/\u003e\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003ey_train = A * x_train * x_train * x_train + B * x_train * x_train + C * x_train + D + torch.randn(x_train.shape) * 2.5  # 添加高斯噪声\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"NkiXhF25\"\u003e\u003cbr/\u003e\u003cbr/\u003e我们使用pytorch构建一个简单的神经网络模型如下：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003eclass DeepNet(nn.Module):\n    def __init__(self, input_dim=1, output_dim=1, hidden_dim=32, num_layers=10, use_layer_norm=True, use_batch_norm=False):\n        super(DeepNet, self).__init__()\n\n        # 输入层\n        layers = []\n        layers.append(nn.Linear(input_dim, hidden_dim))\n        layers.append(nn.ReLU())\n\n        # 中间隐藏层（8层）\n        for _ in range(num_layers - 2):\n            layers.append(nn.Linear(hidden_dim, hidden_dim))\n            layers.append(nn.SiLU())\n\n        # 输出层（无激活函数）\n        layers.append(nn.Linear(hidden_dim, output_dim))\n\n        # 组合所有层\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"llszUzqg\"\u003e\u003cbr/\u003e\u003cbr/\u003e定义损失函数，并且训练模型：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003emodel = DeepNet(input_dim=1, output_dim=1, hidden_dim=16, num_layers=8, use_layer_norm=False, use_batch_norm=False)\ncriterion = nn.MSELoss()  # MSE Loss\noptimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam 优化器\n\n\nfor epoch in range(epochs):\n    # 前向传播\n    y_pred = model(x_train)\n    # 计算 Loss\n    loss = criterion(y_pred, y_train)\n    losses.append(loss.item())\n\n    # 反向传播 + 优化\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"eKX2N7gp\"\u003e\u003cbr/\u003e\u003cbr/\u003e可以得到loss曲线以及拟合结果如下：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-c1e3777cbfb5d2277e67c61a4642e03a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1784\" data-rawheight=\"890\" data-original-token=\"v2-86dcda35fc49a19eaed16d6111928be4\" class=\"origin_image zh-lightbox-thumb\" width=\"1784\" data-original=\"https://pic3.zhimg.com/v2-c1e3777cbfb5d2277e67c61a4642e03a_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-328378b1df7c4186e728d3fd1b950412_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1878\" data-rawheight=\"946\" data-original-token=\"v2-a942e7e39420324216fed64155f790b9\" class=\"origin_image zh-lightbox-thumb\" width=\"1878\" data-original=\"https://pic1.zhimg.com/v2-328378b1df7c4186e728d3fd1b950412_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"y_PqNyZE\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e上面的代码中有一个叫做SiLU的函数，他被称为激活函数，这个函数就是拟合的核心。我们经常听到一句话，激活函数赋予了神经网络非线性性，如果没有激活函数的话一个神经网络是没有非线性的，这句话是什么意思呢？\u003cbr/\u003e我们可以尝试删除激活函数再次训练，可以得到以下的模型。可以看到删除了激活函数之后无论如何训练模型都只能得到一条直线：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-ab6ae286f852def6ac1ce3df9af95065_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1868\" data-rawheight=\"938\" data-original-token=\"v2-48989603588a22ea193236e20b81150c\" class=\"origin_image zh-lightbox-thumb\" width=\"1868\" data-original=\"https://pic2.zhimg.com/v2-ab6ae286f852def6ac1ce3df9af95065_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"vqGgiMEg\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e再次修改模型的layers和hidden_dimmen，把他们都降低到1，相当于这时候模型里面只有一个激活函数，我们尝试拟合，拟合的效果如下，他就是这个激活函数的形状：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-f85b50922690bd0367a2988d23e4b893_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1836\" data-rawheight=\"950\" data-original-token=\"v2-3ff2c245bfd4555e34af06e494257bd6\" class=\"origin_image zh-lightbox-thumb\" width=\"1836\" data-original=\"https://picx.zhimg.com/v2-f85b50922690bd0367a2988d23e4b893_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"6EZ72Vck\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e随着我们不断增加模型的layers以及每一层里面的参数量，模型便可以使用更多的激活函数的组合来拼接出和需要拟合的曲线更加贴近的曲线：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-f707c2a33dab49b6f5b1eee0544062ee_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1746\" data-rawheight=\"944\" data-original-token=\"v2-cae4d243a73a2b5a4f121119d9e10a93\" class=\"origin_image zh-lightbox-thumb\" width=\"1746\" data-original=\"https://pic1.zhimg.com/v2-f707c2a33dab49b6f5b1eee0544062ee_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-48424f5db5d74531e21627f0e742003f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1858\" data-rawheight=\"954\" data-original-token=\"v2-ca3ea3ad7239906a7d603bf93912f23a\" class=\"origin_image zh-lightbox-thumb\" width=\"1858\" data-original=\"https://picx.zhimg.com/v2-48424f5db5d74531e21627f0e742003f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"38DChb5c\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e随着模型的参数量越来越大，曲线由更多的激活函数组成，曲线也更加的平滑，这是一个很像微分的过程。这里就理解了，上面我们说机器学习是用许多简单函数拟合一个复杂函数，\u003cb\u003e这个简单函数就是激活函数。\u003c/b\u003e与其说激活函数赋予了模型非线性性，倒不如说这个复杂函数就是由激活函数组成的。\u003c/p\u003e\u003cp data-pid=\"nXuj1Z-J\"\u003e\u003cbr/\u003e如果学过傅立叶变换的话可以类比为，神经网络中矩阵乘法的部分就是系数，而激活函数对应的就是三角函数；\u003cb\u003e傅立叶变换通过三角函数的组合去拟合时间序列信号，而深度学习网络通过激活函数去拟合更高维度的函数。\u003c/b\u003e有些复杂的函数就连最顶尖的科学家也实在想象不出来要怎么对他建模，举例来讲，要怎么用一堆像素点来建模解出这个像素组成的图像是猫还是狗，或者当我知道了某个人说话的前二十个字之后，怎么用这二十个字来预测他要说的第二十一个字，在这种无法建模的情况下，机器学习赋予了我们跨过建模过程直接拿到结果的能力。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-811b4a88b1e97edb2562072705471843_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1332\" data-rawheight=\"900\" data-original-token=\"v2-913b90fc152b0747ba05a18692e7bb87\" class=\"origin_image zh-lightbox-thumb\" width=\"1332\" data-original=\"https://picx.zhimg.com/v2-811b4a88b1e97edb2562072705471843_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"8NAmE3UW\"\u003e\u003cbr/\u003e模型的参数量越大，拟合的参数项就越多，模型的表达能力也就越强。所以有些很复杂的函数就需要更多的参数、更多的项来拟合，这也是为什么LLM模型有巨大的参数量，因为他要拟合的是一个非常非常复杂的函数，但是无论他有多复杂，都是用无数个SiLU（或者其他激活函数）拼出来的。\u003cbr/\u003e这是机器学习中比较重要的概念，理解了这个点之后后面的操作都是如何帮助这个模型更快更好的拟合到这个复杂函数上面。\u003c/p\u003e\u003ch2\u003e\u003cbr/\u003e\u003cbr/\u003e最小二乘法\u003c/h2\u003e\u003cp data-pid=\"LEn8NkOi\"\u003e\u003cbr/\u003e傅立叶变换里面我们是通过解析解直接求出各个频点的系数，但是在机器学习里不太一样，ML里面使用的是\u003cb\u003e求导+逼近\u003c/b\u003e的做法，为了理解ML是怎么逼近最优解的，这里要先理解什么是损失函数，可以从上面这个简单模型的损失函数来看。\u003cbr/\u003e在上面的拟合曲线里面，我们做了这样一个操作，求所有测量值和预测值的差的平方和最小，这也就是最小二乘法，这也是这个模型的损失函数：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-04277ea3b57664159b5fee8f1fc9eaa4_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"109\" data-original-token=\"v2-b7d31447769809ac5a5121784d234405\" class=\"content_image\" width=\"278\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"5Vuq1Hli\"\u003e\u003cbr/\u003e为什么要做这样一个操作呢？有些地方有讲到使用MSE的原因是他方便计算，也有的说平方可以让一个偏离较远的值加上一个较大的惩罚；这些说法都是有道理的，但是都没有触及本质。这里首先来讲为什么是最小二乘法，然后解释什么是损失函数。\u003cbr/\u003e使用最小二乘法的本质上是这样一个事情：\u003cb\u003e我们的采样本质上是对测量仪器的误差的采样。对结果的估计，其实是对测量仪器误差的建模；而由于测量误差是高斯分布，所以需要用最小二乘法来做极大似然估计。\u003c/b\u003e\u003cbr/\u003e例如我们要用一个温度计来测量某天的温度，那么每一次测量温度其实是对温枪的误差的一次采样；我们每次测量距离，是对测距仪的误差的一次采样。\u003cbr/\u003e举例来讲，假如我们测量了今天的温度，测量结果如下：\u003cbr/\u003e[25.7, 26.1, 26.0, 25.9, 26.2, 25.9]\u003cbr/\u003e我们希望求解温枪的测量误差，应该如何求解？假设温枪的误差为高斯分布：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-8e258cc58b310605b6b685243a97a8e1_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"139\" data-rawheight=\"59\" data-original-token=\"v2-8e258cc58b310605b6b685243a97a8e1\" class=\"content_image\" width=\"139\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"Fm1cc0ca\"\u003e\u003cbr/\u003e则做极大似然估计的目标就是让所有采样的联合分布概率最大，即求所有采样点的概率的乘积，让这个乘积最大：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-d8e8e561b08a27149856b163eba3335c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"98\" data-original-token=\"v2-674ad319e24b8324a7e3492512521807\" class=\"content_image\" width=\"350\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"hKWU1jtq\"\u003e\u003cbr/\u003e也就是：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-624f9929e61548e9f8e5a5d024f8bc95_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"208\" data-original-token=\"v2-e05b656fc8ac28f6691b17f8a5788ee5\" class=\"origin_image zh-lightbox-thumb\" width=\"668\" data-original=\"https://pic4.zhimg.com/v2-624f9929e61548e9f8e5a5d024f8bc95_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"lELjHa-W\"\u003e\u003cbr/\u003e而这又相当于让e的阶数最大（阶数的负数最小）：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-ccd0db0549ea6295eaab25d8f0d70ebf_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"353\" data-rawheight=\"216\" data-original-token=\"v2-0f262bf4761e97fb424d065d7545219a\" class=\"content_image\" width=\"353\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"ItdFLlkY\"\u003e\u003cbr/\u003e\u003cbr/\u003e这个就是最小二乘法的由来。所以说最小二乘法的原因是因为他可以\u003cb\u003e让多次采样高斯噪声的联合分布概率最大。\u003c/b\u003e我们可以通过求导=0的方式来解出这个函数里面的变量mu：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-46f0dd403f647934674616027ce0c445_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"267\" data-rawheight=\"84\" data-original-token=\"v2-66f64aef6a6089cd901eefa594d86b95\" class=\"content_image\" width=\"267\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"6jhVvZf-\"\u003e\u003cbr/\u003e解上面这个式子，我们就得到mu应该等于所有采样点的均值：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-499f04be0df26b7cb71f7b2322a447f2_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"197\" data-rawheight=\"84\" data-original-token=\"v2-499f04be0df26b7cb71f7b2322a447f2\" class=\"content_image\" width=\"197\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"vsxMaXRk\"\u003e这解释了为什么多次测量结果要求平均值，因为求所有采样点的平均值本质上是在计算最小二乘法的解，而计算最小二乘法又是在计算误差的最大似然概率。所以最大似然概率 = 求MSE = 求平均，在高斯分布下这三件事情是相同的，我用均值算出来的结果，和我用最小二乘法逼近出来的结果是相同的。\u003c/p\u003e\u003ch2\u003e\u003cbr/\u003e\u003cbr/\u003e损失函数\u003c/h2\u003e\u003cp data-pid=\"ZMGNzs43\"\u003e\u003cbr/\u003e了解了最小二乘法的原因，就可以开始理解为什么他是拟合类问题的损失函数了，以及更重要的，为什么模型需要一个损失函数了。我们最开始介绍了一种函数拟合的方式，使用解方程的方式直接把解析解给解出来，但是他的前提是我们知道函数的解析式：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-155ec956b60cc355159100e67b2ce181_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"434\" data-original-token=\"v2-ee544c00d10a76c40b8547488a3f6171\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-155ec956b60cc355159100e67b2ce181_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"p3Umjw_o\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e我们又实现了一个使用神经网络拟合函数的方式，然而在这个方式下\u003cb\u003e我们不想去解最优解，我们想通过一种方式不断的逼近最优解，损失函数存在的意义就是帮助去逼近这个最优解，损失函数和最优解的关系是损失函数达到最大值/最小值的时候，正好是最优解的解析解；通过损失函数我们就把一个解方程问题变成了一个逼近问题。\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e对于拟合问题，\u003cb\u003e求均值就是这个解析解，而最小二乘法就是这个损失函数\u003c/b\u003e；当最小二乘法达到最小值的时候，正好也是取均值的时候，通过解析解和拟合的方式都可以得到这个结果。\u003cbr/\u003e所以损失函数Loss Function是怎么来的（这个名字可能有点误导，我们也可以不叫他损失函数，可以叫他逼近函数或者优化函数），就是只要Loss Function在达到最大值/最小值的时候，正好是这个模型的最优解；loss function是模型优化的指南针：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-531f166ab8f6d4b4e077e2a8eaf39c04_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2492\" data-rawheight=\"1312\" data-original-token=\"v2-92535547d03429039cddb78661cdf5fe\" class=\"origin_image zh-lightbox-thumb\" width=\"2492\" data-original=\"https://pica.zhimg.com/v2-531f166ab8f6d4b4e077e2a8eaf39c04_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"dL4-EXUd\"\u003e\u003cbr/\u003e\u003cbr/\u003e那么我们能不能就不用损失函数呢？既然函数我们都已经有了（用非常多个激活函数组合起来的大型缝合函数），是不是也可以直接解出最优解？\u003cbr/\u003e其实这肯定也是可以的，有了解析式了，有了采样的值了，这必然是可以解的。只不过这种方式效率实在太低了，加入一个模型有一百万参数，我们就需要解一个一百万个变量的方程组：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-7d991cb683b977943c52c2e7101591ee_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2400\" data-rawheight=\"1326\" data-original-token=\"v2-c837a36f4b1d8824781140d35b46e8d5\" class=\"origin_image zh-lightbox-thumb\" width=\"2400\" data-original=\"https://pica.zhimg.com/v2-7d991cb683b977943c52c2e7101591ee_r.jpg\"/\u003e\u003c/figure\u003e\u003ch2\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e分类问题的损失函数\u003c/h2\u003e\u003cp data-pid=\"qzZYHf1L\"\u003e\u003cbr/\u003e在看懂了MSE损失函数之后，我们明白了一个道理，损失函数不是因为好计算而设计的，而是符合其真实的自然规律。损失函数要满足的条件第一就是可导，第二就是可以保证在极值的时候恰好是系统的最优解。接下来再看下分类问题的损失函数是怎么定义的，以及为什么要这么定义。\u003cbr/\u003e假设有这样一个二分类问题，举例来讲这是一个抛硬币的场景，抛硬币100次，其中正面40次，背面60次；那么抛硬币一次结果是正面和背面的概率分别是多少呢？当然可以通过解析解直接解出来，他应该是40%和60%，但是这里我们不想这么解，这里要用一个方法把这个结果给逼近出来。\u003cbr/\u003e\u003cbr/\u003e假设这个系统在拟合之后输出正面的概率为p\u0026#39;，输出反面的概率为1-p\u0026#39;，那么这样一个系统扔出(40, 60)的概率是多少呢？我们只要写出这个系统扔出(40, 60)的概率，再让他达到最大值，这个系统就被解出来了。\u003c/p\u003e\u003cp data-pid=\"K3GAp1JE\"\u003e我们假设n=100，m=40，这是一个二项分布，所以我们可以得到分布的概率为\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-c92cff137c53619408b3e6c0553d1d46_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"912\" data-rawheight=\"124\" data-original-token=\"v2-c92cff137c53619408b3e6c0553d1d46\" class=\"origin_image zh-lightbox-thumb\" width=\"912\" data-original=\"https://pica.zhimg.com/v2-c92cff137c53619408b3e6c0553d1d46_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"FVXRjYD3\"\u003e\u003cbr/\u003e我们把两边都开n次根号，可以得到这样的函数：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-4a8a6d403968792967a3abf9d4e316df_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"426\" data-rawheight=\"83\" data-original-token=\"v2-59b1f25e8442b5e415e23de23381994b\" class=\"origin_image zh-lightbox-thumb\" width=\"426\" data-original=\"https://picx.zhimg.com/v2-4a8a6d403968792967a3abf9d4e316df_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"9NIe8_YB\"\u003e\u003cbr/\u003e其中m/n以及(m-n)/n是真实的概率分布，所以我们可以对他进行改写：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-da87885d4600352546f3fd53c7af8486_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"442\" data-rawheight=\"79\" data-original-token=\"v2-dbac1bbee222537209291c2826934cb9\" class=\"origin_image zh-lightbox-thumb\" width=\"442\" data-original=\"https://pic3.zhimg.com/v2-da87885d4600352546f3fd53c7af8486_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"nO1R4ZsG\"\u003e\u003cbr/\u003e我们再对这个函数取log，就得到了这样一个函数：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-378173c84f21ffec06b7d8a723ce7aa0_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1300\" data-rawheight=\"508\" data-original-token=\"v2-62cbd2d12eb5dea2e489b18f6ad1d0d7\" class=\"origin_image zh-lightbox-thumb\" width=\"1300\" data-original=\"https://pic3.zhimg.com/v2-378173c84f21ffec06b7d8a723ce7aa0_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"KPXgj22n\"\u003e\u003cbr/\u003e\u003cbr/\u003e我们可以把这个函数的图给画出来，他的横轴就是我们对于结果的猜想p\u0026#39;。可以看到这个函数在取到极值的时候，p\u0026#39;和真实的概率是相等的。这个函数叫做交叉熵函数，\u003cb\u003e他所刻画的物理含义是在给定一些参数的情况下，一组随机结果出现的概率；而这个概率在最大值的时候，就是极大似然概率，也就是这个随机过程的模型的解。\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-c0acd642bdd3bce2ed25ddc03e5c844b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2400\" data-rawheight=\"1094\" data-original-token=\"v2-ca7903719f34af915e893f83defad902\" class=\"origin_image zh-lightbox-thumb\" width=\"2400\" data-original=\"https://pic4.zhimg.com/v2-c0acd642bdd3bce2ed25ddc03e5c844b_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"ZPIazRwS\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e那我想问能不能不用交叉熵函数，我就想用一个别的函数作为二分类问题的损失函数可不可以呢？就是从原则上来讲，只要符合“在极值的时候是系统的最优解”这个标准的函数都可以，剩下的就是收敛的快不快的问题，我们尝试在二分类问题上面使用最小二乘法，看看会怎么样：\u003cbr/\u003e可以看到在二分类问题上使用MSE也可以取到极值点，并且曲线也是平滑有弧度（比起交叉熵函数的话弧度较小），所以在二分类问题上使用MSE作为损失函数看起来也不是不行的，但是推测训练速度应该会更慢一些。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-31c243faf0acac669cc5ea8163109489_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2056\" data-rawheight=\"1094\" data-original-token=\"v2-5af5e3a4cd9a1333d62bad78628d6397\" class=\"origin_image zh-lightbox-thumb\" width=\"2056\" data-original=\"https://pic4.zhimg.com/v2-31c243faf0acac669cc5ea8163109489_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"6D_f7i4O\"\u003e\u003cbr/\u003e但是如果是多分类问题，这个就不一定了，可以把图画出来查看；而且如果曲线弧度不够的话，那么训练起来会非常的费劲。\u003cbr/\u003e到此，基本上可以理解什么是损失函数、为什么需要损失函数、以及为什么选择MSE和cross Entropy作为损失函数了。Again如果损失函数这个词不是很intuitive的话也可以记为优化函数/逼近函数。\u003cbr/\u003e接下来再看一些训练过程中的问题，也就是怎么沿着损失函数给出的指引不断逼近最优解；这些内容就是一些训练技巧了，他是人们长时间在拟合的过程中得出的一些经验，有些时候他有理论依据，也有时候他只是用起来效果好。\u003c/p\u003e\u003ch2\u003e梯度下降\u003c/h2\u003e\u003cp data-pid=\"oXW4Pjxo\"\u003e理解了复杂函数拟合和损失函数，机器学习最重要的东西就理解了，剩下的就是训练过程中的一些优化技巧了，接下来来看一些训练过程中的优化问题。\u003cbr/\u003e在训练模型的过程中我们使用了Adam优化器对其进行梯度下降的计算，要理解为什么要用这个优化器，我们可以先看下不使用的时候的效果。我们把Adam改成SGD梯度下降进行训练，会发现随着epoch增加，loss在某一刻突然变得非常的大，之后数据便溢出了：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-db27f26b7d61c5ff9d90f801dc73986f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1756\" data-rawheight=\"960\" data-original-token=\"v2-699524f027c73bff26f0344fd0507925\" class=\"origin_image zh-lightbox-thumb\" width=\"1756\" data-original=\"https://picx.zhimg.com/v2-db27f26b7d61c5ff9d90f801dc73986f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"WYTwfzV0\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e可以发现随着训练的过程loss反而越来越大了，可以判断出来模型没有收敛，反而随着训练误差越来越大。猜想原因是设置的学习率太大了，本来应该随着求梯度逐步收敛，但是因为每一步的步长太大，反而发散了，也就是本来是想沿着下坡走进沟里，结果走到天上去了：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-09b52516a2e64b0f8cf2ec624f141576_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2158\" data-rawheight=\"1398\" data-original-token=\"v2-7ae3bd33b19f81c0f7df545da61f8daf\" class=\"origin_image zh-lightbox-thumb\" width=\"2158\" data-original=\"https://pic3.zhimg.com/v2-09b52516a2e64b0f8cf2ec624f141576_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"BgpH-MA4\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e我们可以减少学习率再次尝试，但是可以发现如果学习率设置的太小了的话，又会出现另一种情况，就是模型似乎卡在某一个点上了，无论训练多久参数都不在变化：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-f954d3122807a8abeed7ca6886f342ca_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1796\" data-rawheight=\"956\" data-original-token=\"v2-b091b1e59e1f48487457a2c51df16ca6\" class=\"origin_image zh-lightbox-thumb\" width=\"1796\" data-original=\"https://pica.zhimg.com/v2-f954d3122807a8abeed7ca6886f342ca_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"6LCfjC28\"\u003e\u003cbr/\u003e训练后的模型长这个样子：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-15391592572ea4453d62eca4b964a8f8_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1778\" data-rawheight=\"918\" data-original-token=\"v2-49c864aec2c96fecfea85e27e9eb488e\" class=\"origin_image zh-lightbox-thumb\" width=\"1778\" data-original=\"https://pic1.zhimg.com/v2-15391592572ea4453d62eca4b964a8f8_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"tlfumusc\"\u003e\u003cbr/\u003e我们可以合理的推测，这种情况是模型的训练更新卡在了某一个局部最优点里面，但是由于这个“坑”比较深，而我们设置的学习率又太小，导致模型“跳不出这个坑”，也就永远的卡在了这个坑里面。\u003cbr/\u003e这两种情况的名字分别称为“梯度爆炸”和“梯度消失”。我们把每次训练之后的模型参数打印出来，查看梯度爆炸和梯度消失出现的时候梯度都是什么情况\u003cbr/\u003e梯度爆炸：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-2847852acb02bf0879cf80f1944dffb9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"883\" data-original-token=\"v2-54768a66b4b51b5f79a02b43ca463353\" class=\"origin_image zh-lightbox-thumb\" width=\"1580\" data-original=\"https://pic4.zhimg.com/v2-2847852acb02bf0879cf80f1944dffb9_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"wFg5J4op\"\u003e\u003cbr/\u003e梯度消失：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-57a4779d05dead6658f26d5362db1363_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1839\" data-rawheight=\"831\" data-original-token=\"v2-7cd9b8d98c226b689f60cd7f9d0ae6f8\" class=\"origin_image zh-lightbox-thumb\" width=\"1839\" data-original=\"https://pic4.zhimg.com/v2-57a4779d05dead6658f26d5362db1363_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"_vBiLv3u\"\u003e\u003cbr/\u003e可以看到梯度爆炸的时候，在forward以及backpropagation的时候随着更新，参数越来越大，参数更新的幅度也越来越大，forward时候计算出来的结果也越来越大，最后在一定epoch之后数据溢出；而在梯度消失的时候，梯度由于过小导致参数不再变化。\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003enormalization\u003c/h2\u003e\u003cp data-pid=\"jf0qux_5\"\u003e\u003cbr/\u003e梯度爆炸是一个震荡的过程，每次的前向推理和反向更新都会让参数变大，直到超出计算机能表达的离散数字范围。我们需要一种方式不让数据随着forward/backward而持续的变大/变小，看起来有两办法可以解决这个问题\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"mYRN33V6\"\u003e在不同的层级运用不同的更新参数，例如可以在前面的layers以较小的梯度更新参数，防止累积的参数在后面溢出；\u003c/li\u003e\u003cli data-pid=\"lFdbDdKV\"\u003e每经过一层就对输出做一次调整，把输出拉回一个合理的范围，也可以达到防止溢出的效果：\u003cbr/\u003e\u003cbr/\u003e先来看这第二种方法，这个方法叫做normalization，也叫做归一化，他的效果是把一组数据在分布不变的情况下进行rescale，他的公式以及达到的效果做法如图所示：\u003cbr/\u003e\u003c/li\u003e\u003c/ol\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-a1f6fd3b69c253b5d77ba994409bc1d5_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"142\" data-original-token=\"v2-056849c1f63b957bdbea80f93223dcff\" class=\"origin_image zh-lightbox-thumb\" width=\"490\" data-original=\"https://pic4.zhimg.com/v2-a1f6fd3b69c253b5d77ba994409bc1d5_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"omIi_EAI\"\u003e\u003cbr/\u003e效果：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-a266528d781f4ac8b3f1f0a0b9ab5bcb_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"442\" data-original-token=\"v2-9aed12e2a4a2b4fc7e1638ca93f03f84\" class=\"origin_image zh-lightbox-thumb\" width=\"1272\" data-original=\"https://picx.zhimg.com/v2-a266528d781f4ac8b3f1f0a0b9ab5bcb_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"RXIfxlQA\"\u003e\u003cbr/\u003e常用的有两种方法，batch normalization和layer normalization，他们的实现如下：\u003cbr/\u003ebatch normalize是针对一批数据的；他是把所有数据的每一个纬度，都分别归一化；这样带来的好处是纬度和纬度之间不会有很大的区别：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-5f11e8e081197bf5f9eb69fc7570d90e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1442\" data-rawheight=\"556\" data-original-token=\"v2-8b2544e7d063fb9ae0904d73cc152e73\" class=\"origin_image zh-lightbox-thumb\" width=\"1442\" data-original=\"https://pic1.zhimg.com/v2-5f11e8e081197bf5f9eb69fc7570d90e_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"qBgBSd4o\"\u003e\u003cbr/\u003eLayer Normalization是针对一条数据的，把一条数据的所有纬度进行归一化，但是大纬度还是大纬度，小纬度还是小纬度，这样带来的好处是数据不会有特别大的纬度，梯度不会爆炸；\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-8c6c1d7b8c0dff30ef39752242f37b7c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1516\" data-rawheight=\"468\" data-original-token=\"v2-55c4a3c206b413cb71ce86640b9afd13\" class=\"origin_image zh-lightbox-thumb\" width=\"1516\" data-original=\"https://pica.zhimg.com/v2-8c6c1d7b8c0dff30ef39752242f37b7c_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"qUOmqYSn\"\u003e\u003cbr/\u003e接下来通过对比的方式比较一下不同归一化的效果。在删除了ADAM优化器的模型上面添加batch normalization layer，并且再次训练，可以看到模型相比之前稳定了一些：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-a3dac8da9df956800225849200b090ed_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2294\" data-rawheight=\"1030\" data-original-token=\"v2-cdedc4cd57eddb7d758212f1ed1fe9f6\" class=\"origin_image zh-lightbox-thumb\" width=\"2294\" data-original=\"https://pic4.zhimg.com/v2-a3dac8da9df956800225849200b090ed_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"v45jyY2I\"\u003e\u003cbr/\u003e拟合效果：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-aacc458d956cbf259a1c0547d5401c64_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1796\" data-rawheight=\"944\" data-original-token=\"v2-f3470da64486fdec2c07f3482d55070c\" class=\"origin_image zh-lightbox-thumb\" width=\"1796\" data-original=\"https://pic3.zhimg.com/v2-aacc458d956cbf259a1c0547d5401c64_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"jflnqmFe\"\u003e\u003cbr/\u003e我们再尝试换成layer normalization，可以看到layer normalization相比batch的方式更加不稳定一些：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-8e0f9f5cdc6df762903026f18ae744fc_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1918\" data-rawheight=\"938\" data-original-token=\"v2-630f3dd45c6bcfc4183e2cec41c1f34d\" class=\"origin_image zh-lightbox-thumb\" width=\"1918\" data-original=\"https://pic1.zhimg.com/v2-8e0f9f5cdc6df762903026f18ae744fc_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"T7JJtzez\"\u003e\u003cbr/\u003e拟合出来的效果也有差距：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-c9faf4b944179aa256dea5a053af9acf_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1770\" data-rawheight=\"958\" data-original-token=\"v2-3b242e6e99b1170c7608511d999d6c7c\" class=\"origin_image zh-lightbox-thumb\" width=\"1770\" data-original=\"https://pic2.zhimg.com/v2-c9faf4b944179aa256dea5a053af9acf_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"562nZkaM\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e可以看到normalization通过对每一层数据进行归一化的方式减少了模型训练的不确定性，在每一层结束的时候都把数据“拽回”某一个均值附近，不让他随着层数的加深而变的越来越大。这是一个训练的经验/技巧，他让逼近最优解的过程变得更加平滑快速。\u003c/p\u003e\u003cp data-pid=\"Yekd_iv3\"\u003e\u003cbr/\u003e我们回到之前的问题，为什么adam优化器可以解决参数爆炸的问题？normalization方式是增加一个数据的归一化，防止数据在forward和梯度在backward的时候越来越大，也可以在梯度更新上面下手：如果参数更新的幅度太大了，\u003cb\u003e那我们就减少它的更新率，防止他变得越来越大；如果参数更新幅度太小，那就增加他的更新率，让参数不要卡在某一个点上不动了。\u003c/b\u003e\u003cbr/\u003e结合图从直觉上面理解：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-b4cb1e126ba56e9bdae9c6b4f11fcaa9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2618\" data-rawheight=\"1454\" data-original-token=\"v2-be903f32eea57dc3620cba318bb6e092\" class=\"origin_image zh-lightbox-thumb\" width=\"2618\" data-original=\"https://picx.zhimg.com/v2-b4cb1e126ba56e9bdae9c6b4f11fcaa9_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"g1KfI-Fh\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e所以要解决梯度的问题，要不然就让输入的数据稳定（normalization），要不就让梯度更新稳定（adaptive）。\u003c/p\u003e\u003ch2\u003e\u003cbr/\u003e过拟合\u003c/h2\u003e\u003cp data-pid=\"7yvUMXIr\"\u003e根据之前的分析，使用最小二乘法的目标就是让噪声的最大似然符合高斯分布。这在广义上是正确的，但是在微观层面上，不一定每一次的采样都是能保证噪声是均匀出现的，如果数据不够多的话那么对噪声的建模就无法做到准确，如果无法对噪声准确建模的话，对曲线的拟合就会出现拟合到噪声上面的情况。\u003cbr/\u003e这个是过拟合的现象，例如下图，曲线本身拟合到了噪声上面，而这个问题的本质原因还是噪声的样本太少，\u003cb\u003e不足以让模型分清楚什么是噪声、什么是信号\u003c/b\u003e。这个场景容易出现在数据样本比较少的情况下。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-564823b9aff06f4055433be0110e6940_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1778\" data-rawheight=\"944\" data-original-token=\"v2-3414092e5e68b514bbd7a9beb2151d13\" class=\"origin_image zh-lightbox-thumb\" width=\"1778\" data-original=\"https://pic1.zhimg.com/v2-564823b9aff06f4055433be0110e6940_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"fMNuA7Do\"\u003e\u003cbr/\u003e从本质上来讲，过拟合是因为噪声的样本太少，导致对噪声的建模不准确导致的；这种情况我们可以几种办法解决，例如增加样本的数量，或者也可以减少训练的epoch。（\u003cb\u003e减少训练epoch是一种trick的做法，是用一种误差去掩盖另一种误差。\u003c/b\u003e）\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e高斯分布的推导\u003c/h2\u003e\u003cp data-pid=\"niXiO7A7\"\u003e\u003cbr/\u003e其实前面对于最小二乘法的推导还有一个前提，就是高斯分布的公式是下面这样，我们的推导都是基于高斯分布的。因为高斯分布是这样的，所以我们能用这个公式来计算最小二乘法。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-3d22fe968e2b762507538053ad3c75d9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"368\" data-rawheight=\"94\" data-original-token=\"v2-6b40a62ac984f0c454f5f5d159c9ff54\" class=\"content_image\" width=\"368\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"BX41Wyol\"\u003e\u003cbr/\u003e那么高斯分布为什么会是这样呢？很多推导方式都能推导出高斯分布，但是他们都不太符合直觉，更类似于一种“证明方式”，来看下高斯本人是怎么推导出高斯分布的。高斯本人的推导非常符合直觉，他就是用均值反推出来了高斯分布的表达式。推导过程参考\u003ca href=\"https://link.zhihu.com/?target=https%3A//uploads.cosx.org/2014/07/gamma.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这篇文章\u003c/a\u003e，这里仅做摘抄。\u003cbr/\u003e高斯推导高斯分布的过程很有意思，他恰恰是把我们上面推导“最小二乘法”以及“m=均值”的过程给反过来了。高斯发现既然千百年来大家都认为算术平均是一个好的估计，那极大似然估计导出的就应该是算术平均！所以他猜测：我们要推导的这个误差分布导出的极大似然估计 = 算术平均值。\u003cbr/\u003e他的证明过程如下：设真值为 θ, x1, · · · , xn 为 n 次独立测量值, 每次测量的误差为 ei = xi − θ，假设误差 ei 的密度函数为 f(e), 则测量值的总的误差的联合概率为 n 个误差的联合概率，记为\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-889c203d68a800d7a0dec07ba9ab1e0f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"130\" data-original-token=\"v2-5f466b7e7ce844fdfe574288426994d1\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://picx.zhimg.com/v2-889c203d68a800d7a0dec07ba9ab1e0f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"pZ2ARi2x\"\u003e\u003cbr/\u003e对他求极大似然估计，得到：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-7fc8ffa64d5cbfd5ced195a871141e0e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"245\" data-rawheight=\"82\" data-original-token=\"v2-24033cdf4c1a51580b83ec393ae46ef0\" class=\"content_image\" width=\"245\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"YAELIKfx\"\u003e\u003cbr/\u003e整理可得：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-44eeab1cfe16857115db07fb3b9a323f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"359\" data-rawheight=\"116\" data-original-token=\"v2-f156695c81364228c37831ab654143da\" class=\"content_image\" width=\"359\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"p-ZQ8ymh\"\u003e\u003cbr/\u003e令：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-2e0db7e252a180ed8b383dc8156686f3_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"165\" data-rawheight=\"80\" data-original-token=\"v2-2e0db7e252a180ed8b383dc8156686f3\" class=\"content_image\" width=\"165\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"OxBuVL8X\"\u003e\u003cbr/\u003e可以写为：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-5e8257f896ad81913bca7a7d9b29007a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"116\" data-original-token=\"v2-2f29c094b49bca713a73a75688bc93dd\" class=\"content_image\" width=\"327\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"1TXaBfPY\"\u003e\u003cbr/\u003e由于已经假定了theta的解是x的均值，所以我们可以得到：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-a0dc8f52654cdc46412da3bb9fda3a68_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"290\" data-rawheight=\"109\" data-original-token=\"v2-e97a6eb556e48e0b4618be7bb6c27460\" class=\"content_image\" width=\"290\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"mCzn8lvu\"\u003e\u003cbr/\u003e根据这个式子我们可以得出两个结论，首先假设x均值=0，则可以得到：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-872462c768dca4f1750cdb553b5cb6eb_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"64\" data-original-token=\"v2-2d7247a09bcbbdf7046b83588d1f9ad9\" class=\"content_image\" width=\"240\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"GGqu85jD\"\u003e\u003cbr/\u003e接下来假设n = m + 1, 前m个x都为x，第m+1个x为-mx，则我们得到：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-d25661a1d3fb856657a0702bb528e8d5_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"69\" data-original-token=\"v2-0562512a17f8590b8bb0d53a174cbb00\" class=\"content_image\" width=\"268\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"Umh4rSKR\"\u003e\u003cbr/\u003e也就是说g(x)其实就是这个样子：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-63b9fd6117de0d9b34b0de4260a4a268_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"660\" data-original-token=\"v2-0022b0d1c135d7f022ace02042dad567\" class=\"origin_image zh-lightbox-thumb\" width=\"1192\" data-original=\"https://pic1.zhimg.com/v2-63b9fd6117de0d9b34b0de4260a4a268_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"4USV9p_y\"\u003e\u003cbr/\u003e\u003cbr/\u003e根据这个式子我们就可以求出来g(x) = c * x，于是就把f(x)给解出来了：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-42047c8471984d3a671b077f55948936_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"223\" data-rawheight=\"74\" data-original-token=\"v2-439304dfe4d898b448e73d0bb66e7189\" class=\"content_image\" width=\"223\"/\u003e\u003c/figure\u003e\u003ch2\u003e\u003cbr/\u003e输出函数\u003c/h2\u003e\u003cp data-pid=\"Md66pwjT\"\u003e\u003cbr/\u003e最后我们来看输出函数。上面的有些问题的答案不是经验，而是理论，例如为什么要用最小二乘法，为什么他不是最小三乘或者最小四乘，这是因为噪声的分布是高斯分布；但是有些问题不是理论推导出来的，而是实践中检验出来的经验，例如这个输出函数，当我们问起为什么要选择一个函数作为输出函数的时候很多时候得到的回答是“他的效果比较好”。\u003cbr/\u003e首先想：输出函数一定要是哪个函数吗？这不一定，回忆我们之前讨论的问题，机器学习是在用简单函数拟合复杂函数，那么输出函数是不是一个复杂函数呢？是的。所以他能不能也被拟合出来？也是可以的。所以就像下图展示的一样，分类问题最后一层也不一定要取softmax，\u003cb\u003e大不了我多用几层把softmax模拟出来就可以了\u003c/b\u003e——只这不过这样的模型训练起来可能会比较慢一些。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-0462e0cbdbd0ee93c5608901fe7390a5_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1754\" data-rawheight=\"1250\" data-qrcode-action=\"none\" data-original-token=\"v2-819a0db9286c74c9fb30e4f454776157\" class=\"origin_image zh-lightbox-thumb\" width=\"1754\" data-original=\"https://pic4.zhimg.com/v2-0462e0cbdbd0ee93c5608901fe7390a5_r.jpg\"/\u003e\u003cfigcaption\u003e不用softmax作为输出函数，强行拟合\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"kUK0zB95\"\u003e\u003cbr/\u003e\u003cbr/\u003e所以这里要明确一点，在输出函数的选择上没有绝对的对与错，只有相对的好与坏，没有哪类问题是必须用哪种函数的。我现在就给大家展示在一个分类问题下，如果不用softmax作为输出函数会怎么样，首先我们写一个分类问题模型，并且对MNIST数据集进行分类。由于在pytorch里面 CrossEntropyLoss 是包含了softmax+取log+交叉熵的，所以我们只需要这样定义一个非常简单的模型即可：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003eclass MNISTLogSoftmaxModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )   \n\n    def forward(self, x):\n        x = self.flatten(x)\n        log_probs = self.linear_relu_stack(x)\n        return log_probs\n\n\n# 初始化模型\nmodel = MNISTLogSoftmaxModel()\n\ncriterion = nn.CrossEntropyLoss()  # 关键修改：替换CrossEntropyLoss\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"bmXx7HZq\"\u003e\u003cbr/\u003e我们对模型进行训练，记录loss曲线：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-3774c2f860e1f3248be25e878379bbde_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3244\" data-rawheight=\"1854\" data-original-token=\"v2-3a6767b0598955de927a0f5defd9ec94\" class=\"origin_image zh-lightbox-thumb\" width=\"3244\" data-original=\"https://pica.zhimg.com/v2-3774c2f860e1f3248be25e878379bbde_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"3IpRojX-\"\u003e\u003cbr/\u003e他达到的效果如下：\u003cbr/\u003eTest set: Average loss: 0.0629, Accuracy: 9810/10000 (98.10%)\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e然后接下来我们对模型进行修改，删除他的softmax，在结尾增加logprob并且使用NLLLoss（注意由于要取log，所以同一个relu函数保证数据都是正的）：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-e339fc2a571f46cad6e8780804cab65f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1735\" data-rawheight=\"986\" data-original-token=\"v2-db0c653ecca7e268f6fdda32b17ad2df\" class=\"origin_image zh-lightbox-thumb\" width=\"1735\" data-original=\"https://picx.zhimg.com/v2-e339fc2a571f46cad6e8780804cab65f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"oCC6RIDc\"\u003e\u003cbr/\u003e可以看到模型最开始确实在不断收敛，但是到达0之后并没有停止收敛，而是随着训练误差越来越大了。在0附近曾经达到的最大的准确率可以达到：\u003cbr/\u003eTest set: Average loss: 0.3457, Accuracy: \u003cb\u003e5030/10000 (50.30%)\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e那么为什么softmax就让函数收敛了呢？我们可以做一个猜测，就是softmax里面这个e^x这一项保证了x无论是多少数据都是正的，这样减轻了模型训练参数的压力。为了证明这个猜想我们把softmax改成这样一个参数，这是我们魔改版的softmax，把e^x改成x^2，这样一来这个值也永远都是正的。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-5db0a7f9c7287c105525506e8da14d18_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"641\" data-rawheight=\"156\" data-original-token=\"v2-799c8e489425325d30ef540dfc83e347\" class=\"origin_image zh-lightbox-thumb\" width=\"641\" data-original=\"https://pic1.zhimg.com/v2-5db0a7f9c7287c105525506e8da14d18_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"ucXCffDe\"\u003e\u003cbr/\u003e然后我们对模型进行训练，其损失函数更新如下。可以看到模型也在不断的拟合，但是损失函数相比使用softmax函数收敛的不是很平缓：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-d537b5719ee8daa0e2c8e685a95cc042_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1712\" data-rawheight=\"986\" data-original-token=\"v2-b9f21a49a92a6302ed5e6267f400ac33\" class=\"origin_image zh-lightbox-thumb\" width=\"1712\" data-original=\"https://pic1.zhimg.com/v2-d537b5719ee8daa0e2c8e685a95cc042_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"UV9J7FUg\"\u003e\u003cbr/\u003e\u003cbr/\u003e这个输出函数达到的最好的效果如下，可以看到这个魔改的模型也达到了90%的准确率：\u003cbr/\u003eTest set: Average loss: 0.5393, Accuracy: 9025/10000 (90.25%)\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e最后我们看看softmax里面为什么选择e为底数。我们直接把e改成4，再次训练模型，看看收敛速度和效果有什么区别：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-df1bd64f29daa3a496c42d84fdeed2c9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"928\" data-original-token=\"v2-123ef0d5acec1738c34fbbab30230297\" class=\"origin_image zh-lightbox-thumb\" width=\"1624\" data-original=\"https://picx.zhimg.com/v2-df1bd64f29daa3a496c42d84fdeed2c9_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"XGtOuYXK\"\u003e\u003cbr/\u003e达到的最好效果如下：\u003cbr/\u003eTest set: Average loss: 0.0720, Accuracy: 9819/10000 (98.19%)\u003cbr/\u003e可以看到这个softmax的底在我这个简单场景下使用e还是其他的数字都可以较好的收敛。\u003c/p\u003e\u003cp data-pid=\"CS7qVC5e\"\u003e\u003cbr/\u003e最后我们把四种方式的train loss以及test loss画在一起展示一下，可以看到使用softmax和使用4作为底数都可以达到较好的效果。通常我们使用softmax的一个原因是他的导数计算起来会比较方便一些。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-d3c2d471504f5deec0fd9df867cbf079_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2800\" data-rawheight=\"1328\" data-original-token=\"v2-76b9a037899b5f265479881fd0b4e905\" class=\"origin_image zh-lightbox-thumb\" width=\"2800\" data-original=\"https://pic4.zhimg.com/v2-d3c2d471504f5deec0fd9df867cbf079_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"THbZ4iWS\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e最后来看下分布问题的输出函数，一般来讲他们是softmax函数（多分类问题）和logistic函数（二分类问题）。为什么在二分类上面可以用logistic regression来做，是因为在二分类问题上计算出来了P，那么相对应的1-P也就定下来了，所以在二分类问题上使用了logits即（p/(1-p)）的方式表示概率分布，所以这里可以理解模型最后一层的输出是在拟合ln(p/(1-p))。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-c2801115096391a7b2cc496802194512_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1762\" data-rawheight=\"718\" data-original-token=\"v2-1242e2b7e23599d5699fc11f65c63d41\" class=\"origin_image zh-lightbox-thumb\" width=\"1762\" data-original=\"https://pic1.zhimg.com/v2-c2801115096391a7b2cc496802194512_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"56DRlec9\"\u003e\u003cbr/\u003e\u003cbr/\u003e同样，对于对分类问题softmax也是一样的原理：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-27a4e867160c4de7ee594c444113642c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1656\" data-rawheight=\"1194\" data-original-token=\"v2-c2c0aa72f237d3878770f46377a3ba3e\" class=\"origin_image zh-lightbox-thumb\" width=\"1656\" data-original=\"https://pic1.zhimg.com/v2-27a4e867160c4de7ee594c444113642c_r.jpg\"/\u003e\u003c/figure\u003e\u003ch2\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e总结\u003c/h2\u003e\u003cp data-pid=\"GyCTd-GI\"\u003e\u003cbr/\u003e这篇笔记里梳理了一些机器学习的基本概念。\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e参考\u003cbr/\u003e\u003c/h2\u003e\u003cp data-pid=\"RvLO50X_\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/wxc971231/article/details/123866413\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003eblog.csdn.net/wxc971231\u003c/span\u003e\u003cspan class=\"invisible\"\u003e/article/details/123866413\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cbr/\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//uploads.cosx.org/2014/07/gamma.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003euploads.cosx.org/2014/0\u003c/span\u003e\u003cspan class=\"invisible\"\u003e7/gamma.pdf\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":198,"thumbnails":["https://picx.zhimg.com/50/v2-c3420acb804fef80565bcbb95b825fb6_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-58f2bc3f73d7f10a98a420642240a3ef_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-80ecdac6ead4cc3053f5572dda8a1235_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-c692cd1796a7e9e8c142a875a179d7c4_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-2c6e8bb1bba704b24fc4439f6f63d11e_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-0e75abd0d1ac31b609458e464f8be00d_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-c3c6f88c87cd3a3f800cb83feed73adc_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-aa9aaf392f6e1a3d0bc68e3ae6a9444e_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-9e635b230b885f29e6c9ac33423bcc11_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-78a7648a6b312a38410f837254d40c74_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-514147f3f6b73479d592d0ee4312f57b_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-84dd005db06f09b27bc09f273647f31b_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-211850969e1e77ed990e3cac824f9904_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-e0ee7f2d230e4d05c8e7003f4ba6a253_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-ca56961a4ffda01d6d5631ac0e242420_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-dfcaaed12beaab5d92b42d2c687850ee_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-3a684a92eed2f656a86dc91e98b89605_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-1a55a1ad911346a2377ea363314bad57_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-cce4a46015c399312bd3cae457fcf4f3_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-333186785ce0ca48a0d53c1f02f09f5b_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-7ad65dc534defa74d92495bb2a20f86f_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-d7bab48d37de53b50e9469f31f2b3919_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-3a2bfee57d5ca9e4f5dc607c0cf3c799_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-ea93c075662caeb5931b5841a9712f9a_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-2f2d5d624800498d5010f6da5b9e528c_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-3865bcb72aab480bf3787fdafcbd6ba0_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-893af38c32ed04c22b8448288f18b87c_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-9b9edd88b6b48a38419b226d4bdd61ea_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-1c9db269fe7b4476c298c6a8317c5d8d_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-c48b8e190eeab1deaa5e62ccffe34b62_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-b2b3ea6a2cbebf0bc11b7b481fceddad_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-94a37ea29a9224f9ded5c9cd8c015675_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-8cbe07273ef7fd8235a9f03f299f5ba3_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-c5a6d8f91e4837edf39d71c103162af1_720w.jpg?source=b6762063"],"favorite_count":18,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1918876698696479659}","attached_info":"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","action_card":false},{"id":"152_1750899227.670","type":"feed","offset":152,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899227,"updated_time":1750899227,"target":{"id":"1921036172769097697","type":"answer","url":"https://api.zhihu.com/answers/1921036172769097697","author":{"id":"ab943c8c3651b1bdfb128616ac082301","url":"https://api.zhihu.com/people/ab943c8c3651b1bdfb128616ac082301","user_type":"people","url_token":"chi-zhong-99-32","name":"月见人之多","headline":"法力无边","avatar_url":"https://pica.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":119,"is_following":false,"is_followed":false},"created_time":1750790714,"updated_time":1750874730,"voteup_count":138,"thanks_count":5,"comment_count":54,"is_copyable":true,"question":{"id":"14907123522","type":"question","url":"https://api.zhihu.com/questions/14907123522","author":{"id":"87e20cc1c4a8e65289bf6a208a28c7bc","url":"https://api.zhihu.com/people/87e20cc1c4a8e65289bf6a208a28c7bc","user_type":"people","url_token":"pai-gu-mian","name":"道系男儿","headline":"知乎参与者","avatar_url":"https://picx.zhimg.com/50/v2-eb1bffeb3d98433cc165122f3450f4a4_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":2401,"is_following":false,"is_followed":false},"title":"上海房价会不会再跌百分之50%？","created":1741913104,"answer_count":0,"follower_count":0,"comment_count":2,"bound_topic_ids":[1450,3865,38081,40443,71867],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://picx.zhimg.com/50/v2-6a38b2927d2ce00ba5fcba58b0031cd3_720w.jpg?source=b6762063","excerpt":"上海房子最大的问题是，毫无居住属性。 首先，1500w以下都是纸皮房子，1500w以上会不会厚实一点我不清楚，因为没住过。只能说1500w以下无论多高级的小区，隔音都无限趋进于0。包括不限于隔壁按开关，打电话，情侣吵架，训孩子，宠物奔跑等等，一清二楚。 第二，小区规模都特别小，10栋楼以下的小区比比皆是。同时高架桥修的到处都是，没有同小区大量楼栋互相遮挡的情况下，结果就是10层以上的中高楼层基本没有不被高架桥吵到的可…","excerpt_new":"上海房子最大的问题是，毫无居住属性。 首先，1500w以下都是纸皮房子，1500w以上会不会厚实一点我不清楚，因为没住过。只能说1500w以下无论多高级的小区，隔音都无限趋进于0。包括不限于隔壁按开关，打电话，情侣吵架，训孩子，宠物奔跑等等，一清二楚。 第二，小区规模都特别小，10栋楼以下的小区比比皆是。同时高架桥修的到处都是，没有同小区大量楼栋互相遮挡的情况下，结果就是10层以上的中高楼层基本没有不被高架桥吵到的可…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"Msfm9LWs\"\u003e上海房子最大的问题是，毫无居住属性。\u003c/p\u003e\u003cp data-pid=\"Ug56Czjg\"\u003e首先，1500w以下都是纸皮房子，1500w以上会不会厚实一点我不清楚，因为没住过。只能说1500w以下无论多高级的小区，隔音都无限趋进于0。包括不限于隔壁按开关，打电话，情侣吵架，训孩子，宠物奔跑等等，一清二楚。\u003c/p\u003e\u003cp data-pid=\"ZaUmIxhg\"\u003e第二，小区规模都特别小，10栋楼以下的小区比比皆是。同时高架桥修的到处都是，没有同小区大量楼栋互相遮挡的情况下，结果就是10层以上的中高楼层基本没有不被高架桥吵到的可能性。\u003c/p\u003e\u003cp data-pid=\"vY-1cyvM\"\u003e还有，就是养狗的特别多，如果不幸你的邻居养了，或者在别墅小区，就会每天清晨和傍晚，享受阡陌交通，鸡犬相闻的武陵人桃源生活。再加上小区绿化率通常很高，四点半开始鸟语花香，熬夜党喜提通宵党。\u003c/p\u003e\u003cp data-pid=\"R9646ER9\"\u003e更蛋疼的是，监管部门支持甚至鼓励施工单位夜间在施工，尤其市区，遇上路政持证上岗，凌晨开始一挖一个不吱声。而住宅装修施工的合法时间也很不人性化，早上7点到下午三四点，没有午休，那么工人的准备时间就是清早六点（或者更早），报警没用，而且周末施工貌似也合法（警察不管）。这就要对比其他大城市来看，广深是工作日上午8点以后可以装修，好一点的小区物业会控制到9点，并且中午午休2小时不让开工，晚上施工结束是67点，而且好一点的小区周末也是不让施工的。在装修扰民这方面，广深地区上班族和上学党受到的影响都在可控范围。\u003c/p\u003e\u003cp data-pid=\"cTVnWK9A\"\u003e好多人看到这个回答呀，添加稍微一个邪门的点——上海是国内群租房比例最高的城市top2（不知道北京啥情况，所以严谨起见用top2，广深因为城中村的存在群租并不普及）。首先定义一下群租，在本文的定义里，n➕1就是群租（因为居住属性很差，本文讨论的就是居住属性。防杠，我也知道本地监管部门的定义n➕1不算群租）。如果小区不高档，那么n➕1比例相当高；如果小区品质高档，且附近有高端写字楼，那么n➕1亦不可避免。n➕1难受的点在于，二房东会为了“提高居住属性”，在房间里安装很多很多厕所，对正常邻居的居住体验进行核动力打击。见过在6室6卫0厅里生娃（字面意思，提高人口出生率）的吗？我同事见过。很难想象上下左右如果有正常的业主是什么居住感受，毕竟能改出6室的可不是什么小户型啊，总价便宜不到哪去的。\u003c/p\u003e\u003cp data-pid=\"Kw8vn9Mf\"\u003e所以价格都是其次，上海是一个有钱也住不到正经房子的地方。即使是所谓的“翠湖天地”，一样临街临高架，抽象的一比。\u003c/p\u003e\u003cp data-pid=\"uHQy01iY\"\u003e特别敬佩上海土著的睡眠质量。\u003c/p\u003e\u003cp data-pid=\"5IM3xayh\"\u003e毕竟，“吵”是客观属性，并不以身价和固定资产拥有量而转移，不怕吵的人就是牛逼，值得敬佩，只是我目前修炼还不够～而已～\u003c/p\u003e\u003cp data-pid=\"y2heFFl-\"\u003e——补充——\u003c/p\u003e\u003cp data-pid=\"a-7Ig1x0\"\u003e怎么评论区还有破防的呢？写了这么多只是说在上海市区“吵”的问题很难解决，没有任何一句话是幸灾乐祸上海房价会降，也没有不让人买房啊，甚至全篇都没有参与讨论当前房价或者房租是否合理呀。有房的觉得会涨，没房的觉得会跌，而我只觉得会吵。请涨跌双方坚定自信，都跟这篇文章没关系，和谐社区，友善讨论。\u003c/p\u003e\u003cp data-pid=\"KyFIW74l\"\u003e——再更——\u003c/p\u003e\u003cp data-pid=\"ws7efyAQ\"\u003e说了这么多，全篇没有讨论过房子会不会跌，也没有不让任何人买房，怎么还是破防呢？豪宅卖空与破房子卖不出去不搭噶啊。就好像每天那么多人在上海登记结婚，依然不影响H女士需要在某乎给孩子征婚，对吧？\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-41aed1bc7475edf8f252f073a5d732ab_1440w.jpg\" data-rawwidth=\"600\" data-rawheight=\"156\" data-size=\"normal\" data-original-token=\"v2-41aed1bc7475edf8f252f073a5d732ab\" data-default-watermark-src=\"https://pica.zhimg.com/v2-5b39ec400e55e944571f7f80efcd6ec4_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-41aed1bc7475edf8f252f073a5d732ab_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-0efdc80f4498ee52ded73b70d0adbfa4_1440w.jpg\" data-rawwidth=\"960\" data-rawheight=\"2079\" data-size=\"normal\" data-original-token=\"v2-0efdc80f4498ee52ded73b70d0adbfa4\" data-default-watermark-src=\"https://picx.zhimg.com/v2-5ab3462273854a069c3c77bbb15d45d3_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-0efdc80f4498ee52ded73b70d0adbfa4_r.jpg\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":17585,"thumbnails":["https://picx.zhimg.com/50/v2-6a38b2927d2ce00ba5fcba58b0031cd3_720w.jpg?source=b6762063"],"favorite_count":45,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921036172769097697}","attached_info":"CtIHCNei8J626sH0oQEQBBoJNzMzODYwNjQyILrk68IGKIoBMDZAmAFKaQopVFNfU09VUkNFX0hPVF9DUk9TU19SRUFMX1RJTUVfTkVXX0NPTlRFTlQSNmhvdF9yZWNhbGxfcmVhbHRpbWVfdDpuZXc3ZDoyMDI1LTA2LTI2Om1pZGRsZXlvdW5nc3RlchgAIAA6AFoJMTEzOTYzMDYwYiBhODQ1YjU3Y2VhZDM0MDdiOWM4YTk4Y2EzMjJjNDcxYnITMTkyMTAzNjE3Mjc2OTA5NzY5N4oBCzE0OTA3MTIzNTIyqgEJcmVjb21tZW5kwgEgYWI5NDNjOGMzNjUxYjFiZGZiMTI4NjE2YWMwODIzMDHyAQoIDBIGTm9ybWFs8gEoCAoSJDFmZDBiNGEyLWJjNzUtNDg0MS05OGU0LTUyNTFhYzhlMGQ3NfIBBggLEgIyNoICAIgC1PLnzfoykgIgYWI5NDNjOGMzNjUxYjFiZGZiMTI4NjE2YWMwODIzMDGaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCFlJldmlzaXRWYWx1ZVdlaWdodFJ1bGXKAhhQZXJpb2RJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXaAilUU19TT1VSQ0VfSE9UX0NST1NTX1JFQUxfVElNRV9ORVdfQ09OVEVOVOgCAvoCC05PUk1BTF9GTE9XigMgMDE2MzRiNWVhY2JhNDM3ZDgwMWZmNmRhNmUxN2UwZjaaAw0KAnYyEAAaBW90aGVyqAOxiQHYAwDqAyJob3RDcm9zc1JlYWxUaW1lTmV3Q29udGVudFJlY2FsbGVy+gN9EgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAIQ2AQYnAEiI3YyLTQxYWVkMWJjNzQ3NWVkZjhmMjUyZjA3M2E1ZDczMmFiOi0IAhDABxifECIjdjItMGVmZGM4MGY0NDk4ZWU1MmRlZDczYjcwZDBhZGJmYTSABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQGbWFudWFswgQDMTcwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAACgtb3DP4EFAAAAAAAAAACJBVgb0Y2QhdI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRqQBgCgBpkBqAYAkgIuCgk3MzM4NjA2NDISEzE5MjEwMzYxNzI3NjkwOTc2OTcYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"153_1750899227.178","type":"feed","offset":153,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899227,"updated_time":1750899227,"target":{"id":"90006082787","type":"answer","url":"https://api.zhihu.com/answers/90006082787","author":{"id":"67b57f8eb6e0b1f376072a69ae710d12","url":"https://api.zhihu.com/people/67b57f8eb6e0b1f376072a69ae710d12","user_type":"people","url_token":"zhang-yu-ze-40","name":"影流沼泽","headline":"医疗忍者","avatar_url":"https://pic1.zhimg.com/50/v2-1e6082315a535391e5d8b0c1560ca459_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"医师资格证书持证人"}],"followers_count":11234,"is_following":false,"is_followed":false},"created_time":1738295800,"updated_time":1738384938,"voteup_count":4872,"thanks_count":401,"comment_count":181,"is_copyable":false,"question":{"id":"9092627046","type":"question","url":"https://api.zhihu.com/questions/9092627046","author":{"id":"ab89f831ed1f809bb89044bc19f0c192","url":"https://api.zhihu.com/people/ab89f831ed1f809bb89044bc19f0c192","user_type":"people","url_token":"zhu-meng-qi-hang-87","name":"雨过天晴干货铺子","headline":"喜欢读书.写作.分享","avatar_url":"https://pica.zhimg.com/50/v2-100998171b101e52b530e53f1a4dc9ba_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":266,"is_following":false,"is_followed":false},"title":"孩子为什么越大越不愿意跟我们说话？","created":1736351828,"answer_count":0,"follower_count":0,"comment_count":37,"bound_topic_ids":[428,2146,4719,20377,3769426],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"不光是你孩子，事实上同事，朋友，路人都觉得你说话没货，没有新闻。 没有当头棒喝，没有一语成谶，没有对明天，后天市场波动的完美预测，没有对他人生产力发展，身体心理健康，以及面临迷惑哪怕一点点有效的建议。 历史无数次证明，自以为是的车轱辘话，狗都不听。 路人也不愿意跟你说话，孩子倒了大霉不得不听你逼逼叨叨，无语离开乃是最大的温柔。 综上，人类永远只愿意听四种东西：鼓舞，故事，天气预报和赚钱门路。 爹妈说…","excerpt_new":"不光是你孩子，事实上同事，朋友，路人都觉得你说话没货，没有新闻。 没有当头棒喝，没有一语成谶，没有对明天，后天市场波动的完美预测，没有对他人生产力发展，身体心理健康，以及面临迷惑哪怕一点点有效的建议。 历史无数次证明，自以为是的车轱辘话，狗都不听。 路人也不愿意跟你说话，孩子倒了大霉不得不听你逼逼叨叨，无语离开乃是最大的温柔。 综上，人类永远只愿意听四种东西：鼓舞，故事，天气预报和赚钱门路。 爹妈说…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"YR8R_cAB\"\u003e不光是你孩子，事实上同事，朋友，路人都觉得你说话没货，没有新闻。\u003c/p\u003e\u003cp data-pid=\"qLHwGaNx\"\u003e没有当头棒喝，没有一语成谶，没有对明天，后天市场波动的完美预测，没有对他人生产力发展，身体心理健康，以及面临迷惑哪怕一点点有效的建议。\u003c/p\u003e\u003cp data-pid=\"Dnzpe_6v\"\u003e历史无数次证明，自以为是的车轱辘话，狗都不听。\u003c/p\u003e\u003cp data-pid=\"jE0E4rx4\"\u003e路人也不愿意跟你说话，孩子倒了大霉不得不听你逼逼叨叨，无语离开乃是最大的温柔。\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"Ut_nqTeS\"\u003e综上，人类永远只愿意听四种东西：鼓舞，故事，天气预报和赚钱门路。\u003c/p\u003e\u003cp data-pid=\"eyweD8nf\"\u003e爹妈说不出这四样，就好好把嘴闭上，好好把饭给孩子做好。营养均衡，岁月静好。\u003c/p\u003e\u003cp data-pid=\"cCKOf-HV\"\u003e孩子教育未必不成功。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":259126,"favorite_count":3168,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 90006082787}","attached_info":"Cv8FCNei8J626sH0oQEQBBoJNzExMTE1NzkwIPiT8bwGKIgmMLUBQJkBSkgKH1RTX1NPVVJDRV9aUkVDQUxMX0lURU1DRl9VUFZPVEUSH2RvY190eXBlOiBBbnN3ZXIKaWQ6IDYwOTkzODg0MgoYACAAOgBaCTExMjc5NTA1N2IgYTg0NWI1N2NlYWQzNDA3YjljOGE5OGNhMzIyYzQ3MWJyCzkwMDA2MDgyNzg3igEKOTA5MjYyNzA0NqoBCXJlY29tbWVuZMIBIDY3YjU3ZjhlYjZlMGIxZjM3NjA3MmE2OWFlNzEwZDEy8gEKCAwSBk5vcm1hbPIBKAgKEiQ4MjM4ZmU2NC0xODU3LTQyYjEtYjlmMi02MWRiMzI2ZWIzYjHyAQYICxICMjaCAgCIAtTy5836MpICIDY3YjU3ZjhlYjZlMGIxZjM3NjA3MmE2OWFlNzEwZDEymgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxl2gIfVFNfU09VUkNFX1pSRUNBTExfSVRFTUNGX1VQVk9URegCAvoCC05PUk1BTF9GTE9XigMgMDE2MzRiNWVhY2JhNDM3ZDgwMWZmNmRhNmUxN2UwZjaaAw0KAnYyEAAaBW90aGVyqAO26A/YAwDqAxl0ZXh0QWxsU2l0ZUFjdGlvbkl0ZW1DRlYy+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATKgBACoBACwBAC6BAZtYW51YWzCBAMxNjDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAGDZ0LM/gQUAAAAAAAAAAIkFWBvRjZCF0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFGpAGAKAGmgGoBgCSAiYKCTcxMTExNTc5MBILOTAwMDYwODI3ODcYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"154_1750899227.211","type":"feed","offset":154,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899227,"updated_time":1750899227,"target":{"id":"1921339809072420001","type":"answer","url":"https://api.zhihu.com/answers/1921339809072420001","author":{"id":"350c84beea202cbc0658ac0d5bc06147","url":"https://api.zhihu.com/people/350c84beea202cbc0658ac0d5bc06147","user_type":"people","url_token":"hei-shou-di-guo","name":"青峰哥","headline":"去地球搜：7852742 (交朋友)","avatar_url":"https://picx.zhimg.com/50/v2-44aa04b7e7a46b30b43f41a7e717076d_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":9615,"is_following":false,"is_followed":false},"created_time":1750863107,"updated_time":1750863107,"voteup_count":0,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"4805488130","type":"question","url":"https://api.zhihu.com/questions/4805488130","author":{"id":"cfc44cd63c7eb8fd61cdc2c40925fbed","url":"https://api.zhihu.com/people/cfc44cd63c7eb8fd61cdc2c40925fbed","user_type":"people","url_token":"fu-hua-bei-hou-de-jing-mi","name":"润泽","headline":"自己淋过雨，现在想为别人撑起一把伞！","avatar_url":"https://pica.zhimg.com/50/v2-fe4bc8caf469c9e347950795c8ec1871_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":336,"is_following":false,"is_followed":false},"title":"创业怎么让投资人放心？","created":1732240406,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[1353],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"你要懂，很多规律并不是你以为的那样先后分明，也不是你以为的那样规整，因果有时候是同时发生的，并不存在时间这个概念。 我昨天在抖音刷到一个片段，是宋丹丹和赵本山上台表演之前在对台词。赵本山说：“你不要固定顺序，能够抛出来的时候就抛出来，效果会更好。”宋丹丹对镜头抱怨：“他不让人固定包袱，这谁敢跟他演啊，也就我能接住了。” 讲真的，为什么现在的小品越来越难看了？其实就是这种高手越来越少了。基本上受限于…","excerpt_new":"你要懂，很多规律并不是你以为的那样先后分明，也不是你以为的那样规整，因果有时候是同时发生的，并不存在时间这个概念。 我昨天在抖音刷到一个片段，是宋丹丹和赵本山上台表演之前在对台词。赵本山说：“你不要固定顺序，能够抛出来的时候就抛出来，效果会更好。”宋丹丹对镜头抱怨：“他不让人固定包袱，这谁敢跟他演啊，也就我能接住了。” 讲真的，为什么现在的小品越来越难看了？其实就是这种高手越来越少了。基本上受限于…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"KfCtnlK8\"\u003e你要懂，很多规律并不是你以为的那样先后分明，也不是你以为的那样规整，因果有时候是同时发生的，并不存在时间这个概念。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"T7GAJvKU\"\u003e我昨天在抖音刷到一个片段，是宋丹丹和赵本山上台表演之前在对台词。赵本山说：“你不要固定顺序，能够抛出来的时候就抛出来，效果会更好。”宋丹丹对镜头抱怨：“他不让人固定包袱，这谁敢跟他演啊，也就我能接住了。”\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"wWbnX1FB\"\u003e讲真的，为什么现在的小品越来越难看了？其实就是这种高手越来越少了。基本上受限于各种框架和约定俗成的形式，已经没有创新的土壤了。所以看着就格外尴尬。这是无法勉强的。很多东西随着时代结束了，也就跟着落幕了。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"KGCDzMuN\"\u003e就如同以前唱片好卖的时代，那真的就出了非常多制作精良的歌曲，和实力非常强的歌手。但现在好听且有内涵的歌已经越来越少了，因为靠好歌和唱片为生的时代过去了，注定就决定了资本和人才不会再往这方面走了，自然也就落幕了。你要明白这些道理，看懂一个行业是兴衰，一个人的成败，其实不是自身的才华能够决定的，而是要顺着时代去做的。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"RMvIoiyg\"\u003e我说句严重的话吧，其实以大多数人的能力，今年开春之后蹦跶一整年，到了年底还是捉襟见肘的，也只会随着时间慢慢地走向山穷水尽，这几乎就是定数了。你别不信，也别嫌我说得悲观。如果你真心翻看一下自己这五年、十年走来的路，又有几件事真的顺遂，真的按着正常的路走就有好结果的呢？\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"7uXfQouR\"\u003e看过去，我们都是刺骨且诚实的，但为什么看未来就觉得自己就一定会好起来呢？这是不对的。我们可以对未来充满期许和希望，但依旧要看清楚自己所处的位置和各方面的局限，以及我们当下每一个决策会对未来产生什么样的影响。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"2mvI85Tz\"\u003e我为什么敢说大多数人的命数已经固定了？因为工资是固定的，工作的内容也是固定的，这里面就不会额外多出多少收入。而且每天走的路、际遇和麻烦也是固定的，一年到头其实都是看得到结局的。至于说破局，想都别想了，因为生活安逸的人是看不见潜在的危险的。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"y6HkFp71\"\u003e我也时常抱怨自己，为什么如此愚钝，不能早一些看清楚，不能早一些知道这个世界运行的规律呢。如果在二十出头的时候就知道该多好。但真的翻回去看二十出头时发的朋友圈，以及回想当年自己在做的事情，其实也就明了了——这小子不走到山穷水尽是不会真的开智慧的。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"CwyANSFS\"\u003e最后帮我们醒悟的，是我们经历的那些磨难与痛苦，是因为真的被社会狠狠地摔了几巴掌，才真的懂了自己的位置，懂了自己能够做什么。而且彻底把嘴巴闭上了，疯狂地收敛，极致地发展自己的一亩三分地，不再去惹不该惹的麻烦了，因为你能否强大、能否过得好，无非还是要解决自身的问题罢了。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"jYw1SfzW\"\u003e只要你想走捷径，那么最后一定会被捷径吞噬的。当然这句话很多人是不信的，我也是走过之后才把这个东西给戒了，而且过程极其难受，需要付出巨大的代价。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"avVAfjVg\"\u003e我可以这么说，那种真正富裕、实力强劲的大佬，他们真的就是在走康庄大道的，而且他们挣的每一笔钱都是让自己更好，也让别人开心的。这真的就是一种福报。而我们大多数人挣的钱真的是要把自己累死，然后还吸引了很多不满意的人来，结果你的自卑和别人的贪念狠狠地交织在一起，形成了一次又一次无情的绞杀。这不叫挣钱，这叫自虐。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Hx5iYQMa\"\u003e所以，我年少的时候常常一个人躲到图书馆看书，然后拿出纸笔把我过去和当下做过的事情写下来，然后反复不断地推演这些事情会在未来发生什么样的变化。我猛然发现其实里面很多东西都没有价值，并且会随着时间把我越困越死。这就是一场无解的死局，但无人会过问你的命数，因为大家都麻木地活着，去看本质的人其实很少。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"cfIQyMo_\"\u003e你要成为少数人，才真的有机会超越大多数人。因为财富是不会均匀播撒给大多数人的，真正能够抓住机会的注定是那些少数人。而且你真的往上爬了之后，就会发现自己过去做的事情是如此的蠢，明明有更好的解决方案，但就是因为自己的愚蠢和无知，选择了最难的那一条。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"CrJWNye8\"\u003e以我自己为例吧，我以前接一个单子，真的就是三百块钱的传单改了三四个通宵，客户还是不满意的。最后，因为改了太久了，我和我的合伙人都怒了，然后对方就骂我们是傻逼。我们眼睛一对，决定这一单不做了，也拿着电话一起骂：“你这个蠢货，源文件删了也不给你。”你看吧，有些生意啊，你越是付出就越是会遇到倒霉事的。现在我要挣这三百块钱，可能都不需要出声音了，随便授权一下我的内容给别人转载改编，几千几万就到手了。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"cofVrfyq\"\u003e你一定要去不断地优化自己的视野，优化自己赚钱的方式，然后远离那些太过于低级的挣钱方式。因为那些东西真的不会让人有什么未来。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"MT3qM2W-\"\u003e我以前也觉得行业肯定是平等的，只要你肯干一定是行行出状元的。但真的经历过几年的发展之后，我发现这个思维就是错的。一个行业如果本身就不行，和你五行不合的话，你越努力就会亏得越惨。你让一个本身就很内向的人去跑业务，怎么努力都是错的。你让一个外向的人去坐办公室写报表，他也会觉得如坐牢一般难受。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"nns-FaV7\"\u003e人要因材施教，人也要因势而发展。只有你找到自己适合的，才会真的成为，而不是想着慢慢做，一点点做好。网上有一个很火的段子，说你要先做成一坨狗屎，然后再去慢慢地改，但说真的，这个理论我觉得是不正确的。因为你让大多数人真的去做，他们连一坨狗屎都做不出来的。因为万事开头难，从零到一是最难的。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"E8SLVAER\"\u003e不信的话你可以试着去做一个很烂的账号，看看能不能从不好做到好。不信的话你去做一份烂工作，看看能不能从基层爬上管理岗。很多东西其实一出生就是顶尖水平了，只是随着时间把它本来的样貌露出来而已。你的天赋远比你后天的努力重要。我再说得绝对一些吧，那些真正有天赋的，哪怕不勤奋也会秒杀绝大多数人。之所以我们提倡勤奋，只不过是不希望芸芸众生绝望罢了，都是安慰剂。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"pHkNRAAg\"\u003e做事情的时机，永远大于努力。就像我当年做抖音一上来就是日入三百块的，而且还是很烂的内容，后面才有机会挣到上千万。如果一件事出师不利，那连小成都很难。固然万事开头难，但真的是你天命的事，一定会有甜头先给你。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"A73FOBWv\"\u003e如果你不能去做你真正有天赋的事情，那么你即便用尽所有力量，也不过是勉强解决温饱罢了。甚至越到后面生活都会越来越难，这就是大多数人一生的缩影。他们宁可忍受一生的平庸，却不愿意真的拿出一两年去拼搏。但你也不能苛责他人，毕竟如果你生在那样的家庭，也会无所事事。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"tQ13fg_T\"\u003e一个人的原生家庭，对一个人的制约和影响是非常大的。我甚至觉得那些被父母管太多的孩子，其实是比那些不被爱的孩子可怜的。因为不被爱起码长大了自己可以爱自己，想做什么就去做了。而那些被管得太多的，就形成了思维定势，循规蹈矩没有生气，一点点困难就退缩了。因为他们的意志早就被他们的父母阉割了，他们这一生的阴影就是他们做什么，都得不到父母的满意。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"SA-uwR_o\"\u003e真正的发展其实都是野性的，尤其是对于男性来说，所谓的发展和创造就是充满破坏性和掠夺的。如果这个东西被抹杀了，变成了一个乖乖仔的话，那么他向外界的索取也会变得极其困难。我身边那些真正拥有巨大财富的人，几乎都满足了一个条件，那就是他们不管是索取爱，还是索取财富都是非常自然的。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"yswWrt27\"\u003e这里几乎就与你说透了翻身的秘密，那就是你要创造一个索取爱、索取钱财非常自然的载体。而且在这个世界里面你必须是王者一样的存在，接受崇拜，万人敬仰，充满自信。因为只有一个人是充满自信的、是无边无际的，他才可能伸展自己的能量。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"FG_oTXmm\"\u003e你看那些颓废的、无路可走的人，几乎就是能量被压抑了，而且都是被生活或者工作打压到没有任何自信了，甚至气血和性功能都下降了。难道这不就是一种全新意识形态的阉割么？所以你要小心这一种阉割，你要去做那些能够让你自信的事情，而不是为了挣点钱把自己的尊严拿去卖了。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"goSODSxF\"\u003e我发展到后期，越来越发现，人的发展其实是一个务虚的事情。很多东西如果你不能深层次搞懂，且在思维层面不断地丢掉过去的旧枷锁，那基本上不用找人看，自己都能看清楚这一年的发展会如何了。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"vp4ZZBGB\"\u003e以上与你泄露了太多天机了，千万别等走到了山穷水尽才明白这些。你要利用无常，你要利用混乱，你要利用因果，去达到你的目的。你要不断地向上攀登，去看那些更厉害的人如何思考，如何处理事情。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"iLoVWDC5\"\u003e我很抱歉，并不是人人都能够成功。因为我们人这一生能发展的时间很短，一眨眼十年、二十年就过去了，而且通常踏错一步就没有翻身的机会了。所以我只能说，趁早开悟，趁早醒来。原本这个世界没有你的位置，但只要你出发得足够早，就一定可以利用概率找到一些缝隙。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"bgInGh5m\"\u003e最终成就你的，可能就是你缺乏的东西。比如我小时候就生活在农村，而且爸妈都在外打工，我生病了基本上都是自己扛的，几乎不吃药也不打点滴。长大以后发现自己的身体反而比别人要好。而那些条件比我好的孩子，你会发现他们长大之后，反而身体不如我们这些野孩子了。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"_38yGEFs\"\u003e祸兮，福之所倚；福兮，祸之所伏。福与祸相互依存，又互相转化。事情每时每刻都在不断地发生变化，尤其是今年一定会发生非常多的变化。而你要做的就是趁着大时代的趋势，积极地跟上变化，哪怕变得更糟糕，也许也是一种出路。\u003c/p\u003e\u003cp data-pid=\"zamfFrpg\"\u003e\u003cb\u003e写作费脑，码字不易，欢迎点赞，评论和关注，就当是对我的最大鼓励。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"0RISUZk2\"\u003e\u003cb\u003e知乎干货文章推荐，点击阅读：\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"hGanTEtW\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/449533459\" class=\"internal\" target=\"_blank\"\u003e从业互联网6年(个人经历)\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"Acge43Yh\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/7599675067\" class=\"internal\" target=\"_blank\"\u003e普通人如何改变命运？\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":25,"favorite_count":0,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921339809072420001}","attached_info":"Co0GCNei8J626sH0oQEQBBoJNzM0MDEzMjc2IIOa8MIGKAAwAECaAUokChlUU19TT1VSQ0VfV0FSTV9VUF9OT1JNQUwyEgEwGAAgADoASiIKF1RTX1NPVVJDRV9XQVJNVVBfUlVDRU5FEgEwGAAgADoAWgkxMTE5Mzc3MzFiIGE4NDViNTdjZWFkMzQwN2I5YzhhOThjYTMyMmM0NzFichMxOTIxMzM5ODA5MDcyNDIwMDAxigEKNDgwNTQ4ODEzMKoBCXJlY29tbWVuZMIBIDM1MGM4NGJlZWEyMDJjYmMwNjU4YWMwZDViYzA2MTQ38gEKCAwSBk5vcm1hbPIBKAgKEiRhZDlmMjU5YS05YzY4LTRmOGYtODI3ZS1iMDJmY2NhMTMyYzHyAQYICxICMjaCAgCIAtTy5836MpICIDM1MGM4NGJlZWEyMDJjYmMwNjU4YWMwZDViYzA2MTQ3mgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCGENvbnRlbnRXYXJtVXBCcmVha0luUnVsZdoCGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDLoAgL6AgtOT1JNQUxfRkxPV4oDIDAxNjM0YjVlYWNiYTQzN2Q4MDFmZjZkYTZlMTdlMGY2mgMNCgJ2MhAAGgVvdGhlcqgDGdgDAOoDC3RleHRfcnVjZW5l+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATKgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAwGSIrz+BBQAAAAAAAAAAiQVYG9GNkIXSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUakAYAoAabAagGAZICLgoJNzM0MDEzMjc2EhMxOTIxMzM5ODA5MDcyNDIwMDAxGAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"155_1750899227.378","type":"feed","offset":155,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750899227,"updated_time":1750899227,"target":{"id":"701554123","type":"article","url":"https://api.zhihu.com/articles/701554123","author":{"id":"106d7aa89b8dfd7528960c8f677335d6","url":"https://api.zhihu.com/people/106d7aa89b8dfd7528960c8f677335d6","user_type":"people","url_token":"randxie","name":"Rand Xie","headline":"多模态Reasoning是通往通用机器人的基石","avatar_url":"https://pic1.zhimg.com/50/84c51e44b_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":13103,"is_following":false,"is_followed":false},"title":"转行只有0次和无数次","comment_permission":"all","created":1717481739,"updated":1717481739,"voteup_count":813,"voting":0,"comment_count":39,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"上周, 我被邀请到一家公司去分享我的职场经历和最近一年做大模型的感受. 想着既然都花时间准备了PPT, 那还不如顺手写成文章再分享一波. 我一直相信经历的多样性也能让社会产生多样性. 希望我的分享能够让大家去尝试一些非常规的职业路径, 因为我的经历大概率会被HR当成反面教材, 关键词包括裸辞, 多次转行. 故事线在过去八年的职业生涯里面, 我的转行(职业转型)主要发生了3次. 从机械工程转型成Data Scientist从Data Scientist转…","excerpt_new":"上周, 我被邀请到一家公司去分享我的职场经历和最近一年做大模型的感受. 想着既然都花时间准备了PPT, 那还不如顺手写成文章再分享一波. 我一直相信经历的多样性也能让社会产生多样性. 希望我的分享能够让大家去尝试一些非常规的职业路径, 因为我的经历大概率会被HR当成反面教材, 关键词包括裸辞, 多次转行. 故事线在过去八年的职业生涯里面, 我的转行(职业转型)主要发生了3次. 从机械工程转型成Data Scientist从Data Scientist转…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"gvZmf9aq\"\u003e上周, 我被邀请到一家公司去分享我的职场经历和最近一年做大模型的感受. 想着既然都花时间准备了PPT, 那还不如顺手写成文章再分享一波. 我一直相信经历的多样性也能让社会产生多样性. 希望我的分享能够让大家去尝试一些非常规的职业路径, 因为我的经历大概率会被HR当成反面教材, 关键词包括裸辞, 多次转行.\u003c/p\u003e\u003ch2\u003e故事线\u003c/h2\u003e\u003cp data-pid=\"Wk4seFUi\"\u003e在过去八年的职业生涯里面, 我的转行(职业转型)主要发生了3次.\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"4RgOQCwo\"\u003e从机械工程转型成Data Scientist\u003c/li\u003e\u003cli data-pid=\"GclYXB_m\"\u003e从Data Scientist转型到ML Infra Engineer\u003c/li\u003e\u003cli data-pid=\"GQORyqsP\"\u003e从ML Infra Engineer转型去训练大模型\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"xeqRvh0D\"\u003e我会按照时间线来分享我的经历, 以及提供一些当时转行用的学习资料.\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e从机械工程转成Data Scientist\u003c/h3\u003e\u003cp data-pid=\"lr7YIY5Y\"\u003e我的本科和研究生读的都是机械工程(Mechanical Engineering), 第一份工作虽然Title是软件工程师, 但工作内容还是延续了我研究生时期的研究, 给Matlab写Toolbox. 比较值得分享的一件事情是, 我当初是通过QQ群来找到第一份工作的. 研究生期间, 因为想跟更多同行交流, 就加了各种控制理论的群. 在其中一个群认识了控制大佬姜老师, 就勾搭上了, 然后通过他的推荐拿到了面试. 有时候, 大家会认为社交一定得是\u0026#34;有意义的\u0026#34;. 但没有功利性的社交也许会带来意料不到的惊喜.\u003c/p\u003e\u003cp data-pid=\"FutFNi6t\"\u003e在加入第一家公司之后, 我发现Data science火得飞起. 偶然间, 我发现了Kaggle这个平台. 当时, Kaggle虽然用户已经挺多的, 但认真去打比赛的人还是不多. 印象中, 我打的第一个正式比赛是\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.kaggle.com/competitions/bosch-production-line-performance\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eBosch Production Line Performance\u003c/a\u003e. 想着自己好歹也是学机械工程的, 总得有些优势吧. 结果当然是被现实啪啪啪打脸. 但通过打这个比赛, 我感受到了处理数据的乐趣, 也开始体会到算力的重要性.\u003c/p\u003e\u003cp data-pid=\"4NXItyum\"\u003e因为已经开始工作了, 手头上有点闲钱, 咬咬牙就买了一块Nvidia 1080和各种配件DIY了一个深度学习工作站. 我当时对硬件一窍不通, 但通过研究\u003ca href=\"https://link.zhihu.com/?target=https%3A//pcpartpicker.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ePCPartPicker\u003c/a\u003e和Youtube各种视频, 顺利把机子装起来了.\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-022c6d68b72381ddb255a3f094a84a23_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"1280\" data-original-token=\"v2-a302e06794def64e6aa6a52ed67a937c\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-022c6d68b72381ddb255a3f094a84a23_r.jpg\"/\u003e\u003cfigcaption\u003e我组装的第一台深度学习工作站\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"-qi5TkrF\"\u003e有了武器之后, 我就有条件参加一些Computer Vision的比赛. 当时我的代码能力和深度学习的能力都很弱, 还处于只能把Caffe版本的Faster RCNN编译跑起来, 改改配置文件的阶段. 但我一直相信最高效的学习方法就是动手做起来, 再顺便把需要的知识补全. 于是, 我参加了第二个比赛\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.kaggle.com/competitions/the-nature-conservancy-fisheries-monitoring\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eThe Nature Conservancy Fisheries Monitoring\u003c/a\u003e, 训练一个模型去检测鱼的种类. 由于参与得比较早, 我很快就冲到了前10名, 从中获得了很多正向的反馈, 这让我坚持到了比赛的最后. 在学习一个新的领域, 不要想着一下子成为行业专家, 也不要迷信权威资料, 先尝试找到正反馈, 才可能坚持学习下去.\u003c/p\u003e\u003cp data-pid=\"QeKZYGva\"\u003e在2017年, 只要你在一个Kaggle比赛坚持到底, 拿一个铜牌不是问题; 只要你能把论坛提到的线索整合起来, 就能拿一个银牌; 而要拿金牌, 一个是靠命, 二是去找到别人想不到的线索. 在比赛的那几个月里面, 我在全职工作外, 每天都花4个小时去做各种尝试, 包括:\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Eldvxn2X\"\u003e读各种物体检测的论文, 并找相应的开源代码 (因为以我当时的水平, 实在是写不出来)\u003c/li\u003e\u003cli data-pid=\"-IK2B3RU\"\u003e坚持每天睡前开启模型训练, 早上起来分析一波结果并且开启下一轮训练. 等我下班回来, 又有新的模型可以分析了\u003c/li\u003e\u003cli data-pid=\"4Xlk-Ty3\"\u003e分析模型预测失败的情况, 做数据增强然后看有没有效果, 并且在网上找鱼类相关的数据\u003c/li\u003e\u003cli data-pid=\"M4IwqIo7\"\u003e重装系统, 重装CUDA (有自己服务器的都懂的)\u003c/li\u003e\u003cli data-pid=\"NUeBdlKb\"\u003e补充机器学习的基础知识, 主要啃ESL (\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eElement of Statistical Learning\u003c/a\u003e)和MLAPP (\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eMachine Learning - A probablistic perspective\u003c/a\u003e)\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"rCS5x9bD\"\u003e最后, 我在这个比赛顺利拿到了一个金牌, 之后的半年里面又陆陆续续参加了好几个比赛, 学习了XGBoost的打开方式, 也通过打NLP比赛学习怎么处理自然语言. 后面的这些比赛, 我参与的强度就降下来了. 正如我说的, 只要能够把论坛上的信息整合起来, 就能冲到银牌区. 在这半年里, 我拿了好多个银牌.\u003c/p\u003e\u003cp data-pid=\"-3FkvqM-\"\u003e因为工作比较闲, 我的内心又开始蠢蠢欲动了, 想尝试一份正式的Data Scientist工作. 当时我的本职工作是设计预测性维护的算法(就是预测一台机器什么时候会坏). 所以我一直都在关注这个领域的发展. 当时恰好看到一家芝加哥做工业互联网的Startup招Data Scientist, 面试官曾经也在Kaggle拿过金牌. 我做Predictive maintenance的经验也非常相关, 于是很顺利拿到Offer, 就屁颠屁颠跑过去芝加哥了, 正式成为Data Scientist.\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e从Data Scientist转型到ML Infra Engineer\u003c/h3\u003e\u003cp data-pid=\"nxyrAMy4\"\u003e我刚去Startup还没到一个月, 公司就开启了裁员模式. 真的是天有不测风云. 当时我刚刚拿到H1B工作签证, 一旦被裁只有30天的时间去找到新的工作 (后来这个grace period被延长到了60天). 我听到裁员的消息心里也是日了狗, 毕竟上一家公司可是2008年经济危机都没裁过人的. 第一次跳槽就给我来这种刺激.\u003c/p\u003e\u003cp data-pid=\"zeT5it4p\"\u003e但事情都发生了, 还不如赶紧行动起来. 下定决心要跑路之后, 我投了不少Data Scientist相关的工作, 快速过了一遍\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/1441923225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eAll of Statistics: A Concise Course In Statistical Inference\u003c/a\u003e就上阵面试去了. 基本上都是全军覆灭, 尤其是那些要求A/B Testing的工作. 当时非常天真得以为只要能搞懂Two-sample T-test就能搞定面试, 最后被现实狠狠打脸了.\u003c/p\u003e\u003cp data-pid=\"ZmzDQe5y\"\u003e恰好当时有身边的同事去了Google, 给我指了条明路. 那个时候, 入门级别的工程师只需要刷Leetcode就行. 既然这个面试流程都已经标准化了, 那我还不如直接冲一把. 下班回来基本上所有的时间都花在了研究算法题 (我上班还是很认真的, 也会尝试用学到的算法去优化代码, 一举两得). 这么搞了两三个月之后, 就海投了一把各种公司, 最后成功上岸了Google. 那个时候的美国经济还是欣欣向荣, 大公司都在疯狂扩招, 也算是赶上了一波时代的红利.\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-4006e470bc5c80524baca2bce3d723d1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"898\" data-rawheight=\"1042\" data-original-token=\"v2-f0d483f998fcb99e22d250a655e482e5\" class=\"origin_image zh-lightbox-thumb\" width=\"898\" data-original=\"https://picx.zhimg.com/v2-4006e470bc5c80524baca2bce3d723d1_r.jpg\"/\u003e\u003cfigcaption\u003e按照Tag去刷题\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"ESCp6DPf\"\u003e拿到offer之后, 需要等Google的工作签证办下来. 当时, 公司里面还有别的同事还在准备面试. 我心想反正也有点空, 那不如帮助一下大家, 毕竟都不容易. 于是我自己想了不少机器学习相关的面试题, 另外一个同事每天中午都订了一间会议室, 我们就边吃午饭, 边讨论各种面试题. 我记得有一天我们刚讨论完, 还没擦白板, Engineering Director走进来开下一个会议. 我就赶紧上去把白板擦了, Director看着我擦完白板, 微微一笑. 我记得这个Director在我走的几个月后, 也跑路了.\u003c/p\u003e\u003cp data-pid=\"JBvrKGWp\"\u003e在这次转行, Leetcode is all you need. 我其实也尝试去准备系统设计相关的面试, 读了一下那些经典的分布式系统的论文, e.g. BigTable, MapReduce. 每个字都看得懂, 但连起来看得我云里雾里的. 后来, 在工作中开始自己设计系统的时候, 再回来读这些论文, 就是另外的感受了. 系统设计这件事情, 真的就是实战出真知. 如果没机会去设计一个有足够复杂度的系统, 很难体会到那些论文里面的精妙之处.\u003c/p\u003e\u003cp data-pid=\"t0ZaBsV9\"\u003e顺利入职Google之后, 才发现要学的东西实在太多了. 举个例子, 我刚进去Google的时候, 连Protobuf是什么都不知道 (虽然Caffe也用了Protobuf), 也没用过Python type hint. 除了算法和数据结构, 我对软件工程基本上是一无所知. 但还好, Google是软件工程师的蓝翔技校, 里面有无数的设计文档可以阅读. 我当时在的部门叫Cloud AI, 算是Google里面比较卷的一个部门. 我的Mentor跟我吃饭的时候, 私下跟我说”你看哪个部门亚裔最多, 哪个部门就是最卷的”.\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-788db222a524abd4195e3bd12bbf0131_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1058\" data-rawheight=\"942\" data-original-token=\"v2-3c45debf98f5b04c438c298507c314fd\" class=\"origin_image zh-lightbox-thumb\" width=\"1058\" data-original=\"https://picx.zhimg.com/v2-788db222a524abd4195e3bd12bbf0131_r.jpg\"/\u003e\u003cfigcaption\u003e在谷歌云做的Feature Store产品\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"R6VA8OLT\"\u003e那个时候, 我还没娃. Google对新入职的员工也比较宽容, 我就边看内部的文档, 边看一下公开课去补充自己计算机科学的基础. 我印象中比较好的外部资料包括:\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"dT4S7Xmj\"\u003eDesign Data Intensive Applications\u003c/li\u003e\u003cli data-pid=\"SnNtlO5U\"\u003eCSAPP (Computer Systems: A Programmer\u0026#39;s Perspective)\u003c/li\u003e\u003cli data-pid=\"HFpkQ6sc\"\u003eAndy Pavlo的Database Systems\u003c/li\u003e\u003cli data-pid=\"AnSCYpfL\"\u003eDatabase Internals\u003c/li\u003e\u003cli data-pid=\"TuLeu2yY\"\u003e经典的分布式系统论文, e.g. BigTable, Spanner, Dynamodb, Spark, …\u003c/li\u003e\u003cli data-pid=\"IAThlJxA\"\u003e不同公司的Engineering Blogs\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"x2JBpLJa\"\u003e当时我还发现一件事情, 其实很多Engineers对机器学习算法和基础的统计是不懂的, 导致设计出来的工具看着很别扭, 或者工程味道太浓. 如果能够在ML能力和工程能力都达到一定深度, 是可以看到两边的结合点的. 可惜我当时的级别不够, 就算看到了机会, 也没有能力去真正推动一些事情. 对于Google这样的大型组织, Junior engineers的主要任务就是Execution, 而知识的广度需要在更高的级别才能产生出对应的威力, 因为知识的广度有助于连接不同的团队.\u003c/p\u003e\u003cp data-pid=\"Tno1ysNv\"\u003e在意识到这点之后, 我在Google待了两年半就离开了, 去了一家高速成长的互联网金融公司. 在我去的时候, 这家公司的ML Infra基本上是0. 所以有了一个从0到1搭建起整套ML infra的机会, 也有了更多对公司这种组织结构的理解. 在一个能够给到很多试错空间的环境, 我有很多收获, 包括:\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"rypVzIi0\"\u003e每周面3~4个候选人, 持续了快半年. 可能没到百人, 但也快了.\u003c/li\u003e\u003cli data-pid=\"TASidafV\"\u003e观察到一个组织的韧性, 当时公司经历了GME这个大的风波, 但整个组织在两周后就稳定下来了.\u003c/li\u003e\u003cli data-pid=\"BCqOlYVS\"\u003e在三次裁员中存活下来了, 心理承受能力得到了很大的提高\u003c/li\u003e\u003cli data-pid=\"TYAk6-yx\"\u003e在跟隔壁组的斗争中抢到了一个项目. 无关对错, 只是为了在经济下行的时间, 尽量保住自己的团队.\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"SW3x-WrK\"\u003e在这段经历里面, 我了解到从一个小白成长为一个还不错的工程师, 大概是需要3~4年时间的. 在工程问题以外, 人的问题往往是更加复杂的和不可预测的. 而处理跟人相关问题的能力, 是工程师成长到Senior+之后的一个重要的维度.\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e从ML Infra Engineer转型去训练大模型\u003c/h3\u003e\u003cp data-pid=\"TgSHBEs6\"\u003e首先, 当初我去做ML infra一个原因是觉得模型架构方面的进展不是特别明显, 短期内好像不会太多的突破. 当然, 后面ChatGPT的出现说明了我的判断是多么得不准确. 对于身处研究一线的同学, 应该能有更好的判断. 因为GPT-1, GPT-2, GPT-3的进化是一步一个脚印慢慢做出来的.\u003c/p\u003e\u003cp data-pid=\"1K3vpTa1\"\u003e后面为什么我会选择从ML Infra转型去做大模型呢? 其实最早让我萌生这个想法的是2022年9月份发布的Stable Diffusion. 那个时候, 我才猛然意识到, 原来生成式模型已经能做到这个效果了. 我一直埋头做Infra, 也没有太多关注模型方面的进展. 接着, ChatGPT发布了, 能够更加直接跟它交互, 直到今天, 我依旧觉得非常惊艳.\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-e31390062c2de6e431d02c0c50cbc6b4_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"3384\" data-rawheight=\"2036\" data-original-token=\"v2-4eea37a9b5d6e36c545b94f6b3c3bb27\" class=\"origin_image zh-lightbox-thumb\" width=\"3384\" data-original=\"https://pic3.zhimg.com/v2-e31390062c2de6e431d02c0c50cbc6b4_r.jpg\"/\u003e\u003cfigcaption\u003e当时做的Text-to-SQL产品\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"cpq2-QNd\"\u003e当时, 公司内部有很多人也非常兴奋. 作为这个公司唯一的ML Infra团队, 我自然能找到机会去尝试在产品里面落地. 我自己也在业余做了个Text-to-SQL的项目, 通过不断调用OpenAI的API, 去理解大模型的运转规律和限制. 我虽然发现了大模型的很多缺陷, 但这不影响我内心的兴奋. \u003c/p\u003e\u003cp data-pid=\"Xd7xf2Q9\"\u003e可惜, 当时我在一家互联网金融公司, 90%有价值的机器学习应用都在Fraud Detection (欺诈检测). 而大模型领域每天都在发生翻天覆地的变化, 我在那段时间一直都觉得自己在浪费生命. 于是, 我裸辞了, 可以花更多时间无干扰得学习相关知识. 这里友情提醒: 裸辞一时爽, 医保需谨慎. 如果你在美国工作, 裸辞之前记得要想好医疗保险怎么弄. 如果你的另一半有正式工作, 可以让另一半把你加到医保里面.\u003c/p\u003e\u003cp data-pid=\"LBCqWuL_\"\u003eHR看到这里, 大概率会觉得这个人是不是有毛病. 我的想法是, 如果这波大模型会对社会产生深远的影响, 我一定要去理解大模型里面的机理, 不然我会觉得用着不够安全. 而且我会很担心最强大的模型只掌握在一家公司手中, 这家公司可以决定谁能用, 谁不能用这项技术.\u003c/p\u003e\u003cp data-pid=\"_u38K9RZ\"\u003e在学习大模型的过程中, 我的主要学习资料就是各种Papers. 在之前的工作里面, 我一直都有保持读paper的习惯, 这个习惯能够让我不害怕直接从paper里面学习新的知识. 另外一个好的学习资料就是大模型部署框架, e.g. llama.cpp, vllm. 虽然没办法去真正训练一个大模型, 但我们可以从部署框架打开大模型的黑箱, 去看看里面是怎么一步一步计算出来的.\u003c/p\u003e\u003cp data-pid=\"pEVIBpFM\"\u003e最后的最后, 就是上岸了一家硬核做大模型的公司, 团队里面的人都非常Solid. 和同事交流是一个很有效的学习方式, 一来可以互相印证各自的想法, 二来可以交流分享看到的论文. 在我现在的公司, 终于有机会亲手去炼一个好的大模型, 对内在的机理也有了更加深刻的理解.\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eFinally\u003c/h3\u003e\u003cp data-pid=\"FJ5eSgfC\"\u003e我过去八年的故事就讲完了, 有几点收获吧:\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"39n9qcf0\"\u003e不要用所学的专业限制自己, 保持好奇心\u003c/li\u003e\u003cli data-pid=\"gDovs42t\"\u003e知识的广度可以是很好的工具\u003c/li\u003e\u003cli data-pid=\"KgG-vJ49\"\u003e\u0026#34;无用\u0026#34;的社交也许会带来意外的收获\u003c/li\u003e\u003c/ul\u003e","is_labeled":false,"visited_count":29160,"thumbnails":["https://picx.zhimg.com/50/v2-3ca96e8d41801975291e88c985201032_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-0c767f03f7a2f783e7a056082f171648_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-338182e5c39ab85c9e08097dec94c081_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-49a3d44bf373321eed56fc2e210094ec_720w.jpg?source=b6762063"],"favorite_count":923,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 701554123}","attached_info":"CuEHCNei8J626sH0oQEQBxoJMjQ0MDMxMjk0IIvi+rIGKK0GMCdAmwFKQQosVFNfU09VUkNFX1RXT1RPV0VSX1NIT1JUSU5URVJFU1RfUkVDQUxMX1RFWFQSATAYACAAOgp7InJhdyI6IiJ9YiBhODQ1YjU3Y2VhZDM0MDdiOWM4YTk4Y2EzMjJjNDcxYnIJNzAxNTU0MTIzqgEJcmVjb21tZW5kwgEgMTA2ZDdhYTg5YjhkZmQ3NTI4OTYwYzhmNjc3MzM1ZDbyAQoIDBIGTm9ybWFs8gEoCAoSJGFjMmI5YmFhLWU5NzgtNGJhYy05YTVkLTUwMDQ5MTIxMGI3NPIBBggLEgIyNoICAIgC1PLnzfoykgIgMTA2ZDdhYTg5YjhkZmQ3NTI4OTYwYzhmNjc3MzM1ZDaaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIYUGVyaW9kSW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIUQ29udGVudEFnZVdlaWdodFJ1bGXKAhdUZXN0ZWRBbmRXb3JrV2VpZ2h0UnVsZcoCHEJheWVzRmlyc3RMZXZlbElzb2xhdGlvblJ1bGXaAixUU19TT1VSQ0VfVFdPVE9XRVJfU0hPUlRJTlRFUkVTVF9SRUNBTExfVEVYVOgCA/oCC05PUk1BTF9GTE9XigMgMDE2MzRiNWVhY2JhNDM3ZDgwMWZmNmRhNmUxN2UwZjaaAw0KAnYyEAAaBW90aGVyqAPo4wHYAwDqAxpmZWVkX2F0dG1fdHdvdG93ZXJfdjJfdGV4dPoD2wESDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAxDABxiACiIjdjItYTMwMmUwNjc5NGRlZjY0ZTZhYTZhNTJlZDY3YTkzN2M6LQgCEIIHGJIIIiN2Mi1mMGQ0ODNmOTk4ZmNiOTllMjJkMjUwYTY1NWU0ODJlNTotCAIQoggYrgciI3YyLTNjNDVkZWJmOThmNWIwNGM0MzhjMjk4NTA3YzMxNGZkOi0IAhC4Ghj0DyIjdjItNGVlYTM3YTliNWQ2ZTM2YzU0NWI5NGY2YjNjM2JiMjeABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQGbWFudWFswgQDMTcwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAAAAwaq3P4EFAAAAAAAAAACJBVgb0Y2QhdI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRqQBgCgBpwBqAYAkgIkCgkyNDQwMzEyOTQSCTcwMTU1NDEyMxgHIgpJTUFHRV9URVhU","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=155\u0026desktop=true\u0026end_offset=156\u0026page_number=27\u0026session_token=a845b57cead3407b9c8a98ca322c471b","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=155\u0026desktop=true\u0026end_offset=156\u0026page_number=27\u0026session_token=a845b57cead3407b9c8a98ca322c471b","totals":0},"fresh_text":"推荐已更新"}
