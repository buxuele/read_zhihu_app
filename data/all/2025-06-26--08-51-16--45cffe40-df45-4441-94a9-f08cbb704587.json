{"data":[{"id":"66_1750899122.63","type":"feed","offset":66,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750899122,"updated_time":1750899122,"target":{"id":"1917883331829273687","type":"article","url":"https://api.zhihu.com/articles/1917883331829273687","author":{"id":"a0a65f22d9020a45b318638ffcd22877","url":"https://api.zhihu.com/people/a0a65f22d9020a45b318638ffcd22877","user_type":"people","url_token":"chenhong007","name":"小橘子","headline":"【vibe coding解决100个问题】，免费开源获取","avatar_url":"https://pica.zhimg.com/50/v2-dd9d86f35acdf90f8354c3b4b3cfcc07_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":11710,"is_following":false,"is_followed":false},"title":"百万网友在线热议！Devin炮轰Claude：别再搞Multi-Agent了","image_url":"https://pic1.zhimg.com/v2-478dfa72f5f3d8c62a10ce5626b22043.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1750039382,"updated":1750039382,"voteup_count":32,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"AI从业者，吃瓜群众，一定要停下来手中工作，认真读一下Devin不要构建智能体，以及Anthropic 如何构建智能体的回应。不要只是作为吃瓜群众，而应该认真想一下这些大公司走过的坑，他们都见过千万开发者使用AI，更有发言权，千万不要错过Multi-Agent这场世纪之争。 起因是发布每月500美元Devin的母公司，6月13日Cognition炮轰claude code，agent子任务之间无法并行工作，不要构建多智能体Multi-Agent系统。Anthropic 随后发博客回…","excerpt_new":"AI从业者，吃瓜群众，一定要停下来手中工作，认真读一下Devin不要构建智能体，以及Anthropic 如何构建智能体的回应。不要只是作为吃瓜群众，而应该认真想一下这些大公司走过的坑，他们都见过千万开发者使用AI，更有发言权，千万不要错过Multi-Agent这场世纪之争。 起因是发布每月500美元Devin的母公司，6月13日Cognition炮轰claude code，agent子任务之间无法并行工作，不要构建多智能体Multi-Agent系统。Anthropic 随后发博客回…","preview_type":"default","preview_text":"","column":{"id":"c_1631437224032829440","type":"column","url":"https://api.zhihu.com/columns/c_1631437224032829440","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pic1.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"AIGC大模型由入门到精通","imageUrl":"https://picx.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b","comment_permission":"public","intro":"AIGC大模型不再是少数人的专利","updated":1681745020,"is_following":false},"content":"\u003cp data-pid=\"uiqRaMQ9\"\u003eAI从业者，吃瓜群众，一定要停下来手中工作，认真读一下Devin不要构建智能体，以及Anthropic 如何构建智能体的回应。不要只是作为吃瓜群众，而应该认真想一下这些大公司走过的坑，他们都见过千万开发者使用AI，更有发言权，千万不要错过Multi-Agent这场世纪之争。\u003cbr/\u003e起因是发布每月500美元Devin的母公司，6月13日Cognition炮轰claude code，agent子任务之间无法并行工作，不要构建多智能体Multi-Agent系统。Anthropic 随后发博客回应，如何构建一个多智能体Multi-Agent系统。目前已经有百万人围观这个世纪之争。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-4dee766a1cb7346e7652326949b74beb_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"523\" data-original-token=\"v2-3baf23e80a21e044fd54add61c866d29\" class=\"origin_image zh-lightbox-thumb\" width=\"972\" data-original=\"https://picx.zhimg.com/v2-4dee766a1cb7346e7652326949b74beb_r.jpg\"/\u003e\u003c/figure\u003e\u003ch2\u003e\u003cbr/\u003eCognition：为什么不要构建多智能体Multi-Agent系统\u003cbr/\u003e\u003c/h2\u003e\u003cp data-pid=\"jCHpQmmU\"\u003e很多初学者认为Multi-Agent系统很拉风，各个Agent子任务像老黄牛一样做自己的工作。团队领导发号施令，LLM大模型又能承受100%的PUA，\u003cbr/\u003e你是不是也觉得多代理系统听起来很酷？让不同的AI代理各司其职，像一个团队一样协作完成任务，这想法确实很吸引人。可是现实往往没这么美好。\u003c/p\u003e\u003ch2\u003e从一个实际开发任务说起：经典小游戏Flappy Bird\u003c/h2\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-ddea904c0ed89514477d2f1b5b9c4df8_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"273\" data-rawheight=\"365\" data-original-token=\"v2-77e397a68991731be11a476d522ebafd\" class=\"content_image\" width=\"273\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"boFmze-F\"\u003e\u003cbr/\u003e假如开发一个Flappy Bird游戏，使用两个不同的Agent：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"zLMNrynA\"\u003eAgent A：负责制作游戏背景，管道和碰撞检测\u003c/li\u003e\u003cli data-pid=\"66WvSXQN\"\u003eAgent B：负责实现上下移动的小鸟\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"FTBXemJg\"\u003e实际开发情况，Agent A做出来超级马里奥兄弟那样的背景，而代理B虽然制作了小鸟，移动方式不对，图像是3D真实小鸟，和Agent A完全没办法合并任务。这就像让一个做西餐的厨师和一个做中餐的厨师合作做一道菜，结果是一团糟糕。\u003cbr/\u003eDevin提出：coding Agent开发第一原则是串行执行\u003cbr/\u003eDevin经过无数次血泪教训，总结出来Agent开发第一原则：使用单线程的串行Agent。让每个Agent都共享上下文，并且使明确的信息背景，不要每个Agent做假设。\u003cbr/\u003e这种方法有个巨大的优势：上下文是连续的，不会出现信息不连贯。每一步都能看到前面所有步骤的结果，做出的决策和行动，自然就更加一致。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-d5726a11342d6f4672e7771ab22763f4_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"743\" data-original-token=\"v2-72f83ef29ef4e129f1a2d185167a5c54\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-d5726a11342d6f4672e7771ab22763f4_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"TBmnz7kF\"\u003e\u003cbr/\u003e你可能会担心：如果任务很复杂，信息量过载，超过模型的上下文窗口长度，怎么办？\u003cbr/\u003e包括Cursor/cline等AI native IDE已经在做的一个方案：引入一个专门的LLM模型来压缩历史信息。这个模型的任务就是把之前的操作历史和对话压缩成关键细节、内容摘要总结。还有就是使用类似OpenMemory的机制，存储海量的上下文。\u003cbr/\u003e最有意思的是，Devin炮轰了anthropic公司的Claude Code。Claude code会创建子任务，但是从不让子任务代理并行工作。而是只负责回答问题，不负责写代码。各个子任务之间没有关联的上下文，并行实现肯定冲突。\u003cbr/\u003e2024年，包括Cursor在内容，很多AI native IDE在代码续写方便表现效果很差。当时流行的做法是使用\u0026#34;Apply模型\u0026#34;：让大模型输出编辑说明，然后用小模型来实际重写文件。\u003cbr/\u003e真正实现过的工程师都知道，小模型和大模型没有共享上下文，导致代码工程的代码编辑错误；2025年开始在单个模型完成代码学些的工作，可靠性大幅度提高。\u003c/p\u003e\u003ch2\u003e\u003cbr/\u003eDevin给初学者的建议\u003c/h2\u003e\u003cp data-pid=\"FrNAtzZq\"\u003e\u003cbr/\u003e对于代码类Agent，必须要使用单线程Agent。虽然这中方法不够炫酷，但是运行稳定，结果可预测，线上生产环境不出错，解决大多数代码类的问题。当真正需要处理超长上下文，投入相当多的实验和验证，上下文压缩机制的可靠性。\u003cbr/\u003e对于生产系统而言，安全第一，正常工作第一，不要被华丽酷炫外表迷惑。在2025年，Multi-Agent系统仍然是不可靠的，精力放在单个Agent，AI系统会走的更健壮更更远。\u003c/p\u003e\u003ch2\u003e\u003cbr/\u003eanthropic反驳：如何构建multi-agent research system\u003c/h2\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-3e3ac70191f626e0fad653505a494ef1_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"1000\" data-original-token=\"v2-6a0bf297b14675994331619940e36c95\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic2.zhimg.com/v2-3e3ac70191f626e0fad653505a494ef1_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"q0GswmaM\"\u003e\u003cbr/\u003eanthropic写的内容详实，专业并且具有指导性，但是其实没有回答Devin的问题，在代码领域如何使用Agent。\u003cbr/\u003eMulti-Agent多智能体架构--Research系统\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003eanthropic给出的是研究系统的架构，使用编排器-工作者模式的多智能体架构，其中主智能体协调过程，同时委托给并行操作的专门子智能体。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-a273af87cf68ccad8e0f3f04f6f87c84_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3840\" data-rawheight=\"2160\" data-original-token=\"v2-4bc330923994735ff2ab259f2d9b62ee\" class=\"origin_image zh-lightbox-thumb\" width=\"3840\" data-original=\"https://pic1.zhimg.com/v2-a273af87cf68ccad8e0f3f04f6f87c84_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"OD0TG_Df\"\u003e\u003cbr/\u003e当用户提交查询时，主智能体分析研究范围，制定策略，并产生子智能体同时探索不同方面。如上图所示，子智能体充当智能过滤器，迭代使用搜索工具收集信息，然后向主智能体返回公司列表，以便它可以编译最终答案。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-a6bbdeda7f72ba4da75345949c302435_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3840\" data-rawheight=\"3840\" data-qrcode-action=\"none\" data-original-token=\"v2-819a0fa0a571db5fd0041b26aec50030\" class=\"origin_image zh-lightbox-thumb\" width=\"3840\" data-original=\"https://picx.zhimg.com/v2-a6bbdeda7f72ba4da75345949c302435_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003eCognition VS Anthropic谁对谁错\u003c/h2\u003e\u003cp data-pid=\"g_OjYLVe\"\u003e\u003cbr/\u003eDevin 要实现的单一 Coding 任务，目标是代替高薪程序员，多个子 Agent 之间需要共享上下文，而当前多智能体不太能够进行这种需要长期上下文和主动性的对话，其可靠性远不如单智能体；\u003cbr/\u003eClaude 利用 Multi-Agent 实现其 Research 能力，本质上是由于主任务分派的各子任务的关联性不强；没给任务都可以单独完成：比如Agent A完成X公司的调研报告，Agent B完成Y公司的调研报告；Agent之间不冲突，不需要共享上下文。\u003cbr/\u003e从工作上的稳定性，实用性，复杂度，工作流等，我是坚挺Cognition ，现阶段各个应用都在套壳LLM大模型，本质上是Prompt工程。没必要为了炫技，搞的太复杂。亲爱的读者，你觉得那？\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"RNOjfyVC\"\u003e如果你也想在AI时代开发一个小项目，也许你完全不懂编程？也许你学过编程但是不懂如何从0到1搭建一个MVP项目？欢迎订阅我公众号，这是一个以Augment/Cursor实战为主，以实战案例的形式开发几十个项目，包括：网站、微信小程序、浏览器插件、App应用、Dify+Coze智能体。 \u003c/p\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//xiaojuzi.feishu.cn/wiki/A0ZAwyI2FiZ1B8k8fcocCH1wnqh\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003exiaojuzi.feishu.cn/wiki\u003c/span\u003e\u003cspan class=\"invisible\"\u003e/A0ZAwyI2FiZ1B8k8fcocCH1wnqh\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cp data-pid=\"YpYPq7hT\"\u003e以上就是今天的全部内容，如果你觉得不错，\u003cb\u003e欢迎点赞、在看、转发三连\u003c/b\u003e，帮助更多的朋友，这对我也很重要。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-543ef6b22cbc4067c2488e255d5e338a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"853\" data-rawheight=\"1280\" data-original-token=\"v2-c04048f9684e7727c57bf105b97668d6\" class=\"origin_image zh-lightbox-thumb\" width=\"853\" data-original=\"https://pic3.zhimg.com/v2-543ef6b22cbc4067c2488e255d5e338a_r.jpg\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":2645,"thumbnails":["https://pic1.zhimg.com/v2-478dfa72f5f3d8c62a10ce5626b22043.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-a38b5d1acd010dc3c9ac29cb38bd19e0_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-dd0803b5a803109a133f461d3e65e228_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-14709a927b7ec43910c9edafca295cb5_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-1e790ca651ad0ec4be1464a052026832_720w.jpg?source=b6762063"],"favorite_count":101,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1917883331829273687}","attached_info":"CrgJCOKDvb/vqZLnjwEQBxoJMjU5MTMwMTMxINb2vcIGKCAwAEBCSkkKH1RTX1NPVVJDRV9aUkVDQUxMX0lURU1DRl9VUFZPVEUSIGRvY190eXBlOiBBcnRpY2xlCmlkOiAyNTkxNDYxNTkKGAAgADoAWggxMzI2Mjk4N2IgYTg0NWI1N2NlYWQzNDA3YjljOGE5OGNhMzIyYzQ3MWJyEzE5MTc4ODMzMzE4MjkyNzM2ODeCAV9odHRwczovL3BpYzEuemhpbWcuY29tL3YyLTQ3OGRmYTcyZjVmM2Q4YzYyYTEwY2U1NjI2YjIyMDQzLmpwZz9zb3VyY2U9N2U3ZWY2ZTImbmVlZEJhY2tncm91bmQ9MYoBFWNfMTYzMTQzNzIyNDAzMjgyOTQ0MKoBCXJlY29tbWVuZMIBIGEwYTY1ZjIyZDkwMjBhNDViMzE4NjM4ZmZjZDIyODc38gEKCAwSBk5vcm1hbPIBKAgKEiRkZDUxM2I0Yi04MzY0LTQ1NzAtOGMxYy1hMmI2NjYwNWQ1ZWLyAQYICxICMTKCAgCIAti/4c36MpICIGEwYTY1ZjIyZDkwMjBhNDViMzE4NjM4ZmZjZDIyODc3mgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxlygIXU2FtZUF1dGhvcklzb2xhdGlvblJ1bGXaAh9UU19TT1VSQ0VfWlJFQ0FMTF9JVEVNQ0ZfVVBWT1RF6AIE+gILTk9STUFMX0ZMT1eKAyBlYzc1NWE4MmNkNjQ0OTA0YTYzYjhlMjljZmUxYzMwMpoDDQoCdjIQABoFb3RoZXKoA9UU2AMA6gMZdGV4dEFsbFNpdGVBY3Rpb25JdGVtQ0ZWMvoD6AISDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAhDMBxiLBCIjdjItM2JhZjIzZTgwYTIxZTA0NGZkNTRhZGQ2MWM4NjZkMjk6LQgDEJECGO0CIiN2Mi03N2UzOTdhNjg5OTE3MzFiZTExYTQ3NmQ1MjJlYmFmZDotCAIQsAkY5wUiI3YyLTcyZjgzZWYyOWVmNGUxMjlmMWEyZDE4NTE2N2E1YzU0Oi0IAhCwCRjoByIjdjItNmEwYmYyOTdiMTQ2NzU5OTQzMzE2MTk5NDBlMzZjOTU6LQgCEIAeGPAQIiN2Mi00YmMzMzA5MjM5OTQ3MzVmZjJhYjI1OWYyZDliNjJlZTotCAIQ1QYYgAoiI3YyLWMwNDA0OGY5Njg0ZTc3MjdjNTdiZjEwNWI5NzY2OGQ2Oi0IAhDeBxjxBCIjdjItNDc4ZGZhNzJmNWYzZDhjNjJhMTBjZTU2MjZiMjIwNDOABACIBACSBAZOb3JtYWyaBAE0oAQAqAQAsAQAugQGbWFudWFswgQDMTcwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAABA6bSnP4EFAAAAAAAAAACJBZ81rr+lddI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQyQBgCgBkOoBgGSAi4KCTI1OTEzMDEzMRITMTkxNzg4MzMzMTgyOTI3MzY4NxgHIgpJTUFHRV9URVhU","action_card":false},{"id":"67_1750899122.234","type":"feed","offset":67,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899122,"updated_time":1750899122,"target":{"id":"1921270364211766592","type":"answer","url":"https://api.zhihu.com/answers/1921270364211766592","author":{"id":"721e14499121263ecbe544b56c318ce5","url":"https://api.zhihu.com/people/721e14499121263ecbe544b56c318ce5","user_type":"people","url_token":"a-hu-001","name":"百态网络","headline":"个人公众号：【百态网络】","avatar_url":"https://picx.zhimg.com/50/v2-e6d972c79235d10ea7feb2b9dda82eea_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":57120,"is_following":false,"is_followed":false},"created_time":1750846550,"updated_time":1750846550,"voteup_count":1,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"436263778","type":"question","url":"https://api.zhihu.com/questions/436263778","author":{"id":"a93b96a821ac9e0306e154cd729f4f06","url":"https://api.zhihu.com/people/a93b96a821ac9e0306e154cd729f4f06","user_type":"people","url_token":"benben-54-49","name":"Benben","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-bf8fad8bb4e23e48bf57fe9b5a6f8048_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":1,"is_following":false,"is_followed":false},"title":"普通人如何在一年赚十万？","created":1608780164,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[137247,138833,172578],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"年赚10万是一件很简单的事儿，循环3个动作： 1、锁定一个卖点； 2、每天围绕着一个卖点发50个视频，50篇文章； 3、所有的流量都导入微信。 就这样简单吗？ 对，就这样简单。 做到的人多吗？ 不多，因为时间周期是一年。 买股票的，一直股票持有一年的，连5%的人都不到。 我常说一个词：战略耐性。能理解什么叫战略耐性的人非常少非常少。 1、有的学员，在我这儿学习，学一个月走了。他赚钱吗？肯定赚钱。他回到家还赚钱吗？赚到…","excerpt_new":"年赚10万是一件很简单的事儿，循环3个动作： 1、锁定一个卖点； 2、每天围绕着一个卖点发50个视频，50篇文章； 3、所有的流量都导入微信。 就这样简单吗？ 对，就这样简单。 做到的人多吗？ 不多，因为时间周期是一年。 买股票的，一直股票持有一年的，连5%的人都不到。 我常说一个词：战略耐性。能理解什么叫战略耐性的人非常少非常少。 1、有的学员，在我这儿学习，学一个月走了。他赚钱吗？肯定赚钱。他回到家还赚钱吗？赚到…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"9hB72Xkg\"\u003e年赚10万是一件很简单的事儿，循环3个动作：\u003c/p\u003e\u003cp data-pid=\"MNfCX-kw\"\u003e1、锁定一个卖点；\u003c/p\u003e\u003cp data-pid=\"zqxQqGww\"\u003e2、每天围绕着一个卖点发50个视频，50篇文章；\u003c/p\u003e\u003cp data-pid=\"V4-rIaZD\"\u003e3、所有的流量都导入微信。\u003c/p\u003e\u003cp data-pid=\"vGBcGbAH\"\u003e就这样简单吗？\u003c/p\u003e\u003cp data-pid=\"v8lCKFPt\"\u003e对，就这样简单。\u003c/p\u003e\u003cp data-pid=\"M_gYXjnG\"\u003e做到的人多吗？\u003c/p\u003e\u003cp data-pid=\"ct-l-FOi\"\u003e不多，因为时间周期是一年。\u003c/p\u003e\u003cp data-pid=\"8g4CrkJL\"\u003e买股票的，一直股票持有一年的，连5%的人都不到。\u003c/p\u003e\u003cp data-pid=\"QD8BEaWd\"\u003e我常说一个词：战略耐性。能理解什么叫战略耐性的人非常少非常少。\u003c/p\u003e\u003cp data-pid=\"52tmq_Kp\"\u003e1、有的学员，在我这儿学习，学一个月走了。他赚钱吗？肯定赚钱。他回到家还赚钱吗？赚到钱的概率很低。为什么赚到钱的概率很低啊？他在这儿，一天不发帖就有罪恶感，他回家了，常年累月不发帖，也无罪恶感。\u003c/p\u003e\u003cp data-pid=\"Ji1KtVL9\"\u003e2、有人在这儿呆1年，每天围绕着一个卖点发16小时的短视频，他走的时候，搞到10万是非常轻松的事儿。\u003c/p\u003e\u003cp data-pid=\"LIAXFSS3\"\u003e很多人都高估了自己。比如误以为自己有战略耐性。有战略耐性的人非常少非常少，少到什么程度啊？比上211/985的 人还少。\u003c/p\u003e\u003cp data-pid=\"eFbNtUnJ\"\u003e我认识一个赚钱高手，现在年收入是千万级别的，他常说3句话：\u003c/p\u003e\u003cp data-pid=\"44m7HFsA\"\u003e1、要是这个项目，不打算持有10年，就不要碰它；\u003c/p\u003e\u003cp data-pid=\"6MPRmset\"\u003e2、凡事都要以年为单位，比如10年为一个单位，20年为一个单位，30年为一个单位；\u003c/p\u003e\u003cp data-pid=\"bLAPfPt3\"\u003e3、不换跑道，不走捷径，用最笨的方式跟这个世界互动。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-b9a58e0e89e0822da8e034cbe38eb3dd_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"856\" data-rawheight=\"437\" data-original-token=\"v2-354788eafaf09a42828b0d9f4ddf9711\" class=\"origin_image zh-lightbox-thumb\" width=\"856\" data-original=\"https://pic4.zhimg.com/v2-b9a58e0e89e0822da8e034cbe38eb3dd_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"z-rBpSam\"\u003e他常讲猴子和鳄鱼的故事。猴子的智商，相当于3到10岁小孩的智商。鳄鱼的智商，相当于3岁的小孩的智商。猴子喜欢逗鳄鱼玩。猴子逗鳄鱼玩时，这2个小时，鳄鱼都一动不动，只是眯着眼。2个小时，猴子玩累了，鳄鱼抓到机会，一剑封喉。鳄鱼只要咬到猴子了，无论猴子怎么挣扎，它都不松口。\u003c/p\u003e\u003cp data-pid=\"OO4d9FQE\"\u003e猴子怎样才能避免鳄鱼伤害它啊？就是离鳄鱼远点儿。猴子有这个觉悟吗？没有这个觉悟。\u003c/p\u003e\u003cp data-pid=\"uuwiPcci\"\u003e生活中，很多人都误以为自己是鳄鱼，其实他们都是猴子。鳄鱼有战略耐性，猴子没有战略耐性。\u003c/p\u003e\u003cp data-pid=\"XYvoGnXX\"\u003e这个世界，属于笨人，不属于聪明人。现在生活状况好的，绝对不是聪明人，而是那些不太聪明，也不太傻，踏踏实实干活的人。\u003c/p\u003e\u003cp data-pid=\"F1vJKfIz\"\u003e有个人，很聪明，哪儿工资高往哪儿跑，到了60岁，跑不动了，也就没收入了。另一个人呢，不聪明，就老老实实在一个小区当保安，小区给他交社保，他到了60岁，不干了，每月有1500元的退休金，旱涝保收。\u003c/p\u003e\u003cp data-pid=\"LR4ui7eg\"\u003e聪明是毒药，投机取巧是毒药中的毒药。\u003c/p\u003e\u003cp data-pid=\"qugLLUtY\"\u003e现在有人问：怎么赚钱啊？\u003c/p\u003e\u003cp data-pid=\"L4bBA0q8\"\u003e我只说3句话：\u003c/p\u003e\u003cp data-pid=\"D0Zc1vcb\"\u003e1、锁定一个卖点，持有10年或持有一辈子；\u003c/p\u003e\u003cp data-pid=\"8KOb7eDw\"\u003e2、一直发视频发文章；\u003c/p\u003e\u003cp data-pid=\"aPWqp5vK\"\u003e3、所有的流量导入微信。\u003c/p\u003e\u003cp data-pid=\"TbCpXCiI\"\u003e我的20套房子，就是这样出来的。我这20套房子，我必须说下，就3套是在城市买的，其余的都是在乡下买的（20万/套），没有大家想象的富有。尽管这样，我还是超越了很多同龄人。这10年，我没抄过别人的文章，一直发帖，一直发帖，日收入从来没低于过1000元。\u003c/p\u003e\u003cp data-pid=\"cbVfnTtA\"\u003e我孩子要是读书不行，我就让他们在家发帖，发视频。人生出口看似很多，其实很少很少。人人都想年赚千万或破亿。现实是什么？从兜里掏出来10万都难。承认自己是一个很平凡的人，难道很难吗？\u003c/p\u003e\u003cp data-pid=\"BOwC1t19\"\u003e怎么做的我都讲了，知道和做到，永远是两码事儿。\u003c/p\u003e\u003cp data-pid=\"uTMQFFgl\"\u003e我老婆一直都在奋斗，她赚钱吗？不赚钱。她热爱学习，热爱考证，就她这种好求知的精神，出来创业，连养活自己都难。她啊，喜欢新鲜，喜欢考证。我都说了：你在学校都是学渣（高考只有400分），出来考证，依旧是学渣。你高考不到600分，已说明你在学习上，并无天赋。承认自己是傻逼很难吗？大家找媳妇，遇到爱学习的，直接过滤掉吧，因为她们脑子里的东西，都是浆糊。哎……\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":81,"favorite_count":2,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921270364211766592}","attached_info":"CpgGCOKDvb/vqZLnjwEQBBoJNzMzOTc5ODIxINaY78IGKAEwAEBDSiMKGFRTX1NPVVJDRV9XQVJNX1VQX0JPT1NUMhIBMBgAIAA6AEotCiJUU19TT1VSQ0VfV0FSTV9VUF9ISUdIX0lOVEVSQUNUSU9OEgEwGAAgADoAWgg1OTIyNjUyMGIgYTg0NWI1N2NlYWQzNDA3YjljOGE5OGNhMzIyYzQ3MWJyEzE5MjEyNzAzNjQyMTE3NjY1OTKKAQk0MzYyNjM3NziqAQlyZWNvbW1lbmTCASA3MjFlMTQ0OTkxMjEyNjNlY2JlNTQ0YjU2YzMxOGNlNfIBCggMEgZOb3JtYWzyASgIChIkYzA3OWViMWQtNDMxYy00NGQ4LThhMzAtOTVmNjFjYzA4NTBk8gEGCAsSAjEyggIAiALYv+HN+jKSAiA3MjFlMTQ0OTkxMjEyNjNlY2JlNTQ0YjU2YzMxOGNlNZoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhVRdWVzdGlvbklzb2xhdGlvblJ1bGXaAhhUU19TT1VSQ0VfV0FSTV9VUF9CT09TVDLoAgL6AgtOT1JNQUxfRkxPV4oDIGVjNzU1YTgyY2Q2NDQ5MDRhNjNiOGUyOWNmZTFjMzAymgMNCgJ2MhAAGgVvdGhlcqgDUdgDAOoDHnRleHRfaGlnaF9pbnRlcmFjdGlvbl9yZWNhbGxlcvoDThIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgEENgGGLUDIiN2Mi0zNTQ3ODhlYWZhZjA5YTQyODI4YjBkOWY0ZGRmOTcxMYAEAIgEAJIEBk5vcm1hbJoEATKgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAAJ08sT+BBQAAAAAAAAAAiQWfNa6/pXXSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUMkAYAoAZEqAYDkgIuCgk3MzM5Nzk4MjESEzE5MjEyNzAzNjQyMTE3NjY1OTIYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"68_1750899122.406","type":"feed","offset":68,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899122,"updated_time":1750899122,"target":{"id":"1906778557683836730","type":"answer","url":"https://api.zhihu.com/answers/1906778557683836730","author":{"id":"3c956fc47b11328c4a0969d24e5b3aa2","url":"https://api.zhihu.com/people/3c956fc47b11328c4a0969d24e5b3aa2","user_type":"people","url_token":"fei-fan-74-71","name":"非凡","headline":"风火轮火尖枪混天绫","avatar_url":"https://picx.zhimg.com/50/v2-f8fa45af7ac6e501fd75f71e7eec3366_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"重庆铉晟科技有限公司 总经理"}],"followers_count":167328,"is_following":false,"is_followed":false},"created_time":1747391434,"updated_time":1747391434,"voteup_count":7826,"thanks_count":263,"comment_count":473,"is_copyable":false,"question":{"id":"596279235","type":"question","url":"https://api.zhihu.com/questions/596279235","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pic1.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"老北京话“沏壶高的”是什么意思？","created":1681795950,"answer_count":0,"follower_count":0,"comment_count":22,"bound_topic_ids":[491,1089,7598,34938,182301],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://pic1.zhimg.com/50/v2-f70bfb9229412cf3650dabeda01b3af8_720w.jpg?source=b6762063","excerpt":"高碎不是茶叶铺卖剩的茶。 而是茶叶铺到货以后，就开始筛茶。 不管多好的茶叶，经过长途运输，都有碎的。 条索完整，全须全尾品相好的芽头挑出来，用上好的罐子装起，贴上红纸，给王爷府送去。   王爷府必定要收货，还要打赏。 王爷府这次拒了，下次就不送了。而后全京城都知道，王爷可能有事，吃不起新茶了。 换句话说，新茶到了不给府上送去，就有人上门问罪，恼了一个嘴巴子，大骂：“什么东西？你也敢？” 喝新茶在某种层面上…","excerpt_new":"高碎不是茶叶铺卖剩的茶。 而是茶叶铺到货以后，就开始筛茶。 不管多好的茶叶，经过长途运输，都有碎的。 条索完整，全须全尾品相好的芽头挑出来，用上好的罐子装起，贴上红纸，给王爷府送去。   王爷府必定要收货，还要打赏。 王爷府这次拒了，下次就不送了。而后全京城都知道，王爷可能有事，吃不起新茶了。 换句话说，新茶到了不给府上送去，就有人上门问罪，恼了一个嘴巴子，大骂：“什么东西？你也敢？” 喝新茶在某种层面上…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"KmHuTZ-_\"\u003e高碎不是茶叶铺卖剩的茶。\u003c/p\u003e\u003cp data-pid=\"Zoz3E63O\"\u003e而是茶叶铺到货以后，就开始筛茶。\u003c/p\u003e\u003cp data-pid=\"FSxSEEbp\"\u003e不管多好的茶叶，经过长途运输，都有碎的。\u003c/p\u003e\u003cp data-pid=\"xjAUUlpy\"\u003e条索完整，全须全尾品相好的芽头挑出来，用上好的罐子装起，贴上红纸，给王爷府送去。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-12be5a864fc7f64afeb9277ddf6ebeb3_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"1136\" data-original-token=\"v2-5d14b9caa9c1f399a7586d9f22647060\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-296f1f6cb467aac3e16a47d2019871ef_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"851\" data-original=\"https://picx.zhimg.com/v2-12be5a864fc7f64afeb9277ddf6ebeb3_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"8KKfIGdQ\"\u003e王爷府必定要收货，还要打赏。\u003c/p\u003e\u003cp data-pid=\"q72fzGfT\"\u003e王爷府这次拒了，下次就不送了。而后全京城都知道，王爷可能有事，吃不起新茶了。\u003c/p\u003e\u003cp data-pid=\"RqX8N8qN\"\u003e换句话说，新茶到了不给府上送去，就有人上门问罪，恼了一个嘴巴子，大骂：“什么东西？你也敢？”\u003c/p\u003e\u003cp data-pid=\"_-xD_dnp\"\u003e喝新茶在某种层面上，也是一个风向标。因为新茶贵，在京城是面儿。\u003c/p\u003e\u003cp data-pid=\"LdBs130R\"\u003e次一等的茶片子，用大瓷缸封好，放在铺子头面，写着“新茶品鉴”字样，作为店铺主推新品，卖给大户人家和高档餐厅。\u003c/p\u003e\u003cp data-pid=\"mPesRXH7\"\u003e筛到最后的茶末，用袋子装好，等着茶馆上门。\u003c/p\u003e\u003cp data-pid=\"3ys9JJqp\"\u003e这种茶末是新茶碎末，不是粗茶，所以叫做“高沫”。品相难看，但是味道并不差。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-e9cd146c2fc33f1b6add6438700d6f78_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"1920\" data-original-token=\"v2-600364cd8a25fbfb9b82e4009d30b215\" data-default-watermark-src=\"https://picx.zhimg.com/v2-a4d28179aa1ef6117f4a98fc0a3f1f43_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1920\" data-original=\"https://pica.zhimg.com/v2-e9cd146c2fc33f1b6add6438700d6f78_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"kZIz70eD\"\u003e粗茶实际上是过了采茶季，叶片都长老了，茶叶变得粗壮，口感也变差，喝起来不是味儿，但是价格很便宜，最适合泡茶壶，做茶叶蛋，老百姓家里准备一壶过日子用的。老北京大碗茶，就是这东西。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-b7a93e5c03f4bfb573f301b8df7b4386_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"600\" data-original-token=\"v2-1d6f69f32d4dd292ff9cf7d778723c3a\" data-default-watermark-src=\"https://pica.zhimg.com/v2-227d2e34fce527e8555c8198a3af8f62_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-b7a93e5c03f4bfb573f301b8df7b4386_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"sLZ_XIAO\"\u003e北京城周边，几乎没有好茶叶。最近的也得是河南信阳毛尖，好茶都是通过京杭大运河送来，折腾一番，再好的茶叶也不能江南地区相比。新茶也不是碧绿澄清，在全程冷链以前就这条件。这一点大家要搞明白。\u003c/p\u003e\u003cp data-pid=\"9NAkjN96\"\u003e“沏壶高的”首先就不是价格问题，也不是水的问题，而是一个阶级等级问题。\u003c/p\u003e\u003cp data-pid=\"Hn5eOJuy\"\u003e最好的芽头是不可能流向市场的。因为这货奇缺，全靠人工筛选，要费很大人工，才能挑得出来。而京城官多，王爷多，各级政府班子，有钱人，根本就不够供应。清朝王爷不得出北京城，就指着这些东西活个高兴。京城大员没有好茶叶待客，就会被人闲话：“连好茶叶都搞不到。”\u003c/p\u003e\u003cp data-pid=\"Oa28zKsT\"\u003e所以，京城有个观察窗口。上座，请茶，打开盖碗一看，里面的货色行不行？啥话能说，啥话不能说，这是很重要的一个指标。\u003c/p\u003e\u003cp data-pid=\"Z0x3VJ3s\"\u003e假如说京城一个老板，就是普通的商人，奉出的茶是和王爷府一样的上好芽头。哇，这不简单，和他谈生意靠谱。\u003c/p\u003e\u003cp data-pid=\"Q4au9IUP\"\u003e能拿的出“新茶品鉴”的商人，富裕人家，京城小官，也不错，符合大众认知。\u003c/p\u003e\u003cp data-pid=\"9sBVnTPB\"\u003e兜里有几个子儿，架着鸟去茶馆喝高沫，按我们今天的认知，就是中产阶级。喝高沫的不是穷人，老板看见这样的主儿上门，要打千问安。\u003c/p\u003e\u003cp data-pid=\"hYFPSiy-\"\u003e最穷的人就只能蹲在街边喝大碗茶，从褡裢里掏出个窝窝头，喝一口咬一口。\u003c/p\u003e\u003cp data-pid=\"r0PB7SkJ\"\u003e大碗茶再不济，这是热茶，又解渴，喝下去胃里舒服。\u003c/p\u003e\u003cp data-pid=\"7ynH-cxa\"\u003e说白了，“沏壶高的”就是往上够不着，往下还有富裕。\u003c/p\u003e\u003cp data-pid=\"DNdwwduB\"\u003e不能和达官贵人比，但是看见街边喝大碗茶啃窝头的穷人，又颇有几分自豪。\u003c/p\u003e\u003cp data-pid=\"uKA9G1M4\"\u003e我这是茶馆上座喝“高的”。\u003c/p\u003e\u003cp data-pid=\"u5_gqiTe\"\u003e相当于在星巴克喝咖啡玩手机，拿着本书附庸风雅。\u003c/p\u003e\u003cp data-pid=\"nPcBm-fz\"\u003e概念上是差不多的。\u003c/p\u003e\u003cp data-pid=\"evuPndqM\"\u003e好茶叶碎了，才是“高沫”。别看碎，喝起来和王爷府的味儿一样。\u003c/p\u003e\u003cp data-pid=\"zrG_MXVY\"\u003e喝大碗茶的主儿，吃完了一个窝头，又拿出来一个。正准备吃，一个小女孩儿蓬头垢面直愣愣盯着，直咽口水。哎，谁也别瞧不起谁。\u003c/p\u003e\u003cp data-pid=\"IemVoLeb\"\u003e“小妹妹，拿去吃。”说着递给她。小女孩也不客气，接过来就咬一大口。急切之间竟然咽不下去，梗得难受。“老板，再来一碗茶水。”“别急，喝口茶顺一顺。”女孩儿的妈妈走过来，赶紧作揖：“谢大爷赏饭。”“咱们穷帮穷，不用谢。”\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":359226,"thumbnails":["https://picx.zhimg.com/50/v2-f70bfb9229412cf3650dabeda01b3af8_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-dc242746d61c1785c272c1edc4943c3e_720w.jpg?source=b6762063"],"favorite_count":1489,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1906778557683836730}","attached_info":"CvUGCOKDvb/vqZLnjwEQBBoJNzI3NjgyMDI5IMqnnMEGKJI9MNkDQERKKAodVFNfU09VUkNFX0lOVEVSRVNUX1dPUkRfTUVSR0USATAYACAAOgBaCDk0Nzg3NjcwYiBhODQ1YjU3Y2VhZDM0MDdiOWM4YTk4Y2EzMjJjNDcxYnITMTkwNjc3ODU1NzY4MzgzNjczMIoBCTU5NjI3OTIzNaoBCXJlY29tbWVuZMIBIDNjOTU2ZmM0N2IxMTMyOGM0YTA5NjlkMjRlNWIzYWEy8gEKCAwSBk5vcm1hbPIBKAgKEiQ2MGJkN2JmNi1mMmE2LTQ2NzQtODA4ZS05NTM2N2Q4OGQ1NTTyAQYICxICMTKCAgCIAti/4c36MpICIDNjOTU2ZmM0N2IxMTMyOGM0YTA5NjlkMjRlNWIzYWEymgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxlygIVUXVlc3Rpb25Jc29sYXRpb25SdWxl2gIdVFNfU09VUkNFX0lOVEVSRVNUX1dPUkRfTUVSR0XoAgP6AgtOT1JNQUxfRkxPV4oDIGVjNzU1YTgyY2Q2NDQ5MDRhNjNiOGUyOWNmZTFjMzAymgMNCgJ2MhAAGgVvdGhlcqgDuvYV2AMA6gMiSW50ZXJlc3RXb3JkTWVyZ2VWMU5ld1Bvb2xSZWNhbGxlcvoDrAESDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IBBDTBhjwCCIjdjItNWQxNGI5Y2FhOWMxZjM5OWE3NTg2ZDlmMjI2NDcwNjA6LQgDEIAPGIAPIiN2Mi02MDAzNjRjZDhhMjVmYmZiOWI4MmU0MDA5ZDMwYjIxNTotCAMQgAUY2AQiI3YyLTFkNmY2OWYzMmQ0ZGQyOTJmZjljZjdkNzc4NzIzYzNhgAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEBm1hbnVhbMIEAzE2MMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAoHCkqD+BBQAAAAAAAAAAiQWfNa6/pXXSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUMkAYAoAZFqAYAkgIuCgk3Mjc2ODIwMjkSEzE5MDY3Nzg1NTc2ODM4MzY3MzAYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"69_1750899122.215","type":"feed","offset":69,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899122,"updated_time":1750899122,"target":{"id":"1451778460","type":"answer","url":"https://api.zhihu.com/answers/1451778460","author":{"id":"e0ac97c0a5474f1b81e6cea3a6bf1875","url":"https://api.zhihu.com/people/e0ac97c0a5474f1b81e6cea3a6bf1875","user_type":"people","url_token":"zoushude","name":"邹叔的海外营销","headline":"出海10年，专注海外营销，《出海不出局》zouyangmy1","avatar_url":"https://pic1.zhimg.com/50/v2-b861d469aa07d24377ba89db128d5780_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":36669,"is_following":false,"is_followed":false},"created_time":1599132895,"updated_time":1599132895,"voteup_count":80850,"thanks_count":2901,"comment_count":1954,"is_copyable":true,"question":{"id":"300965902","type":"question","url":"https://api.zhihu.com/questions/300965902","author":{"id":"1918521b7cda549d7eb9f39aa07152d1","url":"https://api.zhihu.com/people/1918521b7cda549d7eb9f39aa07152d1","user_type":"people","url_token":"zhan-yue-85","name":"斩月","headline":"养龟高手中的程序猿","avatar_url":"https://pic1.zhimg.com/50/v2-6eb9296efe37a02ed6053b1329ff59e1_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":359,"is_following":false,"is_followed":false},"title":"一只狗可以通人性到什么程度？","created":1541129377,"answer_count":0,"follower_count":0,"comment_count":8,"bound_topic_ids":[28196],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"网友假装玩手机睡着了，偷拍他家的金毛...","excerpt_new":"网友假装玩手机睡着了，偷拍他家的金毛...","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"Gju3RLG4\"\u003e网友假装玩手机睡着了，偷拍他家的金毛... ​​​​\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-e9a11ec5945d39adcb0babd059920515_1440w.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"280\" data-rawheight=\"342\" data-original-token=\"v2-e9a11ec5945d39adcb0babd059920515\" data-thumbnail=\"https://pic2.zhimg.com/v2-e9a11ec5945d39adcb0babd059920515_b.jpg\" class=\"content_image\" width=\"280\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":6040627,"favorite_count":9510,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1451778460}","attached_info":"CoMHCOKDvb/vqZLnjwEQBBoJMjk0MDMyMjI2IN+pw/oFKNL3BDCiD0BFSi8KHVRTX1NPVVJDRV9GRUVEX1VTRVJfRVhQRVJUX0NGEggxNTY1NjA1MRgAIAA6AFoIMjkxNTc3MzViIGE4NDViNTdjZWFkMzQwN2I5YzhhOThjYTMyMmM0NzFicgoxNDUxNzc4NDYwigEJMzAwOTY1OTAyqgEJcmVjb21tZW5kwgEgZTBhYzk3YzBhNTQ3NGYxYjgxZTZjZWEzYTZiZjE4NzXyAQoIDBIGTm9ybWFs8gEoCAoSJDE3YWFkMzRjLWEwMGYtNDFjNC1iMTZjLWQwODU4OTYwMjlmYvIBBggLEgIxMoICAIgC2L/hzfoykgIgZTBhYzk3YzBhNTQ3NGYxYjgxZTZjZWEzYTZiZjE4NzWaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCFlJldmlzaXRWYWx1ZVdlaWdodFJ1bGXKAhhQZXJpb2RJbnRlcmVzdFdlaWdodFJ1bGXKAhJDbGlwUGVha1dlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZcoCFVF1ZXN0aW9uSXNvbGF0aW9uUnVsZdoCHVRTX1NPVVJDRV9GRUVEX1VTRVJfRVhQRVJUX0NG6AIC+gILTk9STUFMX0ZMT1eKAyBlYzc1NWE4MmNkNjQ0OTA0YTYzYjhlMjljZmUxYzMwMpoDDQoCdjIQABoFb3RoZXKoA7PY8ALYAwDqAxBhaUdyYXBoRW1iVXNlckNG+gNOEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAAQmAIY1gIiI3YyLWU5YTExZWM1OTQ1ZDM5YWRjYjBiYWJkMDU5OTIwNTE1gAQAiAQAkgQGTm9ybWFsmgQBMqAEAKgEALAEALoEBm1hbnVhbMIEAjMwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAAAgd3bAP4EFAAAAAAAAAACJBZ81rr+lddI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQyQBgCgBkaoBgCSAiUKCTI5NDAzMjIyNhIKMTQ1MTc3ODQ2MBgEIgpJTUFHRV9URVhU","action_card":false},{"id":"70_1750899122.649","type":"feed","offset":70,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750899122,"updated_time":1750899122,"target":{"id":"1921234490266747939","type":"article","url":"https://api.zhihu.com/articles/1921234490266747939","author":{"id":"70fff4b1a2f02f672c9e60d4e035f724","url":"https://api.zhihu.com/people/70fff4b1a2f02f672c9e60d4e035f724","user_type":"people","url_token":"daviddeng","name":"闻道客","headline":"常记溪亭日暮，沉醉不知归路。","avatar_url":"https://pic1.zhimg.com/50/f465e4229_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":332,"is_following":false,"is_followed":false},"title":"具身智能：AI未来的主赛道还是最后一公里的陷阱？","comment_permission":"all","created":1750838158,"updated":1750838158,"voteup_count":0,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"2025年的今天，AI的浪潮席卷全球。从ChatGPT的语言魔法到DALL·E的图像创造，虚拟世界的AI已无处不在。但当我们把目光转向现实世界，一个新名词正引发热议：具身智能（Embodied AI）。它被誉为“AI的下一站”，让机器不仅会“思考”，还能“行动”——从工厂的搬运机器人到养老院的陪伴助手，甚至是太空探索的机械先锋。马斯克宣称，特斯拉的Optimus将是“通用智能的载体”；谷歌DeepMind则押注机器人来破解物理世界的难题。 然…","excerpt_new":"2025年的今天，AI的浪潮席卷全球。从ChatGPT的语言魔法到DALL·E的图像创造，虚拟世界的AI已无处不在。但当我们把目光转向现实世界，一个新名词正引发热议：具身智能（Embodied AI）。它被誉为“AI的下一站”，让机器不仅会“思考”，还能“行动”——从工厂的搬运机器人到养老院的陪伴助手，甚至是太空探索的机械先锋。马斯克宣称，特斯拉的Optimus将是“通用智能的载体”；谷歌DeepMind则押注机器人来破解物理世界的难题。 然…","preview_type":"default","preview_text":"","content":"\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-973f550e9d69b4242a3a87e046ba1c51_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"1024\" data-original-token=\"v2-b14f4774c3d12b1a0ddd98847303bce9\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://picx.zhimg.com/v2-973f550e9d69b4242a3a87e046ba1c51_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"fEI-0p3_\"\u003e2025年的今天，AI的浪潮席卷全球。从ChatGPT的语言魔法到DALL·E的图像创造，虚拟世界的AI已无处不在。但当我们把目光转向现实世界，一个新名词正引发热议：具身智能（Embodied AI）。它被誉为“AI的下一站”，让机器不仅会“思考”，还能“行动”——从工厂的搬运机器人到养老院的陪伴助手，甚至是太空探索的机械先锋。马斯克宣称，特斯拉的Optimus将是“通用智能的载体”；谷歌DeepMind则押注机器人来破解物理世界的难题。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"aPpsFRR0\"\u003e然而，现实远比愿景骨感。2025年，具身智能的商用化仍步履蹒跚：成本高企、效率低下、数据瓶颈频现。波士顿动力Spot在巡检场景表现不俗，但价格高达20万元；优必选的Walker搬运箱子耗时数分钟，远逊于人类工人。更深层次的问题是：具身智能真是AI的未来主赛道吗？AI落地的“最后一公里”，究竟需要怎样的突破？\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"0I3c15pI\"\u003e我们从具身智能的定义出发，剖析其技术现状、商用场景与核心瓶颈，探讨它在AI发展中的定位，并展望AI落地的终极路径。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"c39wqAWi\"\u003e\u003cb\u003e具身智能：从“云端大脑”到“物理躯体”\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-d966f976790ccfb675d93b760c1794d0_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" data-original-token=\"v2-d590283278946286a7ba397415434427\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-d966f976790ccfb675d93b760c1794d0_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"S4hmTCX2\"\u003e\u003cb\u003e1. 什么是具身智能？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"W83Mk8QB\"\u003e具身智能指将AI的感知、决策与执行能力集成于物理实体，使其在现实世界中与环境交互并完成任务。不同于传统AI（如语言模型或图像生成）局限于数字空间，具体智能强调“身体”与“智能”的耦合。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"V1Ytv3oS\"\u003e例如：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"3sDcFPU4\"\u003e工业机器人：特斯拉Optimus在工厂搬运零件。\u003c/li\u003e\u003cli data-pid=\"q4YyMzyl\"\u003e服务机器人：软银Pepper在养老院提供情感陪伴。\u003c/li\u003e\u003cli data-pid=\"h8oaArtY\"\u003e探索机器人：NASA的RoboSimian用于火星探测。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"a6A3EVc6\"\u003e其核心在于多模态交互：通过传感器（如视觉、触觉、力觉）感知环境，结合AI算法（如大模型、强化学习）决策，并通过机械结构执行动作。这种“感知-认知-行动”的闭环，被认为是AI从虚拟走向现实的关键。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"vQvI4aHW\"\u003e\u003cb\u003e2. 为什么备受追捧？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Q1OmI2zm\"\u003e2025年，具身智能被热议的原因有三：\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"_hoXUjzi\"\u003e技术驱动：大模型（如GPT-4、Grok 3）与传感器技术的进步，为机器人赋予了更强的环境理解能力。谷歌RT-2模型在2024年实现机器人从视觉输入到动作输出的端到端训练，点燃行业热情。\u003c/li\u003e\u003cli data-pid=\"2PTm89T9\"\u003e市场潜力：据IDC 2025年预测，全球机器人与智能系统市场2024年规模约1350亿美元，2030年将达2740亿美元，年均增长15%。中国市场尤为迅猛，2029年预计突破500亿元。\u003c/li\u003e\u003cli data-pid=\"0X5gPg7Q\"\u003e社会需求：老龄化加剧（如中国2025年老年人口达3亿）、劳动力短缺（如日本制造业缺口超百万），推动机器人替代人工的迫切性。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"RYPNF5JJ\"\u003e然而，热潮背后，具身智能的商用化却面临重重挑战。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"rUVx7U-R\"\u003e\u003cb\u003e商用现状：从实验室到现实的尴尬\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-a9f644fe8dd13b4ade638aeff6fe3aac_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"810\" data-original-token=\"v2-5efcc395d0c70173ea34c3637002d8ea\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-a9f644fe8dd13b4ade638aeff6fe3aac_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"295RkZ-W\"\u003e2025年，具身智能的商用化进展远低于预期。以下是主要场景的现状：\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"ZFanw-H-\"\u003e\u003cb\u003e1.工业制造\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"-S08OtpT\"\u003e案例：特斯拉Optimus试点汽车总装线搬运，优必选Walker尝试物流分拣。\u003c/li\u003e\u003cli data-pid=\"jPfKk7X_\"\u003e现状：效率低下，如Walker搬运箱子需2-4分钟，工人仅需1分钟；成本高企，单机价格40-60万元，回收周期3-4年。\u003c/li\u003e\u003cli data-pid=\"SVrUMW1C\"\u003e挑战：非标任务（如线束整理）需频繁重新训练，出错成本高（如损坏设备损失数十万）。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Kv7paMx_\"\u003e\u003cb\u003e2.服务与零售\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"71OMNBMk\"\u003e案例：软银Pepper在商场引导，Agility Robotics的Digit在酒店送餐。\u003c/li\u003e\u003cli data-pid=\"3mI9Ihsl\"\u003e现状：多为品牌宣传，交互生硬，难以应对复杂人群需求。Pepper在日本零售场景的满意度仅为60%。\u003c/li\u003e\u003cli data-pid=\"Y6t5d9iI\"\u003e优势：提升科技感，吸引流量。\u003c/li\u003e\u003cli data-pid=\"mMCnAfHY\"\u003e挑战：情感交互不成熟，成本远高于人工（如Pepper年维护费超10万元）。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Edf2Ok4A\"\u003e\u003cb\u003e3.医疗与养老\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"UuEOU3iS\"\u003e案例：日本Robear协助老人移动，国内傅利叶智能的康复机器人。\u003c/li\u003e\u003cli data-pid=\"HTyLT3MM\"\u003e现状：试点为主，普及率低。Robear在养老院的部署量不足千台，交互体验受限。\u003c/li\u003e\u003cli data-pid=\"o-mkMvLF\"\u003e潜力：缓解护理短缺（如日本2025年护理缺口50万）。\u003c/li\u003e\u003cli data-pid=\"8EKgGtGY\"\u003e挑战：伦理与隐私问题，高情感交互需求。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"TCr6yO9M\"\u003e\u003cb\u003e4.探索与高危场景\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"VA9PEn-o\"\u003e案例：NASA Valkyrie用于太空任务，波士顿动力Spot在核电站巡检。\u003c/li\u003e\u003cli data-pid=\"Q1e0npO9\"\u003e现状：表现稳定但成本高，Spot单价约20万元，仅限高预算场景。\u003c/li\u003e\u003cli data-pid=\"YuCyy-q_\"\u003e潜力：替代人类进入危险环境。\u003c/li\u003e\u003cli data-pid=\"nb8juYuq\"\u003e挑战：技术复杂，规模化部署受限。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"qAlZ8n6j\"\u003e这些场景显示，具身智能虽有潜力，但效率、成本与场景适配的“三重门”让其商用化举步维艰。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"LvaYiHdI\"\u003e\u003cb\u003e技术瓶颈：具身智能的“三大硬伤”\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-79d0b47b1be3c5cfa3456f126eaa67cb_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"454\" data-original-token=\"v2-6760427394f731808fabf0a639c09e7b\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-79d0b47b1be3c5cfa3456f126eaa67cb_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"fbK6p5Fx\"\u003e为何具身智能如此“低效”？以下是三大技术瓶颈：\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"6_ZahGjr\"\u003e\u003cb\u003e1. 硬件：物理躯体的复杂性\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"AKmm6XHF\"\u003e具身智能需模仿人类的感知与动作，但硬件技术远未成熟：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"xgai5WH4\"\u003e多自由度挑战：人类手臂有7个自由度，抓取物体需视觉、触觉与力觉协同。特斯拉Optimus的灵巧手需定制传感器，成本高昂（单手模组超5万元）。\u003c/li\u003e\u003cli data-pid=\"PlRtJiR2\"\u003e动态稳定性：双足行走涉及重心控制，波士顿动力Atlas在2020年跑步测试中频频跌倒，2025年仍需优化。\u003c/li\u003e\u003cli data-pid=\"pB8KvNfW\"\u003e核心零部件：伺服电机、减速器、力觉传感器多依赖进口，占机器人成本60%以上。国内企业如优必选需支付高额溢价，限制规模化。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"ag8EsLux\"\u003e相比传统机械臂的单一功能，具身智能追求“通用性”，却牺牲了效率与可靠性。例如，工业场景的线束整理需微妙力度，机器人难以匹敌人类。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"jvEQNk3w\"\u003e\u003cb\u003e2. 软件：数据闭环的“天堑”\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Kj9FHNgx\"\u003e具身智能需在三维世界实时决策，数据需求远超虚拟AI：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"0NaHaHup\"\u003e数据稀缺：语言模型（如Grok 3）依托万亿级文本训练，机器人数据却匮乏。谷歌RT-2模型训练需10万条动作数据，耗资千万美元，耗时一年，而验证“scaling law”（数据量增加导致能力跃升）需千万级数据，当前多为百万级。\u003c/li\u003e\u003cli data-pid=\"o0SIuqGh\"\u003e采集难题：汽车通过行驶生成数据，机器人需遥控或仿真采集。DeepMind 2024年报告显示，遥控采集1万条数据需百人团队，成本超百万美元。\u003c/li\u003e\u003cli data-pid=\"CxWDSIzr\"\u003e泛化能力：机器人面对新场景（如不同形状的箱子）需重新训练，优必选Walker在工厂试点需2-3天适配新任务。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"B60qmzGB\"\u003e\u003cb\u003e3. 自主性：从“遥控傀儡”到“独立大脑”\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"poL4Dod_\"\u003e2025年，多数具身智能系统仍依赖人工干预。波士顿动力Spot的巡检任务需后台监控，特斯拉Optimus的复杂动作多由遥控完成。优必选技术负责人焦继超坦言：“机器人不可能靠人在后面拿遥控器操作。”自主性的缺失不仅增加成本，也限制了规模化部署。DeepMind 2025年报告指出，机器人实时决策的延迟仍达0.5-1秒，远逊于人类的0.1秒。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"yDuEA0Ld\"\u003e\u003cb\u003e具身智能是AI的未来主赛道吗？\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-d45ed9ea4e9919ca56d6d0267d9f7a9c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"719\" data-original-token=\"v2-a142d3331a2bfab33053066ef71806cc\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-d45ed9ea4e9919ca56d6d0267d9f7a9c_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"PnFvm1Ta\"\u003e具身智能被寄予厚望，但其在AI发展中的定位引发争议。以下是正反两方的分析：\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"W2GU5BGx\"\u003e\u003cb\u003e1. 正方：具身智能是AI的“终极形态”\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"3-07u9sW\"\u003e支持者认为，具身智能是AI从虚拟到现实的桥梁：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"rcX20Czw\"\u003e通用性潜力：模仿人类身体，机器人可适配多场景（工厂、家庭、太空）。马斯克称：“Optimus将解决99%的人类劳动问题。”\u003c/li\u003e\u003cli data-pid=\"z6do2K_O\"\u003e认知飞跃：具身智能通过物理交互学习环境，类似人类婴儿的“试错”成长。斯坦福2024年研究显示，机器人通过触觉学习抓取，准确率提升30%。\u003c/li\u003e\u003cli data-pid=\"6og27IvF\"\u003e社会需求：老龄化与劳动力短缺（如中国2025年制造业缺口200万）需机器人填补空缺。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"fggcJfey\"\u003e\u003cb\u003e2. 反方：具身智能是“伪需求”与“技术陷阱”\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"zxU4MES0\"\u003e反对者质疑具身智能的必要性：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"B2hVn-Bf\"\u003e非具身AI更高效：语言模型（如ChatGPT）在客服、教育领域已取代人工，成本仅为机器人的1%。轮式机器人（如Roomba）在清洁场景效率更高，价格仅数千元。\u003c/li\u003e\u003cli data-pid=\"6Q2V0aJc\"\u003e技术复杂度：具身智能需解决硬件、软件、自主性三大难题，进度滞后。谷歌2025年报告预测，通用机器人需10-15年成熟。\u003c/li\u003e\u003cli data-pid=\"BM4BRlPc\"\u003e场景适配性：专用设备（如工业机械臂）在特定任务中表现优于通用机器人。波士顿动力Spot在巡检场景的ROI（投资回报率）达80%，远超人形机器人。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"d0ylirpg\"\u003e\u003cb\u003e3. 综合判断\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"972t6h5T\"\u003e具身智能并非AI的“唯一未来”，而是多条赛道之一。虚拟AI（如大模型）将继续主导知识密集型场景，具身智能则适合物理交互场景（如高危任务、养老）。其“通用性”愿景虽诱人，但当前技术与成本瓶颈使其更像“实验室明星”，而非“商用支柱”。高盛2025年报告指出，具身智能的“技术拐点”需5-10年，短期内难以成为主赛道。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"9LPsL7hz\"\u003e\u003cb\u003e从愿景到现实的破局\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-9683d6529e02e017dea1422cc4cfb439_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"926\" data-original-token=\"v2-e57bb90de11e14712d96e45cf63f0659\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-9683d6529e02e017dea1422cc4cfb439_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"7AyQQA37\"\u003eAI落地的“最后一公里”，不仅是技术突破，更是场景、成本与生态的协同。具身智能作为其中一环，需与虚拟AI、专用设备协同发展。以下是关键路径：\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Uz4nm4Ts\"\u003e\u003cb\u003e1. 技术突破\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"8Rit2JE9\"\u003e硬件降本：降低传感器、电机成本，目标机器人价格降至10-20万元，回收周期1.5年。MIT 2025年研究显示，模块化设计可降低30%成本。\u003c/li\u003e\u003cli data-pid=\"kadnyPg3\"\u003e数据闭环：建立开源数据平台，共享千万级动作数据。NVIDIA 2024年推出Isaac平台，已整合百万级仿真数据。\u003c/li\u003e\u003cli data-pid=\"ogiLEiPk\"\u003e自主性提升：优化实时决策算法，降低延迟至0.1秒。DeepMind 2025年RT-3模型将延迟缩短20%。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"58a1pXs5\"\u003e\u003cb\u003e2. 场景优先\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"GgI_MQPL\"\u003e高价值场景：聚焦高危（如核电站巡检）、高需求（如养老护理）场景，优先部署专用机器人。波士顿动力Spot在2024年核电巡检节省500万美元。\u003c/li\u003e\u003cli data-pid=\"2fZN0AFc\"\u003e渐进式通用化：从单一任务（如搬运）向多任务（如搬运+质检）过渡，积累数据与经验。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"NwgbFmup\"\u003e\u003cb\u003e3. 商业模式创新\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"1wmQURXC\"\u003e租赁模式：降低企业初始投入。优必选2025年推出Walker租赁服务，年费10万元，吸引中小客户。\u003c/li\u003e\u003cli data-pid=\"GnhGcYTu\"\u003e云端赋能：通过云端AI降低本地算力需求。亚马逊AWS RoboMaker 2024年报告显示，云端部署可节省40%成本。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"sBp4G9Ge\"\u003e\u003cb\u003e4. 生态协同\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"4_2hyrLX\"\u003e开源生态：推动ROS（机器人操作系统）与大模型整合，降低开发门槛。2025年，ROS社区用户超50万。\u003c/li\u003e\u003cli data-pid=\"zBh9U_hp\"\u003e政策支持：中国“十四五”规划投入百亿补贴机器人产业，欧盟2025年拨款20亿欧元扶持AI硬件。\u003c/li\u003e\u003cli data-pid=\"-SWUvGpz\"\u003e产学研联动：斯坦福、清华大学等高校与企业合作，加速数据与算法迭代。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"OxWbdxDP\"\u003e\u003cb\u003e具身智能的边界与可能\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-9b4c37540038863dab657b972ceccf66_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"937\" data-original-token=\"v2-279ece7f15cdbde1e9d8184e5a0cd1f5\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic1.zhimg.com/v2-9b4c37540038863dab657b972ceccf66_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"iYPW1tXn\"\u003e2025年6月，具身智能站在风口，却也身处迷雾。它承载了AI从云端走向现实的梦想，但高成本、低效率与技术瓶颈，让其商用化之路布满荆棘。更深层次的问题是：AI的未来，是否需要“具身”？虚拟AI的高效与专用设备的高ROI，提醒我们避免陷入“通用性”的执念。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Bftknkoy\"\u003eAI落地的“最后一公里”，不是单一技术的胜利，而是技术、场景与商业的协同突破。具身智能将在高危、交互密集的场景发光，但虚拟AI与专用设备同样不可或缺。对企业而言，聚焦场景、降本增效是当务之急；对投资者而言，技术驱动型企业更具潜力；对政策制定者而言，打造开源生态与数据平台将加速行业成熟。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Z5ti5jRg\"\u003e从波士顿动力Spot的巡检到NASA Valkyrie的太空探索，具身智能正迈出“学步”的第一步。未来十年，它或许能与人类并肩，定义智能时代的新图景——但前提是，先走出“最后一公里”的陷阱。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"AcH3hwA4\"\u003e参考资料：\u003c/p\u003e\u003cp data-pid=\"0IJnQoYf\"\u003eIDC《全球机器人与智能系统市场预测》（2025）\u003c/p\u003e\u003cp data-pid=\"xfVzxG1P\"\u003e高盛《具身智能技术展望》（2025）\u003c/p\u003e\u003cp data-pid=\"N3d0-7Qb\"\u003eDeepMind《RT-3模型技术报告》（2025）\u003c/p\u003e\u003cp data-pid=\"qNjkChID\"\u003e行业访谈与公开数据整理\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-798f55a17de48eb70754f42d45ba2d96_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1048\" data-rawheight=\"673\" data-original-token=\"v2-dae2915394bd32c8d859d15c7f4690d3\" class=\"origin_image zh-lightbox-thumb\" width=\"1048\" data-original=\"https://pica.zhimg.com/v2-798f55a17de48eb70754f42d45ba2d96_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"KJTMu_k1\"\u003e闲云野鹤笔生风，嬉笑怒骂在心中。\u003c/p\u003e\u003cp data-pid=\"TdqiRDYH\"\u003e互联网沉浮数十年，用故事讲道理，让复杂的知识变得简单有趣。\u003c/p\u003e\u003cp data-pid=\"XUOqeAyn\"\u003e喜欢的话，可以关注我。\u003c/p\u003e\u003cp data-pid=\"8761nSd5\"\u003e---------------------------------------------------\u003c/p\u003e\u003cp data-pid=\"why6e8fK\"\u003e图/网络 排版/柏墨 文字/柏墨\u003c/p\u003e\u003cp data-pid=\"8s0HpPz_\"\u003e纯属个人观点，无任何不良引导\u003c/p\u003e\u003cp data-pid=\"M5MkZHMM\"\u003e转载请注明出处\u003c/p\u003e\u003cp data-pid=\"26tmF0SQ\"\u003e【免责声明】\u003c/p\u003e\u003cp data-pid=\"whqRBzib\"\u003e本文部分信息和图片来源网络，如任何个人或单位认为文章涉及著作权等问题，敬请立即通知我，我将第一时间核实予以更改或删除。\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":1,"thumbnails":["https://picx.zhimg.com/50/v2-93cb5239fc6a5ed7f3903b9ea2f5304c_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-5efcc395d0c70173ea34c3637002d8ea_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-fe1a1886641a8cdf7f56daf0f7b64ccd_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-1f7d5706417e1966efefe63f71d26062_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-985331591a1278f707a6e6de167b6ab7_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-5561a178f74e79fdc9508a3990ca0da9_720w.jpg?source=b6762063"],"favorite_count":0,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1921234490266747939}","attached_info":"Cr4ICOKDvb/vqZLnjwEQBxoJMjU5NTQyMjk3II7X7sIGKAAwAEBGSiQKGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDESATAYACAAOgBiIGE4NDViNTdjZWFkMzQwN2I5YzhhOThjYTMyMmM0NzFichMxOTIxMjM0NDkwMjY2NzQ3OTM5qgEJcmVjb21tZW5kwgEgNzBmZmY0YjFhMmYwMmY2NzJjOWU2MGQ0ZTAzNWY3MjTyAQoIDBIGTm9ybWFs8gEoCAoSJDE1ODY3N2VmLWFjYjMtNGM2Mi1iNzAwLTI5ZTE1ODRhOGQ5ZvIBBggLEgIxMoICAIgC2L/hzfoykgIgNzBmZmY0YjFhMmYwMmY2NzJjOWU2MGQ0ZTAzNWY3MjSaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIYUGVyaW9kSW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIYQ29udGVudFdhcm1VcEJyZWFrSW5SdWxl2gIZVFNfU09VUkNFX1dBUk1fVVBfTk9STUFMMegCA/oCC05PUk1BTF9GTE9XigMgZWM3NTVhODJjZDY0NDkwNGE2M2I4ZTI5Y2ZlMWMzMDKaAw0KAnYyEAAaBW90aGVyqAMB2AMA6gMfdGV4dF8xMmhvdXJfdW5pZmluc2hlZF9yZWNhbGxlcvoDlwMSDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAhCACBiACCIjdjItYjE0ZjQ3NzRjM2QxMmIxYTBkZGQ5ODg0NzMwM2JjZTk6LQgCELgIGOAEIiN2Mi1kNTkwMjgzMjc4OTQ2Mjg2YTdiYTM5NzQxNTQzNDQyNzotCAUQuAgYqgYiI3YyLTVlZmNjMzk1ZDBjNzAxNzNlYTM0YzM2MzcwMDJkOGVhOi0IAhC4CBjGAyIjdjItNjc2MDQyNzM5NGY3MzE4MDhmYWJmMGE2MzljMDllN2I6LQgCELgIGM8FIiN2Mi1hMTQyZDMzMzFhMmJmYWIzMzA1MzA2NmVmNzE4MDZjYzotCAIQgAUYngciI3YyLWU1N2JiOTBkZTExZTE0NzEyZDk2ZTQ1Y2Y2M2YwNjU5Oi0IBBCACBipByIjdjItMjc5ZWNlN2YxNWNkYmRlMWU5ZDgxODRlNWEwY2QxZjU6LQgDEJgIGKEFIiN2Mi1kYWUyOTE1Mzk0YmQzMmM4ZDg1OWQxNWM3ZjQ2OTBkM4AEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAYPljij+BBQAAAAAAAAAAiQWfNa6/pXXSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUMkAYAoAZHqAYBkgIuCgkyNTk1NDIyOTcSEzE5MjEyMzQ0OTAyNjY3NDc5MzkYByIKSU1BR0VfVEVYVA==","action_card":false},{"id":"71_1750899122.350","type":"feed","offset":71,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750899122,"updated_time":1750899122,"target":{"id":"525106459","type":"article","url":"https://api.zhihu.com/articles/525106459","author":{"id":"c523fe5774e62fdc7b92f2634a4b7418","url":"https://api.zhihu.com/people/c523fe5774e62fdc7b92f2634a4b7418","user_type":"people","url_token":"ccj-23-56","name":"ewrfcas","headline":"喜欢绕远路","avatar_url":"https://pic1.zhimg.com/50/v2-f1ae83e294ce1c31f9ed44305632adc8_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":1498,"is_following":false,"is_followed":false},"title":"由浅入深了解Diffusion Model","image_url":"https://pic1.zhimg.com/v2-a650b4ed99b02f80b9d1c0687dea0848.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1654863015,"updated":1673251672,"voteup_count":5725,"voting":0,"comment_count":284,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"前言其实早在去年就看过大佬Lil关于diffusion model精彩的介绍 What are Diffusion Models? [1]但是后面一直没深入研究，很快就忘细节了。最近Diffusion Model火到爆炸（GLIDE[2],DALLE2[3],Imagen[4],和一系列Image Editing方法等等），所以又重新建起来学习了下。恐怕diffusion拥有成为下一代图像生成模型的代表的潜力（或者已经是了？）本文主要是对Lil博客进行翻译整理，会添加一些细节的理解和对照代码的思考，主要是为了方…","excerpt_new":"前言其实早在去年就看过大佬Lil关于diffusion model精彩的介绍 What are Diffusion Models? [1]但是后面一直没深入研究，很快就忘细节了。最近Diffusion Model火到爆炸（GLIDE[2],DALLE2[3],Imagen[4],和一系列Image Editing方法等等），所以又重新建起来学习了下。恐怕diffusion拥有成为下一代图像生成模型的代表的潜力（或者已经是了？）本文主要是对Lil博客进行翻译整理，会添加一些细节的理解和对照代码的思考，主要是为了方…","preview_type":"default","preview_text":"","content":"\u003ch2\u003e前言\u003c/h2\u003e\u003cp data-pid=\"oluW1as7\"\u003e其实早在去年就看过大佬Lil关于diffusion model精彩的介绍\u003ca href=\"https://link.zhihu.com/?target=https%3A//lilianweng.github.io/posts/2021-07-11-diffusion-models/%23forward-diffusion-process\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eWhat are Diffusion Models?\u003c/a\u003e \u003csup data-text=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"1\"\u003e[1]\u003c/sup\u003e但是后面一直没深入研究，很快就忘细节了。最近Diffusion Model火到爆炸（GLIDE\u003csup data-text=\"Nichol, Alex, et al. \u0026#34;Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\u0026#34; arXiv preprint arXiv:2112.10741 (2021).\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"2\"\u003e[2]\u003c/sup\u003e,DALLE2\u003csup data-text=\"Ramesh, Aditya, et al. \u0026#34;Hierarchical text-conditional image generation with clip latents.\u0026#34; arXiv preprint arXiv:2204.06125 (2022).\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"3\"\u003e[3]\u003c/sup\u003e,Imagen\u003csup data-text=\"Saharia, Chitwan, et al. \u0026#34;Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.\u0026#34; arXiv preprint arXiv:2205.11487 (2022).\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"4\"\u003e[4]\u003c/sup\u003e,和一系列Image Editing方法等等），所以又重新建起来学习了下。恐怕diffusion拥有成为下一代图像生成模型的代表的潜力（或者已经是了？）本文主要是对Lil博客进行翻译整理，会添加一些细节的理解和对照代码的思考，主要是为了方便自己学习记录，如果有理解错误的地方还请指出。\u003c/p\u003e\u003ch2\u003e什么是Diffusion Model（扩散模型）？\u003c/h2\u003e\u003cp data-pid=\"jb_uqxyM\"\u003e首先我们来看一下最近火爆各个公众号的text-to-image结果：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-500e1dbe9318c07aff33f45b66c7ab32_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"576\" data-original-token=\"v2-ee7ab4d360888b994cfb23e09a267407\" class=\"origin_image zh-lightbox-thumb\" width=\"1558\" data-original=\"https://pic1.zhimg.com/v2-500e1dbe9318c07aff33f45b66c7ab32_r.jpg\"/\u003e\u003cfigcaption\u003e图1. DALLE2生成结果\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-d82920823897c075ffb7ac8eff085635_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2116\" data-rawheight=\"788\" data-original-token=\"v2-830dee6ad2bd21825f8fc1095360844f\" class=\"origin_image zh-lightbox-thumb\" width=\"2116\" data-original=\"https://pic4.zhimg.com/v2-d82920823897c075ffb7ac8eff085635_r.jpg\"/\u003e\u003cfigcaption\u003e图2. Imagen生成结果\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"i4EguehP\"\u003e上述图片的结果都非常惊人，无论从真实度还是还原度都几乎无可挑剔。这里我们从由浅入深来了解一下Diffusion Model。首先还是放一个各类生成模型对比图:\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-42181e6098a90635a05cfeb1c1091afe_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"3416\" data-rawheight=\"2362\" data-original-token=\"v2-515bc44814d4827693ec51b7089364a6\" class=\"origin_image zh-lightbox-thumb\" width=\"3416\" data-original=\"https://pica.zhimg.com/v2-42181e6098a90635a05cfeb1c1091afe_r.jpg\"/\u003e\u003cfigcaption\u003e图3. 不同生成模型对比图（来源：Lil博客）\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"hpVithPV\"\u003ediffusion model和其他模型最大的区别是它的latent code(z)和原图是同尺寸大小的，当然最近也有基于压缩的latent diffusion model\u003csup data-text=\"Rombach, Robin, et al. \u0026#34;High-Resolution Image Synthesis with Latent Diffusion Models.\u0026#34; arXiv preprint arXiv:2112.10752 (2021).\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"5\"\u003e[5]\u003c/sup\u003e，不过是后话了。一句话概括diffusion model，即存在一系列高斯噪声（ \u003cimg src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/\u003e 轮），将输入图片 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e 变为纯高斯噪声 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T\" alt=\"x_T\" eeimg=\"1\"/\u003e 。而我们的模型则负责将 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T\" alt=\"x_T\" eeimg=\"1\"/\u003e 复原回图片 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e 。这样一来其实diffusion model和GAN很像，都是给定噪声 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T\" alt=\"x_T\" eeimg=\"1\"/\u003e生成图片 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e ，但是要强调的是，这里噪声 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T\" alt=\"x_T\" eeimg=\"1\"/\u003e 与图片\u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e是\u003cb\u003e同维度\u003c/b\u003e的。\u003c/p\u003e\u003cp data-pid=\"giFBy8a1\"\u003ediffusion model有很多种理解，这里介绍是基于denoising diffusion probabilistic models (DDPM)\u003csup data-text=\"Ho, Jonathan, Ajay Jain, and Pieter Abbeel. \u0026#34;Denoising diffusion probabilistic models.\u0026#34; Advances in Neural Information Processing Systems 33 (2020): 6840-6851.\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"6\"\u003e[6]\u003c/sup\u003e的\u003ci\u003e。\u003c/i\u003e\u003c/p\u003e\u003ch2\u003eDiffusion前向过程\u003c/h2\u003e\u003cp data-pid=\"t6Wr_2N-\"\u003e所谓前向过程，即往图片上加噪声的过程。虽然这个步骤无法做到图片生成，但是这是理解diffusion model以及\u003cb\u003e构建训练样本GT\u003c/b\u003e至关重要的一步。\u003c/p\u003e\u003cp data-pid=\"YPxCSL0r\"\u003e给定真实图片 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0%5Csim+q%28x%29\" alt=\"x_0\\sim q(x)\" eeimg=\"1\"/\u003e ,diffusion前向过程通过 \u003cimg src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/\u003e 次累计对其添加高斯噪声，得到 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2%2C...%2Cx_T\" alt=\"x_1,x_2,...,x_T\" eeimg=\"1\"/\u003e ，如下图的q过程。这里需要给定一系列的高斯分布方差的超参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5C%7B%5Cbeta_t%5Cin%280%2C1%29%5C%7D_%7Bt%3D1%7D%5E%7BT%7D\" alt=\"\\{\\beta_t\\in(0,1)\\}_{t=1}^{T}\" eeimg=\"1\"/\u003e .前向过程由于每个时刻 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 只与 \u003cimg src=\"https://www.zhihu.com/equation?tex=t-1\" alt=\"t-1\" eeimg=\"1\"/\u003e 时刻有关，所以也可以看做马尔科夫过程：\u003c/p\u003e\u003cp data-pid=\"GPKtAkwA\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_t%7Cx_%7Bt-1%7D%29%3D%5Cmathcal%7BN%7D%28x_t%3B%5Csqrt%7B1-%5Cbeta_t%7Dx_%7Bt-1%7D%2C%5Cbeta_t%5Cmathbf%7BI%7D%29%2C+q%28x_%7B1%3AT%7D%7Cx_0%29%3D%5Cprod_%7Bt%3D1%7D%5ET+q%28x_t%7Cx_%7Bt-1%7D%29.~%5Ctag%7B1%7D\" alt=\"q(x_t|x_{t-1})=\\mathcal{N}(x_t;\\sqrt{1-\\beta_t}x_{t-1},\\beta_t\\mathbf{I}), q(x_{1:T}|x_0)=\\prod_{t=1}^T q(x_t|x_{t-1}).~\\tag{1}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"PFUDpZCc\"\u003e这个过程中，随着 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 的增大， \u003cimg src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/\u003e 越来越接近纯噪声。当 \u003cimg src=\"https://www.zhihu.com/equation?tex=T%5Crightarrow%5Cinfty\" alt=\"T\\rightarrow\\infty\" eeimg=\"1\"/\u003e ， \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T\" alt=\"x_T\" eeimg=\"1\"/\u003e 是完全的高斯噪声（下面会证明，且与均值系数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csqrt%7B1-%5Cbeta_t%7D\" alt=\"\\sqrt{1-\\beta_t}\" eeimg=\"1\"/\u003e 的选择有关）。且实际中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta_t\" alt=\"\\beta_t\" eeimg=\"1\"/\u003e 随着t增大是递增的，即 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta_1%3C%5Cbeta_2%3C...%3C%5Cbeta_T\" alt=\"\\beta_1\u0026lt;\\beta_2\u0026lt;...\u0026lt;\\beta_T\" eeimg=\"1\"/\u003e 。在GLIDE的code中， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta_t\" alt=\"\\beta_t\" eeimg=\"1\"/\u003e 是由0.0001 到0.02线性插值（以 \u003cimg src=\"https://www.zhihu.com/equation?tex=T%3D1000\" alt=\"T=1000\" eeimg=\"1\"/\u003e 为基准， \u003cimg src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/\u003e 增加， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta_t\" alt=\"\\beta_t\" eeimg=\"1\"/\u003e 对应降低）。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-65bf3a96fb73eda390b02fdf602f0750_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"3832\" data-rawheight=\"1348\" data-qrcode-action=\"none\" data-original-token=\"v2-d7a4348eaae4d788cebda93dc7d83c4d\" class=\"origin_image zh-lightbox-thumb\" width=\"3832\" data-original=\"https://pic1.zhimg.com/v2-65bf3a96fb73eda390b02fdf602f0750_r.jpg\"/\u003e\u003cfigcaption\u003e图4. diffusion的前向(q)和逆向(p)过程，来源：DDPM\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"VAz3Omi5\"\u003e前向过程介绍结束前，需要讲述一下diffusion在实现和推导过程中要用到的两个重要特性。\u003c/p\u003e\u003ch3\u003e特性1：重参数（reparameterization trick）\u003c/h3\u003e\u003cp data-pid=\"XI9jrbcB\"\u003e重参数技巧在很多工作（gumbel softmax, VAE）中有所引用。如果我们要从某个分布中随机采样(高斯分布)一个样本，这个过程是无法反传梯度的。而这个通过高斯噪声采样得到 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/\u003e 的过程在diffusion中到处都是，因此我们需要通过重参数技巧来使得他可微。最通常的做法是吧随机性通过一个独立的随机变量( \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/\u003e )引导过去。举个例子，如果要从高斯分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=z%5Csim+%5Cmathcal%7BN%7D%28z%3B%5Cmu_%5Ctheta%2C%5Csigma_%5Ctheta%5E2%5Cmathbf%7BI%7D%29\" alt=\"z\\sim \\mathcal{N}(z;\\mu_\\theta,\\sigma_\\theta^2\\mathbf{I})\" eeimg=\"1\"/\u003e 采样一个z，我们可以写成:\u003c/p\u003e\u003cp data-pid=\"YKrwZ-85\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=z%3D%5Cmu_%5Ctheta%2B%5Csigma_%5Ctheta%5Codot%5Cepsilon%2C+%5Cepsilon%5Csim%5Cmathcal%7BN%7D%280%2C%5Cmathbf%7BI%7D%29.~%5Ctag%7B2%7D\" alt=\"z=\\mu_\\theta+\\sigma_\\theta\\odot\\epsilon, \\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}).~\\tag{2}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"V4I3-CJm\"\u003e上式的z依旧是有随机性的， 且满足均值为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmu_%5Ctheta\" alt=\"\\mu_\\theta\" eeimg=\"1\"/\u003e 方差为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_%5Ctheta%5E2\" alt=\"\\sigma_\\theta^2\" eeimg=\"1\"/\u003e 的高斯分布。这里的\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmu_%5Ctheta\" alt=\"\\mu_\\theta\" eeimg=\"1\"/\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%EF%BC%8C%5Csigma_%5Ctheta%5E2\" alt=\"，\\sigma_\\theta^2\" eeimg=\"1\"/\u003e可以是由参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e 的神经网络推断得到的。整个“采样”过程依旧梯度可导，随机性被转嫁到了\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/\u003e上。\u003c/p\u003e\u003ch3\u003e特性2：任意时刻的 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/\u003e 可以由 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/\u003e 表示\u003c/h3\u003e\u003cp data-pid=\"lyqxjaLa\"\u003e能够通过 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/\u003e 快速得到 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/\u003e 对后续diffusion model的推断和推导有巨大作用。首先我们假设 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Calpha_t%3D1-%5Cbeta_t\" alt=\"\\alpha_t=1-\\beta_t\" eeimg=\"1\"/\u003e ，并且 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Coverline%7B%5Calpha%7D_t%3D%5Cprod_%7Bi%3D1%7D%5E%7BT%7D%5Calpha_i\" alt=\"\\overline{\\alpha}_t=\\prod_{i=1}^{T}\\alpha_i\" eeimg=\"1\"/\u003e ，展开 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/\u003e 可以得到:\u003c/p\u003e\u003cp data-pid=\"_2jGdH9R\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+x_t%26%3D%5Csqrt%7Ba_t%7Dx_%7Bt-1%7D%2B%5Csqrt%7B1-%5Calpha_t%7Dz_%7B1%7D+%5Cquad+%5Cmathrm%7Bwhere%7D%5Cquad+z_%7B1%7D%2Cz_%7B2%7D%2C...%5Csim%5Cmathcal%7BN%7D%280%2C%5Cmathbf%7BI%7D%29%3B%5C%5C+%26%3D%5Csqrt%7Ba_t%7D%28%5Csqrt%7Ba_%7Bt-1%7D%7Dx_%7Bt-2%7D%2B%5Csqrt%7B1-%5Calpha_%7Bt-1%7D%7Dz_%7B2%7D%29%2B%5Csqrt%7B1-%5Calpha_t%7Dz_%7B1%7D%5C%5C+%26%3D%5Csqrt%7Ba_t+a_%7Bt-1%7D%7Dx_%7Bt-2%7D%2B%28%5Csqrt%7Ba_t%281-%5Calpha_%7Bt-1%7D%29%7Dz_%7B2%7D%2B%5Csqrt%7B1-%5Calpha_t%7Dz_%7B1%7D%29%5C%5C+%26%3D%5Csqrt%7Ba_t+a_%7Bt-1%7D%7Dx_%7Bt-2%7D%2B%5Csqrt%7B1-%5Calpha_%7Bt%7D%5Calpha_%7Bt-1%7D%7D%5Coverline%7Bz%7D_%7B2%7D+%5Cquad+%5Cmathrm%7Bwhere%7D%5Cquad%5Coverline%7Bz%7D_%7B2%7D%5Csim%5Cmathcal%7BN%7D%280%2C%5Cmathbf%7BI%7D%29%3B%5C%5C+%26%3D...%5C%5C+%26%3D%5Csqrt%7B%5Coverline%7B%5Calpha%7D_t%7Dx_0%2B%5Csqrt%7B1-%5Coverline%7B%5Calpha%7D_t%7D%5Coverline%7Bz%7D_t.~%5Ctag%7B3%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} x_t\u0026amp;=\\sqrt{a_t}x_{t-1}+\\sqrt{1-\\alpha_t}z_{1} \\quad \\mathrm{where}\\quad z_{1},z_{2},...\\sim\\mathcal{N}(0,\\mathbf{I});\\\\ \u0026amp;=\\sqrt{a_t}(\\sqrt{a_{t-1}}x_{t-2}+\\sqrt{1-\\alpha_{t-1}}z_{2})+\\sqrt{1-\\alpha_t}z_{1}\\\\ \u0026amp;=\\sqrt{a_t a_{t-1}}x_{t-2}+(\\sqrt{a_t(1-\\alpha_{t-1})}z_{2}+\\sqrt{1-\\alpha_t}z_{1})\\\\ \u0026amp;=\\sqrt{a_t a_{t-1}}x_{t-2}+\\sqrt{1-\\alpha_{t}\\alpha_{t-1}}\\overline{z}_{2} \\quad \\mathrm{where}\\quad\\overline{z}_{2}\\sim\\mathcal{N}(0,\\mathbf{I});\\\\ \u0026amp;=...\\\\ \u0026amp;=\\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha}_t}\\overline{z}_t.~\\tag{3} \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"gYUzcUoL\"\u003e由于独立高斯分布可加性，即 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BN%7D%280%2C%5Csigma_1%5E2%5Cmathbf%7BI%7D%29%2B%5Cmathcal%7BN%7D%280%2C%5Csigma_2%5E2%5Cmathbf%7BI%7D%29%5Csim%5Cmathcal%7BN%7D%280%2C%28%5Csigma_1%5E2%2B%5Csigma_2%5E2%29%5Cmathbf%7BI%7D%29\" alt=\"\\mathcal{N}(0,\\sigma_1^2\\mathbf{I})+\\mathcal{N}(0,\\sigma_2^2\\mathbf{I})\\sim\\mathcal{N}(0,(\\sigma_1^2+\\sigma_2^2)\\mathbf{I})\" eeimg=\"1\"/\u003e ,所以\u003c/p\u003e\u003cp data-pid=\"zYgCsx9_\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26%5Csqrt%7Ba_t%281-%5Calpha_%7Bt-1%7D%29%7Dz_%7B2%7D%5Csim%5Cmathcal%7BN%7D%280%2Ca_t%281-%5Calpha_%7Bt-1%7D%29%5Cmathbf%7BI%7D%29%5C%5C+%26%5Csqrt%7B1-%5Calpha_t%7Dz_%7B1%7D%5Csim%5Cmathcal%7BN%7D%280%2C%281-%5Calpha_t%29%5Cmathbf%7BI%7D%29%5C%5C+%26%5Csqrt%7Ba_t%281-%5Calpha_%7Bt-1%7D%29%7Dz_%7B2%7D%2B%5Csqrt%7B1-%5Calpha_t%7Dz_%7B1%7D%5Csim%5Cmathcal%7BN%7D%280%2C%5B%5Calpha_t%281-%5Calpha_%7Bt-1%7D%29%2B%281-%5Calpha_t%29%5D%5Cmathbf%7BI%7D%29%5C%5C+%26%3D%5Cmathcal%7BN%7D%280%2C%281-%5Calpha_t+%5Calpha_%7Bt-1%7D%29%5Cmathbf%7BI%7D%29.~%5Ctag%7B4%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} \u0026amp;\\sqrt{a_t(1-\\alpha_{t-1})}z_{2}\\sim\\mathcal{N}(0,a_t(1-\\alpha_{t-1})\\mathbf{I})\\\\ \u0026amp;\\sqrt{1-\\alpha_t}z_{1}\\sim\\mathcal{N}(0,(1-\\alpha_t)\\mathbf{I})\\\\ \u0026amp;\\sqrt{a_t(1-\\alpha_{t-1})}z_{2}+\\sqrt{1-\\alpha_t}z_{1}\\sim\\mathcal{N}(0,[\\alpha_t(1-\\alpha_{t-1})+(1-\\alpha_t)]\\mathbf{I})\\\\ \u0026amp;=\\mathcal{N}(0,(1-\\alpha_t \\alpha_{t-1})\\mathbf{I}).~\\tag{4} \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"BF2t7zkF\"\u003e因此可以混合两个高斯分布得到标准差为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csqrt%7B1-%5Calpha_t+%5Calpha_%7Bt-1%7D%7D\" alt=\"\\sqrt{1-\\alpha_t \\alpha_{t-1}}\" eeimg=\"1\"/\u003e 的混合高斯分布，然而Eq(3)中的 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Coverline%7Bz%7D_%7B2%7D\" alt=\"\\overline{z}_{2}\" eeimg=\"1\"/\u003e 仍然是标准高斯分布。而任意时刻的 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/\u003e 满足 \u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_t%7Cx_0%29%3D%5Cmathcal%7BN%7D%28x_t%3B%5Csqrt%7B%5Coverline%7Ba%7D_t%7Dx_0%2C+%281-%5Coverline%7Ba%7D_t%29%5Cmathbf%7BI%7D%29\" alt=\"q(x_t|x_0)=\\mathcal{N}(x_t;\\sqrt{\\overline{a}_t}x_0, (1-\\overline{a}_t)\\mathbf{I})\" eeimg=\"1\"/\u003e .\u003c/p\u003e\u003cp data-pid=\"o-Ynfnth\"\u003e一开始笔者一直不清楚为什么Eq(1)中diffusion的均值每次要乘上 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csqrt%7B1-%5Cbeta_t%7D\" alt=\"\\sqrt{1-\\beta_t}\" eeimg=\"1\"/\u003e .明明 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta_t\" alt=\"\\beta_t\" eeimg=\"1\"/\u003e 只是方差系数，怎么会影响均值呢？替换为任何一个新的超参数，保证它\u0026lt;1，也能够保证值域并且使得最后均值收敛到0（但是方差并不为1）. 然而通过Eq(3)(4)，可以发现当 \u003cimg src=\"https://www.zhihu.com/equation?tex=T%5Crightarrow%5Cinfty\" alt=\"T\\rightarrow\\infty\" eeimg=\"1\"/\u003e , \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T%5Csim%5Cmathcal%7BN%7D%280%2C%5Cmathbf%7BI%7D%29\" alt=\"x_T\\sim\\mathcal{N}(0,\\mathbf{I})\" eeimg=\"1\"/\u003e .所以\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csqrt%7B1-%5Cbeta_t%7D\" alt=\"\\sqrt{1-\\beta_t}\" eeimg=\"1\"/\u003e的均值系数能够稳定保证 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T\" alt=\"x_T\" eeimg=\"1\"/\u003e 最后收敛到方差为1的标准高斯分布，且在Eq(4)的推导中也更为简洁优雅。（注:很遗憾，笔者并没有系统地学习过随机过程，也许 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csqrt%7B1-%5Cbeta_t%7D\" alt=\"\\sqrt{1-\\beta_t}\" eeimg=\"1\"/\u003e 就是diffusion model前向过程收敛到标准高斯分布的唯一解，读者有了解也欢迎评论）\u003c/p\u003e\u003ch2\u003eDiffusion逆向（推断）过程\u003c/h2\u003e\u003cp data-pid=\"TdD5NafS\"\u003e如果说前向过程(forward)是加噪的过程，那么逆向过程(reverse)就是diffusion的去噪推断过程。如果我们能够逐步得到逆转后的分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_%7Bt-1%7D%7Cx_t%29\" alt=\"q(x_{t-1}|x_t)\" eeimg=\"1\"/\u003e ，就可以从完全的标准高斯分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T%5Csim%5Cmathcal%7BN%7D%280%2C%5Cmathbf%7BI%7D%29\" alt=\"x_T\\sim\\mathcal{N}(0,\\mathbf{I})\" eeimg=\"1\"/\u003e 还原出原图分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e .在文献\u003csup data-text=\"Feller, William. \u0026#34;On the theory of stochastic processes, with particular reference to applications.\u0026#34; Proceedings of the [First] Berkeley Symposium on Mathematical Statistics and Probability. University of California Press, 1949.\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"7\"\u003e[7]\u003c/sup\u003e中证明了如果 \u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_%7Bt%7D%7Cx_%7Bt-1%7D%29\" alt=\"q(x_{t}|x_{t-1})\" eeimg=\"1\"/\u003e 满足高斯分布且 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta_t\" alt=\"\\beta_t\" eeimg=\"1\"/\u003e 足够小，\u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_%7Bt-1%7D%7Cx_t%29\" alt=\"q(x_{t-1}|x_t)\" eeimg=\"1\"/\u003e仍然是一个高斯分布。然而我们无法简单推断\u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_%7Bt-1%7D%7Cx_t%29\" alt=\"q(x_{t-1}|x_t)\" eeimg=\"1\"/\u003e，因此我们使用深度学习模型（参数为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e ，目前主流是U-Net+attention的结构）去预测这样的一个逆向的分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%5Ctheta\" alt=\"p_\\theta\" eeimg=\"1\"/\u003e（类似VAE） ：\u003c/p\u003e\u003cp data-pid=\"nKG1aAcV\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+p_%5Ctheta%28X_%7B0%3AT%7D%29%26%3Dp%28x_T%29%5Cprod_%7Bt%3D1%7D%5ET+p_%5Ctheta%28x_%7Bt-1%7D%7Cx_t%29%3B~%5Ctag%7B5-1%7D%5C%5C+p_%5Ctheta%28x_%7Bt-1%7D%7Cx_t%29%26%3D%5Cmathcal%7BN%7D%28x_%7Bt-1%7D%3B%5Cmu_%5Ctheta%28x_t%2Ct%29%2C%5CSigma_%5Ctheta%28x_t%2Ct%29%29.~%5Ctag%7B5-2%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} p_\\theta(X_{0:T})\u0026amp;=p(x_T)\\prod_{t=1}^T p_\\theta(x_{t-1}|x_t);~\\tag{5-1}\\\\ p_\\theta(x_{t-1}|x_t)\u0026amp;=\\mathcal{N}(x_{t-1};\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t)).~\\tag{5-2} \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"Ewyt-yOq\"\u003e虽然我们无法得到逆转后的分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_%7Bt-1%7D%7Cx_t%29\" alt=\"q(x_{t-1}|x_t)\" eeimg=\"1\"/\u003e，但是如果知道 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e ，是可以通过贝叶斯公式得到 \u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_%7Bt-1%7D%7Cx_t%2Cx_0%29\" alt=\"q(x_{t-1}|x_t,x_0)\" eeimg=\"1\"/\u003e 为：\u003c/p\u003e\u003cp data-pid=\"8xtmjihr\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_%7Bt-1%7D%7Cx_t%2Cx_0%29%3D%5Cmathcal%7BN%7D%28x_%7Bt-1%7D%3B%5Ctilde%7B%5Cmu%7D%28x_t%2Cx_0%29%2C%5Ctilde%7B%5Cbeta_t%7D%5Cmathbf%7BI%7D%29~%5Ctag%7B6%7D\" alt=\"q(x_{t-1}|x_t,x_0)=\\mathcal{N}(x_{t-1};\\tilde{\\mu}(x_t,x_0),\\tilde{\\beta_t}\\mathbf{I})~\\tag{6}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"NPPJStB4\"\u003e过程如下：\u003c/p\u003e\u003cp data-pid=\"HX8vVT8V\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+q%28x_%7Bt-1%7D%7Cx_t%2Cx_0%29%26%3Dq%28x_t%7Cx_%7Bt-1%7D%2Cx_0%29%5Cfrac%7Bq%28x_%7Bt-1%7D%7Cx_0%29%7D%7Bq%28x_t%7Cx_0%29%7D~%5Ctag%7B7-1%7D%5C%5C+%26%5Cpropto+%5Cexp%5CBigg%28-%5Cfrac%7B1%7D%7B2%7D%5CBig%28%5Cfrac%7B%28x_t-%5Csqrt%7B%5Calpha_t%7Dx_%7Bt-1%7D%29%5E2%7D%7B%5Cbeta_t%7D%2B%5Cfrac%7B%28x_%7Bt-1%7D-%5Csqrt%7B%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7Dx_%7B0%7D%29%5E2%7D%7B1-%5Coverline%7Ba%7D_%7Bt-1%7D%7D-%5Cfrac%7B%28x_t-%5Csqrt%7B%5Coverline%7B%5Calpha%7D_t%7Dx_%7B0%7D%29%5E2%7D%7B1-%5Coverline%7Ba%7D_t%7D%5CBig%29%5CBigg%29~%5Ctag%7B7-2%7D%5C%5C+%26%3D%5Cexp%5CBigg%28-%5Cfrac%7B1%7D%7B2%7D%5CBig%28%5Cunderbrace%7B%28%5Cfrac%7B%5Calpha_t%7D%7B%5Cbeta_t%7D%2B%5Cfrac%7B1%7D%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7D%29x%5E2_%7Bt-1%7D%7D_%7Bx_%7Bt-1%7D%E6%96%B9%E5%B7%AE%7D-%5Cunderbrace%7B%28%5Cfrac%7B2%5Csqrt%7B%5Calpha_t%7D%7D%7B%5Cbeta_t%7Dx_t%2B%5Cfrac%7B2%5Csqrt%7B%5Coverline%7Ba%7D_%7Bt-1%7D%7D%7D%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7Dx_0%29x_%7Bt-1%7D%7D_%7Bx_%7Bt-1%7D%E5%9D%87%E5%80%BC%7D%2B%5Cunderbrace%7BC%28x_t%2Cx_0%29%7D_%7B%E4%B8%8Ex_%7Bt-1%7D%E6%97%A0%E5%85%B3%7D%5CBig%29%5CBigg%29.~%5Ctag%7B7-3%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} q(x_{t-1}|x_t,x_0)\u0026amp;=q(x_t|x_{t-1},x_0)\\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}~\\tag{7-1}\\\\ \u0026amp;\\propto \\exp\\Bigg(-\\frac{1}{2}\\Big(\\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{\\beta_t}+\\frac{(x_{t-1}-\\sqrt{\\overline{\\alpha}_{t-1}}x_{0})^2}{1-\\overline{a}_{t-1}}-\\frac{(x_t-\\sqrt{\\overline{\\alpha}_t}x_{0})^2}{1-\\overline{a}_t}\\Big)\\Bigg)~\\tag{7-2}\\\\ \u0026amp;=\\exp\\Bigg(-\\frac{1}{2}\\Big(\\underbrace{(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\overline{\\alpha}_{t-1}})x^2_{t-1}}_{x_{t-1}方差}-\\underbrace{(\\frac{2\\sqrt{\\alpha_t}}{\\beta_t}x_t+\\frac{2\\sqrt{\\overline{a}_{t-1}}}{1-\\overline{\\alpha}_{t-1}}x_0)x_{t-1}}_{x_{t-1}均值}+\\underbrace{C(x_t,x_0)}_{与x_{t-1}无关}\\Big)\\Bigg).~\\tag{7-3} \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"rGCGqke8\"\u003e上式(7-1)巧妙地将\u003cb\u003e逆向\u003c/b\u003e过程全部变回了\u003cb\u003e前向\u003c/b\u003e，即 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28x_%7Bt-1%7D%2Cx_0%29%5Crightarrow+x_t%3B%5Cquad+x_0%5Crightarrow+x_t%3B%5Cquad+x_0%5Crightarrow+x_%7Bt-1%7D\" alt=\"(x_{t-1},x_0)\\rightarrow x_t;\\quad x_0\\rightarrow x_t;\\quad x_0\\rightarrow x_{t-1}\" eeimg=\"1\"/\u003e ，而(7-2)分别写出其对应的高斯概率密度函数，(7-3)则整理成了 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_%7Bt-1%7D\" alt=\"x_{t-1}\" eeimg=\"1\"/\u003e 的高斯分布概率密度函数形式。一般的高斯概率密度函数的指数部分应该写为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cexp%5CBig%28%7B-%5Cfrac%7B%28x-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D%7D%5CBig%29%3D%5Cexp%5CBig%28-%5Cfrac%7B1%7D%7B2%7D%28%5Cfrac%7B1%7D%7B%5Csigma%5E2%7Dx%5E2-%5Cfrac%7B2%5Cmu%7D%7B%5Csigma%5E2%7Dx%2B%5Cfrac%7B%5Cmu%5E2%7D%7B%5Csigma%5E2%7D%29%5CBig%29\" alt=\"\\exp\\Big({-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\Big)=\\exp\\Big(-\\frac{1}{2}(\\frac{1}{\\sigma^2}x^2-\\frac{2\\mu}{\\sigma^2}x+\\frac{\\mu^2}{\\sigma^2})\\Big)\" eeimg=\"1\"/\u003e ，因此稍加整理我们可以得到(6)中的方差和均值为：\u003c/p\u003e\u003cp data-pid=\"7n4OSTJG\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26%5Cfrac%7B1%7D%7B%5Csigma%5E2%7D%3D%5Cfrac%7B1%7D%7B%5Ctilde%7B%5Cbeta%7D_t%7D%3D%28%5Cfrac%7B%5Calpha_t%7D%7B%5Cbeta_t%7D%2B%5Cfrac%7B1%7D%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7D%29%3B%5Cquad+%5Ctilde%7B%5Cbeta%7D_t%3D%5Cfrac%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7D%7B1-%5Coverline%7B%5Calpha%7D_t%7D%5Ccdot%5Cbeta_t~%5Ctag%7B8-1%7D%5C%5C+%26%5Cfrac%7B2%5Cmu%7D%7B%5Csigma%5E2%7D%3D%5Cfrac%7B2%5Ctilde%7B%5Cmu%7D_t%28x_t%2Cx_0%29%7D%7B%5Ctilde%7B%5Cbeta%7D_t%7D%3D%28%5Cfrac%7B2%5Csqrt%7B%5Calpha_t%7D%7D%7B%5Cbeta_t%7Dx_t%2B%5Cfrac%7B2%5Csqrt%7B%5Coverline%7Ba%7D_%7Bt-1%7D%7D%7D%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7Dx_0%29%3B%5C%5C%26%5Ctilde%7B%5Cmu%7D_t%28x_t%2Cx_0%29%3D%5Cfrac%7B%5Csqrt%7Ba%7D_t%281-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%29%7D%7B1-%5Coverline%7B%5Calpha%7D_t%7Dx_t%2B%5Cfrac%7B%5Csqrt%7B%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7D%5Cbeta_t%7D%7B1-%5Coverline%7B%5Calpha%7D_t%7Dx_0.~%5Ctag%7B8-2%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} \u0026amp;\\frac{1}{\\sigma^2}=\\frac{1}{\\tilde{\\beta}_t}=(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\overline{\\alpha}_{t-1}});\\quad \\tilde{\\beta}_t=\\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_t}\\cdot\\beta_t~\\tag{8-1}\\\\ \u0026amp;\\frac{2\\mu}{\\sigma^2}=\\frac{2\\tilde{\\mu}_t(x_t,x_0)}{\\tilde{\\beta}_t}=(\\frac{2\\sqrt{\\alpha_t}}{\\beta_t}x_t+\\frac{2\\sqrt{\\overline{a}_{t-1}}}{1-\\overline{\\alpha}_{t-1}}x_0);\\\\\u0026amp;\\tilde{\\mu}_t(x_t,x_0)=\\frac{\\sqrt{a}_t(1-\\overline{\\alpha}_{t-1})}{1-\\overline{\\alpha}_t}x_t+\\frac{\\sqrt{\\overline{\\alpha}_{t-1}}\\beta_t}{1-\\overline{\\alpha}_t}x_0.~\\tag{8-2} \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"EfR3iJJu\"\u003e根据\u003cb\u003e特性2\u003c/b\u003e，我们得知 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B%5Coverline%7Ba%7D_t%7D%7D%28x_t-%5Csqrt%7B1-%5Coverline%7Ba%7D_t%7D%5Coverline%7Bz%7D_t%29\" alt=\"x_0=\\frac{1}{\\sqrt{\\overline{a}_t}}(x_t-\\sqrt{1-\\overline{a}_t}\\overline{z}_t)\" eeimg=\"1\"/\u003e ，因此带入(8-2)可以得到\u003c/p\u003e\u003cp data-pid=\"MjOtVznT\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cmu%7D_t%3D%5Cfrac%7B1%7D%7B%5Csqrt%7Ba_t%7D%7D%28x_t-%5Cfrac%7B%5Cbeta_t%7D%7B%5Csqrt%7B1-%5Coverline%7Ba%7D_t%7D%7D%5Coverline%7Bz%7D_t%29%2C~%5Ctag%7B8-3%7D\" alt=\"\\tilde{\\mu}_t=\\frac{1}{\\sqrt{a_t}}(x_t-\\frac{\\beta_t}{\\sqrt{1-\\overline{a}_t}}\\overline{z}_t),~\\tag{8-3}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"XXVmm9Yr\"\u003e其中高斯分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Coverline%7Bz%7D_t\" alt=\"\\overline{z}_t\" eeimg=\"1\"/\u003e 为深度模型所预测的噪声（用于去噪），可看做为 \u003cimg src=\"https://www.zhihu.com/equation?tex=z_%5Ctheta%28x_t%2Ct%29\" alt=\"z_\\theta(x_t,t)\" eeimg=\"1\"/\u003e ，即得到：\u003c/p\u003e\u003cp data-pid=\"w5xy2n8o\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmu_%5Ctheta%28x_t%2Ct%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7Ba_t%7D%7D%28x_t-%5Cfrac%7B%5Cbeta_t%7D%7B%5Csqrt%7B1-%5Coverline%7Ba%7D_t%7D%7Dz_%5Ctheta%28x_t%2Ct%29%29.~%5Ctag%7B9%7D\" alt=\"\\mu_\\theta(x_t,t)=\\frac{1}{\\sqrt{a_t}}(x_t-\\frac{\\beta_t}{\\sqrt{1-\\overline{a}_t}}z_\\theta(x_t,t)).~\\tag{9}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"1qQLGAZr\"\u003e这样一来,DDPM的每一步的推断可以总结为：\u003c/p\u003e\u003cp data-pid=\"-TQyKTQX\"\u003e\u003cb\u003e1) 每个时间步通过 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/\u003e 和\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 来预测高斯噪声\u003cimg src=\"https://www.zhihu.com/equation?tex=z_%5Ctheta%28x_t%2Ct%29\" alt=\"z_\\theta(x_t,t)\" eeimg=\"1\"/\u003e，随后根据(9)得到均值 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmu_%5Ctheta%28x_t%2Ct%29\" alt=\"\\mu_\\theta(x_t,t)\" eeimg=\"1\"/\u003e .\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Wln52msX\"\u003e\u003cb\u003e2) 得到方差 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CSigma_%5Ctheta%28x_t%2Ct%29\" alt=\"\\Sigma_\\theta(x_t,t)\" eeimg=\"1\"/\u003e ，DDPM中使用untrained \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CSigma_%5Ctheta%28x_t%2Ct%29%3D%5Ctilde%7B%5Cbeta%7D_t\" alt=\"\\Sigma_\\theta(x_t,t)=\\tilde{\\beta}_t\" eeimg=\"1\"/\u003e ，且认为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cbeta%7D_t%3D%5Cbeta_t\" alt=\"\\tilde{\\beta}_t=\\beta_t\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cbeta%7D_t%3D%5Cfrac%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7D%7B1-%5Coverline%7B%5Calpha%7D_t%7D%5Ccdot%5Cbeta_t\" alt=\"\\tilde{\\beta}_t=\\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_t}\\cdot\\beta_t\" eeimg=\"1\"/\u003e 结果近似，在GLIDE中则是根据网络预测trainable方差 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CSigma_%5Ctheta%28x_t%2Ct%29\" alt=\"\\Sigma_\\theta(x_t,t)\" eeimg=\"1\"/\u003e .\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"t2-CYGL_\"\u003e\u003cb\u003e3) 根据(5-2)得到 \u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_%7Bt-1%7D%7Cx_t%29\" alt=\"q(x_{t-1}|x_t)\" eeimg=\"1\"/\u003e ，利用重参数得到 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_%7Bt-1%7D\" alt=\"x_{t-1}\" eeimg=\"1\"/\u003e .\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-c6f8d4c04be1f74a58a7011c39405bd1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1220\" data-rawheight=\"272\" data-original-token=\"v2-a650b4ed99b02f80b9d1c0687dea0848\" class=\"origin_image zh-lightbox-thumb\" width=\"1220\" data-original=\"https://pic4.zhimg.com/v2-c6f8d4c04be1f74a58a7011c39405bd1_r.jpg\"/\u003e\u003cfigcaption\u003e在x0和xt反复横跳的diffusion逆向过程\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003eDiffusion训练\u003c/h2\u003e\u003cp data-pid=\"YEC4n--a\"\u003e搞清楚diffusion的逆向过程之后，我们算是搞清楚diffusion的推断过程了。但是如何训练diffusion model以得到靠谱的\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmu_%5Ctheta%28x_t%2Ct%29\" alt=\"\\mu_\\theta(x_t,t)\" eeimg=\"1\"/\u003e和\u003cimg src=\"https://www.zhihu.com/equation?tex=%5CSigma_%5Ctheta%28x_t%2Ct%29\" alt=\"\\Sigma_\\theta(x_t,t)\" eeimg=\"1\"/\u003e呢？通过对真实数据分布下，最大化模型预测分布的对数似然，即优化在 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0%5Csim+q%28x_0%29\" alt=\"x_0\\sim q(x_0)\" eeimg=\"1\"/\u003e 下的 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%5Ctheta%28x_0%29\" alt=\"p_\\theta(x_0)\" eeimg=\"1\"/\u003e 交叉熵：\u003c/p\u003e\u003cp data-pid=\"8bqvk2TX\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D%5Cmathbb%7BE%7D_%7Bq%28x_0%29%7D%5B-%5Clog+p_%5Ctheta%28x_0%29%5D.~%5Ctag%7B10%7D\" alt=\"\\mathcal{L}=\\mathbb{E}_{q(x_0)}[-\\log p_\\theta(x_0)].~\\tag{10}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"8tEmlVoP\"\u003e从图4可以得知这个过程很像VAE，即可以使用变分下限(VLB)来优化负对数似然。由于KL散度非负，可得到：\u003c/p\u003e\u003cp data-pid=\"zyuFIBq4\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csmall%5Cbegin%7Balign%7D+-%5Clog+p_%5Ctheta%28x_0%29%26%5Cleq-%5Clog+p_%5Ctheta%28x_0%29%2BD_%7BKL%7D%28q%28x_%7B1%3AT%7D%7Cx_0%29%7C%7Cp_%5Ctheta%28x_%7B1%3AT%7D%7Cx_0%29%29%5C%5C+%26%3D-%5Clog+p_%5Ctheta%28x_0%29%2B%5Cmathbb%7BE%7D_%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%5Cleft%5B%5Clog%5Cfrac%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%2Fp_%5Ctheta%28x_0%29%7D%5Cright%5D%3B%5Cquad%5Cmathrm%7Bwhere%7D%5Cquad+p_%5Ctheta%28x_%7B1%3AT%7D%7Cx_0%29%3D%5Cfrac%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%7D%7Bp_%5Ctheta%28x_0%29%7D%5C%5C+%26%3D-%5Clog+p_%5Ctheta%28x_0%29%2B%5Cmathbb%7BE%7D_%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%5Cleft%5B%5Clog%5Cfrac%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%7D%2B%5Cunderbrace%7B%5Clog+p_%5Ctheta%28x_0%29%7D_%7B%E4%B8%8Eq%E6%97%A0%E5%85%B3%7D%5Cright%5D%5C%5C+%26%3D%5Cmathbb%7BE%7D_%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%5Cleft%5B%5Clog%5Cfrac%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%7D%5Cright%5D.~%5Ctag%7B11%7D+%5Cend%7Balign%7D\" alt=\"\\small\\begin{align} -\\log p_\\theta(x_0)\u0026amp;\\leq-\\log p_\\theta(x_0)+D_{KL}(q(x_{1:T}|x_0)||p_\\theta(x_{1:T}|x_0))\\\\ \u0026amp;=-\\log p_\\theta(x_0)+\\mathbb{E}_{q(x_{1:T}|x_0)}\\left[\\log\\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})/p_\\theta(x_0)}\\right];\\quad\\mathrm{where}\\quad p_\\theta(x_{1:T}|x_0)=\\frac{p_\\theta(x_{0:T})}{p_\\theta(x_0)}\\\\ \u0026amp;=-\\log p_\\theta(x_0)+\\mathbb{E}_{q(x_{1:T}|x_0)}\\left[\\log\\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})}+\\underbrace{\\log p_\\theta(x_0)}_{与q无关}\\right]\\\\ \u0026amp;=\\mathbb{E}_{q(x_{1:T}|x_0)}\\left[\\log\\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})}\\right].~\\tag{11} \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"Ra8WStps\"\u003e对(11)左右取期望 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7Bq%28x_0%29%7D\" alt=\"\\mathbb{E}_{q(x_0)}\" eeimg=\"1\"/\u003e ，利用到重积分中的\u003ca href=\"https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E5%25AF%258C%25E6%25AF%2594%25E5%25B0%25BC%25E5%25AE%259A%25E7%2590%2586\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eFubini\u003c/a\u003e定理：\u003c/p\u003e\u003cp data-pid=\"LvNPsmvo\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csmall%5Cmathcal%7BL%7D_%7BVLB%7D%3D%5Cunderbrace%7B%5Cmathbb%7BE%7D_%7Bq%28x_0%29%7D%5Cleft%28%5Cmathbb%7BE%7D_%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%5Cleft%5B%5Clog%5Cfrac%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%7D%5Cright%5D%5Cright%29%3D%5Cmathbb%7BE%7D_%7Bq%28x_%7B0%3AT%7D%29%7D%5Cleft%5B%5Clog%5Cfrac%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%7D%5Cright%5D%7D_%7BFubini%E5%AE%9A%E7%90%86%7D%5Cgeq%5Cmathbb%7BE%7D_%7Bq%28x_0%29%7D%5B-%5Clog+p_%5Ctheta%28x_0%29%5D.~%5Ctag%7B12%7D\" alt=\"\\small\\mathcal{L}_{VLB}=\\underbrace{\\mathbb{E}_{q(x_0)}\\left(\\mathbb{E}_{q(x_{1:T}|x_0)}\\left[\\log\\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})}\\right]\\right)=\\mathbb{E}_{q(x_{0:T})}\\left[\\log\\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})}\\right]}_{Fubini定理}\\geq\\mathbb{E}_{q(x_0)}[-\\log p_\\theta(x_0)].~\\tag{12}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"mxatjGH6\"\u003e能够最小化 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7BVLB%7D\" alt=\"\\mathcal{L}_{VLB}\" eeimg=\"1\"/\u003e 即可最小化我们的目标损失(10)。\u003c/p\u003e\u003cp data-pid=\"LkxFlSXo\"\u003e另一方面，通过\u003ca href=\"https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E7%25B0%25A1%25E6%25A3%25AE%25E4%25B8%258D%25E7%25AD%2589%25E5%25BC%258F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eJensen不等式\u003c/a\u003e也可以得到一样的目标：\u003c/p\u003e\u003cp data-pid=\"oC0gPptA\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cmathcal%7BL%7D%26%3D%5Cmathbb%7BE%7D_%7Bq%28x_0%29%7D%5B-%5Clog+p_%5Ctheta%28x_0%29%5D%5C%5C+%26%3D-%5Cmathbb%7BE%7D_%7Bq%28x_0%29%7D%5Clog%5Cleft%28p_%5Ctheta%28x_0%29%5Ccdot%5Cint+p_%5Ctheta%28x_%7B1%3AT%7D%29dx_%7B1%3AT%7D%5Cright%29%5C%5C+%26%3D-%5Cmathbb%7BE%7D_%7Bq%28x_0%29%7D%5Clog%5Cleft%28%5Cint+p_%5Ctheta%28x_%7B0%3AT%7D%29dx_%7B1%3AT%7D%5Cright%29%5C%5C+%26%3D-%5Cmathbb%7BE%7D_%7Bq%28x_0%29%7D%5Clog%5Cleft%28%5Cint+q%28x_%7B1%3AT%7D%7Cx_0%29%5Cfrac%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%7D%7B+q%28x_%7B1%3AT%7D%7Cx_0%29%7Ddx_%7B1%3AT%7D%5Cright%29%5C%5C+%26%3D-%5Cmathbb%7BE%7D_%7Bq%28x_0%29%7D%5Clog%5Cleft%28%5Cmathbb%7BE%7D_%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%5Cfrac%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%7D%7B+q%28x_%7B1%3AT%7D%7Cx_0%29%7D%5Cright%29%5C%5C+%26%5Cleq-%5Cmathbb%7BE%7D_%7Bq%28x_%7B0%3AT%7D%29%7D%5Clog%5Cfrac%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%7D%7B+q%28x_%7B1%3AT%7D%7Cx_0%29%7D%3B%5Cqquad%5Cqquad+Jensen%E4%B8%8D%E7%AD%89%E5%BC%8F%5C%5C+%26%3D%5Cmathbb%7BE%7D_%7Bq%28x_%7B0%3AT%7D%29%7D%5Clog%5Cfrac%7Bq%28x_%7B1%3AT%7D%7Cx_0%29%7D%7Bp_%5Ctheta%28x_%7B0%3AT%7D%29%7D%3D%5Cmathcal%7BL%7D_%7BVLB%7D.~%5Ctag%7B13%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} \\mathcal{L}\u0026amp;=\\mathbb{E}_{q(x_0)}[-\\log p_\\theta(x_0)]\\\\ \u0026amp;=-\\mathbb{E}_{q(x_0)}\\log\\left(p_\\theta(x_0)\\cdot\\int p_\\theta(x_{1:T})dx_{1:T}\\right)\\\\ \u0026amp;=-\\mathbb{E}_{q(x_0)}\\log\\left(\\int p_\\theta(x_{0:T})dx_{1:T}\\right)\\\\ \u0026amp;=-\\mathbb{E}_{q(x_0)}\\log\\left(\\int q(x_{1:T}|x_0)\\frac{p_\\theta(x_{0:T})}{ q(x_{1:T}|x_0)}dx_{1:T}\\right)\\\\ \u0026amp;=-\\mathbb{E}_{q(x_0)}\\log\\left(\\mathbb{E}_{q(x_{1:T}|x_0)}\\frac{p_\\theta(x_{0:T})}{ q(x_{1:T}|x_0)}\\right)\\\\ \u0026amp;\\leq-\\mathbb{E}_{q(x_{0:T})}\\log\\frac{p_\\theta(x_{0:T})}{ q(x_{1:T}|x_0)};\\qquad\\qquad Jensen不等式\\\\ \u0026amp;=\\mathbb{E}_{q(x_{0:T})}\\log\\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})}=\\mathcal{L}_{VLB}.~\\tag{13} \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"4kaMUJxd\"\u003e进一步对\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7BVLB%7D\" alt=\"\\mathcal{L}_{VLB}\" eeimg=\"1\"/\u003e推导，可以得到熵与多个KL散度的累加，具体可见文献\u003csup data-text=\"Sohl-Dickstein, Jascha, et al. \u0026#34;Deep unsupervised learning using nonequilibrium thermodynamics.\u0026#34; International Conference on Machine Learning. PMLR, 2015.\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"8\"\u003e[8]\u003c/sup\u003e.这里我就复制一波Lil的博客中的推导过程：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-8e5259ec37ddfc2886d7515a47e4a5d0_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1540\" data-rawheight=\"1148\" data-original-token=\"v2-313dbca3c5a16398a5ff9e7737f7fc91\" class=\"origin_image zh-lightbox-thumb\" width=\"1540\" data-original=\"https://pic3.zhimg.com/v2-8e5259ec37ddfc2886d7515a47e4a5d0_r.jpg\"/\u003e\u003cfigcaption\u003e进一步推导VLB，得到组合的KL散度和熵\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"V5cSrdxu\"\u003e也可写为：\u003c/p\u003e\u003cp data-pid=\"uTSxw0q2\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26%5Cmathcal%7BL%7D_%7BVLB%7D%3DL_T%2BL_%7BT-1%7D%2B...%2BL_0~%5Ctag%7B14-1%7D%5C%5C+%26L_T%3DD_%7BKL%7D%28q%28x_T%7Cx_0%29%7C%7Cp_%5Ctheta%28x_T%29%29~%5Ctag%7B14-2%7D%5C%5C+%26L_t%3DD_%7BKL%7D%28q%28x_t%7Cx_%7Bt%2B1%7D%2Cx_0%29%7C%7Cp_%5Ctheta%28x_t%7Cx_%7Bt%2B1%7D%29%29%3B%5Cqquad+1%5Cleq+t+%5Cleq+T-1~%5Ctag%7B14-3%7D%5C%5C+%26L_0%3D-%5Clog+p_%5Ctheta%28x_0%7Cx_1%29.~%5Ctag%7B14-4%7D%5C%5C+%5Cend%7Balign%7D\" alt=\"\\begin{align} \u0026amp;\\mathcal{L}_{VLB}=L_T+L_{T-1}+...+L_0~\\tag{14-1}\\\\ \u0026amp;L_T=D_{KL}(q(x_T|x_0)||p_\\theta(x_T))~\\tag{14-2}\\\\ \u0026amp;L_t=D_{KL}(q(x_t|x_{t+1},x_0)||p_\\theta(x_t|x_{t+1}));\\qquad 1\\leq t \\leq T-1~\\tag{14-3}\\\\ \u0026amp;L_0=-\\log p_\\theta(x_0|x_1).~\\tag{14-4}\\\\ \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"GU7-i-IN\"\u003e由于前向 \u003cimg src=\"https://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\"/\u003e 没有可学习参数，而 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T\" alt=\"x_T\" eeimg=\"1\"/\u003e则是纯高斯噪声， \u003cimg src=\"https://www.zhihu.com/equation?tex=L_T\" alt=\"L_T\" eeimg=\"1\"/\u003e 可以当做常量忽略。而 \u003cimg src=\"https://www.zhihu.com/equation?tex=L_t\" alt=\"L_t\" eeimg=\"1\"/\u003e 则可以看做拉近2个高斯分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=q%28x_%7Bt-1%7D%7Cx_t%2Cx_0%29%3D%5Cmathcal%7BN%7D%28x_%7Bt-1%7D%3B%5Ctilde%7B%5Cmu%7D%28x_t%2Cx_0%29%2C%5Ctilde%7B%5Cbeta_t%7D%5Cmathbf%7BI%7D%29\" alt=\"q(x_{t-1}|x_t,x_0)=\\mathcal{N}(x_{t-1};\\tilde{\\mu}(x_t,x_0),\\tilde{\\beta_t}\\mathbf{I})\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%5Ctheta%28x_%7Bt-1%7D%7Cx_t%29%3D%5Cmathcal%7BN%7D%28x_%7Bt-1%7D%3B%5Cmu_%5Ctheta%28x_t%2Ct%29%2C%5CSigma_%5Ctheta%29\" alt=\"p_\\theta(x_{t-1}|x_t)=\\mathcal{N}(x_{t-1};\\mu_\\theta(x_t,t),\\Sigma_\\theta)\" eeimg=\"1\"/\u003e ，根据\u003ca href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence%23Multivariate_normal_distributions\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e多元高斯分布的KL散度求解\u003c/a\u003e：\u003c/p\u003e\u003cp data-pid=\"dF1tfBLi\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=L_t%3D%5Cmathbb%7BE%7D_q%5Cleft%5B%5Cfrac%7B1%7D%7B2%7C%7C%5CSigma_%5Ctheta%28x_t%2Ct%29%7C%7C_2%5E2%7D%7C%7C%5Ctilde%7B%5Cmu%7D_t%28x_t%2Cx_0%29-%5Cmu_%5Ctheta%28x_t%2Ct%29%7C%7C%5E2%5Cright%5D%2BC%2C~%5Ctag%7B15%7D\" alt=\"L_t=\\mathbb{E}_q\\left[\\frac{1}{2||\\Sigma_\\theta(x_t,t)||_2^2}||\\tilde{\\mu}_t(x_t,x_0)-\\mu_\\theta(x_t,t)||^2\\right]+C,~\\tag{15}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"GMHqd2Fl\"\u003e其中C是与模型参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e 无关的常量。吧(8-3)的 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cmu%7D_t%28x_t%2Cx_0%29\" alt=\"\\tilde{\\mu}_t(x_t,x_0)\" eeimg=\"1\"/\u003e (9)的 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmu_%5Ctheta%28x_t%2Ct%29\" alt=\"\\mu_\\theta(x_t,t)\" eeimg=\"1\"/\u003e 和(3)的 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/\u003e 带入(15)可以得到：\u003c/p\u003e\u003cp data-pid=\"AK49aUXy\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+L_t%26%3D%5Cmathbb%7BE%7D_%7Bx_0%2C%5Coverline%7Bz%7D_t%7D%5Cleft%5B%5Cfrac%7B1%7D%7B2%7C%7C%5CSigma_%5Ctheta%28x_t%2Ct%29%7C%7C_2%5E2%7D%7C%7C%5Ctilde%7B%5Cmu%7D_t%28x_t%2Cx_0%29-%5Cmu_%5Ctheta%28x_t%2Ct%29%7C%7C%5E2%5Cright%5D%5C%5C+%26%3D%5Cmathbb%7BE%7D_%7Bx_0%2C%5Coverline%7Bz%7D_t%7D%5Cleft%5B%5Cfrac%7B1%7D%7B2%7C%7C%5CSigma_%5Ctheta%28x_t%2Ct%29%7C%7C_2%5E2%7D%7C%7C%5Cfrac%7B1%7D%7B%5Csqrt%7B%5Coverline%7Ba%7D_t%7D%7D%28x_t-%5Cfrac%7B%5Cbeta_t%7D%7B%5Csqrt%7B1-%5Coverline%7Ba%7D_t%7D%7D%5Coverline%7Bz%7D_t%29-%5Cfrac%7B1%7D%7B%5Csqrt%7B%5Coverline%7Ba%7D_t%7D%7D%28x_t-%5Cfrac%7B%5Cbeta_t%7D%7B%5Csqrt%7B1-%5Coverline%7Ba%7D_t%7D%7Dz_%5Ctheta%28x_t%2Ct%29%29%7C%7C%5E2%5Cright%5D%5C%5C+%26%3D%5Cmathbb%7BE%7D_%7Bx_0%2C%5Coverline%7Bz%7D_t%7D%5Cleft%5B%5Cfrac%7B%5Cbeta_t%5E2%7D%7B2%5Calpha_t%281-%5Coverline%7B%5Calpha%7D_t%7C%7C%5CSigma_%5Ctheta%7C%7C_2%5E2%29%7D%7C%7C%5Coverline%7Bz%7D_t-z_%5Ctheta%28x_t%2Ct%29%7C%7C%5E2%5Cright%5D%5C%5C+%26%3D%5Cmathbb%7BE%7D_%7Bx_0%2C%5Coverline%7Bz%7D_t%7D%5Cleft%5B%5Cfrac%7B%5Cbeta_t%5E2%7D%7B2%5Calpha_t%281-%5Coverline%7B%5Calpha%7D_t%7C%7C%5CSigma_%5Ctheta%7C%7C_2%5E2%29%7D%7C%7C%5Coverline%7Bz%7D_t-z_%5Ctheta%28%5Csqrt%7B%5Coverline%7B%5Calpha%7D_t%7Dx_0%2B%5Csqrt%7B1-%5Coverline%7B%5Calpha%7D_t%7D%5Coverline%7Bz%7D_t%2Ct%29%7C%7C%5E2%5Cright%5D.~%5Ctag%7B16%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} L_t\u0026amp;=\\mathbb{E}_{x_0,\\overline{z}_t}\\left[\\frac{1}{2||\\Sigma_\\theta(x_t,t)||_2^2}||\\tilde{\\mu}_t(x_t,x_0)-\\mu_\\theta(x_t,t)||^2\\right]\\\\ \u0026amp;=\\mathbb{E}_{x_0,\\overline{z}_t}\\left[\\frac{1}{2||\\Sigma_\\theta(x_t,t)||_2^2}||\\frac{1}{\\sqrt{\\overline{a}_t}}(x_t-\\frac{\\beta_t}{\\sqrt{1-\\overline{a}_t}}\\overline{z}_t)-\\frac{1}{\\sqrt{\\overline{a}_t}}(x_t-\\frac{\\beta_t}{\\sqrt{1-\\overline{a}_t}}z_\\theta(x_t,t))||^2\\right]\\\\ \u0026amp;=\\mathbb{E}_{x_0,\\overline{z}_t}\\left[\\frac{\\beta_t^2}{2\\alpha_t(1-\\overline{\\alpha}_t||\\Sigma_\\theta||_2^2)}||\\overline{z}_t-z_\\theta(x_t,t)||^2\\right]\\\\ \u0026amp;=\\mathbb{E}_{x_0,\\overline{z}_t}\\left[\\frac{\\beta_t^2}{2\\alpha_t(1-\\overline{\\alpha}_t||\\Sigma_\\theta||_2^2)}||\\overline{z}_t-z_\\theta(\\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha}_t}\\overline{z}_t,t)||^2\\right].~\\tag{16} \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"48lPQhbC\"\u003e 从(16)可以看出，diffusion训练的核心就是取学习高斯噪声 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Coverline%7Bz%7D_t\" alt=\"\\overline{z}_t\" eeimg=\"1\"/\u003e , \u003cimg src=\"https://www.zhihu.com/equation?tex=z_%5Ctheta\" alt=\"z_\\theta\" eeimg=\"1\"/\u003e 之间的MSE。\u003c/p\u003e\u003cp data-pid=\"KwTuie9v\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=L_0%3D-%5Clog+p_%5Ctheta%28x_0%7Cx_1%29\" alt=\"L_0=-\\log p_\\theta(x_0|x_1)\" eeimg=\"1\"/\u003e 相当于最后一步的熵，DDPM论文指出，从 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_1\" alt=\"x_1\" eeimg=\"1\"/\u003e 到 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e 应该是一个离散化过程，因为图像RGB值都是离散化的。DDPM针对 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%5Ctheta%28x_0%7Cx_1%29\" alt=\"p_\\theta(x_0|x_1)\" eeimg=\"1\"/\u003e 构建了一个离散化的分段积分累乘，有点类似基于分类目标的自回归(auto-regressive)学习。有兴趣的同学可以去参考原文。\u003c/p\u003e\u003cp data-pid=\"lpORSx1y\"\u003eDDPM将loss进一步简化为:\u003c/p\u003e\u003cp data-pid=\"H49Rfsk2\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=L_t%5E%7Bsimple%7D%3D%5Cmathbb%7BE%7D_%7Bx_0%2C%5Coverline%7Bz%7D_t%7D%5Cleft%5B%7C%7C%5Coverline%7Bz%7D_t-z_%5Ctheta%28%5Csqrt%7B%5Coverline%7B%5Calpha%7D_t%7Dx_0%2B%5Csqrt%7B1-%5Coverline%7B%5Calpha%7D_t%7D%5Coverline%7Bz%7D_t%2Ct%29%7C%7C%5E2%5Cright%5D.~%5Ctag%7B17%7D\" alt=\"L_t^{simple}=\\mathbb{E}_{x_0,\\overline{z}_t}\\left[||\\overline{z}_t-z_\\theta(\\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha}_t}\\overline{z}_t,t)||^2\\right].~\\tag{17}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"yTYHdr5f\"\u003e正如之前提过的，DDPM并没有将模型预测的方差 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CSigma_%5Ctheta%28x_t%2Ct%29\" alt=\"\\Sigma_\\theta(x_t,t)\" eeimg=\"1\"/\u003e 考虑到训练和推断中，而是通过untrained \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta_t\" alt=\"\\beta_t\" eeimg=\"1\"/\u003e 或者(8-1) \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cbeta%7D_t\" alt=\"\\tilde{\\beta}_t\" eeimg=\"1\"/\u003e 代替。他们发现 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5CSigma_%5Ctheta\" alt=\"\\Sigma_\\theta\" eeimg=\"1\"/\u003e 可能导致训练的不稳定。\u003c/p\u003e\u003cp data-pid=\"oDqYmvO5\"\u003e训练过程可以看做：\u003c/p\u003e\u003cp data-pid=\"W1NKtdRI\"\u003e\u003cb\u003e1）获取输入 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e ，从 \u003cimg src=\"https://www.zhihu.com/equation?tex=1...T\" alt=\"1...T\" eeimg=\"1\"/\u003e 随机采样一个 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e .\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"igVRWFdD\"\u003e\u003cb\u003e2)  从标准高斯分布采样一个噪声 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Coverline%7Bz%7D_t%5Csim%5Cmathcal%7BN%7D%280%2C%5Cmathbf%7BI%7D%29\" alt=\"\\overline{z}_t\\sim\\mathcal{N}(0,\\mathbf{I})\" eeimg=\"1\"/\u003e .\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"N77ZshRw\"\u003e\u003cb\u003e3)  最小化 \u003cimg src=\"https://www.zhihu.com/equation?tex=%7C%7C%5Coverline%7Bz%7D_t-z_%5Ctheta%28%5Csqrt%7B%5Coverline%7B%5Calpha%7D_t%7Dx_0%2B%5Csqrt%7B1-%5Coverline%7B%5Calpha%7D_t%7D%5Coverline%7Bz%7D_t%2Ct%29%7C%7C\" alt=\"||\\overline{z}_t-z_\\theta(\\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha}_t}\\overline{z}_t,t)||\" eeimg=\"1\"/\u003e .\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"svgpnAgN\"\u003e最后再附上DDPM提供的训练/测试（采样）流程图\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-6a41afbb1bf22710efc37646b69ea085_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"4098\" data-rawheight=\"1014\" data-original-token=\"v2-117c91f1b7052bbeee4f8d31b6fd1d20\" class=\"origin_image zh-lightbox-thumb\" width=\"4098\" data-original=\"https://pic2.zhimg.com/v2-6a41afbb1bf22710efc37646b69ea085_r.jpg\"/\u003e\u003cfigcaption\u003eDDPM训练测试算法流程图（来源：DDPM）\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e加速Diffusion采样和方差的选择(DDIM)\u003c/h2\u003e\u003cp data-pid=\"XNSAmqUG\"\u003eDDPM的高质量生成依赖于较大的 \u003cimg src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/\u003e （一般为1000或以上），这就导致diffusion的前向过程非常缓慢。在denoising diffusion implicit model (DDIM)\u003csup data-text=\"Song, Jiaming, Chenlin Meng, and Stefano Ermon. \u0026#34;Denoising diffusion implicit models.\u0026#34; arXiv preprint arXiv:2010.02502 (2020).\" data-url=\"\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"9\"\u003e[9]\u003c/sup\u003e中提出了一种牺牲多样性来换取更快推断的手段。\u003c/p\u003e\u003cp data-pid=\"_JgqR1_T\"\u003e根据\u003cb\u003e特性2\u003c/b\u003e和独立高斯分布可加性\u003cb\u003e，\u003c/b\u003e我们可以得到 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_%7Bt-1%7D\" alt=\"x_{t-1}\" eeimg=\"1\"/\u003e 为：\u003c/p\u003e\u003cp data-pid=\"5T9lQxru\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+x_%7Bt-1%7D%26%3D%5Csqrt%7B%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7Dx_0%2B%5Csqrt%7B1-%5Coverline%7Ba%7D_%7Bt-1%7D%7D%5Coverline%7Bz%7D_%7Bt-1%7D%5C%5C+%26%3D%5Csqrt%7B%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7Dx_0%2B%5Csqrt%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D-%5Csigma_t%5E2%7D%5Coverline%7Bz%7D_t%2B%5Csigma_t+z_t%5C%5C+%26%3D%5Csqrt%7B%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7Dx_0%2B%5Csqrt%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D-%5Csigma_t%5E2%7D%28%5Cfrac%7Bx_t-%5Csqrt%7B%5Coverline%7Ba%7D_t%7Dx_0%7D%7B%5Csqrt%7B1-%5Coverline%7B%5Calpha%7D_t%7D%7D%29%2B%5Csigma_t+z_t%5C%5C+q_%5Csigma%28x_%7Bt-1%7D%7Cx_t%2Cx_0%29%26%3D%5Cmathcal%7BN%7D%28x_%7Bt-1%7D%3B%5Csqrt%7B%5Coverline%7Ba%7D_%7Bt-1%7D%7Dx_0%2B%5Csqrt%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D-%5Csigma%5E2_t%7D%28%5Cfrac%7Bx_t-%5Csqrt%7B%5Coverline%7Ba%7D_t%7Dx_0%7D%7B%5Csqrt%7B1-%5Coverline%7B%5Calpha%7D_t%7D%7D%29%2C+%5Csigma_t%5E2%5Cmathbf%7BI%7D%29.~%5Ctag%7B18%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} x_{t-1}\u0026amp;=\\sqrt{\\overline{\\alpha}_{t-1}}x_0+\\sqrt{1-\\overline{a}_{t-1}}\\overline{z}_{t-1}\\\\ \u0026amp;=\\sqrt{\\overline{\\alpha}_{t-1}}x_0+\\sqrt{1-\\overline{\\alpha}_{t-1}-\\sigma_t^2}\\overline{z}_t+\\sigma_t z_t\\\\ \u0026amp;=\\sqrt{\\overline{\\alpha}_{t-1}}x_0+\\sqrt{1-\\overline{\\alpha}_{t-1}-\\sigma_t^2}(\\frac{x_t-\\sqrt{\\overline{a}_t}x_0}{\\sqrt{1-\\overline{\\alpha}_t}})+\\sigma_t z_t\\\\ q_\\sigma(x_{t-1}|x_t,x_0)\u0026amp;=\\mathcal{N}(x_{t-1};\\sqrt{\\overline{a}_{t-1}}x_0+\\sqrt{1-\\overline{\\alpha}_{t-1}-\\sigma^2_t}(\\frac{x_t-\\sqrt{\\overline{a}_t}x_0}{\\sqrt{1-\\overline{\\alpha}_t}}), \\sigma_t^2\\mathbf{I}).~\\tag{18} \\end{align}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"eSzUWIH4\"\u003e不同于(6)和(9)，(18)\u003cb\u003e将方差 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_t%5E2\" alt=\"\\sigma_t^2\" eeimg=\"1\"/\u003e 引入到了均值中\u003c/b\u003e，当 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_t%5E2%3D%5Ctilde%7B%5Cbeta%7D_t%3D%5Cfrac%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7D%7B1-%5Coverline%7B%5Calpha%7D_t%7D%5Cbeta_t\" alt=\"\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_t}\\beta_t\" eeimg=\"1\"/\u003e 时，(18)等价于(6)。\u003c/p\u003e\u003cp data-pid=\"NKr8uQr9\"\u003e在DDIM中吧由(18)经过贝叶斯得到的 \u003cimg src=\"https://www.zhihu.com/equation?tex=q_%5Csigma%28x_t%7Cx_%7Bt-1%7D%2Cx_0%29\" alt=\"q_\\sigma(x_t|x_{t-1},x_0)\" eeimg=\"1\"/\u003e 称为非马尔科夫过程，因为 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/\u003e 的概率同时依赖于 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_%7Bt-1%7D\" alt=\"x_{t-1}\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_0\" alt=\"x_0\" eeimg=\"1\"/\u003e 。（笔者并不了解刻意强调这个非马尔科夫是原因，也许是为了使得(18)中方差出现在均值合理化？）DDIM进一步定义了 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_t%28%5Ceta%29%5E2%3D%5Ceta%5Ccdot%5Ctilde%7B%5Cbeta%7D_t\" alt=\"\\sigma_t(\\eta)^2=\\eta\\cdot\\tilde{\\beta}_t\" eeimg=\"1\"/\u003e .当 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ceta%3D0\" alt=\"\\eta=0\" eeimg=\"1\"/\u003e 时，diffusion的sample过程会丧失所有随机性从而得到一个deterministic的结果（但是可以改变 \u003cimg src=\"https://www.zhihu.com/equation?tex=x_T\" alt=\"x_T\" eeimg=\"1\"/\u003e ）。而 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ceta%3D1\" alt=\"\\eta=1\" eeimg=\"1\"/\u003e 则DDIM等价于DDPM(使用 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cbeta%7D_t\" alt=\"\\tilde{\\beta}_t\" eeimg=\"1\"/\u003e 作为方差的版本).用随机性换取生成性能的类似操作在GAN中也可以通过对latent code操作实现。\u003c/p\u003e\u003cp data-pid=\"XAmU7KMz\"\u003e对于方差 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_t%5E2\" alt=\"\\sigma_t^2\" eeimg=\"1\"/\u003e 的选择，我们在这里重新整理一下\u003c/p\u003e\u003cp data-pid=\"6ziuXny5\"\u003eDDPM：\u003c/p\u003e\u003cp data-pid=\"z5gnOsj_\"\u003e1） \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_%7Bt%2C%5Ctheta%7D%5E2%3D%5CSigma_%5Ctheta%28x_t%2Ct%29\" alt=\"\\sigma_{t,\\theta}^2=\\Sigma_\\theta(x_t,t)\" eeimg=\"1\"/\u003e 相当于模型学习的方差，DDPM称为learned，实际没有使用（但是GLIDE使用的是这种方差）。\u003c/p\u003e\u003cp data-pid=\"wPVML-qX\"\u003e2） \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_%7Bt%2Cs%7D%5E2%3D%5Ctilde%7B%5Cbeta%7D_t%3D%5Cfrac%7B1-%5Coverline%7B%5Calpha%7D_%7Bt-1%7D%7D%7B1-%5Coverline%7B%5Calpha%7D_t%7D%5Cbeta_t\" alt=\"\\sigma_{t,s}^2=\\tilde{\\beta}_t=\\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_t}\\beta_t\" eeimg=\"1\"/\u003e ，由(8-1)得到，DDPM称为fixedsmall，用于celebahq和lsun。\u003c/p\u003e\u003cp data-pid=\"mN-17Z-C\"\u003e3） \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_%7Bt%2Cl%7D%5E2%3D%5Cbeta_t\" alt=\"\\sigma_{t,l}^2=\\beta_t\" eeimg=\"1\"/\u003e ，DDPM称为fixedlarge，用于cifar10，注意 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_%7Bt%2Cl%7D%3E%5Csigma_%7Bt%2Cs%7D\" alt=\"\\sigma_{t,l}\u0026gt;\\sigma_{t,s}\" eeimg=\"1\"/\u003e ，\u003cb\u003efixedlarge的方差大于fixedsmall的\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"1Z7VscYF\"\u003eDDIM：\u003c/p\u003e\u003cp data-pid=\"3EE9iVH4\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_t%28%5Ceta%29%5E2%3D%5Ceta%5Ccdot%5Ctilde%7B%5Cbeta%7D_t\" alt=\"\\sigma_t(\\eta)^2=\\eta\\cdot\\tilde{\\beta}_t\" eeimg=\"1\"/\u003e ，DDIM所选择的是基于fixedsmall版本上再乘以一个 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/\u003e .\u003c/p\u003e\u003cp data-pid=\"AokQ-ETG\"\u003e假设总的采样步 \u003cimg src=\"https://www.zhihu.com/equation?tex=T%3D1000\" alt=\"T=1000\" eeimg=\"1\"/\u003e ，间隔是 \u003cimg src=\"https://www.zhihu.com/equation?tex=Q\" alt=\"Q\" eeimg=\"1\"/\u003e ，DDIM采样的步数为 \u003cimg src=\"https://www.zhihu.com/equation?tex=S%3DT%2FQ\" alt=\"S=T/Q\" eeimg=\"1\"/\u003e ，S 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/\u003e 的实验结果如下：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-37e25ef62de1ac70a53a8ab3bff0d34d_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"3262\" data-rawheight=\"758\" data-original-token=\"v2-b5c0b052feec7a34e79e7d6b371392eb\" class=\"origin_image zh-lightbox-thumb\" width=\"3262\" data-original=\"https://pic4.zhimg.com/v2-37e25ef62de1ac70a53a8ab3bff0d34d_r.jpg\"/\u003e\u003cfigcaption\u003e来自DDIM的FID结果（不同步数S和方差设置）\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"TWNwDidS\"\u003e可以发现在 \u003cimg src=\"https://www.zhihu.com/equation?tex=S\" alt=\"S\" eeimg=\"1\"/\u003e 很小的时候 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ceta%3D0\" alt=\"\\eta=0\" eeimg=\"1\"/\u003e 取得了最好的结果。值得一提的是， \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ceta%3D1\" alt=\"\\eta=1\" eeimg=\"1\"/\u003e 是等价于DDPM的fixedsmall版本。而 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7B%5Csigma%7D%3D%5Csqrt%7B%5Cbeta_t%7D\" alt=\"\\hat{\\sigma}=\\sqrt{\\beta_t}\" eeimg=\"1\"/\u003e 表示的是DDPM的fixedlarge版本。因此当 \u003cimg src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/\u003e 足够大的时候使用更大的方差 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csigma_t%5E2\" alt=\"\\sigma_t^2\" eeimg=\"1\"/\u003e 能取得更好的结果。\u003c/p\u003e","is_labeled":false,"visited_count":831240,"thumbnails":["https://pic1.zhimg.com/v2-a650b4ed99b02f80b9d1c0687dea0848.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-c7319b988f4414e076a1579a84104aff_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-ce68214431bb7d58a73954a4ad9ca022_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-961ec4a14cb9efda96b7e92e77c7ccc0_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-539350bd3dfd42450b2641b9f4929bb5_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-7bcc327a0ab68c971ae2a4ab51c26099_720w.jpg?source=b6762063"],"favorite_count":8842,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 525106459}","attached_info":"Cq0JCOKDvb/vqZLnjwEQBxoJMjA0ODIyNjkwIKfpjJUGKN0sMJwCQEdKMAoGSXRlbUNGEiBkb2NfdHlwZTogQXJ0aWNsZQppZDogMjU5MTM3MTg0ChgAIAA6AGIgYTg0NWI1N2NlYWQzNDA3YjljOGE5OGNhMzIyYzQ3MWJyCTUyNTEwNjQ1OYIBX2h0dHBzOi8vcGljMS56aGltZy5jb20vdjItYTY1MGI0ZWQ5OWIwMmY4MGI5ZDFjMDY4N2RlYTA4NDguanBnP3NvdXJjZT03ZTdlZjZlMiZuZWVkQmFja2dyb3VuZD0xqgEJcmVjb21tZW5kwgEgYzUyM2ZlNTc3NGU2MmZkYzdiOTJmMjYzNGE0Yjc0MTjyAQoIDBIGTm9ybWFs8gEoCAoSJDI3MjViY2ZjLTJlZjYtNDkxMi04NDg4LWZlNzE0NTZiYTA5NvIBBggLEgIxMoICAIgC2b/hzfoykgIgYzUyM2ZlNTc3NGU2MmZkYzdiOTJmMjYzNGE0Yjc0MTiaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIYUGVyaW9kSW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIUQ29udGVudEFnZVdlaWdodFJ1bGXKAhdUZXN0ZWRBbmRXb3JrV2VpZ2h0UnVsZcoCHEJheWVzRmlyc3RMZXZlbElzb2xhdGlvblJ1bGXKAhdTYW1lQXV0aG9ySXNvbGF0aW9uUnVsZdoCBkl0ZW1DRugCBPoCC05PUk1BTF9GTE9XigMgZWM3NTVhODJjZDY0NDkwNGE2M2I4ZTI5Y2ZlMWMzMDKaAw0KAnYyEAAaBW90aGVyqAOI3jLYAwDqAxV0ZXh0QWxsU2l0ZU12SXRlbUNGVjL6A+gCEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAMQlgwYwAQiI3YyLWVlN2FiNGQzNjA4ODhiOTk0Y2ZiMjNlMDlhMjY3NDA3Oi0IAxDEEBiUBiIjdjItODMwZGVlNmFkMmJkMjE4MjVmOGZjMTA5NTM2MDg0NGY6LQgEENgaGLoSIiN2Mi01MTViYzQ0ODE0ZDQ4Mjc2OTNlYzUxYjcwODkzNjRhNjotCAMQxAkYkAIiI3YyLWE2NTBiNGVkOTliMDJmODBiOWQxYzA2ODdkZWEwODQ4Oi0IAxCEDBj8CCIjdjItMzEzZGJjYTNjNWExNjM5OGE1ZmY5ZTc3MzdmN2ZjOTE6LQgDEIIgGPYHIiN2Mi0xMTdjOTFmMWI3MDUyYmJlZWU0ZjhkMzFiNmZkMWQyMDotCAQQvhkY9gUiI3YyLWI1YzBiMDUyZmVlYzdhMzRlNzllN2Q2YjM3MTM5MmVigAQAiAQAkgQGTm9ybWFsmgQBNKAEAKgEALAEALoEBm1hbnVhbMIEAzE3MMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAoIffoD+BBQAAAAAAAAAAiQWfNa6/pXXSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUMkAYAoAZIqAYAkgIkCgkyMDQ4MjI2OTASCTUyNTEwNjQ1ORgHIgpJTUFHRV9URVhU","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=71\u0026desktop=true\u0026end_offset=72\u0026page_number=13\u0026session_token=a845b57cead3407b9c8a98ca322c471b","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=71\u0026desktop=true\u0026end_offset=72\u0026page_number=13\u0026session_token=a845b57cead3407b9c8a98ca322c471b","totals":0},"fresh_text":"推荐已更新"}
