{"data":[{"id":"114_1753853260.329","type":"feed","offset":114,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853260,"updated_time":1753853260,"target":{"id":"1933315494762485315","type":"article","url":"https://api.zhihu.com/articles/1933315494762485315","author":{"id":"3dccbba2df276ed4eb31b0d6beca76d8","url":"https://api.zhihu.com/people/3dccbba2df276ed4eb31b0d6beca76d8","user_type":"people","url_token":"lei-yi-43-4","name":"LeonYi","headline":"脚踏实地，拒绝纸上谈兵","avatar_url":"https://pica.zhimg.com/50/v2-507e55d6de18d16e2600e66cc481d4ba_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"四川大学 计算机技术硕士"}],"followers_count":2995,"is_following":false,"is_followed":false},"title":"RLHF系列：万字长文详解策略梯度算法（原理+代码）","image_url":"https://pic1.zhimg.com/v2-676e978bc10b6bf054526ea4c0cfe022.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1753802218,"updated":1753807927,"voteup_count":1,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"前言我是LeonYi，这是我的第1篇强化学习笔记。以知识梳理和复习笔记为目的。系列内容基于 动手学强化学习 + EasyRL + 李宏毅老师强化学习为主线，辅以Sutton强化学习 + 强化学习的数学原理，以及相应论文。从策略梯度开始，目标是彻底搞定RLHF理论及其实践 (力图搞懂所有卡点)。 文章首发于知乎，同步于公众号和我的博客园；内容概要本篇内容为 策略梯度的原理和代码实践，主要是从基于策略的强化学习的视角进行讲解。为什么要学…","excerpt_new":"前言我是LeonYi，这是我的第1篇强化学习笔记。以知识梳理和复习笔记为目的。系列内容基于 动手学强化学习 + EasyRL + 李宏毅老师强化学习为主线，辅以Sutton强化学习 + 强化学习的数学原理，以及相应论文。从策略梯度开始，目标是彻底搞定RLHF理论及其实践 (力图搞懂所有卡点)。 文章首发于知乎，同步于公众号和我的博客园；内容概要本篇内容为 策略梯度的原理和代码实践，主要是从基于策略的强化学习的视角进行讲解。为什么要学…","preview_type":"default","preview_text":"","content":"\u003ch2\u003e前言\u003c/h2\u003e\u003cblockquote data-pid=\"0XWcyeDh\"\u003e我是LeonYi，这是我的第1篇强化学习笔记。以知识梳理和复习笔记为目的。系列内容基于 \u003cb\u003e动手学强化学习\u003c/b\u003e + \u003cb\u003eEasyRL\u003c/b\u003e + \u003cb\u003e李宏毅老师强化学习\u003c/b\u003e为主线，辅以\u003cb\u003eSutton强化学习\u003c/b\u003e + \u003cb\u003e强化学习的数学原理\u003c/b\u003e，以及相应论文。从策略梯度开始，目标是彻底搞定RLHF理论及其实践 (力图搞懂所有卡点)。\u003cbr/\u003e文章首发于知乎，同步于公众号和我的博客园；\u003c/blockquote\u003e\u003ch3\u003e内容概要\u003c/h3\u003e\u003cp data-pid=\"-Fx-3si2\"\u003e本篇内容为\u003cb\u003e策略梯度\u003c/b\u003e的原理和代码实践，主要是从基于策略的强化学习的视角进行讲解。为什么要学习策略梯度，因为后续的AC、PPO和GRPO等变体都属于策略梯度体系。\u003c/p\u003e\u003cblockquote data-pid=\"N30szIVp\"\u003e不同之处在于如何控制策略梯度估计的方差、基于优势函数的变体来平衡方差与偏差以及假设重要性采样和策略更新限制，以及在RLHF上的调整。\u003c/blockquote\u003e\u003cp data-pid=\"dtgFzreY\"\u003e所以尽量从原理和实现层搞懂策略梯度，特别是策略梯度定理对于理解上述算法，有纲举目张的作用（理解其他的变体也不在话下，比如REINFORCE++\\RLOO等)。\u003c/p\u003e\u003cp data-pid=\"G_vBhirv\"\u003e\u003cb\u003e文章思路\u003c/b\u003e\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"5eS0mLFG\"\u003e强化学习的背景知识；\u003c/li\u003e\u003cli data-pid=\"xaWcGT7A\"\u003e如何优化策略（从监督学习的视角）；\u003c/li\u003e\u003cli data-pid=\"wnY5dI7n\"\u003e策略梯度的通俗推导（从轨迹分布的视角）;\u003c/li\u003e\u003cli data-pid=\"Txr6Bbon\"\u003e策略梯度实现之REINFORCE;\u003c/li\u003e\u003c/ol\u003e\u003cblockquote data-pid=\"kHWd_Jii\"\u003e由于内容过多，策略梯度定理的内容，将在本系列的下一篇给出。\u003c/blockquote\u003e\u003cp data-pid=\"P0FOECay\"\u003e\u003cb\u003e后续计划\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"HqCNJnuf\"\u003e1、策略梯度定理的详细介绍:\u003c/li\u003e\u003cli data-pid=\"_smKrZTC\"\u003e结合贝尔曼期望方程+贝尔曼最优方程，时序差分+蒙特卡洛的值函数估计; 推导并解释分析Policy Gradient Theorem\u003c/li\u003e\u003cli data-pid=\"Sxt9xgJG\"\u003e2、策略梯度的方差控制:\u003c/li\u003e\u003cli data-pid=\"vQA7GpQ2\"\u003e介绍ReinForce with Baseline以及Actor Critic\u003c/li\u003e\u003cli data-pid=\"rmEGn0ik\"\u003e从时序差分和值函数近似估计到GAE广义优势估计；\u003c/li\u003e\u003cli data-pid=\"8XRBKrKJ\"\u003e3、PPO、GRPO与REINFORCE++原理和核心代码解读;\u003c/li\u003e\u003cli data-pid=\"sB9IVXOg\"\u003e4、RLHF实战和调参经验；\u003c/li\u003e\u003cli data-pid=\"DBqVoW2Y\"\u003eGRPO实战/PPO实战\u003c/li\u003e\u003cli data-pid=\"uV_m_neP\"\u003eRewardModel和DPO\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e碎碎念\u003c/h3\u003e\u003cblockquote data-pid=\"3_cmBccB\"\u003e大概5年前初步了解基本的强化学习，毕竟当时落地的RL还是少数，RL方向的实际需求也并不多。但没怎深入，而且也并不理解教材一上来就给出的价值函数、动作价值函数和贝尔曼方程，遂中止。\u003c/blockquote\u003e\u003cp data-pid=\"l3BwY3qB\"\u003e今时不同往日，OpenAI打通了RLHF的道路，而且DS-R1进一步验证落地了强化学习的激发LLM推理能力的技术路线。在实际的业务场景也能有一些RL后训练进一步提升效果的探索和落地。\u003c/p\u003e\u003ch2\u003e一、RL背景知识\u003c/h2\u003e\u003ch3\u003e1.1 基础概念\u003c/h3\u003e\u003cblockquote data-pid=\"k_4SyuK5\"\u003e本文专注于离散动作的强化学习。\u003c/blockquote\u003e\u003cp data-pid=\"vO5OiBND\"\u003e强化学习（Reinforce Learing, RL），有时又叫做最优控制。RL的最终目标，是找到一个能实现期望累计奖励最大化的最优策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi%5E%7B%2A%7D\" alt=\"\\pi^{*}\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cp data-pid=\"TrMxzqOH\"\u003e\u003cbr/\u003e图1.1 强化学习的基本组成\u003c/p\u003e\u003cp data-pid=\"hJwZYOyl\"\u003e\u003cb\u003e强化学习的组成部分\u003c/b\u003e：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"SKxfCS40\"\u003e\u003cb\u003eActor\u003c/b\u003e：即智能体、代理Agent或翻译为演员，本文保留Actor。Actor是基于策略的RL的最终目标产物；\u003c/li\u003e\u003cli data-pid=\"v_YFWK4P\"\u003e\u003cb\u003e环境 (Enviroment)\u003c/b\u003e：即环境；\u003c/li\u003e\u003cli data-pid=\"OtdcgJ-B\"\u003e\u003cb\u003e动作空间\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BA%7D\" alt=\"\\mathbb{A}\" eeimg=\"1\"/\u003e：\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BA%7D\" alt=\"\\mathbb{A}\" eeimg=\"1\"/\u003e即为Action的集合，指智能体所有可能动作的集合;\u003c/li\u003e\u003cli data-pid=\"JM9QzovD\"\u003e\u003cb\u003e动作 (Action)\u003c/b\u003e：\u003cimg src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/\u003e，Actor在动作空间\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BA%7D\" alt=\"\\mathbb{A}\" eeimg=\"1\"/\u003e中能够采取的行为, \u003cimg src=\"https://www.zhihu.com/equation?tex=a+%5Cin+%5Cmathbb%7BA%7D\" alt=\"a \\in \\mathbb{A}\" eeimg=\"1\"/\u003e;\u003c/li\u003e\u003cli data-pid=\"nKMnN5Of\"\u003e\u003cb\u003e状态空间\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BS%7D\" alt=\"\\mathbb{S}\" eeimg=\"1\"/\u003e：\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BS%7D\" alt=\"\\mathbb{S}\" eeimg=\"1\"/\u003e指环境中所有可能状态的集合;\u003c/li\u003e\u003cli data-pid=\"C5YuIxzP\"\u003e\u003cb\u003e状态 (State)\u003c/b\u003e：\u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e指Actor在某一时刻所处的环境状态，如游戏画面、棋盘等; \u003cimg src=\"https://www.zhihu.com/equation?tex=s+%5Cin+%5Cmathbb%7BS%7D\" alt=\"s \\in \\mathbb{S}\" eeimg=\"1\"/\u003e;\u003c/li\u003e\u003cli data-pid=\"bMgMEBpa\"\u003e\u003cb\u003e策略 (Policy)\u003c/b\u003e：\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e代表策略，指Actor在某一状态下采取动作的概率分布。根据动作的类型，可分为离散或连续的概率分布；\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"nQM8Jc6I\"\u003e\u003cbr/\u003e图1.2 参数化的策略\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"A2THMh66\"\u003e\u003cb\u003e马尔可夫决策过程（MDP）\u003c/b\u003e：通常建模为五元组 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28%5Cmathbb%7BS%7D%2C+%5Cmathbb%7BA%7D%2C+p%2C+r%2C+%5Cgamma%29\" alt=\"(\\mathbb{S}, \\mathbb{A}, p, r, \\gamma)\" eeimg=\"1\"/\u003e\u003c/li\u003e\u003cli data-pid=\"mExmAYf2\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BS%7D\" alt=\"\\mathbb{S}\" eeimg=\"1\"/\u003e状态空间，\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BA%7D\" alt=\"\\mathbb{A}\" eeimg=\"1\"/\u003e动作空间，\u003cimg src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/\u003e是奖励函数\u003c/li\u003e\u003cli data-pid=\"JfL6Ahc7\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=p%28s_%7Bt%2B1%7D%7Cs_t%2Ca_t%29\" alt=\"p(s_{t+1}|s_t,a_t)\" eeimg=\"1\"/\u003e: 状态转移概率, 表示在状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e执行动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e之后到达状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt%2B1%7D\" alt=\"s_{t+1}\" eeimg=\"1\"/\u003e的概率\u003c/li\u003e\u003cli data-pid=\"jVpvZGr2\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/\u003e: 未来奖励的折扣因子，考虑到动作奖励的延后性质；\u003c/li\u003e\u003cli data-pid=\"4e4dj69j\"\u003e\u003cb\u003e奖励 (Reward)\u003c/b\u003e：\u003cimg src=\"https://www.zhihu.com/equation?tex=r%28s%2C+a%29\" alt=\"r(s, a)\" eeimg=\"1\"/\u003e，在状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e采取动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/\u003e获得的即时奖励;\u003c/li\u003e\u003cli data-pid=\"Z1VkAKNG\"\u003e\u003cb\u003e回报（Return）\u003c/b\u003e: \u003cimg src=\"https://www.zhihu.com/equation?tex=R+%3D+%5Csum_%7Bt%3D1%7D%5E%7BT%7D++r_%7Bt%7D\" alt=\"R = \\sum_{t=1}^{T}  r_{t}\" eeimg=\"1\"/\u003e  为回报，又称总奖励， 即所有时间步的累计奖励。回报通常用于评估RL算法的好坏（一般也用符号\u003cimg src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/\u003e的表示）；\u003c/li\u003e\u003cli data-pid=\"HXQ1dGAl\"\u003e\u003cb\u003e价值函数 (Value Function)\u003c/b\u003e：用于衡量策略在一个状态采取所有行为或在一个状态采取一个动作后的一个价值的期望值\u003c/li\u003e\u003cli data-pid=\"Ys5QUxs6\"\u003e状态价值函数\u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7B%5Cpi%7D%28s%29\" alt=\"V_{\\pi}(s)\" eeimg=\"1\"/\u003e为策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e在某一状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e后的累计折扣奖励的期望, \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%5Cpi%28s%29+%3D+%5Cmathbb%7BE%7D_%5Cpi+%5Cleft%5B+%5Csum_%7Bk%3D0%7D%5E%5Cinfty+%5Cgamma%5Ek+r_%7Bt%2Bk%7D+%5Cmid+s_t+%3D+s+%5Cright%5D\" alt=\"V_\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k r_{t+k} \\mid s_t = s \\right]\" eeimg=\"1\"/\u003e\u003c/li\u003e\u003cli data-pid=\"KVi0KDKO\"\u003e动作价值函数\u003cimg src=\"https://www.zhihu.com/equation?tex=Q_%7B%5Cpi%7D%28s%2C+a%29\" alt=\"Q_{\\pi}(s, a)\" eeimg=\"1\"/\u003e为策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e在某一状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e下采取动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/\u003e后的期望累计折扣奖励，\u003cimg src=\"https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2C+a%29+%3D+%5Cmathbb%7BE%7D_%5Cpi+%5Cleft%5B+%5Csum_%7Bk%3D0%7D%5E%5Cinfty+%5Cgamma%5Ek+r_%7Bt%2Bk%7D+%5Cmid+s_t+%3D+s%2C+a_t+%3D+a+%5Cright%5D\" alt=\"Q_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k r_{t+k} \\mid s_t = s, a_t = a \\right]\" eeimg=\"1\"/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"6XsZLd-B\"\u003e\u003cb\u003eRL的特征\u003c/b\u003e\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"qMNRFtfe\"\u003e\u003cb\u003e探索与利用的平衡trial-and-error search\u003c/b\u003e：策略需在尝试新动作（探索）和选择已知最优动作（利用）之间取得平衡；\u003c/li\u003e\u003cli data-pid=\"me4PQDNm\"\u003e\u003cb\u003eMDP建模当前状态和动作的关系\u003c/b\u003e：MDP简化了序列决策问题的建模，但它也能间接推动出多阶的状态转移概率，即考虑多步时间关联性。每个时间步的动作选择不仅影响即时奖励，还会改变环境状态，进而影响后续所有决策；(譬如，游戏画面具有时间连续性，前后两个游戏画面通常是有关系的，而且存在多步的依赖关系)\u003c/li\u003e\u003cli data-pid=\"ku2MA2VE\"\u003e\u003cb\u003e奖励延迟\u003c/b\u003e：有时采取某些动作即时奖励很低，但其效果可能需要多步之后才能显现； 举一个经典的在凹形轨道用撞击的方式使小球移动的例子。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"MEOzScf6\"\u003e\u003cbr/\u003e图1.3 以移动小球为例介绍奖励延迟\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"V08hbRB9\"\u003e动作空间为：{ 左移动， 右移动 }。目标是让小球从左侧轨道出去，但只能当小球处于最底部时采取动作（一次撞击仅能将小球移动到2/3的高度）。\u003c/li\u003e\u003cli data-pid=\"PpUbbCT4\"\u003e直观理解，左移动的奖励会比较大，因为更靠近目标，但出不去。\u003cb\u003e但先右移积累能量，再利用惯性 + 左移，就能达成目标\u003c/b\u003e（在累计奖励最大的指引下，RL算法往往能探索出这种反直觉的策略）。第一步先右移看起来奖励很低，但如果第二步采取左移，第一步右移的累计奖励并不小。\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e1.2 基于策略的RL\u003c/h3\u003e\u003cp data-pid=\"0DRbI2uC\"\u003e\u003cbr/\u003e图1.4 基于策略的RL\u003c/p\u003e\u003cblockquote data-pid=\"CI_-wFDJ\"\u003e不同于基于价值的RL算法去间接地学习最优的价值函数（价值迭代或策略迭代两种方式），再导出最优的策略。\u003c/blockquote\u003e\u003cp data-pid=\"yMKuPmJj\"\u003e基于策略的RL算法有着非常直接的学习目标，那就是直接优化参数化的策略网络。\u003c/p\u003e\u003cp data-pid=\"iNKerqtD\"\u003e\u003cb\u003e基于策略的RL算法的核心思想\u003c/b\u003e:\u003c/p\u003e\u003cp data-pid=\"-JzCyKas\"\u003e给定最终的奖励信号做引导，无需直接的监督信号，策略网络通过尝试与环境不断地交互并迭代优化参数，自行探索出能够获得最大期望回报的策略。\u003c/p\u003e\u003cblockquote data-pid=\"OT_S4M_d\"\u003e当然，还是有点类似于机器学习的独立同分布的假设。在环境中不断交互获取数据，迭代更新策略，直到当前采样的训练数据上的回报达到目标阈值。这样训练出的策略网络，应该在同样的环境还是有不错的表现。\u003c/blockquote\u003e\u003ch3\u003e1.2 目标函数\u003c/h3\u003e\u003cp data-pid=\"h7s52HSl\"\u003e\u003cbr/\u003e 图1.5 强化学习的示例\u003c/p\u003e\u003cp data-pid=\"59dGVroK\"\u003e如上图所示，在让模型玩视频游戏时：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"edZdMq1m\"\u003e\u003cb\u003eActor\u003c/b\u003e负责操控游戏的摇杆，比如向左、向右、开火等操作；\u003c/li\u003e\u003cli data-pid=\"nUfQLjwb\"\u003e\u003cb\u003e环境\u003c/b\u003e就是游戏的主机，负责控制游戏的画面、负责控制怪兽的移动等；\u003c/li\u003e\u003cli data-pid=\"Bj1og8A-\"\u003e\u003cb\u003e奖励函数\u003c/b\u003e就是当Actor做什么事情、发生什么状况的时候，能得到多少收益，比如打败一只怪兽得20个金币 等。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"SH4KUPP0\"\u003e\u003cbr/\u003e 图1.6 奖励函数\u003c/p\u003e\u003cp data-pid=\"k49R4o4t\"\u003eActor要决定在给定的状态下，应该采取什么动作。Actor里的策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e决定了Actor的动作，即给定一个输入，它会输出Actor下一步应该执行的动作。\u003c/p\u003e\u003cp data-pid=\"0kyAowgT\"\u003e环境会根据Actorc采取动作，给出一个奖励，而RL的训练目标就是让Actor能获得的奖励尽可能高。\u003c/p\u003e\u003cblockquote data-pid=\"Ck4m3j80\"\u003e在RL中，环境不是我们可以控制的，它们在学习之前就给定好了。唯一需要做的就是调整Actor的策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e，使得Actor可以得到最大的奖励。（当然奖励函数往往也需要自行定义， 这取决于训练的最终目标）\u003c/blockquote\u003e\u003cp data-pid=\"YMSA4Vm_\"\u003e\u003cbr/\u003e 图1.7 策略网络\u003c/p\u003e\u003cp data-pid=\"UyZNJat-\"\u003e令策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e为一个深度神经网络，用 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e 来代表 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e 的参数。策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e的输入是Actor看到的东西。如果让Actor玩视频游戏，Actor看到的就是游戏画面。\u003c/p\u003e\u003cblockquote data-pid=\"5_zz-fT3\"\u003e可用向量或矩阵来表示Actor的观测，并将观测输入策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e，策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e就会输出Actor要采取的动作。\u003c/blockquote\u003e\u003cp data-pid=\"IviDN4Tv\"\u003e\u003cb\u003eActor与环境交互的过程\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"p7QAJAvS\"\u003e输入一个状态后，策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e为每个动作输出一个分数，即离散动作空间上的概率分布。Actor根据动作的概率分布来决定它要采取的动作。\u003c/p\u003e\u003cblockquote data-pid=\"GushIpF-\"\u003e比如 0.7 的概率向左走、0.2 的概率向右走、0.1的概率开火等。概率分布不同，Actor采取的动作就会不一样。\u003c/blockquote\u003e\u003cp data-pid=\"1rHGoHhT\"\u003e\u003cbr/\u003e 图1.8 Actor的交互过程\u003c/p\u003e\u003cp data-pid=\"Fhdx8LBG\"\u003e如上图的视频游戏示例，首先Actor会看到一个视频游戏的初始画面，接下来它会根据其策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e来决定一个动作。其中：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"y81gQQHB\"\u003e每个画面帧对应一个时间步\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e；\u003c/li\u003e\u003cli data-pid=\"IRctCYqq\"\u003e躲避子弹的决策会影响后续数秒的游戏状态；\u003c/li\u003e\u003cli data-pid=\"EeNht43v\"\u003e最终击败Boss的奖励需要经过数百次连续正确决策才能获得；\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"ad7HEi7z\"\u003e假设\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e决定的动作是向右，在采取动作后，它就会得到一个奖励（奖励代表它采取这个动作以后，得到的收益）。整体流程如下：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"2BXpMSIR\"\u003e将游戏的初始画面（环境状态）记作 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_1\" alt=\"s_1\" eeimg=\"1\"/\u003e， 把第1次执行的动作记作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_1\" alt=\"a_1\" eeimg=\"1\"/\u003e，把第1次执行动作后得到的奖励记作 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_1\" alt=\"r_1\" eeimg=\"1\"/\u003e；\u003c/li\u003e\u003cli data-pid=\"tgeY-3tU\"\u003eActor决定一个动作以后，就会看到一个新的游戏画面\u003cimg src=\"https://www.zhihu.com/equation?tex=s_2\" alt=\"s_2\" eeimg=\"1\"/\u003e;\u003c/li\u003e\u003cli data-pid=\"c9vIICc3\"\u003e把 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_2\" alt=\"s_2\" eeimg=\"1\"/\u003e 输入给Actor，Actor决定要开火，它可能打败了一只怪兽，就得到5分;\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"7j1GWAbI\"\u003e这个过程反复地持续下去，直到在某一个时间步执行某一个动作，得到奖励之后，环境决定这个游戏结束。\u003c/p\u003e\u003cblockquote data-pid=\"H1crTdf2\"\u003e例如，在这个游戏中，去控制宇宙飞船去击杀怪兽。如果宇宙飞船被毁或是把所有的怪兽都清空了，游戏就结束啦。\u003c/blockquote\u003e\u003cp data-pid=\"hlGcRKd6\"\u003e\u003cbr/\u003e 图1.9 RL的整体交互流程\u003c/p\u003e\u003cblockquote data-pid=\"nVtKDpzU\"\u003e如上图所示，一场游戏称为轨迹trajectory或一个回合episode。\u003c/blockquote\u003e\u003cp data-pid=\"MBzVMt8s\"\u003eActor通过与环境交互，可以得到轨迹(trajectory) 或回合(episode) 数据, 即 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau%3D%28s_1%2Ca_1%2Cr_1%2C...%2Cs_T%2Ca_T%2Cr_T%29\" alt=\"\\tau=(s_1,a_1,r_1,...,s_T,a_T,r_T)\" eeimg=\"1\"/\u003e，其表示从初始状态到终止状态的完整轨迹。\u003c/p\u003e\u003cp data-pid=\"2iI4Dh7H\"\u003e轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e的基本组成部分即，状态、动作和奖励三元组\u003cimg src=\"https://www.zhihu.com/equation?tex=%28s_t%2Ca_t%2Cr_t%29\" alt=\"(s_t,a_t,r_t)\" eeimg=\"1\"/\u003e。将一个轨迹里面得到的每一步的奖励都加起来，就是\u003cb\u003e总奖励（total reward）\u003c/b\u003e，即\u003cb\u003e回报\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=R\" alt=\"R\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cp data-pid=\"jcujlyhl\"\u003e轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e是一个随机变量（因为样本在采样前未知，所以每个样本都可以视为一个随机变量），它是从\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e决定的概率分布中进行采样的，并可能受环境的不确定性影响。\u003c/p\u003e\u003cblockquote data-pid=\"y4aTkQjL\"\u003e在LLM中，已经不存在环境的影响。历史的上下文即状态，LLM预测的下一个token即动作，下一个状态确定性地由：历史的上下文 +下一个token构成。\u003c/blockquote\u003e\u003cp data-pid=\"Z0DHCsy5\"\u003e而RL的优化目标可形式化为，最大化如下的这个期望累积奖励目标函数： \u003cimg src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D_%7B%5Ctau+%5Csim+%5Cpi_%7B%5Ctheta%7D%7D+%5Cleft%5B+%5Csum_%7Bt%3D1%7D%5E%7B%5Cinfty%7D+%5Cgamma%5E%7Bt-1%7D+r%28s_t%2Ca_t%29+%5Cright%5D+%3D+V_%7B%5Cpi%7D%28s_1%29+%3D+%5Csum_%7Ba+%5Cin+%5Cmathbb%7BA%7D%7D+%5Cpi%28a%7Cs_1%29Q_%7B%5Cpi%7D%28s_1%2C+a%29+%5Ctag%7B1.1%7D\" alt=\"J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=1}^{\\infty} \\gamma^{t-1} r(s_t,a_t) \\right] = V_{\\pi}(s_1) = \\sum_{a \\in \\mathbb{A}} \\pi(a|s_1)Q_{\\pi}(s_1, a) \\tag{1.1}\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cblockquote data-pid=\"hS3DASlP\"\u003e这里令\u003cimg src=\"https://www.zhihu.com/equation?tex=s_1\" alt=\"s_1\" eeimg=\"1\"/\u003e为初始状态。注意，这里的累计奖励是从一个轨迹的初始状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_1\" alt=\"s_1\" eeimg=\"1\"/\u003e开始的累计折扣奖励。它用于衡量策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e的期望回报。如果有不同的初始状态，则还要套一层求和。\u003c/blockquote\u003e\u003cp data-pid=\"u1iEQMhT\"\u003e换句话说，目标函数就是最大化每个轨迹的初始状态的状态值函数的期望，希望策略的回报越大越好。通常，会采样轨迹来近似计算这个期望。\u003c/p\u003e\u003cp data-pid=\"vUYMCkLK\"\u003e那么，通过收集轨迹数据\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BT%7D%3D+%5C%7B%5Ctau_1%2C+%5Ctau_2%2C+...%2C+%5Ctau_N%5C%7D\" alt=\"\\mathbb{T}= \\{\\tau_1, \\tau_2, ..., \\tau_N\\}\" eeimg=\"1\"/\u003e (基于\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e从环境中采样数据) ，可以用于训练RL算法，从而想办法优化Actor的参数来最大化它可以得到的\u003cb\u003e回报\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=R\" alt=\"R\" eeimg=\"1\"/\u003e的期望。\u003c/p\u003e\u003cblockquote data-pid=\"jzJsucGh\"\u003e注意，通常衡量强化学习学到的策略好坏，是直接用不带折扣的累计奖励。但是，在训练时往往是用带折扣的累计折扣奖励作为目标。\u003c/blockquote\u003e\u003ch2\u003e二、如何优化策略\u003c/h2\u003e\u003ch3\u003e2.1 从监督分类到优化策略网络\u003c/h3\u003e\u003cp data-pid=\"DBzODDps\"\u003e可将强化学习视为一个特殊的分类问题，即将采样的轨迹数据中的状态动作对\u003cimg src=\"https://www.zhihu.com/equation?tex=%28s_t%2C+a_t%29\" alt=\"(s_t, a_t)\" eeimg=\"1\"/\u003e拆分，状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e作为分类器的输入，输出是动作的类别概率分布。\u003c/p\u003e\u003cblockquote data-pid=\"2Po5V1WL\"\u003e这个分类器就决定了策略的概率分布\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Cpi_%7B%5Ctheta%7D%7D%28a_t%7Cs_t%29\" alt=\"p_{\\pi_{\\theta}}(a_t|s_t)\" eeimg=\"1\"/\u003e；\u003c/blockquote\u003e\u003cp data-pid=\"_s59hhCM\"\u003e\u003cbr/\u003e 图2.1 监督分类任务\u003c/p\u003e\u003cblockquote data-pid=\"xp92rReZ\"\u003e以游戏图片为输入，具体可用CNN或VIT来提取图片特征，甚至可用Transformer来处理历史所有的画面。\u003c/blockquote\u003e\u003cp data-pid=\"y4JnqQec\"\u003e在解决分类问题时，优化目标为:  \u003cb\u003e最小化交叉熵损失函数（cross entropy）= 最小化负对数似然（likelihood） = 最大化似然（\u003c/b\u003e即对参数化的伯努利分布进行极大似然估计，要调整参数使得样本出现的概率最大）\u003c/p\u003e\u003cp data-pid=\"lyRJxCiF\"\u003e在实现时，交叉熵损失就是统计所有样本的真实标签 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat+%7B+%5Cmathbf+a%7D\" alt=\"\\hat { \\mathbf a}\" eeimg=\"1\"/\u003e的类别上的预测类别的负对数概率（即负对数似然）。\u003c/p\u003e\u003cp data-pid=\"ZUJlDRg9\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++L%28%5Cmathbb%7BT%7D%3B%7B%5Ctheta%7D%29+%3D+-%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5Ctag%7B2.1%7D%5C%5C\" alt=\"    L(\\mathbb{T};{\\theta}) = -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right) \\tag{2.1}\\\\\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cblockquote data-pid=\"EdYUeKgT\"\u003e这只是尽量提高模型在groudtruth label上的预测概率 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Chat%7Ba%7D+%7C+s%29\" alt=\"p_{\\theta}(\\hat{a} | s)\" eeimg=\"1\"/\u003e 。但这样做，反过来就会降低模型在给定 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_t+\" alt=\"s_t \" eeimg=\"1\"/\u003e 后，在标签 \u003cimg src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/\u003e 之外的所有动作类别上的概率。\u003c/blockquote\u003e\u003cp data-pid=\"hvtIFyEt\"\u003e在这里令，为动作\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7Ba%7D\" alt=\"\\hat{a}\" eeimg=\"1\"/\u003e上的交叉熵损失， 即Actor在状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e时选择动作\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7Ba%7D\" alt=\"\\hat{a}\" eeimg=\"1\"/\u003e的负对数概率。\u003c/p\u003e\u003cp data-pid=\"Iug87Cnk\"\u003e\u003cbr/\u003e 图2.2 如何调整优化策略\u003c/p\u003e\u003cp data-pid=\"mwKmJHeO\"\u003e那么，要调整Actor网络的策略，本质上就是\u003cb\u003e在给定状态，提升某些动作出现概率，降低另一些动作的的概率\u003c/b\u003e（直接调整采样到的动作的预测概率）。\u003c/p\u003e\u003cblockquote data-pid=\"jiLj53lV\"\u003e基于策略的RL，会将已经采样到的动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e视为要预测的类别标签，其目标是根据某种信号来调整在状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e时预测动作类别\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e的概率，从而达到调整策略的目的。\u003c/blockquote\u003e\u003cp data-pid=\"WMqM4EcH\"\u003e要达到这个目的，一种朴素的实现方式为:\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"UXGV0sWU\"\u003e想要采取的动作，使用正的损失\u003cimg src=\"https://www.zhihu.com/equation?tex=e\" alt=\"e\" eeimg=\"1\"/\u003e；\u003c/li\u003e\u003cli data-pid=\"9HoKR0-G\"\u003e不想采取的动作，使用负的损失\u003cimg src=\"https://www.zhihu.com/equation?tex=-e\" alt=\"-e\" eeimg=\"1\"/\u003e（即给当前动作的损失e乘以负号）；\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"QPF_nDAN\"\u003e\u003cbr/\u003e 图2.3 策略优化的实现\u003c/p\u003e\u003cblockquote data-pid=\"NAcfZZ_N\"\u003e在某个状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e下，采取了动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e，会得到对应的奖励值去加权损失（有的行为是要被采取的，奖励为+1；有的行为是不希望被采取的，奖励为-1）。\u003c/blockquote\u003e\u003cp data-pid=\"g40_zvr5\"\u003e这样做，对于损失为负的动作，仍要将其损失最小化，就会引导Actor让该动作的概率向着0减小，反过来就将其他动作的概率就变大了。那么起到的效果就是，调整后的策略，会更倾向于选择我们认为更好的动作。\u003c/p\u003e\u003cp data-pid=\"Kf7Un7rp\"\u003e为了直观展示这个思路，我绘制了交叉熵（负对数，\u003cimg src=\"https://www.zhihu.com/equation?tex=-log_%7B2%7D%28x%29\" alt=\"-log_{2}(x)\" eeimg=\"1\"/\u003e）和 负的交叉熵（正对数，\u003cimg src=\"https://www.zhihu.com/equation?tex=log_%7B2%7D%28x%29\" alt=\"log_{2}(x)\" eeimg=\"1\"/\u003e ，即似然值)在概率\u003cimg src=\"https://www.zhihu.com/equation?tex=x+%5Cin+%280%2C1%5D\" alt=\"x \\in (0,1]\" eeimg=\"1\"/\u003e的范围的曲线：\u003c/p\u003e\u003cp data-pid=\"XSF_fHDO\"\u003e\u003cbr/\u003e图2.4 交叉熵的可视化\u003c/p\u003e\u003cp data-pid=\"taYuhDD2\"\u003e如上图所示，可以看到：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"ayBvhy1O\"\u003e最小化交叉熵（ \u003cimg src=\"https://www.zhihu.com/equation?tex=min\" alt=\"min\" eeimg=\"1\"/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=-log_%7B2%7D%28x%29\" alt=\"-log_{2}(x)\" eeimg=\"1\"/\u003e= \u003cimg src=\"https://www.zhihu.com/equation?tex=max\" alt=\"max\" eeimg=\"1\"/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=log_%7B2%7D%28x%29\" alt=\"log_{2}(x)\" eeimg=\"1\"/\u003e，即最大化对数概率）就是要让目标类别的概率\u003cimg src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/\u003e向右➡逼近1\u003c/li\u003e\u003cli data-pid=\"dXCMWFz-\"\u003e最小化负的交叉熵（\u003cimg src=\"https://www.zhihu.com/equation?tex=min\" alt=\"min\" eeimg=\"1\"/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=-%28-log_%7B2%7D%28x%29%29\" alt=\"-(-log_{2}(x))\" eeimg=\"1\"/\u003e= \u003cimg src=\"https://www.zhihu.com/equation?tex=min\" alt=\"min\" eeimg=\"1\"/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=log_%7B2%7D%28x%29\" alt=\"log_{2}(x)\" eeimg=\"1\"/\u003e, 即最小化对数概率）就是要让目标类别的概率\u003cimg src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/\u003e向左⬅逼近0。\u003c/li\u003e\u003c/ul\u003e\u003cblockquote data-pid=\"zRzuC8uO\"\u003e存在特殊情况，就是给定状态采取好动作的概率已经极大，那么此时已经不用调整（实际因为损失极小也调整不动了）\u003c/blockquote\u003e\u003cp data-pid=\"mD5i_LeQ\"\u003e那么，为了训练Actor来调整其策略，我们需要通过采样收集轨迹数据作为训练资料。可定义损失函数\u003cimg src=\"https://www.zhihu.com/equation?tex=L+%3D+%2B+e_%7B1%7D++%2B+%28-+e_%7B2%7D%29+%2B++e_%7B3%7D+%2B+...+%2B+%28-+e_%7BN%7D%29\" alt=\"L = + e_{1}  + (- e_{2}) +  e_{3} + ... + (- e_{N})\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cp data-pid=\"NfinK35Q\"\u003e\u003cbr/\u003e 图2.5 策略优化的损失函数\u003c/p\u003e\u003cp data-pid=\"U0lRgv0-\"\u003e然后，就可以去minimize这个损失\u003cimg src=\"https://www.zhihu.com/equation?tex=L\" alt=\"L\" eeimg=\"1\"/\u003e，来训练Actor。那最小化这个损失，会发生什么呢?\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"iAadQ43i\"\u003e认为\u003cb\u003eYes\u003c/b\u003e的好动作✅，就会调大其在给定状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e下出现的概率；\u003c/li\u003e\u003cli data-pid=\"CTqlaDqW\"\u003e认为\u003cb\u003eNo\u003c/b\u003e的坏动作❌，就会调小其在给定状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e下出现的概率；\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Ucw_l17d\"\u003e按这个思路还可进一步设定，每一个动作不是只有好\u003cimg src=\"https://www.zhihu.com/equation?tex=%2B1\" alt=\"+1\" eeimg=\"1\"/\u003e或坏\u003cimg src=\"https://www.zhihu.com/equation?tex=-1\" alt=\"-1\" eeimg=\"1\"/\u003e这种二元奖励，还可用一个数值来进一步细粒度地表示好或坏的程度差别：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"q6TZmpp0\"\u003e有非常好的✅✅✅\u003c/li\u003e\u003cli data-pid=\"862-WELm\"\u003e有推荐执行的✅\u003c/li\u003e\u003cli data-pid=\"FcDiFX8X\"\u003e不好不坏 \u003c/li\u003e\u003cli data-pid=\"_-ZRGsue\"\u003e有点糟糕❌\u003c/li\u003e\u003cli data-pid=\"1E7YWGau\"\u003e非常差的❌❌❌\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Bu4snsvf\"\u003e这时，奖励分数可以视为，我们有多希望在看到状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e时，采取了动作a。_t\u003c/p\u003e\u003cp data-pid=\"GSbKpbGi\"\u003e\u003cbr/\u003e 图2.6 优化奖励分值\u003c/p\u003e\u003cp data-pid=\"OLJiSaYW\"\u003e但是，我们可以通过奖励值来间接地指导模型：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"lB1UmGra\"\u003e奖励为正的且越大的动作，越要加大剂量；\u003c/li\u003e\u003cli data-pid=\"PNhScD8z\"\u003e奖励为负且越小的动作，越不要做（能做的就是在该样本的损失上乘以负的奖励值，从而将奖励为负的动作的概率减小）；\u003c/li\u003e\u003c/ul\u003e\u003cblockquote data-pid=\"t9r0FJ9G\"\u003e这相当于正负样本的交叉熵损失加权。本质上是在使用基于梯度的优化算法时，是沿着样本估计出的梯度方向，加大了学习的步长，相当于学得更猛了。\u003c/blockquote\u003e\u003cp data-pid=\"sp1fBnuP\"\u003e将\u003cimg src=\"https://www.zhihu.com/equation?tex=A_t\" alt=\"A_t\" eeimg=\"1\"/\u003e带入前面的损失函数，可以得到如下最终损失函数：\u003c/p\u003e\u003cp data-pid=\"Qe0Ot3OW\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+++++L%28%5Cmathbb%7BT%7D%3B%7B%5Ctheta%7D%29+%26%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+A_%7Bt%7D+%2A+e_n+%5C%5C%26%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+A_%7Bt%7D+%2A+-%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5C%5C%26%3D+-%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+A_%7Bt%7D+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5Cend%7Baligned%7D++%5Ctag%7B2.2%7D\" alt=\"\\begin{aligned}     L(\\mathbb{T};{\\theta}) \u0026amp;= \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} A_{t} * e_n \\\\\u0026amp;= \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} A_{t} * -\\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right) \\\\\u0026amp;= -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} A_{t} \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right) \\end{aligned}  \\tag{2.2}\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cp data-pid=\"AzD8rr3D\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=L%28%5Cmathbb%7BT%7D%3B%7B%5Ctheta%7D%29\" alt=\"L(\\mathbb{T};{\\theta})\" eeimg=\"1\"/\u003e可视为基于采样到的轨迹样本构造的原始目标函数\u003cimg src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/\u003e的一种近似估计。最小化损失函数\u003cimg src=\"https://www.zhihu.com/equation?tex=L%28%5Cmathbb%7BT%7D%3B%7B%5Ctheta%7D%29\" alt=\"L(\\mathbb{T};{\\theta})\" eeimg=\"1\"/\u003e，等价于通过最大化负的损失函数，使得动作对数概率\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Clog+p_%7B%5Ctheta%7D%28+%5Chat%7Ba%7D+%7C+s%29\" alt=\"\\log p_{\\theta}( \\hat{a} | s)\" eeimg=\"1\"/\u003e加权的奖励之和尽可能最大，来调整策略网络参数。\u003c/p\u003e\u003cblockquote data-pid=\"mtwTmrO0\"\u003e优化过程类似于常规分类任务，可通过Pytorch反向传播计算梯度+优化器更新网络参数实现。\u003c/blockquote\u003e\u003cp data-pid=\"lCfVandw\"\u003e那么，不断采样数据并训练Acotr，当Actor在采样的到轨迹数据上的累计奖励稳定到一个足够大的值，可以认为策略网络已经收敛到一个足够好的水平。\u003c/p\u003e\u003ch3\u003e2.2 监督学习和RL的区别联系\u003c/h3\u003e\u003cp data-pid=\"wRpvfbon\"\u003e到目前为止，基于策略梯度的RL似乎和传统的监督分类任务差别好像没有那么大。区别在于就是样本和加权方式 。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"qsdiYttK\"\u003e\u003cb\u003e监督分类\u003c/b\u003e:\u003c/li\u003e\u003cli data-pid=\"cRG511Iv\"\u003e训练样本：事先收集的训练样本；\u003c/li\u003e\u003cli data-pid=\"yGhRdhkC\"\u003e训练目标：最小化交叉熵损失，直接最大化训练样本的条件概率\u003cimg src=\"https://www.zhihu.com/equation?tex=p%28y%7Cx%2C+%5Ctheta%29\" alt=\"p(y|x, \\theta)\" eeimg=\"1\"/\u003e；\u003c/li\u003e\u003cli data-pid=\"4xrR1Agr\"\u003e加权方式：一般不加权（有时对于常见的类别不平衡，也可按样本标签的分布加权交叉熵或使用FocalLoss）；\u003c/li\u003e\u003cli data-pid=\"q2lZwLrM\"\u003e\u003cb\u003e基于策略梯度的RL\u003c/b\u003e:\u003c/li\u003e\u003cli data-pid=\"wGEO75tE\"\u003e训练样本: 来源于基于策略网络从环境中采样到的轨迹样本；\u003c/li\u003e\u003cli data-pid=\"9n_CY_LL\"\u003e训练目标：最大化训练样本上的期望累计奖励；\u003c/li\u003e\u003cli data-pid=\"soCoAXh3\"\u003e加权方式：基于这些样本用奖励函数算出的奖励值或更高级的的优势值，来加权交叉熵损失（不同程度地鼓励好动作或压制坏动作）;\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"P1giuO8K\"\u003e再进一步，以LLM的SFT和RLHF为例子：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"LGvM9MKy\"\u003e\u003cb\u003eSFT是有监督分类的特殊版本\u003c/b\u003e，它只收集一定数量的高质量好样本，保证指令和回答的多样性、不同难易度，尽量覆盖所有case。\u003c/li\u003e\u003cli data-pid=\"FbW26kLq\"\u003e训练目标就是最大化似然，即调整LLM参数，使得在给定Instruction的情况下，生成正确的Answer的条件概率尽量大。属于\u003cb\u003e死记硬背\u003c/b\u003e。\u003c/li\u003e\u003cli data-pid=\"mBjMdyIF\"\u003eSFT所需的大量直接监督信号的代价太高，特别是状态和动作空间极大的场景。而且不是所有高质量的训练样本都可以依靠人工标注出来，比如国际奥林数学竞赛难题得最优解。\u003c/li\u003e\u003cli data-pid=\"JCJ5EFWs\"\u003e\u003cb\u003eRLHF属于基于策略的RL\u003c/b\u003e。不事先确定每一个指令的最优回答是什么，\u003cb\u003e而是在大量采样到的回答上，通过奖励函数导出的累积奖励做为每一步策略调整的信号，来探索更优的答案路径\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"ioL0InP8\"\u003e前提是奖励函数要准，比如数学题、代码或给定数据可验证的任务（例如基于SQL生成的特征因子挖掘）。通过偏好的偏序数据来训练奖励模型，在一定程度上解决了好样本标注很困难的问题（如果有奖励得分，就还可以在pair-wise损失上加入奖励值的回归损失）。奖励模型具备了奖励打分和一定的泛化能力，能够引导LLM进一步对齐细粒度的偏好（覆盖SFT不能触达的场景），并且有能力探索出更高质量的回答（以奖励模型为衡量尺度）。\u003c/li\u003e\u003cli data-pid=\"IO2j_NoG\"\u003e策略网络初始化不能太差（预训练和SFT的LLM不能够太拉跨）；往往在探索的过程中，策略会输出各种用提示词都不会出现的bad_case，所以需要通过加强提示词、SFT微调初始化以及加入更多关于格式或回答过程的细粒度的规则奖励约束，避免在奖励稀疏（好回答少这种类型）和奖励Hacking。\u003c/li\u003e\u003c/ul\u003e\u003cblockquote data-pid=\"vLW6l0Kv\"\u003e不得不同意，RLHF真是极好的idea \u003c/blockquote\u003e\u003ch3\u003e2.3 策略网络的不同优化选项\u003c/h3\u003e\u003cp data-pid=\"hcqD2TTd\"\u003e那么现在的关键是\u003cimg src=\"https://www.zhihu.com/equation?tex=A_t\" alt=\"A_t\" eeimg=\"1\"/\u003e的计算方式。实际上\u003cimg src=\"https://www.zhihu.com/equation?tex=A_t\" alt=\"A_t\" eeimg=\"1\"/\u003e有多个候选项。\u003c/p\u003e\u003cp data-pid=\"qZz6c0gd\"\u003e\u003cb\u003eVersion0: 直接使用时间步\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e\u003cb\u003e的即时奖励\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=r_t\" alt=\"r_t\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"G0_2jzsn\"\u003e\u003cbr/\u003e 图2.7 版本零：即时奖励作为策略调整信号\u003c/p\u003e\u003cp data-pid=\"h-PUt_2z\"\u003e这种方式，训练出来的模型是短视的，换句话说就是只知道立即爽，而不会推迟奖励。\u003c/p\u003e\u003cblockquote data-pid=\"_zygAwOb\"\u003e比如，Atari游戏中，动作空间为{←左移，→右移， 开火}，而往往而只有开火命中了外星人 才会有奖励 。 那么按照即时奖励的方式去训练模型，会学到一个只会一直开火   的糟糕策略（这种就是由于初始策略差导致好奖励稀疏，使得策略优化走了斜路的错误捷径，发生了奖励Hacking ）。 通常，需要涉及更细粒度的反馈，即奖励Reshaping的手段，提供轨迹中过程动作的奖励，来缓解问题。\u003c/blockquote\u003e\u003cp data-pid=\"TBbZ1evB\"\u003e自然，考虑到奖励延迟（当前动作会影响之后的状态和动作，导致后续动作有更高或更低的奖励），可以使用累计奖励的方式。\u003c/p\u003e\u003cp data-pid=\"cXJ5u7y2\"\u003e\u003cb\u003eVersion1: 使用时间步\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e\u003cb\u003e的累计奖励\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"QTIi1WQZ\"\u003e\u003cbr/\u003e 图2.8 版本一：累计奖励作为策略调整信号\u003c/p\u003e\u003cp data-pid=\"dXN2EfuJ\"\u003e为每一个时间步的动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e，都赋予一个累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cp data-pid=\"FzWxGNXy\"\u003e累计奖励让模型牺牲了即时奖励，但是却能学到在当前状态采取累计奖励最大的动作的策略，从而期待其达到长期奖励最大的目标。\u003c/p\u003e\u003cp data-pid=\"yZgg5vrv\"\u003e但累计奖励将后续时间步的即时奖励直接相加，\u003cb\u003e没有区别对待当前时间步和后续时间步的即时奖励，从而放大了后续时间步奖励的对当前时间步的影响\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"obG8XNNB\"\u003e\u003cb\u003eVersion2: 使用时间步\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e\u003cb\u003e的累计折扣奖励\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"po-IHWux\"\u003e\u003cbr/\u003e 图2.9 版本二：累计折扣奖励作为策略调整信号\u003c/p\u003e\u003cp data-pid=\"-XhuHkaz\"\u003e那么，引入一个折扣因子\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/\u003e，\u003cimg src=\"https://www.zhihu.com/equation?tex=0+%5Cleq+%5Cgamma+%5Cleq+1\" alt=\"0 \\leq \\gamma \\leq 1\" eeimg=\"1\"/\u003e，它决定了当前时间步\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e对未来奖励的考虑程度:\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"GnbOYnvq\"\u003e如果 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cgamma+%3D+0\" alt=\"\\gamma = 0\" eeimg=\"1\"/\u003e，那么\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t+%3D+r_%7Bt%7D\" alt=\"G_t = r_{t}\" eeimg=\"1\"/\u003e，Actor就是短视的。\u003c/li\u003e\u003cli data-pid=\"IqFOZfrf\"\u003e如果 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/\u003e趋近于1，那么Actor就会更强烈地考虑未来的奖励，变得很远视。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"prDcrJPX\"\u003e通常将累计折扣奖励（实际是移动指数平均EMA的特例），定义为\u003cb\u003e回报（return）\u003c/b\u003e ，即为从第\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e时间步的状态开始，直到终止状态时，所有奖励的衰减之和：\u003c/p\u003e\u003cp data-pid=\"ySKLiAdY\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++G_%7Bt%7D%3Dr_%7Bt%7D%2B%5Cgamma+r_%7Bt%2B1%7D%2B%5Cgamma%5E%7B2%7D+r_%7Bt%2B2%7D%2B%5Cldots+%3D+%5Csum%5E%7BT%7D_%7Bk%3D0%7D+%5Cgamma%5E%7Bk%7D+r_%7Bt%2Bk%7D+%5Ctag%7B2.3%7D%5C%5C\" alt=\"  G_{t}=r_{t}+\\gamma r_{t+1}+\\gamma^{2} r_{t+2}+\\ldots = \\sum^{T}_{k=0} \\gamma^{k} r_{t+k} \\tag{2.3}\\\\\" eeimg=\"1\"/\u003e其中，\u003cimg src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/\u003e为轨迹的总长度, 往往是有限。\u003c/p\u003e\u003cblockquote data-pid=\"DSDLYN-Y\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=G_%7Bt%7D+%3D+%5Csum%5E%7BT%7D_%7Bk%3D0%7D+%5Cgamma%5E%7Bk%7D+r_%7Bt%2Bk%7D\" alt=\"G_{t} = \\sum^{T}_{k=0} \\gamma^{k} r_{t+k}\" eeimg=\"1\"/\u003e的定义稍微有点问题，因为k最多取到\u003cimg src=\"https://www.zhihu.com/equation?tex=T-t\" alt=\"T-t\" eeimg=\"1\"/\u003e，在轨迹结束后没有奖励了，所以它和正确的定义\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t+%3D+%5Csum_%7Bk%3Dt%7D%5E%7BT_%7Bn%7D%7D+%5Cgamma%5E%7Bk-t%7D+r_%7Bk%7D\" alt=\"G_t = \\sum_{k=t}^{T_{n}} \\gamma^{k-t} r_{k}\" eeimg=\"1\"/\u003e的值是一样的。习惯用它，就保留了。 通过变量替换，可得如下递推公式：\u003c/blockquote\u003e\u003cp data-pid=\"D8xWQQFy\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++G_%7Bt%7D%3Dr_%7Bt%7D%2B%5Cgamma%28r_%7Bt%2B1%7D%2B%5Cgamma%5E%7B1%7D+r_%7Bt%2B2%7D%2B+%5Cgamma%5E%7B2%7D+r_%7Bt%2B3%7D+%2B+%5Cldots%29+%3D+r_%7Bt%7D%2B%5Cgamma+G_%7Bt%2B1%7D+%5Ctag%7B2.4%7D%5C%5C\" alt=\"  G_{t}=r_{t}+\\gamma(r_{t+1}+\\gamma^{1} r_{t+2}+ \\gamma^{2} r_{t+3} + \\ldots) = r_{t}+\\gamma G_{t+1} \\tag{2.4}\\\\\" eeimg=\"1\"/\u003e注意，传统定义将时间步\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e的状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e采取动作\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e得到的奖励，表示\u003cimg src=\"https://www.zhihu.com/equation?tex=r_%7Bt%2B1%7D\" alt=\"r_{t+1}\" eeimg=\"1\"/\u003e。这里统一用状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e、动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e和奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=r_t\" alt=\"r_t\" eeimg=\"1\"/\u003e来表示。\u003c/p\u003e\u003cp data-pid=\"vJvUyFIi\"\u003e\u003cb\u003eVersion3: 使用时间步\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e\u003cb\u003e的减去基线的累计折扣奖励\u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"BlTBH9fI\"\u003e\u003cbr/\u003e 图2.10 版本三：减去基线的累计折扣奖励作为策略调整信号\u003c/p\u003e\u003cp data-pid=\"8CJO0iOU\"\u003e如果奖励都是正的，其实也存在2个问题:\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"lsFIXxD5\"\u003e\u003cb\u003e轨迹依赖于采样\u003c/b\u003e，但如果采样到轨迹中的奖励都为正，正的奖励就会提高动作出现的概率。但存在潜在的好动作未被采样，相对来说就被降低了出现的概率；\u003c/li\u003e\u003cli data-pid=\"rIND_fE4\"\u003e\u003cb\u003e梯度估计高方差\u003c/b\u003e：策略梯度依赖整个轨迹来估计，估计出的梯度方差过大，会导致训练不稳定，更难收敛（特别是轨迹很长时）；\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"j0Mz-VmQ\"\u003e让奖励有正有负的一个实现，就是用回报\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e减去回报的均值，即baseline基线\u003cimg src=\"https://www.zhihu.com/equation?tex=b%28s_t%29\" alt=\"b(s_t)\" eeimg=\"1\"/\u003e，对其进行中心化（但注意，奖励值为0是个非常差的设定）。\u003c/p\u003e\u003cblockquote data-pid=\"YcaKuW_D\"\u003e具体的一种实现，就是采样多个轨迹。计算单个轨迹下各时间步的回报，并统计各状态下的回报，并计算相应均值。这其实就是状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e的状态值函数\u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7B%5Cpi%7D%28s_t%29\" alt=\"V_{\\pi}(s_t)\" eeimg=\"1\"/\u003e的在轨迹维度的\u003cb\u003e蒙特卡洛估计\u003c/b\u003e。\u003c/blockquote\u003e\u003cp data-pid=\"WnGgb_2h\"\u003e该方法的另一种解释就是，基线\u003cimg src=\"https://www.zhihu.com/equation?tex=b%28s_t%29\" alt=\"b(s_t)\" eeimg=\"1\"/\u003e代表在状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e后的平均回报。那么，回报\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e减去基线\u003cimg src=\"https://www.zhihu.com/equation?tex=b%28s_t%29\" alt=\"b(s_t)\" eeimg=\"1\"/\u003e，代表的就是采取动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e相对于平均水平的优势（好坏程度）。（基线\u003cimg src=\"https://www.zhihu.com/equation?tex=b%28s_t%29\" alt=\"b(s_t)\" eeimg=\"1\"/\u003e的实现有很多变体，可以直接采样估计，也可以用一个Critic Model来拟合）\u003c/p\u003e\u003cp data-pid=\"eQXGR_hF\"\u003e通常\u003cimg src=\"https://www.zhihu.com/equation?tex=A_t+%3D+G_t+-+b%28s_t%29\" alt=\"A_t = G_t - b(s_t)\" eeimg=\"1\"/\u003e被称为优势Advantage，用于衡量动作的相对好坏。 | 这里只做概念上的的介绍，下一篇将做详细分析\u003c/p\u003e\u003cp data-pid=\"QzKzrFgQ\"\u003e\u003cb\u003e策略梯度的实现\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"wNjwIvT3\"\u003e\u003cbr/\u003e 图2.11 Policy Graident的实现\u003c/p\u003e\u003cp data-pid=\"zcpoWgXg\"\u003e到目前为止，如果使用版本三基于减去基线的累计折扣奖励作为优势Advantage，来优化调整策略网络，其实就是策略梯度算法中的REINFORCE with baseline的一种实现。其损失如下所示：\u003c/p\u003e\u003cp data-pid=\"ut58otI1\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+++++L%28%5Cmathbb%7BT%7D%3B%7B%5Ctheta%7D%29+%3D+-%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+A_%7Bt%7D+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5Cend%7Baligned%7D+%5Ctag%7B2.5%7D\" alt=\"\\begin{aligned}     L(\\mathbb{T};{\\theta}) = -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} A_{t} \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right) \\end{aligned} \\tag{2.5}\" eeimg=\"1\"/\u003e其中，\u003cimg src=\"https://www.zhihu.com/equation?tex=e%3D++-%5Clog+p_%7B%5Ctheta%7D%28+%5Chat%7Ba%7D+%7C+s%29\" alt=\"e=  -\\log p_{\\theta}( \\hat{a} | s)\" eeimg=\"1\"/\u003e ，为动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7Ba%7D\" alt=\"\\hat{a}\" eeimg=\"1\"/\u003e上的负对数似然即交叉熵损失; \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BT%7D\" alt=\"\\mathbb{T}\" eeimg=\"1\"/\u003e为采样到的\u003cimg src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/\u003e个轨迹的集合，\u003cimg src=\"https://www.zhihu.com/equation?tex=T_n\" alt=\"T_n\" eeimg=\"1\"/\u003e为第\u003cimg src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/\u003e个轨迹的总长度。\u003c/p\u003e\u003cp data-pid=\"KM9qV85_\"\u003e到这里为止，已经可以用pytorch去实现策略梯度算法了：\u003c/p\u003e\u003cblockquote data-pid=\"Rt7lOS-5\"\u003e在每一轮迭代中，都要基于当前的Actor采样轨迹数据。并前向传播计算损失函数。然后，靠pytorch自动微分求得梯度，随后基于优化算法更新Actor的参数。\u003c/blockquote\u003e\u003cp data-pid=\"spdCopnU\"\u003e可知损失的梯度为：\u003c/p\u003e\u003cp data-pid=\"ToQv4sGt\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctheta%7D+L%28%5Cmathbb%7BT%7D%3B%7B%5Ctheta%7D%29+%3D+-%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+A_%7Bt%7D+%5Cnabla_%7B%5Ctheta%7D+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5Ctag%7B2.6%7D\" alt=\"\\nabla_{\\theta} L(\\mathbb{T};{\\theta}) = -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} A_{t} \\nabla_{\\theta} \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right) \\tag{2.6}\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cblockquote data-pid=\"JpFBi8YT\"\u003e后续的优化重点就在于基线怎么估计、优势的进一步优化，以及提升采样的样本的利用率和相关的策略更新幅度的限制等事项；\u003c/blockquote\u003e\u003cp data-pid=\"waQqQePE\"\u003e到这里，策略梯度直观实现已经完成，接下来将从轨迹生成概率的视角，推导一下目标函数。\u003c/p\u003e\u003ch2\u003e三、策略梯度的推导\u003c/h2\u003e\u003ch3\u003e3.1 策略梯度的流程\u003c/h3\u003e\u003cp data-pid=\"6EPFj-6d\"\u003e策略梯度的核心思想：基于当前策略采样轨迹数据，并通过梯度上升算法对策略网络参数进行优化，使得策略网络在使得在给定轨迹数据下获得的期望奖励能最大化。\u003c/p\u003e\u003cblockquote data-pid=\"dP8eY4o8\"\u003e从轨迹分布的视角出发，策略梯度算法的目标函数就是轨迹的累计折扣奖励的期望。那么通过不断调整策略网络的参数，使得其在训练的轨迹数据上，能有最大的累计奖励，从而找到一个最优的策略。\u003c/blockquote\u003e\u003cp data-pid=\"Z9fKPvau\"\u003e策略梯度的核心流程为3步：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"Q39tLV-i\"\u003e\u003cb\u003e采样轨迹数据\u003c/b\u003e：基于策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e采样一批轨迹，即与环境交互得到轨迹数据，轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e由状态、动作和奖励组成；\u003c/li\u003e\u003cli data-pid=\"dQGUZeBZ\"\u003e\u003cb\u003e计算轨迹的概率\u003c/b\u003e：计算轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e出现的概率（在最终的式子中会简化掉和目标函数无关的概率项）；\u003c/li\u003e\u003cli data-pid=\"AhFY2PUg\"\u003e\u003cb\u003e计算梯度并更新策略网络参数\u003c/b\u003e：计算轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e出现的概率关于策略网络参数的梯度，并使用奖励加权。再基于梯度上升，更新策略网络参数。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"rT-8mFy6\"\u003e接下来，会详细介绍策略梯度算法细节。\u003c/p\u003e\u003cblockquote data-pid=\"VS1jvS72\"\u003e注意，因为图片大部分源于李宏毅老师的课件，所以符号直接沿用。后续的\u003cimg src=\"https://www.zhihu.com/equation?tex=R_t\" alt=\"R_t\" eeimg=\"1\"/\u003e和上文的\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e是等价的。有带折扣的和不带折扣的2个版本。\u003c/blockquote\u003e\u003ch3\u003e3.2 轨迹的概率计算\u003c/h3\u003e\u003cp data-pid=\"WvpdAYxW\"\u003e一般来说，一个轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e出现的概率\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e取决于Actor的动作和环境的动作（但在LLM的场景中，就只取决于大模型本身）。\u003c/p\u003e\u003cp data-pid=\"GZcXBmKU\"\u003e\u003cb\u003eActor和环境的概率分布\u003c/b\u003e如下：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"HyDcy3ID\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D+%7C+s_%7Bt%7D%5Cright%29\" alt=\"p_{\\theta}\\left(a_{t} | s_{t}\\right)\" eeimg=\"1\"/\u003e由策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e决定：\u003c/li\u003e\u003cli data-pid=\"TPUn-07W\"\u003e给定一个状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e，Actor要采取的动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e 取决于其参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e。策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%5Ctheta\" alt=\"\\pi_\\theta\" eeimg=\"1\"/\u003e的输出是一个概率分布\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D+%7C+s_%7Bt%7D%5Cright%29\" alt=\"p_{\\theta}\\left(a_{t} | s_{t}\\right)\" eeimg=\"1\"/\u003e，基于这个分布做采样，就能决定Actor要采取的动作是什么。\u003c/li\u003e\u003cli data-pid=\"m_qCeI1w\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28s_%7Bt%2B1%7D+%7C+s_%7Bt%7D%2C+a_%7Bt%7D%5Cright%29\" alt=\"p\\left(s_{t+1} | s_{t}, a_{t}\\right)\" eeimg=\"1\"/\u003e 由环境决定：\u003c/li\u003e\u003cli data-pid=\"aB7_wgCI\"\u003e环境的动作是指环境根据其内部的参数或规则采取的动作，导致状态的转移。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Kfv2_ehg\"\u003e以RL打游戏为例子。在一场游戏里面，将环境输出的 \u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e 与Actor输出的动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/\u003e 全部组合起来，就是一个轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau%3D%5Cleft%5C%7Bs_%7B1%7D%2C+a_%7B1%7D%2C+s_%7B2%7D%2C+a_%7B2%7D%2C+%5Ccdots%2C+s_%7BT%7D%2C+a_%7BT%7D%5Cright%5C%7D\" alt=\"\\tau=\\left\\{s_{1}, a_{1}, s_{2}, a_{2}, \\cdots, s_{T}, a_{T}\\right\\}\" eeimg=\"1\"/\u003e（有时也会把即时奖励涵盖进去）。\u003c/p\u003e\u003cp data-pid=\"qCg6fZpj\"\u003e\u003cbr/\u003e 图3.1 Actor和环境\u003c/p\u003e\u003cp data-pid=\"Uv_wk7h-\"\u003e环境一开始根据一个初始状态分布，输出一个初始状态（游戏画面 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_1\" alt=\"s_1\" eeimg=\"1\"/\u003e），Actor看到游戏画面 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_1\" alt=\"s_1\" eeimg=\"1\"/\u003e 以后，根据其策略分布\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D+%7C+s_%7Bt%7D%5Cright%29\" alt=\"p_{\\theta}\\left(a_{t} | s_{t}\\right)\" eeimg=\"1\"/\u003e采样输出动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_1\" alt=\"a_1\" eeimg=\"1\"/\u003e。环境把动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_1\" alt=\"a_1\" eeimg=\"1\"/\u003e 当作它的输入，再输出新的游画面 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_2\" alt=\"s_2\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cp data-pid=\"DpXmyP7N\"\u003eActor看到新的游戏画面\u003cimg src=\"https://www.zhihu.com/equation?tex=s_2\" alt=\"s_2\" eeimg=\"1\"/\u003e，再采取新的动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_2\" alt=\"a_2\" eeimg=\"1\"/\u003e。环境看到 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_2\" alt=\"a_2\" eeimg=\"1\"/\u003e，再输出 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_3\" alt=\"s_3\" eeimg=\"1\"/\u003e，...。按这样，反复执行下去，\u003cb\u003e直到大厦崩塌 \u003c/b\u003e。额，当然是直到环境觉得应该要停止为止。\u003c/p\u003e\u003cp data-pid=\"sh_vZ2WD\"\u003e给定Actor的参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e，按MDP的设定，可以计算某个轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e发生的概率：\u003c/p\u003e\u003cp data-pid=\"DeZOa8Sh\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+++++++++p_%7B%5Ctheta%7D%28%5Ctau%29+++++++++%26%3Dp%5Cleft%28s_%7B1%7D%5Cright%29+p_%7B%5Ctheta%7D%5Cleft%28a_%7B1%7D+%7C+s_%7B1%7D%5Cright%29+p%5Cleft%28s_%7B2%7D+%7C+s_%7B1%7D%2C+a_%7B1%7D%5Cright%29+p_%7B%5Ctheta%7D%5Cleft%28a_%7B2%7D+%7C+s_%7B2%7D%5Cright%29+p%5Cleft%28s_%7B3%7D+%7C+s_%7B2%7D%2C+a_%7B2%7D%5Cright%29+%5Ccdots+%5C%5C+++++++++%26%3Dp%5Cleft%28s_%7B1%7D%5Cright%29+%5Cprod_%7Bt%3D1%7D%5E%7BT%7D+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D+%7C+s_%7Bt%7D%5Cright%29+p%5Cleft%28s_%7Bt%2B1%7D+%7C+s_%7Bt%7D%2C+a_%7Bt%7D%5Cright%29+++++++++%5Cend%7Baligned%7D+%5Ctag%7B3.1%7D\" alt=\"\\begin{aligned}         p_{\\theta}(\\tau)         \u0026amp;=p\\left(s_{1}\\right) p_{\\theta}\\left(a_{1} | s_{1}\\right) p\\left(s_{2} | s_{1}, a_{1}\\right) p_{\\theta}\\left(a_{2} | s_{2}\\right) p\\left(s_{3} | s_{2}, a_{2}\\right) \\cdots \\\\         \u0026amp;=p\\left(s_{1}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(a_{t} | s_{t}\\right) p\\left(s_{t+1} | s_{t}, a_{t}\\right)         \\end{aligned} \\tag{3.1}\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cblockquote data-pid=\"KKGlX3a7\"\u003e其假设为一阶马尔科夫链，将概率依赖关系限定在了相邻2步（与多步的历史无关），从而简化了多阶MDP带来的状态空间爆炸和计算复杂度问题（当然，通过多次1阶概率转移矩阵相乘也可近似多阶依赖）。\u003c/blockquote\u003e\u003cp data-pid=\"fVlKt689\"\u003e根据条件概率的乘法公式和马尔可夫决策过程的性质，即当前状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt%7D\" alt=\"s_{t}\" eeimg=\"1\"/\u003e和动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_%7Bt%7D\" alt=\"a_{t}\" eeimg=\"1\"/\u003e只受上一时刻的状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_%7Bt-1%7D\" alt=\"s_{t-1}\" eeimg=\"1\"/\u003e和动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_%7Bt-1%7D\" alt=\"a_{t-1}\" eeimg=\"1\"/\u003e影响。\u003c/p\u003e\u003cp data-pid=\"B2x7Xcnb\"\u003e所以轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e出现的联合概率\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e，为初始状态的概率\u003cimg src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28s_%7B1%7D%5Cright%29\" alt=\"p\\left(s_{1}\\right)\" eeimg=\"1\"/\u003e与各个条件概率 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D+%7C+s_%7Bt%7D%5Cright%29\" alt=\"p_{\\theta}\\left(a_{t} | s_{t}\\right)\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28s_%7Bt%2B1%7D+%7C+s_%7Bt%7D%2C+a_%7Bt%7D%5Cright%29\" alt=\"p\\left(s_{t+1} | s_{t}, a_{t}\\right)\" eeimg=\"1\"/\u003e 的连乘。\u003c/p\u003e\u003cblockquote data-pid=\"wuTKGsyz\"\u003e如果仅按条件概率拆解联合概率公式，有\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28s_1%2Ca_1%2Cs_2%2Ca_2%29+%3D+p_%7B%5Ctheta%7D%28s_1%2Ca_1%2Cs_2%29+%5Ctimes+p_%7B%5Ctheta%7D%28a_2%7C+s_1%2C+a_1%2C+s_2%29\" alt=\"p_{\\theta}(s_1,a_1,s_2,a_2) = p_{\\theta}(s_1,a_1,s_2) \\times p_{\\theta}(a_2| s_1, a_1, s_2)\" eeimg=\"1\"/\u003e。但根据1阶MDP的性质，直接简化为了：\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28a_2%7C+s_1%2C+a_1%2C+s_2%29++%3D+p_%7B%5Ctheta%7D%28a_2%7C+s_2%29\" alt=\"p_{\\theta}(a_2| s_1, a_1, s_2)  = p_{\\theta}(a_2| s_2)\" eeimg=\"1\"/\u003e。\u003c/blockquote\u003e\u003cp data-pid=\"3XuSSJRA\"\u003e具体地，在马尔可夫决策过程中，\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e = {\u003cimg src=\"https://www.zhihu.com/equation?tex=s_1\" alt=\"s_1\" eeimg=\"1\"/\u003e, \u003cimg src=\"https://www.zhihu.com/equation?tex=a_1\" alt=\"a_1\" eeimg=\"1\"/\u003e, \u003cimg src=\"https://www.zhihu.com/equation?tex=s_2\" alt=\"s_2\" eeimg=\"1\"/\u003e, \u003cimg src=\"https://www.zhihu.com/equation?tex=a_2\" alt=\"a_2\" eeimg=\"1\"/\u003e} 发生的概率\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29+%3Dp_%7B%5Ctheta%7D+%5Cleft%28s_1%2Ca_%7B1%7D%2C+s_%7B2%7D%2C+a_2+%5Cright%29\" alt=\"p_{\\theta}(\\tau) =p_{\\theta} \\left(s_1,a_{1}, s_{2}, a_2 \\right)\" eeimg=\"1\"/\u003e可以拆分为 {\u003cimg src=\"https://www.zhihu.com/equation?tex=s_1\" alt=\"s_1\" eeimg=\"1\"/\u003e, \u003cimg src=\"https://www.zhihu.com/equation?tex=a_1\" alt=\"a_1\" eeimg=\"1\"/\u003e, \u003cimg src=\"https://www.zhihu.com/equation?tex=s_2\" alt=\"s_2\" eeimg=\"1\"/\u003e}\u003cimg src=\"https://www.zhihu.com/equation?tex=%E5%8F%91%E7%94%9F%E7%9A%84%E6%A6%82%E7%8E%87+p_%7B%5Ctheta%7D+%5Cleft%28s_1%2Ca_%7B1%7D%2C+s_%7B2%7D%5Cright%29\" alt=\"发生的概率 p_{\\theta} \\left(s_1,a_{1}, s_{2}\\right)\" eeimg=\"1\"/\u003e乘以给定\u003cimg src=\"https://www.zhihu.com/equation?tex=s_2\" alt=\"s_2\" eeimg=\"1\"/\u003e时\u003cimg src=\"https://www.zhihu.com/equation?tex=a_2\" alt=\"a_2\" eeimg=\"1\"/\u003e发生的概率 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%5Cleft%28a_%7B2%7D+%7C+s_%7B2%7D%5Cright%29\" alt=\"p_{\\theta}\\left(a_{2} | s_{2}\\right)\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cp data-pid=\"yZp3lOJR\"\u003e所以\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28s_1%2Ca_1%2Cs_2%2Ca_2%29\" alt=\"p_{\\theta}(s_1,a_1,s_2,a_2)\" eeimg=\"1\"/\u003e的整体公式为：\u003c/p\u003e\u003cp data-pid=\"C3d1hT1Z\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+p_%7B%5Ctheta%7D%28s_1%2Ca_1%2Cs_2%2Ca_2%29+%26%3D+p_%7B%5Ctheta%7D%28s_1%2Ca_1%2Cs_2%29+%5Ctimes+p_%7B%5Ctheta%7D%28a_2%7Cs_2%29++%5C%5C%26%3D+p%28s_1%29p_%7B%5Ctheta%7D%28a_1%7Cs_1%29p%28s_2%7Cs_1%2Ca_1%29p_%7B%5Ctheta%7D%28a_2%7Cs_2%29++%5Cend%7Baligned%7D++%5Ctag%7B3.2%7D\" alt=\"\\begin{aligned} p_{\\theta}(s_1,a_1,s_2,a_2) \u0026amp;= p_{\\theta}(s_1,a_1,s_2) \\times p_{\\theta}(a_2|s_2)  \\\\\u0026amp;= p(s_1)p_{\\theta}(a_1|s_1)p(s_2|s_1,a_1)p_{\\theta}(a_2|s_2)  \\end{aligned}  \\tag{3.2}\" eeimg=\"1\"/\u003e其中，\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28s_1%2Ca_1%2Cs_2%29+%3D+p%28s_1%29+p_%7B%5Ctheta%7D%28a_1%7Cs_1%29p%28s_2%7Cs_1%2Ca_1%29\" alt=\"p_{\\theta}(s_1,a_1,s_2) = p(s_1) p_{\\theta}(a_1|s_1)p(s_2|s_1,a_1)\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cblockquote data-pid=\"gQcb8Ysq\"\u003e例如，假设\u003cimg src=\"https://www.zhihu.com/equation?tex=p%28s_1%29%3D0.6\" alt=\"p(s_1)=0.6\" eeimg=\"1\"/\u003e，\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28a_1%7Cs_1%29%3D0.7\" alt=\"p_{\\theta}(a_1|s_1)=0.7\" eeimg=\"1\"/\u003e，\u003cimg src=\"https://www.zhihu.com/equation?tex=p%28s_2%7Cs_1%2Ca_1%29%3D0.8\" alt=\"p(s_2|s_1,a_1)=0.8\" eeimg=\"1\"/\u003e，\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28a_2%7Cs_2%29%3D0.9\" alt=\"p_{\\theta}(a_2|s_2)=0.9\" eeimg=\"1\"/\u003e，则\u003cimg src=\"https://www.zhihu.com/equation?tex=p%28s_1%2Ca_1%2Cs_2%2Ca_2%29%3D0.6%C3%970.7%C3%970.8%C3%970.9+%3D+0.3024\" alt=\"p(s_1,a_1,s_2,a_2)=0.6×0.7×0.8×0.9 = 0.3024\" eeimg=\"1\"/\u003e。\u003c/blockquote\u003e\u003cp data-pid=\"VgCnyVmg\"\u003e然而，通常大部分环境是未知项，加上环境无法被控制。而且，后续的优化策略的目标函数和环境的动态不相关。所以，能做的就是调整Actor。\u003c/p\u003e\u003cblockquote data-pid=\"ren0O-Bj\"\u003e无需获取环境的状态转移概率 \u003cimg src=\"https://www.zhihu.com/equation?tex=p%28s_%7Bt%2B1%7D+%7C+s_%7Bt%7D%2C+a_%7Bt%7D%29\" alt=\"p(s_{t+1} | s_{t}, a_{t})\" eeimg=\"1\"/\u003e去计算完整的\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e。这就是无模型RL的思路。\u003c/blockquote\u003e\u003ch3\u003e3.3 期望累计奖励\u003c/h3\u003e\u003cp data-pid=\"Q2iKGWYp\"\u003e\u003cbr/\u003e 图3.2 期望累计奖励最大的示例\u003c/p\u003e\u003cp data-pid=\"e1nA2uJK\"\u003e如上图所示，奖励函数根据在状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e采取的动\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e决定这个动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e决可以得到的分数。基于奖励函数，在时间步\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e，就能得到一个确定性的即时奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_t\" alt=\"r_t\" eeimg=\"1\"/\u003e(随机性的奖励比较少见)。\u003c/p\u003e\u003cblockquote data-pid=\"uOH-f4lV\"\u003e对奖励函数而言: 输入 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5C%7Bs_1%2Ca_1%5C%7D\" alt=\"\\{s_1,a_1\\}\" eeimg=\"1\"/\u003e，它会输出\u003cimg src=\"https://www.zhihu.com/equation?tex=r_1\" alt=\"r_1\" eeimg=\"1\"/\u003e, 输入 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5C%7Bs_2%2Ca_2%5C%7D\" alt=\"\\{s_2,a_2\\}\" eeimg=\"1\"/\u003e，它会输出 \u003cimg src=\"https://www.zhihu.com/equation?tex=r_2\" alt=\"r_2\" eeimg=\"1\"/\u003e。\u003c/blockquote\u003e\u003cp data-pid=\"V-sLl2ZQ\"\u003e如果把轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e所有的即时奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5C%7Br_1%2C+r_2%2C+...%2C+r_T%5C%7D\" alt=\"\\{r_1, r_2, ..., r_T\\}\" eeimg=\"1\"/\u003e都加起来，就得到了 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29+%3D+%5Csum%5E%7BT%7D_%7Bt%3D1%7D+r_t\" alt=\"R(\\tau) = \\sum^{T}_{t=1} r_t\" eeimg=\"1\"/\u003e ，即轨迹 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e 的即时奖励之和，也称回报（总奖励）或累计奖励。\u003c/p\u003e\u003cp data-pid=\"2NT1-saM\"\u003e累计奖励将作为Actor好坏的衡量尺度。那么显而易见，要去优化Actor，那就是尽量让Actor在尽可能多的轨迹上的累计奖励的都尽可能大，换句话就是期望累计奖励最大。\u003c/p\u003e\u003cblockquote data-pid=\"pNdAEVAq\"\u003e譬如，下一局围棋，赢了能得到一个好的回报 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e。那最终的目标就是调整Actor内部的参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e， 使得尽可能多的围棋场次上的\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e的值都越大越好。 理想策略就是AlphaGo了\u003c/blockquote\u003e\u003cp data-pid=\"UCoc88cs\"\u003e但实际上，累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e并不只是一个标量（scalar），它是一个随机变量（因为\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e 是一个随机变量，所以\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e 的函数\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e也是一个随机变量）：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"XbqOumhw\"\u003eActor在给定同样的状态下会采取的动作是有随机性的。因为，动作是从Actor的\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e所决定的策略概率分布 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e 中采样得到的。\u003c/li\u003e\u003cli data-pid=\"b9EZfhBv\"\u003e环境在给定同样的观测时要采取什么样的动作，要产生什么样的观测，本身也是有随机性的，\u003c/li\u003e\u003cli data-pid=\"3qDhVokv\"\u003e每一步的动作和状态都是随机变量，那每一步的奖励实际就是随机变量的函数。（多个随机变量的函数之和，还是随机变量）\u003c/li\u003e\u003c/ul\u003e\u003cblockquote data-pid=\"nwb7UCvS\"\u003e具体而言，如果是LLM，其环境就没有随机性了，通常随机性是取决于Actor的。所以通常是去估计的\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e的期望值。例如自动驾驶，那环境有随机性。\u003c/blockquote\u003e\u003cp data-pid=\"3poMcKLa\"\u003e\u003cbr/\u003e 图3.3 回报和期望累计奖励\u003c/p\u003e\u003cp data-pid=\"4ykgeUSf\"\u003e给定Actor的参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e，可计算其期望奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e :\u003c/p\u003e\u003cp data-pid=\"awnDRm5M\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Cbar%7BR%7D_%7B%5Ctheta%7D%3D%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+p_%7B%5Ctheta%7D%28%5Ctau%29%3D%5Cmathbb%7BE%7D_%7B%5Ctau+%5Csim+p_%7B%5Ctheta%7D%28%5Ctau%29%7D%5BR%28%5Ctau%29%5D+%5Ctag%7B3.3%7D%5C%5C\" alt=\"    \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[R(\\tau)] \\tag{3.3}\\\\\" eeimg=\"1\"/\u003e理论上，为了计算这个期望奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e （即累计奖励的期望），要穷举所有可能的轨迹 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e， 而每一个轨迹 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e 可以根据 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e 算出其出现的概率 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e。但轨迹的数量是无穷的，期望值注定无法通过穷举，只能通过采样近似计算。\u003c/p\u003e\u003cblockquote data-pid=\"gPzOyweT\"\u003e如果期望奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e 在时间步展开并加上累计折扣因子，基本与之前的式子(1.1)目标函数\u003cimg src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/\u003e等价。\u003c/blockquote\u003e\u003cp data-pid=\"rSjVig-G\"\u003e注意的是，策略梯度的目的并不是估计这个期望奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e，而是为了用策略网络采样出的轨迹数据去估计其参数关于期望奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e这个目标函数的梯度（这个梯度是有关轨迹奖励和轨迹概率的，没有轨迹概率就无法优化策略）。\u003c/p\u003e\u003cblockquote data-pid=\"meG07DHw\"\u003e如果用一定数量轨迹的累计奖励的平均来估计期望奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e，有\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D+%3D+%5Cmathbb%7BE%7D_%7B%5Ctau+%5Csim+p_%7B%5Ctheta%7D%28%5Ctau%29%7D%5Cleft%5BR%28%5Ctau%29+%5Cright%5D+%5Capprox+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+R%5Cleft%28%5Ctau%5E%7Bn%7D%5Cright%29\" alt=\"\\bar{R}_{\\theta} = \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\right] \\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right)\" eeimg=\"1\"/\u003e。这时轨迹奖励均值 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+R%5Cleft%28%5Ctau%5E%7Bn%7D%5Cright%29\" alt=\"\\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right)\" eeimg=\"1\"/\u003e 只是期望奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e的基于样本的无偏的极大似然估计，它与策略网络的参数无关，只是策略效果的评估，所以它并不是真正的目标函数（无法用于优化策略，而且直观来看就是\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D\" alt=\"\\frac{1}{N}\" eeimg=\"1\"/\u003e做权重了）。\u003c/blockquote\u003e\u003ch3\u003e3.4 策略梯度\u003c/h3\u003e\u003cp data-pid=\"-KJqRcCD\"\u003e强化学习目标是找到一个策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e，能使得期望累计奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e越大越好。假设基于当前的Actor采样了一批轨迹数据，其中\u003cimg src=\"https://www.zhihu.com/equation?tex=%28s_t%2Ca_t%29\" alt=\"(s_t,a_t)\" eeimg=\"1\"/\u003e 是在整个轨迹 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e 的里面的某一个状态和动作。\u003c/p\u003e\u003cp data-pid=\"wiLqcSRs\"\u003e\u003cbr/\u003e 图3.4 策略梯度\u003c/p\u003e\u003cp data-pid=\"F24rHlvp\"\u003e给定采样到的轨迹，如何优化策略呢。很直观的做法就是，假设在 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e 采取了动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"bR6LnbiZ\"\u003e如果在 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e 执行 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e 观察到正的奖励，那么增加在 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e 执行 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e 的概率。\u003c/li\u003e\u003cli data-pid=\"twueNDl0\"\u003e反之，如果在 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e 执行 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e 观察到负的奖励，那就减少在 \u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e 执行 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e 的概率。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"yqel4nK1\"\u003e只要按这个机制去调整参数化的策略网络的参数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e。那么自然就能提升在给定采样到的训练数据上的近似的期望奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e，从而最终得到最优策略。那具体怎么调整Actor的参数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e呢？\u003c/p\u003e\u003cp data-pid=\"I-A-riJ6\"\u003e一个最朴素的端到端得优化思路，将期望奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e做为目标函数，令\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\nabla \\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e为Actor的参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e 关于期望累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D%3D%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"\\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e的梯度，并根据策略网络的梯度来优化。\u003c/p\u003e\u003cp data-pid=\"T6GDkdPZ\"\u003e具体地，给定一个参数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7B%5Ctheta%7D\" alt=\"\\mathbf{\\theta}\" eeimg=\"1\"/\u003e的策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e，往往是使用该策略网络\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cmathbf%7B%5Ctheta%7D%7D\" alt=\"\\pi_{\\mathbf{\\theta}}\" eeimg=\"1\"/\u003e采样一定数量的轨迹 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BT%7D+%3D+%5C%7B+%5Ctau%5E%7B1%7D%2C+...%2C+%5Ctau%5E%7BN%7D+%5C%7D\" alt=\"\\mathbb{T} = \\{ \\tau^{1}, ..., \\tau^{N} \\}\" eeimg=\"1\"/\u003e （上标代表某次轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/\u003e，下标代表轨迹的某个时间步\u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e），并基于对应奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e和轨迹出现的概率\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e，来计算理论策略梯度的期望形式\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"\\nabla \\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e的无偏估计\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7B%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D%7D\" alt=\"\\hat{\\nabla \\bar{R}_{\\theta}}\" eeimg=\"1\"/\u003e，并使用 \u003cb\u003e梯度上升\u003c/b\u003e 来优化当前策略网络参数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cblockquote data-pid=\"DEEmJ9P8\"\u003e注意采样轨迹的目的是为了估计策略梯度。而基于采样的轨迹得到累计奖励的期望的估计，已经和原始的目标函数无关了。\u003c/blockquote\u003e\u003cp data-pid=\"p7sjR1ow\"\u003e\u003cb\u003e策略梯度的实际推导逻辑\u003c/b\u003e：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"0V_aofwr\"\u003e从目标函数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e出发，直接从梯度上升的视角，去推导策略网络参数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e关于目标函数的梯度形式\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\nabla \\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e；\u003c/li\u003e\u003cli data-pid=\"iqGkv8AO\"\u003e策略梯度\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\nabla \\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e仍然是一个期望，对其进行采样估计近似后，获得策略梯度\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\nabla \\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e的估计值。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"RCcFgjub\"\u003e从此时的近似估计的梯度形式倒推的近似目标函数自然不等价于原始目标函数。估计是目标函数的近似，和目标函数不成正比，更不能是目标函数本身。最后，用样本来估计这个期望。\u003c/p\u003e\u003cblockquote data-pid=\"lowXNU0W\"\u003e从直觉出发，实际目标函数需要考虑策略网络对奖励的影响，所以会使用给定策略轨迹出现的概率。这其实也是离散随机变量的期望的近似项。原本的策略梯度目标函数是包含策略网络的概率这项的。\u003c/blockquote\u003e\u003cp data-pid=\"943ZVQkp\"\u003e要进行梯度上升，那就要计算策略网络参数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7B%5Ctheta%7D\" alt=\"\\mathbf{\\theta}\" eeimg=\"1\"/\u003e关于期望奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e 的梯度 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\nabla \\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e的估计 。先对期望奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e 做梯度运算：\u003c/p\u003e\u003cp data-pid=\"i5xbrdSR\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D%3D%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+%5Cnabla+p_%7B%5Ctheta%7D%28%5Ctau%29+%5Ctag%7B3.4%7D%5C%5C\" alt=\"    \\nabla \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau) \\tag{3.4}\\\\\" eeimg=\"1\"/\u003e其中，奖励函数\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e 不需要是可微的，故只有 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e 与 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e 有关。\u003c/p\u003e\u003cp data-pid=\"4wEKH5ig\"\u003e所以 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\nabla \\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e实际是\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e 乘以 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"\\nabla p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e（即\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e关于轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e的出现概率\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e的梯度）。实际上是每一个轨迹的梯度，乘以 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e 的梯度。\u003c/p\u003e\u003cp data-pid=\"hGpgUJZR\"\u003e\u003cbr/\u003e 图3.5 Policy Gradiet推导\u003c/p\u003e\u003cp data-pid=\"xe7tC7Jk\"\u003e基于对数函数的求导公式（对数函数 \u003cimg src=\"https://www.zhihu.com/equation?tex=f%28x%29%3D%5Clog+x\" alt=\"f(x)=\\log x\" eeimg=\"1\"/\u003e 的导数为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bx%7D\" alt=\"\\frac{1}{x}\" eeimg=\"1\"/\u003e）可得:\u003c/p\u003e\u003cp data-pid=\"GOMCWYi0\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Cnabla+f%28x%29%3Df%28x%29%5Cnabla+%5Clog+f%28x%29+%5Ctag%7B3.5%7D%5C%5C\" alt=\"    \\nabla f(x)=f(x)\\nabla \\log f(x) \\tag{3.5}\\\\\" eeimg=\"1\"/\u003e对\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"\\nabla p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e 使用该log-trick公式，可得 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+p_%7B%5Ctheta%7D%28%5Ctau%29%3Dp_%7B%5Ctheta%7D%28%5Ctau%29++%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"\\nabla p_{\\theta}(\\tau)=p_{\\theta}(\\tau)  \\nabla \\log p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e。接下来，公式两边变换下形式，可得：\u003c/p\u003e\u003cp data-pid=\"7SuIbk8a\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Cfrac%7B%5Cnabla+p_%7B%5Ctheta%7D%28%5Ctau%29%7D%7Bp_%7B%5Ctheta%7D%28%5Ctau%29%7D%3D+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29+%5Ctag%7B3.6%7D%5C%5C\" alt=\"    \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)}= \\nabla \\log p_{\\theta}(\\tau) \\tag{3.6}\\\\\" eeimg=\"1\"/\u003e期望奖励关于策略参数的梯度，即策略梯度\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\nabla \\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e就可按期望的形式对式子(3.4)进行展开。如式(3.7)所示，对所有可能的 \u003cimg src=\"https://www.zhihu.com/equation?tex=%CF%84\" alt=\"τ\" eeimg=\"1\"/\u003e 进行求和，把 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e  乘  \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"\\nabla\\log p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e并用 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e 加权：\u003c/p\u003e\u003cp data-pid=\"OFmU61h_\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Cbegin%7Baligned%7D+++++++++%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D+%26%3D%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+%5Cnabla+p_%7B%5Ctheta%7D%28%5Ctau%29+%5C%5C%26%3D%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+p_%7B%5Ctheta%7D%28%5Ctau%29+%5Cfrac%7B%5Cnabla+p_%7B%5Ctheta%7D%28%5Ctau%29%7D%7Bp_%7B%5Ctheta%7D%28%5Ctau%29%7D+%5C%5C%26%3D+++++++++%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+p_%7B%5Ctheta%7D%28%5Ctau%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29+%5C%5C+++++++++%26%3D%5Cmathbb%7BE%7D_%7B%5Ctau+%5Csim+p_%7B%5Ctheta%7D%28%5Ctau%29%7D%5Cleft%5BR%28%5Ctau%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29%5Cright%5D+++++++++%5Cend%7Baligned%7D+%5Ctag%7B3.7%7D\" alt=\"    \\begin{aligned}         \\nabla \\bar{R}_{\\theta} \u0026amp;=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau) \\\\\u0026amp;=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)} \\\\\u0026amp;=         \\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla \\log p_{\\theta}(\\tau) \\\\         \u0026amp;=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]         \\end{aligned} \\tag{3.7}\" eeimg=\"1\"/\u003e其中，\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"\\nabla \\log p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e 的具体计算过程，可基于\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e的计算公式(2.1) ，简化为：\u003c/p\u003e\u003cp data-pid=\"WRJtsLFg\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+++%5Cbegin%7Baligned%7D+++++++++%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29+%26%3D+%5Cnabla+%5Cleft%28%5Clog+p%28s_1%29%2B%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Clog+p_%7B%5Ctheta%7D%28a_t%7Cs_t%29%2B+%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Clog+p%28s_%7Bt%2B1%7D%7Cs_t%2Ca_t%29+%5Cright%29+%5C%5C+++++++++%26%3D+%5Cnabla+%5Clog+p%28s_1%29%2B+%5Cnabla+%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Clog+p_%7B%5Ctheta%7D%28a_t%7Cs_t%29%2B++%5Cnabla+%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Clog+p%28s_%7Bt%2B1%7D%7Cs_t%2Ca_t%29+%5C%5C+++++++++%26%3D%5Cnabla+%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Clog+p_%7B%5Ctheta%7D%28a_t%7Cs_t%29%5C%5C+++++++++%26%3D%5Csum_%7Bt%3D1%7D%5E%7BT%7D+%5Cnabla%5Clog+p_%7B%5Ctheta%7D%28a_t%7Cs_t%29+++++++++%5Cend%7Baligned%7D+%5Ctag%7B3.8%7D\" alt=\"   \\begin{aligned}         \\nabla \\log p_{\\theta}(\\tau) \u0026amp;= \\nabla \\left(\\log p(s_1)+\\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)+ \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t) \\right) \\\\         \u0026amp;= \\nabla \\log p(s_1)+ \\nabla \\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)+  \\nabla \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t) \\\\         \u0026amp;=\\nabla \\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)\\\\         \u0026amp;=\\sum_{t=1}^{T} \\nabla\\log p_{\\theta}(a_t|s_t)         \\end{aligned} \\tag{3.8}\" eeimg=\"1\"/\u003e注意，\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%5Ctheta%28a_t%7Cs_t%29\" alt=\"p_\\theta(a_t|s_t)\" eeimg=\"1\"/\u003e 来自Actor。而\u003cimg src=\"https://www.zhihu.com/equation?tex=p%28s_1%29\" alt=\"p(s_1)\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=p%28s_%7Bt%2B1%7D%7Cs_t%2Ca_t%29\" alt=\"p(s_{t+1}|s_t,a_t)\" eeimg=\"1\"/\u003e 由环境决定，与 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e 无关。因此 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Clog+p%28s_1%29%3D0\" alt=\"\\nabla \\log p(s_1)=0\" eeimg=\"1\"/\u003e ，\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Clog+p%28s_%7Bt%2B1%7D%7Cs_t%2Ca_t%29%3D0\" alt=\"\\nabla \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t)=0\" eeimg=\"1\"/\u003e。而期望值 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7B%5Ctau+%5Csim+p_%7B%5Ctheta%7D%28%5Ctau%29%7D%5Cleft%5BR%28%5Ctau%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29%5Cright%5D\" alt=\"\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]\" eeimg=\"1\"/\u003e 将通过采样来近似计算。\u003c/p\u003e\u003cp data-pid=\"N_TVqfaB\"\u003e基于Actor，采样 \u003cimg src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/\u003e 个轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e，并计算每个轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e 的 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e和\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"\\nabla\\log p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e值，然后加权求和再平均就得到了最终的梯度值 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D\" alt=\"\\nabla \\bar{R}_{\\theta}\" eeimg=\"1\"/\u003e。其具体展开过程，如下所示:\u003c/p\u003e\u003cp data-pid=\"S4coCJVp\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+++++++++%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D++++++++++%26%3D+%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+%5Cnabla+p_%7B%5Ctheta%7D%28%5Ctau%29+++++++++%3D+%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+p_%7B%5Ctheta%7D%28%5Ctau%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29++++++++++%5C%5C%26+%3D%5Cmathbb%7BE%7D_%7B%5Ctau+%5Csim+p_%7B%5Ctheta%7D%28%5Ctau%29%7D%5Cleft%5BR%28%5Ctau%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%28%5Ctau%29%5Cright%5D++++++++++%5C%5C%26+%5Capprox+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+R%5Cleft%28%5Ctau%5E%7Bn%7D%5Cright%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28%5Ctau%5E%7Bn%7D%5Cright%29++%5C%5C%26%3D%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+R%5Cleft%28%5Ctau%5E%7Bn%7D%5Cright%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+++++++++%5Cend%7Baligned%7D+%5Ctag%7B3.9%7D\" alt=\"\\begin{aligned}         \\nabla \\bar{R}_{\\theta}          \u0026amp;= \\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau)         = \\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla \\log p_{\\theta}(\\tau)          \\\\\u0026amp; =\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]          \\\\\u0026amp; \\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right)  \\\\\u0026amp;=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)         \\end{aligned} \\tag{3.9}\" eeimg=\"1\"/\u003e可以看到，式子(3.9)就是对期望累计奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D_%7B%5Ctheta%7D%3D%5Csum_%7B%5Ctau%7D+R%28%5Ctau%29+p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"\\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e的 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e做了一个巧妙的求对数梯度 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla%5Clog\" alt=\"\\nabla\\log\" eeimg=\"1\"/\u003e的操作（从策略梯度期望的求和形式， 凑了一个轨迹概率，变回了期望，再做了一次采样近似，而刚好 \u003cimg src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28%5Ctau%29\" alt=\"p_{\\theta}(\\tau)\" eeimg=\"1\"/\u003e是累乘和对数天作之合） 。有：\u003c/p\u003e\u003cp data-pid=\"WE6Q-mND\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D+%5Capprox+%5Chat%7B%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D%7D+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+R%5Cleft%28%5Ctau%5E%7Bn%7D%5Cright%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%7C+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5Ctag%7B3.10%7D%5C%5C\" alt=\"    \\nabla \\bar{R}_{\\theta} \\approx \\hat{\\nabla \\bar{R}_{\\theta}} = \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right) \\tag{3.10}\\\\\" eeimg=\"1\"/\u003e有了梯度后，怎么优化策略呢？可以沿着估计出的梯度 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7B%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D%7D\" alt=\"\\hat{\\nabla \\bar{R}_{\\theta}}\" eeimg=\"1\"/\u003e 的正方向以 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/\u003e的步长(学习率)，从局部最大化目标函数的角度更新参数 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e，即梯度上升法（Gradient Ascend）:\u003c/p\u003e\u003cp data-pid=\"xUpHjMSM\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Ctheta+%5Cleftarrow+%5Ctheta+%2B+%5Ceta+%5Chat%7B%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D%7D++%5Ctag%7B3.11%7D%5C%5C\" alt=\"    \\theta \\leftarrow \\theta + \\eta \\hat{\\nabla \\bar{R}_{\\theta}}  \\tag{3.11}\\\\\" eeimg=\"1\"/\u003e当然，也可用 Adam等方法来基于梯度的统计量（梯度的移动指数平均EMA获取动量， 梯度的二阶原点矩来自适应调整学习率），获取每次优化的参数更新。\u003c/p\u003e\u003cp data-pid=\"O4sHD_Mn\"\u003e实际中，通常是每次采样若干轨迹，所以是mini-batch的梯度上升。这也会导致估计的梯度存在高方差的问题。\u003c/p\u003e\u003cblockquote data-pid=\"jHwRwjIy\"\u003e策略梯度算法作为一种在线（Online）的同策略（On-Policy）强化算法，天然存在方差过大，导致训练难收敛的问题\u003c/blockquote\u003e\u003cp data-pid=\"TZj2tLXB\"\u003e\u003cbr/\u003e 图3.6 策略梯度计算过程\u003c/p\u003e\u003cp data-pid=\"-1NW8FC4\"\u003e理论上，为了计算策略梯度的估计值\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7B%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D%7D\" alt=\"\\hat{\\nabla \\bar{R}_{\\theta}}\" eeimg=\"1\"/\u003e，需用参数为 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e的Actor与环境交互，来获取大量的轨迹数据\u003cimg src=\"https://www.zhihu.com/equation?tex=%5C%7B+%5Ctau%5E%7B1%7D%2C+...%2C+%5Ctau%5E%7BN%7D+%5C%7D\" alt=\"\\{ \\tau^{1}, ..., \\tau^{N} \\}\" eeimg=\"1\"/\u003e ，从而收集很多\u003cimg src=\"https://www.zhihu.com/equation?tex=%28s_t%2Ca_t%29\" alt=\"(s_t,a_t)\" eeimg=\"1\"/\u003e及其奖励值\u003cimg src=\"https://www.zhihu.com/equation?tex=r_t\" alt=\"r_t\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cp data-pid=\"4NqaSwsr\"\u003e然后，把采样到的数据代入式子(3.10)里，即计算在状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e下采取动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e的对数概率\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%7C+s_%7Bt%7D%5E%7Bn%7D%5Cright%29\" alt=\"\\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\" eeimg=\"1\"/\u003e，对这个概率求梯度\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%7C+s_%7Bt%7D%5E%7Bn%7D%5Cright%29\" alt=\"\\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\" eeimg=\"1\"/\u003e，并在梯度前面乘该轨迹的奖励值作为权重。在估计出梯度\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7B%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D%7D\" alt=\"\\hat{\\nabla \\bar{R}_{\\theta}}\" eeimg=\"1\"/\u003e后，就可以更新模型参数。\u003c/p\u003e\u003cp data-pid=\"UZSzGYlI\"\u003e当更新完模型参数之后，要重新采样数据，才能进行下一次模型参数更新。很直观的，因为梯度估计使用的轨迹数据，是用当前模型参数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e采样的。\u003c/p\u003e\u003cblockquote data-pid=\"zS-7OhK1\"\u003e注意， \u003cb\u003e策略梯度（policy gradient）\u003c/b\u003e 采样的数据只用一次。因为其梯度的估计只能用同一个策略采样的数据，不然会带来梯度估计的误差。所以它是on-policy算法，数据利用效率低下。\u003c/blockquote\u003e\u003cp data-pid=\"H-8IuAOL\"\u003e但实际上会怎么实现呢？\u003c/p\u003e\u003cp data-pid=\"DBoKUQB6\"\u003e首先，\u003cimg src=\"https://www.zhihu.com/equation?tex=p_%5Ctheta%28a_t%7Cs_t%29\" alt=\"p_\\theta(a_t|s_t)\" eeimg=\"1\"/\u003e是在给定状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e选取动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e的概率，而策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/\u003e本身就是在做一个根据状态预测动作空间上的概率分布的一个分类任务。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-359fc8e5aa943718ba29cfbc943a9d16_1440w.png\" data-size=\"normal\" data-original-token=\"v2-359fc8e5aa943718ba29cfbc943a9d16\" class=\"content_image\"/\u003e\u003cfigcaption\u003e 图3.7 策略梯度实现细节\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"rjJwuEA4\"\u003e在解决分类问题时，需要收集样本数据，Pytorch实现对数似然的目标函数（最小化交叉熵cross entropy损失函数,即负对数似然）的前向计算，用然后再backward和opitimizer.step()来更新参数，这样训练就能逐步最大化该目标函数：\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29\" alt=\"\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\" eeimg=\"1\"/\u003e，梯度项 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29\" alt=\"\\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\" eeimg=\"1\"/\u003e的计算，通过Pytorch反向传播自动实现。\u003c/p\u003e\u003cp data-pid=\"2wd0EvDD\"\u003e而强化学习是通过采样来获得轨迹数据，其中状态当作分类器的输入，动作作为ground truth，奖励将会用于计算\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%5En%29\" alt=\"R(\\tau^n)\" eeimg=\"1\"/\u003e作为似然项的加权。\u003c/p\u003e\u003cp data-pid=\"6lYvyCvF\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=+++++%5Chat%7BJ%28%5Ctheta%29+%7D%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D+R%5Cleft%28%5Ctau%5E%7Bn%7D%5Cright%29++%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5Ctag%7B3.12%7D%5C%5C\" alt=\"     \\hat{J(\\theta) }= \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right)  \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right) \\tag{3.12}\\\\\" eeimg=\"1\"/\u003e可以看到，目标函数(3.12)的\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Chat%7BJ%28%5Ctheta%29%7D\" alt=\"\\hat{J(\\theta)}\" eeimg=\"1\"/\u003e基本等价于损失函数式子(2.15)\u003cimg src=\"https://www.zhihu.com/equation?tex=L%28%5Ctheta%29\" alt=\"L(\\theta)\" eeimg=\"1\"/\u003e取负 （具体的加权项的实现不同而已）。相比第二节所述的直接调整策略网络的目标入手，本节推导的是从目标函数入手，但两者殊途同归(最大化似然等于最小化交叉损失)。\u003c/p\u003e\u003cp data-pid=\"UvFzVRia\"\u003e那么在训练时，通过优化算法来优化Actor网络的参数，以调整策略，就能使得Actor在采样到的轨迹数据上的累计奖励能尽量得大。具体的训练步骤，即：\u003cb\u003e3.1小节的策略梯度的流程\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"d3yAtcVW\"\u003e这里可以看到，采样的训练数据的质量以及多样性，对RL训练是决定性。\u003c/p\u003e\u003cblockquote data-pid=\"uisymUui\"\u003e甚至可以说，RL训练就是很看运气 。如果采样的样本都是很相似，要么都好，要么都坏，或者奖励信号很稀疏，策略很难得到优化。而且初始策略也很关键，因为一直调低坏动作，不见得就能把策略优化到好的动作。\u003c/blockquote\u003e\u003ch3\u003e3.5 策略梯度实现技巧\u003c/h3\u003e\u003cp data-pid=\"MEv7504s\"\u003e下面我们介绍一些在实现策略梯度时可以使用的技巧。\u003c/p\u003e\u003cblockquote data-pid=\"FA5ExmRc\"\u003e这里其实和2.2小节策略网络的不同优化选项的内容基本一致，所以只简要介绍下。\u003c/blockquote\u003e\u003cp data-pid=\"9bcyodbW\"\u003e\u003cb\u003e第一个技巧：添加基线（baseline）\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"uMg8WCuI\"\u003e从式子(3.11）可知，目前还是用整个轨迹的累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e对轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e的所有状态动作对的交叉熵损失进行加权。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"-TfxURIN\"\u003e如果轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e的累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e是负的，就要 降低轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e的所有 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28s%2Ca%29\" alt=\"(s,a)\" eeimg=\"1\"/\u003e 中采取\u003cimg src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/\u003e 动作的概率。\u003c/li\u003e\u003cli data-pid=\"FaUc8Lpg\"\u003e相反，如果轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e的累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e是正的，就要 增加轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e的所有 \u003cimg src=\"https://www.zhihu.com/equation?tex=%28s%2Ca%29\" alt=\"(s,a)\" eeimg=\"1\"/\u003e 中采取\u003cimg src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/\u003e 动作的概率。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"h3h-pc5S\"\u003e实际上，在很多场景\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e总是正的，比如一场足球比赛的最终得分。那么假设我们直接使用式子(3.11)，相当于在训练的时告诉策略网络，不管是什么动作，都应该提升它的概率。\u003c/p\u003e\u003cp data-pid=\"e32L25Wg\"\u003e然而，尽管\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e总是正的，但它的值是有大有小的，也就是说提升的程度是不一样的。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-437bf3f042fa9f1eb0d3f170d38122c7_1440w.png\" data-size=\"normal\" data-original-token=\"v2-437bf3f042fa9f1eb0d3f170d38122c7\" class=\"content_image\"/\u003e\u003cfigcaption\u003e 图3.8 策略梯度的Baseline变体\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"MjnwG7E2\"\u003e如上图所示，假设在某一个状态有a、b和c 3 个动作可选。实际上可能出现如下两种情况：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"2iu8pL2H\"\u003e理想的情况，3个动作都在采样到的轨迹中，这3个动作的概率概率都提高\u003c/li\u003e\u003cli data-pid=\"pRYYqjAi\"\u003e实际采样时，可能只采样到了部分动作，例如b、c（没采样到a，推广的情况是一些动作从未被采样到），那么这2个动作的概率会提高。尽管，a不一定是坏的动作，只是没有被采样到，但由于b和c的概率卷了起来，a的概率反而降下去了，导致了一定程度错误的策略优化；\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"fXovrUmi\"\u003e​要怎么解决这个问题呢？可以把累计奖励减其期望\u003cimg src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/\u003e，从而进行中心化，让 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29-b\" alt=\"R(\\tau)-b\" eeimg=\"1\"/\u003e 这一项有正有负（但别来0奖励），即\u003c/p\u003e\u003cp data-pid=\"jfwI5jB0\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D+%5Capprox+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D%5Cleft%28R%5Cleft%28%5Ctau%5E%7Bn%7D%5Cright%29-b%5Cright%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5Ctag%7B3.14%7D%5C%5C\" alt=\"    \\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(R\\left(\\tau^{n}\\right)-b\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right) \\tag{3.14}\\\\\" eeimg=\"1\"/\u003e其中，\u003cimg src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/\u003e 称为基线。\u003cimg src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/\u003e怎么设置呢？训练时，不断地把 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e 记录下来，更新 \u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e 的均值，作为累计奖励期望的近似，即\u003cimg src=\"https://www.zhihu.com/equation?tex=E%5BR%28%5Ctau%29%5D+%5Capprox+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+R%28%5Ctau%5E%7Bn%7D%29+%3D+b\" alt=\"E[R(\\tau)] \\approx \\frac{1}{N} \\sum_{n=1}^{N} R(\\tau^{n}) = b\" eeimg=\"1\"/\u003e（当前还是轨迹级别的奖励）。\u003c/p\u003e\u003cblockquote data-pid=\"EKRIfR-L\"\u003e如果这个优化一些，就是一个状态即prompt，生成一组response，用局部的奖励均值估计一个基线。就是GRPO的思路。\u003c/blockquote\u003e\u003cp data-pid=\"k1FTY9Jb\"\u003e\u003cb\u003e第二个技巧：分配合适的回报\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"325aQAYO\"\u003e在3.1节中，一个轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau%5E%7Bn%7D\" alt=\"\\tau^{n}\" eeimg=\"1\"/\u003e 中所有的状态-动作都使用同样的累计奖励进行加权。实际上，轨迹\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau%5E%7Bn%7D\" alt=\"\\tau^{n}\" eeimg=\"1\"/\u003e 中不同时间步采取的动作可能有不同的累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t+%3D+%5Csum_%7Bt%27%3Dt%7D%5E%7BT_%7Bn%7D%7D\" alt=\"G_t = \\sum_{t\u0026#39;=t}^{T_{n}}\" eeimg=\"1\"/\u003e，即：未来收益+即时奖励。\u003c/p\u003e\u003cblockquote data-pid=\"NbvC5_xW\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t+%3D+%5Csum_%7Bt%27%3Dt%7D%5E%7BT_%7Bn%7D%7D+r_%7Bt%27%7D\" alt=\"G_t = \\sum_{t\u0026#39;=t}^{T_{n}} r_{t\u0026#39;}\" eeimg=\"1\"/\u003e和\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t+%3D+%5Csum_%7Bk%3D0%7D%5E%7BT_%7Bn%7D%7D+r_%7Bt%2Bk%7D\" alt=\"G_t = \\sum_{k=0}^{T_{n}} r_{t+k}\" eeimg=\"1\"/\u003e\u003c/blockquote\u003e\u003cp data-pid=\"iwtFftC2\"\u003e而且可能存在：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"__RiJv7I\"\u003e整个轨迹是好的✅，但是某个时间步采取的动作不好❌；\u003c/li\u003e\u003cli data-pid=\"TuPJTY53\"\u003e整个轨迹是不好的❌，但是某个时间步采取的动作还不错✅；\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"iTuTB-re\"\u003e所以，给每一个动作分配合适的回报值，是很重要的。\u003c/p\u003e\u003cblockquote data-pid=\"SZ1jb5fY\"\u003e\u003cb\u003e时间维度稀疏的奖励稀疏\u003c/b\u003e：很多时间步拿不到奖励，只有最后有奖励。容易导致奖励Hacking。 \u003cb\u003e状态稀疏或奖励尺度稀疏的奖励稀疏\u003c/b\u003e：很多情况又拿到一样的奖励，比奖励稀疏好一些，但动作得不到细粒度的反馈， 可能会降低训练的效果（好回答少，或者奖励模型区分性差）。\u003c/blockquote\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-7bd9e1119928f3410e346707c7830326_1440w.png\" data-size=\"normal\" data-original-token=\"v2-7bd9e1119928f3410e346707c7830326\" class=\"content_image\"/\u003e\u003cfigcaption\u003e 图3.9 策略梯度分配合适的奖励\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"-u1PZFIg\"\u003e那一个轨迹里面的坏动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e，就应该用其累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e加权，而不是整个轨迹的回报\u003cimg src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cp data-pid=\"VCg90TYO\"\u003e推广的话，每个动作应该使用其合理的累计奖励即\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e进行加权，从而正确地调整其策略。\u003c/p\u003e\u003cblockquote data-pid=\"HC3Uvh0r\"\u003e累计奖即\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e能真实地反映动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e的好坏。（因为在执行动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e之前发生的事与动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e无关，所以在执行动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e之前得到的奖励并不能算动作\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e的贡献）\u003c/blockquote\u003e\u003cp data-pid=\"fZzrPW23\"\u003e分配合适的累计奖励：\u003c/p\u003e\u003cp data-pid=\"IyeqcDQh\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D+%5Capprox+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D%5Cleft%28%5Csum_%7Bt%5E%7B%5Cprime%7D%3Dt%7D%5E%7BT_%7Bn%7D%7D+r_%7Bt%5E%7B%5Cprime%7D%7D%5E%7Bn%7D-b%28s_t%29%5Cright%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5Ctag%7B3.15%7D%5C%5C\" alt=\"    \\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(\\sum_{t^{\\prime}=t}^{T_{n}} r_{t^{\\prime}}^{n}-b(s_t)\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right) \\tag{3.15}\\\\\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cp data-pid=\"pGN7Sf4I\"\u003e接下来更进一步，把累计奖励加上折扣因子\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/\u003e，即\u003c/p\u003e\u003cp data-pid=\"xAL4JGES\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=++++%5Cnabla+%5Cbar%7BR%7D_%7B%5Ctheta%7D+%5Capprox+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+%5Csum_%7Bt%3D1%7D%5E%7BT_%7Bn%7D%7D%5Cleft%28%5Csum_%7Bt%5E%7B%5Cprime%7D%3Dt%7D%5E%7BT_%7Bn%7D%7D+%5Cgamma%5E%7Bt%5E%7B%5Cprime%7D-t%7D+r_%7Bt%5E%7B%5Cprime%7D%7D%5E%7Bn%7D-b%28s_t%29%5Cright%29+%5Cnabla+%5Clog+p_%7B%5Ctheta%7D%5Cleft%28a_%7Bt%7D%5E%7Bn%7D+%5Cmid+s_%7Bt%7D%5E%7Bn%7D%5Cright%29+%5Ctag%7B3.16%7D%5C%5C\" alt=\"    \\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(\\sum_{t^{\\prime}=t}^{T_{n}} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}^{n}-b(s_t)\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right) \\tag{3.16}\\\\\" eeimg=\"1\"/\u003e可以发现，式子(3.16)和式子(2.5)是一样的了。其中，\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csum_%7Bt%5E%7B%5Cprime%7D%3Dt%7D%5E%7BT_%7Bn%7D%7D+%5Cgamma%5E%7Bt%5E%7B%5Cprime%7D-t%7D+r_%7Bt%5E%7B%5Cprime%7D%7D%5E%7Bn%7D\" alt=\"\\sum_{t^{\\prime}=t}^{T_{n}} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}^{n}\" eeimg=\"1\"/\u003e就是之前提到的累计折扣奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e。而基线\u003cimg src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/\u003e的将依赖于状态的更细粒度计算，即变成\u003cimg src=\"https://www.zhihu.com/equation?tex=b%28s_t%29\" alt=\"b(s_t)\" eeimg=\"1\"/\u003e：\u003c/p\u003e\u003cblockquote data-pid=\"GvnIwIl0\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=b%28s%29\" alt=\"b(s)\" eeimg=\"1\"/\u003e在这里实际就是策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/\u003e的状态价值函数\u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7B%5Cpi%7D%28s%29\" alt=\"V_{\\pi}(s)\" eeimg=\"1\"/\u003e（即状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e的期望累计奖励）的估计值。具体的计算方式：\u003c/blockquote\u003e\u003cul\u003e\u003cli data-pid=\"G4b_8A8G\"\u003e蒙特卡洛估计（MC）：在轨迹维度统计估计\u003cimg src=\"https://www.zhihu.com/equation?tex=b%28s%29\" alt=\"b(s)\" eeimg=\"1\"/\u003e，即采样到的轨迹 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e 中所有状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e为\u003cimg src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/\u003e的\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e的均值，\u003cimg src=\"https://www.zhihu.com/equation?tex=E%5BG_t%7Cs_t%3Ds%5D+%5Capprox+%5Cfrac%7B1%7D%7BN_%7Bs%7D%7D+%5Csum+G_t+%7C+s_t+%3D+s\" alt=\"E[G_t|s_t=s] \\approx \\frac{1}{N_{s}} \\sum G_t | s_t = s\" eeimg=\"1\"/\u003e。具体可参考《动手学强化学习》第3章3.5节的介绍和代码实现 [\u003ca href=\"https://link.zhihu.com/?target=https%3A//hrl.boyuai.com/chapter/1/%25E9%25A9%25AC%25E5%25B0%2594%25E5%258F%25AF%25E5%25A4%25AB%25E5%2586%25B3%25E7%25AD%2596%25E8%25BF%2587%25E7%25A8%258B\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003ehrl.boyuai.com/chapter/\u003c/span\u003e\u003cspan class=\"invisible\"\u003e1/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e]\u003c/li\u003e\u003cli data-pid=\"XXP0d1Ub\"\u003e基线用一个Critic价值网络拟合：\u003cimg src=\"https://www.zhihu.com/equation?tex=b%28s_t%29\" alt=\"b(s_t)\" eeimg=\"1\"/\u003e是Critic Model的输出\u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7B%5Cpi_%7B%5Ctheta%7D%7D%28s%29\" alt=\"V_{\\pi_{\\theta}}(s)\" eeimg=\"1\"/\u003e。而\u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7B%5Cpi_%7B%5Ctheta%7D%7D%28s%29\" alt=\"V_{\\pi_{\\theta}}(s)\" eeimg=\"1\"/\u003e的训练损失就是回归的MSE，样本构造可用时序差分以及蒙特卡洛估计这2种方式，训练目标是回归到均值上。这就是ActorCritic的架构了。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"7Fk1NQ8o\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t+-b%28s_t%29\" alt=\"G_t -b(s_t)\" eeimg=\"1\"/\u003e这一项即优势\u003cimg src=\"https://www.zhihu.com/equation?tex=A_t\" alt=\"A_t\" eeimg=\"1\"/\u003e，是 \u003cb\u003e优势函数（advantage function）\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Ctheta%7D%28s_t%2Ca_t%29\" alt=\"A^{\\theta}(s_t,a_t)\" eeimg=\"1\"/\u003e 的输出。\u003c/p\u003e\u003cp data-pid=\"T3LsXTEj\"\u003e直观理解，优势\u003cimg src=\"https://www.zhihu.com/equation?tex=A_t\" alt=\"A_t\" eeimg=\"1\"/\u003e的意义是在状态\u003cimg src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/\u003e 执行动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e，相较于其他可能的动作（因为\u003cimg src=\"https://www.zhihu.com/equation?tex=b%28s_t%29\" alt=\"b(s_t)\" eeimg=\"1\"/\u003e代表的就是采取所有可能动作的平均回报），\u003cimg src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/\u003e相对来说有多好，即\u003cb\u003e相对优势（relative advantage）\u003c/b\u003e。\u003c/p\u003e\u003ch2\u003e四、REINFORCE算法\u003c/h2\u003e\u003cp data-pid=\"u10Id56E\"\u003e接下来我们讲一些代码级别实现细节，即经典的策略梯度的REINFORCE算法（蒙特卡洛策略梯度）的在倒立摆场景的实现细节。\u003c/p\u003e\u003cp data-pid=\"HAlqGKpz\"\u003eREINFORCE是策略梯度的最基础版本，直接计算累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e，来估计策略参数的梯度。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-494670247e054b1c495873033aa47473_1440w.png\" data-size=\"normal\" data-original-token=\"v2-494670247e054b1c495873033aa47473\" class=\"content_image\"/\u003e\u003cfigcaption\u003e 图4.1 REINFORCE算法的流程\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Pkg9OUjx\"\u003e可以看到REINFORCE算法的流程很简洁。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-676e978bc10b6bf054526ea4c0cfe022_1440w.png\" data-size=\"normal\" data-original-token=\"v2-676e978bc10b6bf054526ea4c0cfe022\" class=\"content_image\"/\u003e\u003cfigcaption\u003e 图4.2 REINFORCE算法的示例\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"tg6UFrHV\"\u003e给定一个需要训练的策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/\u003e，其参数为\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"PUnE-DZL\"\u003e\u003cb\u003eSample阶段\u003c/b\u003e: 基于当前策略\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/\u003e，采样一批轨迹数据\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BT%7D%3D%5C%7B%5Ctau%5E%7B1%7D%2C+%5Ctau%5E%7B2%7D%EF%BC%8C+...%2C+%5Ctau%5E%7BB%7D+%5C%7D\" alt=\"\\mathbb{T}=\\{\\tau^{1}, \\tau^{2}， ..., \\tau^{B} \\}\" eeimg=\"1\"/\u003e, 其中\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau%5E%7Bb%7D%3D%28s_1%2Ca_1%2Cr_1%2C...%2Cs_T%2Ca_T%2Cr_T%29\" alt=\"\\tau^{b}=(s_1,a_1,r_1,...,s_T,a_T,r_T)\" eeimg=\"1\"/\u003e（1步训练，可以采样1个轨迹, 也可以多个轨迹）\u003c/li\u003e\u003cli data-pid=\"fAST29V9\"\u003e\u003cb\u003e计算累计奖励\u003c/b\u003e: 基于此递推公式\u003cimg src=\"https://www.zhihu.com/equation?tex=G_%7Bt%7D%3D+r_%7Bt%7D%2B%5Cgamma+G_%7Bt%2B1%7D\" alt=\"G_{t}= r_{t}+\\gamma G_{t+1}\" eeimg=\"1\"/\u003e，从后往前计算，获得轨迹 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/\u003e的每个时间步的累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=%5C%7BG_T%2C+G_%7BT-1%7D%2C+++++++...%2C+G_%7B1%7D%5C%7D\" alt=\"\\{G_T, G_{T-1},       ..., G_{1}\\}\" eeimg=\"1\"/\u003e\u003c/li\u003e\u003cli data-pid=\"jkb22oqY\"\u003e计算每个状态动作对\u003cimg src=\"https://www.zhihu.com/equation?tex=%28s_t%2C+a_t%29\" alt=\"(s_t, a_t)\" eeimg=\"1\"/\u003e的对数概率\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Clog+p_%7B%5Ctheta%7D%28a_t%7Cs_t%29\" alt=\"\\log p_{\\theta}(a_t|s_t)\" eeimg=\"1\"/\u003e，并用累计奖励\u003cimg src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/\u003e加权\u003c/li\u003e\u003cli data-pid=\"JJ2-Uwhw\"\u003e求和轨迹的所有时间步的加权对数概率\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Csum_%7Bt%3D1%7D%5E%7BT%7DG_t+%5Clog+p_%7B%5Ctheta%7D%28a_t%7Cs_t%29\" alt=\"\\sum_{t=1}^{T}G_t \\log p_{\\theta}(a_t|s_t)\" eeimg=\"1\"/\u003e，完成前向传播\u003c/li\u003e\u003cli data-pid=\"dqFOo8n0\"\u003e反向传播更新梯度，基于优化算法更新的参数\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"FoaMYUyY\"\u003e只要，基于Pytorch定义好前向传播过程，就可通过自动反向传播计算出策略梯度的估计，并基于优化器自动更新参数。\u003c/p\u003e\u003cp data-pid=\"aSMgOwR6\"\u003e接下来看下代码实现：\u003c/p\u003e\u003cblockquote data-pid=\"PfT5AFKk\"\u003e这里使用的是《动手学强化学习》的REINFORCE代码实现[ \u003ca href=\"https://link.zhihu.com/?target=https%3A//hrl.boyuai.com/chapter/2/%25E7%25AD%2596%25E7%2595%25A5%25E6%25A2%25AF%25E5%25BA%25A6%25E7%25AE%2597%25E6%25B3%2595/%2394-reinforce-%25E4%25BB%25A3%25E7%25A0%2581%25E5%25AE%259E%25E8%25B7%25B5\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e策略梯度算法\u003c/a\u003e]\u003c/blockquote\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003e## 版本依赖\n# !pip install torch==2.6.0+cu124\n# !pip install gym==0.22.0\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n\nclass PolicyNet(nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        \u0026#34;\u0026#34;\u0026#34;使用SwiGLU\u0026#34;\u0026#34;\u0026#34;\n        super().__init__()\n        self.linear_in = nn.Linear(state_dim, hidden_dim, bias=False)\n        # 门控线性层\n        self.linear_gate = nn.Linear(state_dim, hidden_dim, bias=False)\n        # 第三个线性层，从隐藏层到输出层\n        self.linear_out = nn.Linear(hidden_dim, action_dim, bias=False)\n\n    def forward(self, x):\n        # 计算使用SiLU激活的门控输出\n        gate_output = F.silu(self.linear_gate(x))\n        # 计算输入线性变化 乘以 门控输出\n        hidden_output = self.linear_in(x) * gate_output\n        logits = self.linear_out(hidden_output)\n        probabilities = F.softmax(logits, dim=1)\n        return probabilities # 输出策略的概率分布\n\nclass REINFORCE:\n    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,\n                 device):\n        self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n        # 使用AdamW优化器\n        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(),\n                                           lr=learning_rate\n        )  \n        self.gamma = gamma  # 折扣因子\n        self.device = device\n\n    def take_action(self, state):  \n        # 根据策略网络输出的动作概率分布(类别分布)采样动作\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        probs = self.policy_net(state)\n        action_dist = torch.distributions.Categorical(probs)\n        action = action_dist.sample()\n        return action.item()\n\n    def update(self, transition_dict):\n        #  transition_dict记录了MDP的轨迹数据（状态、动作、奖励）\n        state_list = transition_dict[\u0026#39;states\u0026#39;]\n        action_list = transition_dict[\u0026#39;actions\u0026#39;]\n        reward_list = transition_dict[\u0026#39;rewards\u0026#39;]\n\n        # 计算单个轨迹的各时间步的累计折扣奖励\n        G = 0\n        self.optimizer.zero_grad()\n        for i in reversed(range(len(reward_list))):  \n            # 从最后一步算起， 每一步更新一次\n            reward = reward_list[i]\n            state = torch.tensor([state_list[i]],\n                                 dtype=torch.float).to(self.device)\n            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)\n            log_prob = torch.log(self.policy_net(state).gather(1, action))\n            G = self.gamma * G + reward # 反向计算累计折扣奖励 \n            loss = -log_prob * G        # 每一步的损失函数：G_t加权的交叉熵损失\n            loss.backward()             # 反向传播计算梯度\n        \n        # 优化器会使用从轨迹中所有时间步累积的梯度来更新策略网络\n        self.optimizer.step()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cblockquote data-pid=\"xp4NsUx2\"\u003e模型代码（这里我把2层FFN替换成了SwishGLU, 即SiLU激活函数+门控线性单元层GLU）。\u003c/blockquote\u003e\u003cp data-pid=\"DbQO1xxX\"\u003e然后，是基于梯度下降的优化算法来优化策略网络的，实际就是把累计奖励加权的对数似然取负，变成了加权交叉熵损失，训练目标就变成了最小化损失。\u003c/p\u003e\u003cp data-pid=\"u72z4T6Q\"\u003e值得注意的是，梯度计算和优化和之前的常规REINFORCE算法流程略有不同：\u003c/p\u003e\u003cblockquote data-pid=\"AN-GLiw0\"\u003e通常来说，REINFORCE 算法是在一个完整的轨迹结束后，计算整个轨迹的累积奖励，然后根据这个累积奖励计算所有时间步的损失，求和或平均后，一次大的反向传播来更新策略网络。\u003c/blockquote\u003e\u003cp data-pid=\"pI2Ljwd-\"\u003e而这里，在每一步的时间步上，都计算了当前时间步的损失并执行反向传播，然后在整个轨迹结束后，调用 self.optimizer.step() 来统一做一次优化器更新。\u003c/p\u003e\u003cp data-pid=\"C3E36EJO\"\u003eupdate方法反向遍历reward_list列表，并在每个时间步计算损失并执行反向传播。实际上, 计算每个时间步的损失loss=- G_t * log_prob, 然后调用loss.backward() 执行对应时间步的反向传播 （相当于分开计算计每一步的策略网络参数关于损失的梯度，而这些梯度会累加到在每个参数的.grad属性中）\u003c/p\u003e\u003cp data-pid=\"zOHakNBs\"\u003e最后调用self.optimizer.step()，统一做一次优化器更新。此时，优化器会使用从轨迹中所有时间步累积的梯度来更新策略网络。\u003c/p\u003e\u003cp data-pid=\"AwPYQH1B\"\u003e这种方法与计算整个轨迹的总损失再进行一次反向传播和更新的效果类似。\u003cb\u003e但对于非常长的轨迹，这种方式可能更节省内存，因为每一步的反向传播执行完了后，不需要再存储当前loss的计算图的所有中间变量。\u003c/b\u003e（这其实就是梯度累计的计算流程）\u003c/p\u003e\u003cp data-pid=\"pEoIWgi2\"\u003e训练代码实现：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003elearning_rate = 1e-3\nnum_episodes = 1000\nhidden_dim = 128\ngamma = 0.98\ndevice = torch.device(\u0026#34;cuda\u0026#34;) if torch.cuda.is_available() else torch.device(\n    \u0026#34;cpu\u0026#34;)\n\nenv_name = \u0026#34;CartPole-v1\u0026#34;\nenv = gym.make(env_name)\ntorch.manual_seed(0)\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nagent = REINFORCE(state_dim, hidden_dim, action_dim, learning_rate, gamma, device)\n\nreturn_list = []\nfor i in range(10):\n    with tqdm(total=int(num_episodes / 10), desc=\u0026#39;Iteration %d\u0026#39; % i) as pbar:\n        for i_episode in range(int(num_episodes / 10)):\n            # Step1: 采样一个轨迹数据\n            episode_return = 0\n            transition_dict = {\n                \u0026#39;states\u0026#39;: [],\n                \u0026#39;actions\u0026#39;: [],\n                \u0026#39;next_states\u0026#39;: [],\n                \u0026#39;rewards\u0026#39;: [],\n                \u0026#39;dones\u0026#39;: []\n            }\n            state = env.reset()\n            done = False\n            while not done:\n                action = agent.take_action(state)\n                next_state, reward, done, _ = env.step(action)\n                transition_dict[\u0026#39;states\u0026#39;].append(state)\n                transition_dict[\u0026#39;actions\u0026#39;].append(action)\n                transition_dict[\u0026#39;next_states\u0026#39;].append(next_state)\n                transition_dict[\u0026#39;rewards\u0026#39;].append(reward)\n                transition_dict[\u0026#39;dones\u0026#39;].append(done)\n                state = next_state\n                episode_return += reward\n            return_list.append(episode_return)\n    \n            # Step2: 计算累计折扣奖励并更新策略网络\n            agent.update(transition_dict)\n            if (i_episode + 1) % 10 == 0:\n                pbar.set_postfix({\n                    \u0026#39;episode\u0026#39;:\n                    \u0026#39;%d\u0026#39; % (num_episodes / 10 * i + i_episode + 1),\n                    \u0026#39;return\u0026#39;:\n                    \u0026#39;%.3f\u0026#39; % np.mean(return_list[-10:])\n                })\n            pbar.update(1)\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"bB3c_8-q\"\u003e训练过程\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003eAgent: \u0026lt;__main__.REINFORCE object at 0x783aa0ebbe50\u0026gt;\nIteration 0: 100%|██████████| 200/200 [00:10\u0026lt;00:00, 18.53it/s, episode=200, return=24.100]\nIteration 1: 100%|██████████| 200/200 [00:09\u0026lt;00:00, 21.65it/s, episode=400, return=26.800]\nIteration 2: 100%|██████████| 200/200 [00:10\u0026lt;00:00, 18.54it/s, episode=600, return=38.700]\nIteration 3: 100%|██████████| 200/200 [00:11\u0026lt;00:00, 16.67it/s, episode=800, return=29.800]\nIteration 4: 100%|██████████| 200/200 [00:15\u0026lt;00:00, 13.18it/s, episode=1000, return=50.600]\nIteration 5: 100%|██████████| 200/200 [00:14\u0026lt;00:00, 13.62it/s, episode=1200, return=47.800]\nIteration 6: 100%|██████████| 200/200 [00:18\u0026lt;00:00, 10.94it/s, episode=1400, return=62.700]\nIteration 7: 100%|██████████| 200/200 [00:20\u0026lt;00:00,  9.68it/s, episode=1600, return=47.500]\nIteration 8: 100%|██████████| 200/200 [00:20\u0026lt;00:00,  9.83it/s, episode=1800, return=68.400]\nIteration 9: 100%|██████████| 200/200 [00:24\u0026lt;00:00,  8.07it/s, episode=2000, return=51.400]\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-c672a0276871e4f36eade0e7931715d2_1440w.png\" data-size=\"normal\" data-original-token=\"v2-c672a0276871e4f36eade0e7931715d2\" class=\"content_image\"/\u003e\u003cfigcaption\u003e 图4.3 累计奖励曲线\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"qyVMDhyP\"\u003e训练时的回报曲线。但是这个倒立摆的效果不太好：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-98240735f6d679ced6ea7c2329ab286b_1440w.gif\" data-size=\"normal\" data-thumbnail=\"https://picx.zhimg.com/v2-98240735f6d679ced6ea7c2329ab286b_b.jpg\" data-original-token=\"v2-98240735f6d679ced6ea7c2329ab286b\" class=\"content_image\"/\u003e\u003cfigcaption\u003e 图4.5 倒立摆效果\u003c/figcaption\u003e\u003c/figure\u003e\u003cblockquote data-pid=\"zqy5YkwC\"\u003e本文由 \u003ca href=\"https://zhuanlan.zhihu.com/p/1901622331102696374\" class=\"internal\" target=\"_blank\"\u003eZhihu on Obsidian\u003c/a\u003e 创作并发布\u003c/blockquote\u003e","is_labeled":false,"visited_count":108,"thumbnails":["https://pic1.zhimg.com/v2-676e978bc10b6bf054526ea4c0cfe022.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-402ba860b94a632974872ae37d85ecfd_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-6f5b445094b2337fb8e7313902631d4b_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-34e10e225a3fe3845c6f60f1e241f930_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-5ea1a122b77f27102aedc086edcb850f_720w.jpg?source=b6762063"],"favorite_count":14,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1933315494762485315}","attached_info":"CvgICLOQq8rj2evBaRAHGgkyNjA5NDY2Nzcg6sujxAYoATAAQHJKIwoYVFNfU09VUkNFX1dBUk1fVVBfQk9PU1QyEgEwGAAgADoASi8KJFRTX1NPVVJDRV9XQVJNVVBfVFdPVE9XRVJfRVhQVjJfVEVYVBIBMBgAIAA6AGIgZDMyYzdkMzc4OTUwMjk5YzA0M2M0NjVkYzVjYWMwYzZyEzE5MzMzMTU0OTQ3NjI0ODUzMTWCAV9odHRwczovL3BpYzEuemhpbWcuY29tL3YyLTY3NmU5NzhiYzEwYjZiZjA1NDUyNmVhNGMwY2ZlMDIyLmpwZz9zb3VyY2U9N2U3ZWY2ZTImbmVlZEJhY2tncm91bmQ9MaoBCXJlY29tbWVuZMIBIDNkY2NiYmEyZGYyNzZlZDRlYjMxYjBkNmJlY2E3NmQ48gEKCAwSBk5vcm1hbPIBKAgKEiRmMDFhMDVlMi0yZTMzLTQ2NjEtYTVjMS0yYTUxODZiMWZiZDLyAQYICxICMjCCAgCIAo3js86FM5ICIDNkY2NiYmEyZGYyNzZlZDRlYjMxYjBkNmJlY2E3NmQ4mgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZdoCGFRTX1NPVVJDRV9XQVJNX1VQX0JPT1NUMugCA/oCC05PUk1BTF9GTE9XigMgNjVjOTliMGZmZDBiNDY2OWEzYWRlMjIwMTk1MWRmNTSaAw0KAnYyEAAaBW90aGVyqANs2AMA6gMuY29udGVudFdhcm11cFR3b1Rvd2VyVHZwVGV4dEJvb3N0RXhwVjJSZWNhbGxlcvoD6AISDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAhCQCRjUBiIjdjItMzU5ZmM4ZTVhYTk0MzcxOGJhMjljZmJjOTQzYTlkMTY6LQgCEIEJGLYGIiN2Mi00MzdiZjNmMDQyZmE5ZjFlYjBkM2YxNzBkMzgxMjJjNzotCAQQqQgY0QUiI3YyLTdiZDllMTExOTkyOGYzNDEwZTM0NjcwN2M3ODMwMzI2Oi0IAhD7Bhj4AiIjdjItNDk0NjcwMjQ3ZTA1NGIxYzQ5NTg3MzAzM2FhNDc0NzM6LQgDENsMGNoGIiN2Mi02NzZlOTc4YmMxMGI2YmYwNTQ1MjZlYTRjMGNmZTAyMjotCAIQuwQYxwMiI3YyLWM2NzJhMDI3Njg3MWU0ZjM2ZWFkZTBlNzkzMTcxNWQyOi0IABDYBBiQAyIjdjItOTgyNDA3MzVmNmQ2NzljZWQ2ZWE3YzIzMjlhYjI4NmKABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAKDuPoQ/gQUAAAAAAAAAAIkFxH8O5xd10z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFFJAGAKAGcqgGAZICLgoJMjYwOTQ2Njc3EhMxOTMzMzE1NDk0NzYyNDg1MzE1GAciCklNQUdFX1RFWFQ=","action_card":false},{"id":"115_1753853260.67","type":"feed","offset":115,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853260,"updated_time":1753853260,"target":{"id":"1933699548313806296","type":"answer","url":"https://api.zhihu.com/answers/1933699548313806296","author":{"id":"58d3e915bde4dd1effd64e652c5b50b3","url":"https://api.zhihu.com/people/58d3e915bde4dd1effd64e652c5b50b3","user_type":"people","url_token":"91-40-47-31","name":"宋野顾丞","headline":"水瓶座\n慢慢～\n沉淀 沉淀 再沉淀","avatar_url":"https://picx.zhimg.com/50/v2-308b972b6ff27c286f3edabae6d90ccf_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":1745,"is_following":false,"is_followed":false},"created_time":1753809898,"updated_time":1753809898,"voteup_count":1,"thanks_count":0,"comment_count":1,"is_copyable":true,"question":{"id":"663585211","type":"question","url":"https://api.zhihu.com/questions/663585211","author":{"id":"ece651995c7ca23bc7d10ca852fe5d16","url":"https://api.zhihu.com/people/ece651995c7ca23bc7d10ca852fe5d16","user_type":"people","url_token":"27-93-90-41","name":"细雨","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-55a2414016c640cb4657f069b623334b_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":2,"is_following":false,"is_followed":false},"title":"能给我推荐一些书吗？","created":1722919913,"answer_count":0,"follower_count":0,"comment_count":17,"bound_topic_ids":[53,404,478],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://picx.zhimg.com/50/v2-9870237c4f0fb3696a22f29eb474840a_720w.jpg?source=b6762063","excerpt":"书名：《关于下北泽》 作者：[日]吉本芭娜娜 译者：董海涛 这是日本作家吉本芭娜娜关于下北泽的回忆之书。 在书里，她回忆了与下北泽的缘分，用平淡的语言不加修饰地讲述记忆里深藏的快乐、治愈、苦痛和悲伤的时刻。 因为想让孩子居住在有商店街的地方，吉本搬离了居住十年的上马，在这里她与房东和邻居都相处得十分愉快。在她搬离之前，楼上甚至住进了一只小猪，咚咚的脚步声令人觉得幸福。\t 含泪告别房东太太后，吉本住到了下…","excerpt_new":"书名：《关于下北泽》 作者：[日]吉本芭娜娜 译者：董海涛 这是日本作家吉本芭娜娜关于下北泽的回忆之书。 在书里，她回忆了与下北泽的缘分，用平淡的语言不加修饰地讲述记忆里深藏的快乐、治愈、苦痛和悲伤的时刻。 因为想让孩子居住在有商店街的地方，吉本搬离了居住十年的上马，在这里她与房东和邻居都相处得十分愉快。在她搬离之前，楼上甚至住进了一只小猪，咚咚的脚步声令人觉得幸福。\t 含泪告别房东太太后，吉本住到了下…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"60_Ycq95\"\u003e书名：《关于下北泽》\u003c/p\u003e\u003cp data-pid=\"W4MwKy6Y\"\u003e作者：[日]吉本芭娜娜\u003c/p\u003e\u003cp data-pid=\"yiDsuT-h\"\u003e译者：董海涛\u003c/p\u003e\u003cp data-pid=\"5CFu7BJy\"\u003e这是日本作家吉本芭娜娜关于下北泽的回忆之书。\u003c/p\u003e\u003cp data-pid=\"rEfurFl4\"\u003e在书里，她回忆了与下北泽的缘分，用平淡的语言不加修饰地讲述记忆里深藏的快乐、治愈、苦痛和悲伤的时刻。\u003c/p\u003e\u003cp data-pid=\"9aRswi7C\"\u003e因为想让孩子居住在有商店街的地方，吉本搬离了居住十年的上马，在这里她与房东和邻居都相处得十分愉快。在她搬离之前，楼上甚至住进了一只小猪，咚咚的脚步声令人觉得幸福。\t\u003c/p\u003e\u003cp data-pid=\"LzCTVbJT\"\u003e含泪告别房东太太后，吉本住到了下北泽，这个与她有着奇妙缘分的商店街。\u003c/p\u003e\u003cp data-pid=\"wjgYAFuH\"\u003e吉本回忆里的下北泽是一个自由、毫无修饰、充满活力，做什么都不会觉得奇怪的地方。走两步就能遇见艺术家，有超市、书店、酒馆、咖啡店、玩具店，还有很多好吃又便宜的生鱼片店。\u003c/p\u003e\u003cp data-pid=\"A5GseWJp\"\u003e“白天的下北泽，有很多穿着奇装异服、来路不明的人来来往往。小酒馆也从傍晚就开始热火朝天。”\u003c/p\u003e\u003cp data-pid=\"au8jhIb3\"\u003e光是看文字就令人无限向往。\t\u003c/p\u003e\u003cp data-pid=\"SU0ZGXRs\"\u003e但这样的下北泽，也在不断地经历消失。\t\u003c/p\u003e\u003cp data-pid=\"LAJe9UUF\"\u003e菜店消失了，大型超市开了起来；“二丁目三番地”玩具店不在了，换成了一家餐厅；可以吃饭、喝果汁、做按摩的PIRKATANTO书店关了门，总是帮吉本开车的店长所经营的ONE LOVE BOOKS书店也歇了业。\u003c/p\u003e\u003cp data-pid=\"IcCZIEvJ\"\u003e就像所有世间事物会经历的规律一样，下北泽也在新生与消亡中不断轮回，恰如吉本家新的生命到来，而老去的生命依次落幕。\u003c/p\u003e\u003cp data-pid=\"n2Hkw02D\"\u003e这样的规律令吉本感到悲伤的同时，也提醒着她要活在当下，珍视现在的生活，过好眼前的日子。她在很多篇文章里都流露出这样的表达。\u003c/p\u003e\u003cp data-pid=\"BmKae8UR\"\u003e也许你不熟悉下北泽，但每个人的生命时间线里都有一个“下北泽”，那个让你无比眷恋无比怀念却再也回不去的地方。\u003c/p\u003e\u003cp data-pid=\"09xYbORR\"\u003e你能做的，就是把那个“深夜窗前穿着漂亮睡衣给花草浇水的老奶奶的样子，一直留在记忆里”，然后好好地生活下去。\u003c/p\u003e\u003cp data-pid=\"bu15syjN\"\u003e这是吉本的书能带给读者感动和治愈之处。她带着读者去触摸记忆深处的幸福时刻，穿过时间的屏障，回到属于读者自己的“下北泽”，摩挲那些记忆里让人温暖的碎片，带着这些治愈的力量回到现实后，直面未来的生活。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-11506ae34a69c5db24b613aa8c7ee226_1440w.jpg\" data-rawwidth=\"1170\" data-rawheight=\"1377\" data-size=\"normal\" data-qrcode-action=\"none\" data-original-token=\"v2-b4664a917330ef72f42e8609a9817c90\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-aef30fd388157395604f454e4f627681_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1170\" data-original=\"https://pic3.zhimg.com/v2-11506ae34a69c5db24b613aa8c7ee226_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-c657170e38ea31190edc21008116dda6_1440w.jpg\" data-rawwidth=\"1170\" data-rawheight=\"1380\" data-size=\"normal\" data-original-token=\"v2-502a9e244d7cbeb7b0fb29453cf623d5\" data-default-watermark-src=\"https://pic2.zhimg.com/v2-951acde7ff43ab6285cee0b29b3ccddb_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1170\" data-original=\"https://pic1.zhimg.com/v2-c657170e38ea31190edc21008116dda6_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-1d43f6f37447d0759d9cb773b3dc747f_1440w.jpg\" data-rawwidth=\"1170\" data-rawheight=\"1382\" data-size=\"normal\" data-original-token=\"v2-5e72a2e46e4207b6dc38bd7cae0f5799\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-4016f8c181e788dcc7fe23c81609f47c_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1170\" data-original=\"https://pic4.zhimg.com/v2-1d43f6f37447d0759d9cb773b3dc747f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-7bf8d73c629b87ed974090c14748a795_1440w.jpg\" data-rawwidth=\"1170\" data-rawheight=\"1318\" data-size=\"normal\" data-original-token=\"v2-e96db95a18830fa49f70e388dec96655\" data-default-watermark-src=\"https://picx.zhimg.com/v2-82b632879508cdd380084c82e4da66cd_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1170\" data-original=\"https://pic4.zhimg.com/v2-7bf8d73c629b87ed974090c14748a795_r.jpg\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":41,"thumbnails":["https://picx.zhimg.com/50/v2-9870237c4f0fb3696a22f29eb474840a_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-aceb31c75f5d1f003627ec4915de3143_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-4a415ae11f298a86edea289df6e58fec_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-03061cf3e5ddd49da3af05d46d85e855_720w.jpg?source=b6762063"],"favorite_count":4,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1933699548313806296}","attached_info":"Co4HCLOQq8rj2evBaRAEGgk3Mzk1MjI5MTAg6oekxAYoATABQHNKIwoYVFNfU09VUkNFX1dBUk1fVVBfQk9PU1QyEgEwGAAgADoASi0KIlRTX1NPVVJDRV9XQVJNX1VQX0hJR0hfSU5URVJBQ1RJT04SATAYACAAOgBaCTEwOTc0MTA3MGIgZDMyYzdkMzc4OTUwMjk5YzA0M2M0NjVkYzVjYWMwYzZyEzE5MzM2OTk1NDgzMTM4MDYyOTaKAQk2NjM1ODUyMTGqAQlyZWNvbW1lbmTCASA1OGQzZTkxNWJkZTRkZDFlZmZkNjRlNjUyYzViNTBiM/IBCggMEgZOb3JtYWzyASgIChIkZjk3ZWRiMTktYzMxOS00MzVjLWJiODItYWIzOGU2NmQ5NzE08gEGCAsSAjIwggIAiAKN47POhTOSAiA1OGQzZTkxNWJkZTRkZDFlZmZkNjRlNjUyYzViNTBiM5oCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXaAhhUU19TT1VSQ0VfV0FSTV9VUF9CT09TVDLoAgP6AgtOT1JNQUxfRkxPV4oDIDY1Yzk5YjBmZmQwYjQ2NjlhM2FkZTIyMDE5NTFkZjU0mgMNCgJ2MhAAGgVvdGhlcqgDKdgDAOoDHnRleHRfaGlnaF9pbnRlcmFjdGlvbl9yZWNhbGxlcvoD2wESDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IBBCSCRjhCiIjdjItYjQ2NjRhOTE3MzMwZWY3MmY0MmU4NjA5YTk4MTdjOTA6LQgCEJIJGOQKIiN2Mi01MDJhOWUyNDRkN2NiZWI3YjBmYjI5NDUzY2Y2MjNkNTotCAIQkgkY5goiI3YyLTVlNzJhMmU0NmU0MjA3YjZkYzM4YmQ3Y2FlMGY1Nzk5Oi0IBBCSCRimCiIjdjItZTk2ZGI5NWExODgzMGZhNDlmNzBlMzg4ZGVjOTY2NTWABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAACDBQaA/gQUAAAAAAAAAAIkFxH8O5xd10z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFFJAGAKAGc6gGA5ICLgoJNzM5NTIyOTEwEhMxOTMzNjk5NTQ4MzEzODA2Mjk2GAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"116_1753853260.60","type":"feed","offset":116,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853260,"updated_time":1753853260,"target":{"id":"1933440672410637586","type":"article","url":"https://api.zhihu.com/articles/1933440672410637586","author":{"id":"8c4d0ef0ce31b6110e76b17471a010d7","url":"https://api.zhihu.com/people/8c4d0ef0ce31b6110e76b17471a010d7","user_type":"people","url_token":"gong-kong-zi-liao-ku","name":"工控技术分享","headline":"工控人","avatar_url":"https://picx.zhimg.com/50/v2-c85dabe1e0c2016b2025244f1d47cf54_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":48,"is_following":false,"is_followed":false},"title":"十年PLC老鸟的血泪总结：比技术更重要的，是这些思维！","image_url":"https://pic1.zhimg.com/v2-1e573a320655e09589138d9fd14d6c65.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1753748779,"updated":1753748779,"voteup_count":1,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"凌晨三点，车间冰冷的金属气味混着调试失败的焦糊味儿，屏幕上的故障码像在嘲笑我。这场景，十年PLC生涯里经历了太多次。从初出茅庐的愣头青，到如今勉强能称一声“老鸟”，烧过的模块、熬过的通宵、掉过的头发，都成了学费。 一路走来，最大的感悟是： PLC技术本身，学精了是基础，但真正决定你天花板高度、项目成败、甚至头发浓密度的，是那些“看不见”的思维模式。 今天掏心窝子聊聊那些比梯形图、ST语言更重要的东西。思维…","excerpt_new":"凌晨三点，车间冰冷的金属气味混着调试失败的焦糊味儿，屏幕上的故障码像在嘲笑我。这场景，十年PLC生涯里经历了太多次。从初出茅庐的愣头青，到如今勉强能称一声“老鸟”，烧过的模块、熬过的通宵、掉过的头发，都成了学费。 一路走来，最大的感悟是： PLC技术本身，学精了是基础，但真正决定你天花板高度、项目成败、甚至头发浓密度的，是那些“看不见”的思维模式。 今天掏心窝子聊聊那些比梯形图、ST语言更重要的东西。思维…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"-4484MqK\"\u003e凌晨三点，车间冰冷的金属气味混着调试失败的焦糊味儿，屏幕上的故障码像在嘲笑我。这场景，十年PLC生涯里经历了太多次。从初出茅庐的愣头青，到如今勉强能称一声“老鸟”，烧过的模块、熬过的通宵、掉过的头发，都成了学费。\u003c/p\u003e\u003cp data-pid=\"eY5QqC8t\"\u003e一路走来，最大的感悟是：\u003cb\u003ePLC技术本身，学精了是基础，但真正决定你天花板高度、项目成败、甚至头发浓密度的，是那些“看不见”的思维模式。\u003c/b\u003e 今天掏心窝子聊聊那些比梯形图、ST语言更重要的东西。\u003c/p\u003e\u003chr/\u003e\u003ch2\u003e思维一：系统思维 —— 别只盯着PLC那块“砖头”\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"hh2Lghlz\"\u003e\u003cb\u003e踩过的坑：\u003c/b\u003e 早年接到项目，一头扎进PLC编程，画梯形图那叫一个酣畅淋漓。结果呢？传感器选型不合理，信号干扰严重；执行机构响应慢，跟不上PLC节奏；上位机通讯协议没定好，数据传不上来……整个系统成了一锅夹生饭，PLC程序写得再漂亮也白搭。\u003c/li\u003e\u003cli data-pid=\"Oxif3mLf\"\u003e\u003cb\u003e血泪教训：\u003c/b\u003e PLC是工业自动化系统的\u003cb\u003e核心控制器\u003c/b\u003e，但绝不是\u003cb\u003e唯一\u003c/b\u003e。它需要和传感器、执行器、HMI、SCADA、MES甚至IT系统协同工作。\u003c/li\u003e\u003cli data-pid=\"guq_KR3O\"\u003e\u003cb\u003e老鸟心得：\u003c/b\u003e\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"SQv859uG\"\u003e\u003cb\u003e项目启动先画“地图”：\u003c/b\u003e 在动手写第一行代码前，强迫自己画系统架构图，理清信号流、数据流、能源流。明确每个环节的接口、协议、性能要求。\u003c/li\u003e\u003cli data-pid=\"vHmZNHgB\"\u003e\u003cb\u003e预见“边界摩擦”：\u003c/b\u003e 思考PLC程序如何与上下游（比如，机械的极限位置、仪表的精度误差、网络的延迟）优雅地“握手”和“容错”。\u003c/li\u003e\u003cli data-pid=\"IQYSbQ47\"\u003e\u003cb\u003e工具助力：\u003c/b\u003e 这时候，像\u003cb\u003e工控技术平台APP\u003c/b\u003e这类工具的价值就体现了。在项目现场遇到一个不熟悉的传感器通讯协议卡壳？与其抓瞎翻厚厚的纸质手册（如果还找得到的话），不如快速在APP里搜索型号，查协议细节、接线图、常见故障处理，甚至看看同行有没有类似案例分享。\u003cb\u003e它解决的是“现场知识断层”的燃眉之急，让你把精力聚焦在系统级的思考和整合上，而不是被某个小零件的文档绊住脚。\u003c/b\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e思维二：调试思维 —— “分而治之”与“大胆假设，小心求证”\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"hP6YNLX6\"\u003e\u003cb\u003e踩过的坑：\u003c/b\u003e 面对一整个系统的故障，像无头苍蝇一样东测一下西改一点，改了半天可能只是掩盖了真正的问题，甚至引入新BUG。或者，过于迷信经验，对异常信号视而不见，结果是小问题拖成大事故。\u003c/li\u003e\u003cli data-pid=\"uoT6pYFS\"\u003e\u003cb\u003e血泪教训：\u003c/b\u003e 调试不是碰运气，是\u003cb\u003e逻辑推理\u003c/b\u003e和\u003cb\u003e科学实验\u003c/b\u003e的结合。需要结构化方法和开放心态。\u003c/li\u003e\u003cli data-pid=\"UH_LVIzc\"\u003e\u003cb\u003e老鸟心得：\u003c/b\u003e\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"KvaT9Okg\"\u003e\u003cb\u003e模块化隔离：\u003c/b\u003e 把大系统按功能或区域拆分成独立小单元。先保证电源、IO基础测试通过，再逐级打通信号链路，最后做整体联动。\u003cb\u003e分段上电、分段调试！\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"nhWCcb7s\"\u003e\u003cb\u003e建立“故障树”：\u003c/b\u003e 从现象（比如，电机不转）出发，列出所有可能的原因（电源？信号？PLC输出？接触器？电机本身？），然后设计最有效的测试路径去逐一排除。\u003c/li\u003e\u003cli data-pid=\"XsZmYkh_\"\u003e\u003cb\u003e数据说话，不猜不蒙：\u003c/b\u003e 万用表、示波器、在线监测变量是王道。PLC的在线监控是金钥匙。\u003cb\u003e记录关键数据变化！\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"A-MrMeLP\"\u003e\u003cb\u003e工具助力：\u003c/b\u003e 遇到复杂通讯问题或诡异干扰，APP里的故障排查指南、经典案例分析，或者社区里同行的一句经验之谈（“某个型号变频器的接地要特别注意…”），往往能提供新的排查思路或验证你的假设，少走弯路。这相当于把众多老鸟的“踩坑”经验装进了口袋。\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e思维三：文档思维 —— “烂笔头”胜过最强大脑\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"xoz07ou5\"\u003e\u003cb\u003e踩过的坑：\u003c/b\u003e 项目紧急，想着“先搞定功能，文档回头补”。结果呢？“回头”就是永远。半年后设备要改造，或者别人接手维护，面对一堆没有注释的程序、消失的图纸、记不清的修改点，直接崩溃。或者，自己都忘了当初某个参数为什么设成这个值。\u003c/li\u003e\u003cli data-pid=\"QhZhBuh-\"\u003e\u003cb\u003e血泪教训：\u003c/b\u003e \u003cb\u003e好记性不如烂笔头，清晰的文档是给自己和别人最大的仁慈，是项目的“生命线”。\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"XWEm8w0c\"\u003e\u003cb\u003e老鸟心得：\u003c/b\u003e\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"SzV3sAYk\"\u003e\u003cb\u003e即时记录，拒绝拖延：\u003c/b\u003e 修改了程序？马上加注释，说明\u003cb\u003e为什么改\u003c/b\u003e（而不仅仅是改了哪里）。调整了参数？记录调整依据（测试数据或客户要求）。接线变更？立刻更新图纸（哪怕是手绘草图拍照）。\u003c/li\u003e\u003cli data-pid=\"rDhXGEjO\"\u003e\u003cb\u003e标准化模板：\u003c/b\u003e 建立自己的程序注释规范、设备清单模板、IO表模板、调试记录模板。\u003cb\u003e一致性\u003c/b\u003e是关键。\u003c/li\u003e\u003cli data-pid=\"oe9navwU\"\u003e\u003cb\u003e版本管理：\u003c/b\u003e 再小的修改也要留痕。日期+修改人+修改内容摘要，这是底线。\u003c/li\u003e\u003cli data-pid=\"Z--hALiX\"\u003e\u003cb\u003e工具助力：\u003c/b\u003e 随身携带的\u003cb\u003e工控技术平台APP\u003c/b\u003e可以成为你的移动文档库。把常用的设备手册（电子版）、标准接线图、自己总结的调试checklist、甚至关键的程序截图/注释片段存在里面。现场需要参考或记录零散信息时，比纸质笔记本更高效，且不易丢失。更重要的是，一些APP支持云端同步，意味着你在办公室整理的笔记，现场也能随时调用（当然，注意保密性要求）。\u003cb\u003e它解决的是文档的“即时性”和“可携带性”问题，让“随手记”变得更容易。\u003c/b\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e思维四：沟通思维 —— 技术人不能只和机器对话\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"5YjGHMdZ\"\u003e\u003cb\u003e踩过的坑：\u003c/b\u003e 跟机械工程师说“你那个气缸位置信号不稳”，对方回“我调好了啊，是你PLC没读到吧？”；跟操作工说“按这个按钮启动”，结果他按错了，还说“你这机器设计得不人性化”；跟客户经理拍胸脯“三天搞定”，结果遇到意外硬件故障拖了一周…\u003c/li\u003e\u003cli data-pid=\"fbIeKrsh\"\u003e\u003cb\u003e血泪教训：\u003c/b\u003e \u003cb\u003e自动化项目是团队作战。\u003c/b\u003e 清晰、有效、换位的沟通，能消除至少50%的“技术难题”和100%的背锅风险。\u003c/li\u003e\u003cli data-pid=\"W4vw8pxH\"\u003e\u003cb\u003e老鸟心得：\u003c/b\u003e\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"Kdg1_IFv\"\u003e\u003cb\u003e说人话，用场景：\u003c/b\u003e 跟非电控人员交流，避免堆砌术语。用“当XX发生时，设备会YY动作，我们需要ZZ信号来保证安全”这样的方式。\u003c/li\u003e\u003cli data-pid=\"pQfy3En2\"\u003e\u003cb\u003e明确责任边界：\u003c/b\u003e 调试前，和机械、电气、工艺明确好各自的调试范围、接口定义、联调节点。白纸黑字最好。\u003c/li\u003e\u003cli data-pid=\"R9-jrwQS\"\u003e\u003cb\u003e管理预期：\u003c/b\u003e 给领导或客户汇报进度和问题，要客观、及时。遇到困难，尽早提出并给出解决方案建议，而不是最后才说“搞不定了”。\u003c/li\u003e\u003cli data-pid=\"yznxuUrM\"\u003e\u003cb\u003e倾听操作工反馈：\u003c/b\u003e 他们是最直接的使用者，他们的抱怨往往藏着真问题（可能是设计缺陷，也可能是你的程序逻辑不够健壮）。\u003c/li\u003e\u003cli data-pid=\"fesY4Oxv\"\u003e\u003cb\u003e工具助力？沟通本身工具帮不上大忙，但\u003c/b\u003eAPP里如果有一些标准化的设备操作说明模板、安全注意事项图示，你可以快速调取修改后给操作工培训，比临时手写更规范清晰。这也算间接支持了有效沟通。\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e思维五：持续学习与开放思维 —— 技术日新月异，别当“老古董”\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"pbIx7BW6\"\u003e\u003cb\u003e踩过的坑：\u003c/b\u003e 抱着一种品牌/一种编程语言不放，对新出现的技术（如工业物联网IIoT、OPC UA、软PLC、机器视觉集成）嗤之以鼻，觉得“花架子”、“不如我的继电器实在”。结果市场需要时，自己技能脱节。\u003c/li\u003e\u003cli data-pid=\"zZ9SfAzC\"\u003e\u003cb\u003e血泪教训：\u003c/b\u003e 工控领域技术迭代越来越快。\u003cb\u003e“舒适区”就是“淘汰区”。\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"5jGsesWq\"\u003e\u003cb\u003e老鸟心得：\u003c/b\u003e\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"Ouw0I4Us\"\u003e\u003cb\u003e保持好奇心：\u003c/b\u003e 主动了解行业新趋势、新协议、新工具。不一定要精，但要懂其原理和应用场景。\u003c/li\u003e\u003cli data-pid=\"wLN_gtbn\"\u003e\u003cb\u003e拥抱“跨界”：\u003c/b\u003e IT知识（网络、数据库、信息安全）、基础的数据分析能力，越来越成为工控人的加分项甚至必备项。\u003c/li\u003e\u003cli data-pid=\"o_gnvzf1\"\u003e\u003cb\u003e善用资源：\u003c/b\u003e 技术论坛、线上课程、专业书籍、厂商培训都是途径。\u003c/li\u003e\u003cli data-pid=\"MzdeuekQ\"\u003e\u003cb\u003e工具助力：\u003c/b\u003e \u003cb\u003e工控技术平台APP\u003c/b\u003e这类工具本身就是信息聚合的入口。利用它关注行业动态、学习新技术文章、参与技术讨论（哪怕只是潜水看看），能高效地利用碎片时间保持技术敏感度。看到一篇讲OPC UA over TSN的文章，可能就为你下一个项目打开了思路。\u003cb\u003e它解决的是信息获取的“便捷性”和“时效性”问题。\u003c/b\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003chr/\u003e\u003ch2\u003e结语\u003c/h2\u003e\u003cp data-pid=\"D19hg-LC\"\u003e十年磨一剑，磨的不只是技术这把“利刃”，更是上面这些决定你能走多远、飞多高的“内功心法”。技术细节终有过时的一天，但优秀的工程思维、解决问题的逻辑、沟通协作的能力、持续学习的习惯，才是这个行业里真正的硬通货。\u003c/p\u003e\u003cp data-pid=\"deurHAYn\"\u003e那些让你抓狂的现场问题、通宵的调试、掉过的坑，最终都会沉淀成你的思维肌肉。\u003cb\u003e工控技术平台APP这样的工具，就像一位随身携带的“老法师”，在你需要时提供信息支持、思路启发，帮你更高效地实践这些宝贵的思维，把精力用在更值得的刀刃上——去思考、去设计、去解决真正有挑战的问题。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"NQQUAjyk\"\u003e记住，我们不是写代码的机器，我们是\u003cb\u003e用技术创造价值的工程师\u003c/b\u003e。共勉！\u003c/p\u003e","is_labeled":false,"visited_count":152,"thumbnails":["https://pic1.zhimg.com/v2-1e573a320655e09589138d9fd14d6c65.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-53c703b3cc10be83d98170a167e37d3a_720w.jpg?source=b6762063"],"favorite_count":10,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1933440672410637586}","attached_info":"Ct4GCLOQq8rj2evBaRAHGgkyNjA5NTAxNjAgq6qgxAYoATAAQHRKIwoYVFNfU09VUkNFX1dBUk1fVVBfQk9PU1QyEgEwGAAgADoASi8KJFRTX1NPVVJDRV9XQVJNVVBfVFdPVE9XRVJfRVhQVjJfVEVYVBIBMBgAIAA6AGIgZDMyYzdkMzc4OTUwMjk5YzA0M2M0NjVkYzVjYWMwYzZyEzE5MzM0NDA2NzI0MTA2Mzc1ODaCAV9odHRwczovL3BpYzEuemhpbWcuY29tL3YyLTFlNTczYTMyMDY1NWUwOTU4OTEzOGQ5ZmQxNGQ2YzY1LmpwZz9zb3VyY2U9N2U3ZWY2ZTImbmVlZEJhY2tncm91bmQ9MaoBCXJlY29tbWVuZMIBIDhjNGQwZWYwY2UzMWI2MTEwZTc2YjE3NDcxYTAxMGQ38gEKCAwSBk5vcm1hbPIBKAgKEiQwNjU1NmE2MS1mOTk3LTRkYzUtOWEwZC1jMzc2ZWNlZTU0NTfyAQYICxICMjCCAgCIAo3js86FM5ICIDhjNGQwZWYwY2UzMWI2MTEwZTc2YjE3NDcxYTAxMGQ3mgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZdoCGFRTX1NPVVJDRV9XQVJNX1VQX0JPT1NUMugCA/oCC05PUk1BTF9GTE9XigMgNjVjOTliMGZmZDBiNDY2OWEzYWRlMjIwMTk1MWRmNTSaAw0KAnYyEAAaBW90aGVyqAOYAdgDAOoDLmNvbnRlbnRXYXJtdXBUd29Ub3dlclR2cFRleHRCb29zdEV4cFYyUmVjYWxsZXL6A04SDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAhCACBiACCIjdjItMWU1NzNhMzIwNjU1ZTA5NTg5MTM4ZDlmZDE0ZDZjNjWABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAKDShpA/gQUAAAAAAAAAAIkFxH8O5xd10z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFFJAGAKAGdKgGAJICLgoJMjYwOTUwMTYwEhMxOTMzNDQwNjcyNDEwNjM3NTg2GAciCklNQUdFX1RFWFQ=","action_card":false},{"id":"117_1753853260.966","type":"feed","offset":117,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853260,"updated_time":1753853260,"target":{"id":"1929469460571457382","type":"article","url":"https://api.zhihu.com/articles/1929469460571457382","author":{"id":"3a900822348249be5dfae95ecd09140a","url":"https://api.zhihu.com/people/3a900822348249be5dfae95ecd09140a","user_type":"people","url_token":"lin-mx-97","name":"Juvy猪尾","headline":"以真相照心，以智慧炼心。","avatar_url":"https://picx.zhimg.com/50/v2-22877ea0f4889952f7f000c5a23b31df_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":16,"is_following":false,"is_followed":false},"title":"学习方法论","comment_permission":"all","created":1753626609,"updated":1753626609,"voteup_count":4,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"——写一些思考给不知如何学习的知友和自己，供参考。前面已经写过如何做到专心学习，学习的前提条件已经有了。 现在的问题是： 如何学习？现在回想起自己的高中时候，花大量时候去“学习”，效果却不咋地，最重要的原因就是没有掌握系统的学习方法论。 这个情况到第一次考研时，也同样出现这种情况，没有掌握系统的学习方法论，这也是导致一战没有达到目标分数的根本原因之一。另一个根本原因就是没有专心学习！ 现在想想悔恨不…","excerpt_new":"——写一些思考给不知如何学习的知友和自己，供参考。前面已经写过如何做到专心学习，学习的前提条件已经有了。 现在的问题是： 如何学习？现在回想起自己的高中时候，花大量时候去“学习”，效果却不咋地，最重要的原因就是没有掌握系统的学习方法论。 这个情况到第一次考研时，也同样出现这种情况，没有掌握系统的学习方法论，这也是导致一战没有达到目标分数的根本原因之一。另一个根本原因就是没有专心学习！ 现在想想悔恨不…","preview_type":"default","preview_text":"","column":{"id":"c_1925584530615542899","type":"column","url":"https://api.zhihu.com/columns/c_1925584530615542899","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pic1.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"猪尾的反思日记","imageUrl":"https://picx.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b","comment_permission":"private","intro":"","updated":1751875128,"is_following":false},"content":"\u003cp data-pid=\"6LG2argm\"\u003e\u003cb\u003e——写一些思考给不知如何学习的知友和自己，供参考。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"LZC5wsFw\"\u003e前面已经写过如何做到专心学习，学习的前提条件已经有了。\u003c/p\u003e\u003cp data-pid=\"ScW-Oou6\"\u003e现在的问题是：\u003cb\u003e如何学习？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"JaFpcPDR\"\u003e现在回想起自己的高中时候，花大量时候去“学习”，效果却不咋地，最重要的原因就是没有掌握系统的学习方法论。\u003c/p\u003e\u003cp data-pid=\"-YkSkAnV\"\u003e这个情况到第一次考研时，也同样出现这种情况，没有掌握系统的学习方法论，这也是导致一战没有达到目标分数的根本原因之一。另一个根本原因就是没有专心学习！\u003c/p\u003e\u003cp data-pid=\"KeXfsF1i\"\u003e现在想想悔恨不已，但悔恨没用。\u003c/p\u003e\u003cp data-pid=\"6RYwCeOW\"\u003e\u003cb\u003e真理只掌握在少数人手里！\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"97HNhcF7\"\u003e后来也去翻一翻关于如何学习的书，还是很有必要去看看的。\u003c/p\u003e\u003cp data-pid=\"ayPHFNse\"\u003e我看的有《大脑学习法》《刻意练习》《认知天性》《好好学习》《好好思考》《费曼学习法》等等，也在知乎上看了好多篇高赞回答。\u003c/p\u003e\u003cp data-pid=\"8sgOK73a\"\u003e都讲的各有侧重点。\u003c/p\u003e\u003cp data-pid=\"3a6DfI_6\"\u003e现在也基本掌握了科学的\u003cb\u003e系统思维方式\u003c/b\u003e。现在要做的就是用该思维方式去分析和解决一些现实的和历史遗留的问题，做一些思考，以指导实践。\u003c/p\u003e\u003cp data-pid=\"9OtjJYEU\"\u003e下面是我结合过去已经学习的内容，所做出的一些思考。\u003c/p\u003e\u003cp data-pid=\"2jxd2SGs\"\u003e浅浅回答\u003cb\u003e如何学习？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"COZehAz_\"\u003e\u003cb\u003e学习就是我们的目的。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"3SCb4DS5\"\u003e这里的学习指的是能够深入理解并实践运用知识or技能的学习，而不是死记硬背式的学习，虽然后者在某种程度上也有用处。\u003c/p\u003e\u003cp data-pid=\"1xoorw6k\"\u003e\u003cb\u003e那要达到此目的，需要哪些条件？\u003c/b\u003e我觉得学习需要三步走。\u003c/p\u003e\u003ch2\u003e第一步，\u003cb\u003e理解\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"V17CLCW-\"\u003e顾名思义，就是要理解所需要学习的内容。\u003c/p\u003e\u003cp data-pid=\"GgXgsUbC\"\u003e这也是\u003cb\u003e大脑里的神经元建立连接\u003c/b\u003e的过程。\u003c/p\u003e\u003cp data-pid=\"55klZzkJ\"\u003e（你的大脑大约有860亿个神经元，他们通过突触连接，形成庞大的信息处理网络）\u003c/p\u003e\u003cp data-pid=\"hcoFEDsk\"\u003e这要求，深刻理解知识点or技能，摸清它的来龙去脉，有何逻辑与规律？是什么？为什么？怎么办？如何操作？有什么用？和其他事物有什么联系等等\u003c/p\u003e\u003cp data-pid=\"jIW9SsEO\"\u003e因为咱们不是要建立几个神经元之间的连接，而是要让大脑里的神经元就像疯狂生长的树根，不断建立新的链接，让很多很多神经元连接在一起，形成一张越来越密的\u003cb\u003e神经网络。\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-e403e6d80d1825b04847bcef4a76b20b_1440w.jpg\" data-rawwidth=\"908\" data-rawheight=\"524\" data-size=\"normal\" data-original-token=\"v2-09ed5d6e3cdba39a7e53d081644bea95\" class=\"origin_image zh-lightbox-thumb\" width=\"908\" data-original=\"https://pic4.zhimg.com/v2-e403e6d80d1825b04847bcef4a76b20b_r.jpg\"/\u003e\u003cfigcaption\u003e图源：百度百科\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"a2Kflpf9\"\u003e就是以该知识点or技能为中心，不断向外蔓延和拓展相关信息。理解该知识点与其他知识点or技能or与其有联系的事物。\u003c/p\u003e\u003cp data-pid=\"FcRKN9ix\"\u003e联系、拓展与之相关的信息了解越多，神经网络就更密集，理解就更深刻。\u003c/p\u003e\u003cp data-pid=\"FUb_Heul\"\u003e这也是深度学习的理论和生物学基础，这\u003cb\u003e要求学新东西时要慢，理解要透彻。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"GgLXb45a\"\u003e这是上课时为什么说花时间去消化吸收知识点的原因\u003cb\u003e。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"LopegV3f\"\u003e\u003cb\u003e俗话说：好记性不如烂笔头。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"1fbIPlBa\"\u003e多记笔记，然后思考和理解。不要假装很努力，不动脑子去理解和思考而只知道记笔记，甚至死记硬背和背答案。\u003c/p\u003e\u003cp data-pid=\"4lR9MOVA\"\u003e\u003cb\u003e这也是为后边的记忆和运用打基础\u003c/b\u003e。\u003c/p\u003e\u003ch2\u003e第二步，\u003cb\u003e记忆\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"3r7hwtpb\"\u003e好了，假设现在已经理解了知识点or技能。\u003c/p\u003e\u003cp data-pid=\"6RVoGiLR\"\u003e理解记忆的开始，也是遗忘的开始。\u003c/p\u003e\u003cp data-pid=\"i4H1EBfR\"\u003e\u003cb\u003e学而时习之。\u003c/b\u003e如果不对知识点进行刻意的记忆和复习，那也很快就忘光光了。\u003c/p\u003e\u003cp data-pid=\"zn0bM142\"\u003e当你频繁激活和使用某条神经通路，比如反复练习乘法口诀，神经元之间连接的线路会更粗，这条路径就会变得更加高效。\u003c/p\u003e\u003cp data-pid=\"agCzgw2u\"\u003e反之，如长期不用，对应的连接就会逐渐弱化，甚至断开。\u003c/p\u003e\u003cp data-pid=\"XJ2fOFTo\"\u003e 科学家也之前也曾做过一个实验，将猴子的一根手指砍断，那么仅仅过去两个月，这根手指对应的大脑神经元区域就会萎缩了。\u003c/p\u003e\u003cp data-pid=\"iC1GKl6n\"\u003e大脑遵循的是\u003cb\u003e用进废退\u003c/b\u003e原则。\u003c/p\u003e\u003cp data-pid=\"mIgY1eCI\"\u003e根据大家应该都听过的艾斯浩宾遗忘曲线，\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-36341ab9127c53d09ed52ec67087e51d_1440w.jpg\" data-rawwidth=\"1748\" data-rawheight=\"1280\" data-size=\"normal\" data-original-token=\"v2-f2782bd2f221fc52b367b742784ff904\" class=\"origin_image zh-lightbox-thumb\" width=\"1748\" data-original=\"https://pic2.zhimg.com/v2-36341ab9127c53d09ed52ec67087e51d_r.jpg\"/\u003e\u003cfigcaption\u003e图源：百度百科\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"V6utdL6p\"\u003e第1、2、3、5、7、15、30、45天后都要回忆一遍。\u003c/p\u003e\u003cp data-pid=\"yP4bu3ZQ\"\u003e有个点要注意：稍微\u003cb\u003e让回忆有点困难\u003c/b\u003e和难度，比如回忆时间可以隔长一些，再回忆可以让记忆很牢固。\u003c/p\u003e\u003cp data-pid=\"4aX5XBXn\"\u003e这一点可以根据个人习惯来。\u003c/p\u003e\u003cp data-pid=\"PPfRseF1\"\u003e在这里，我还要讲十分重要的一点：“\u003cb\u003e好笔头也不如好记忆\u003c/b\u003e”。\u003c/p\u003e\u003cp data-pid=\"UsH72tT0\"\u003e\u003cb\u003e怎么记忆，那也是有规律的。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"3zf-WxiH\"\u003e对有序、有规律的记忆内容，大脑自然而然的会记得很牢固和轻松，激活和使用对应的神经元通路时也会又快又准。\u003c/p\u003e\u003cp data-pid=\"rdOpqMRk\"\u003e最简单，让你记得我叫12345678，估计你一个月之后都记得。\u003c/p\u003e\u003cp data-pid=\"WJQUGdSO\"\u003e但是让你记忆我叫37259374，估计没几秒你就忘记了，就算你特意去记忆，也没有12345678记得牢固。\u003c/p\u003e\u003cp data-pid=\"j4An0iQM\"\u003e在这里推荐建议大家\u003cb\u003e系统记忆知识点，把各个知识点连接起来，建立一个有序的知识体系。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"izTZb1at\"\u003e类似\u003cb\u003e思维导图，让无序的内容变有序，\u003c/b\u003e让知识点输入变成一大片的有序的神经网络。\u003c/p\u003e\u003cp data-pid=\"m5GcSpOO\"\u003e也可以理解为文件夹。\u003c/p\u003e\u003cp data-pid=\"K5uNx1Sw\"\u003e这本质上是为了强化神经元之间的连接，让大脑在检索记忆时更迅速，有路径可循。\u003c/p\u003e\u003cp data-pid=\"EkMqJWR3\"\u003e这就很考验各位了，不过问题不大，\u003cb\u003e目录\u003c/b\u003e是个好东西，每本书都有目录。\u003c/p\u003e\u003cp data-pid=\"UvMhtr1W\"\u003e\u003cb\u003e目录是有必要去记一记的，因为它能把你的知识点全部串联起来，形成一个知识体系。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"MkLchXsa\"\u003e就像下面这样的。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-8d76cb0f2739343a9b95a0889f3fb54e_1440w.jpg\" data-rawwidth=\"3072\" data-rawheight=\"4096\" data-size=\"normal\" data-original-token=\"v2-9d9c11f3149e1c09b5212856014ce165\" class=\"origin_image zh-lightbox-thumb\" width=\"3072\" data-original=\"https://pica.zhimg.com/v2-8d76cb0f2739343a9b95a0889f3fb54e_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"LLl9F6xG\"\u003e这本书分几部分，每部分有几个一级标题，每个一级标题又分几个二级标题，每个二级标题里面又有几个三级标题，逐级细化。\u003c/p\u003e\u003cp data-pid=\"Iu8sebXZ\"\u003e还有下面这样的\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-d17b36391bea39c82868731de467d102_1440w.jpg\" data-rawwidth=\"3072\" data-rawheight=\"4096\" data-size=\"normal\" data-original-token=\"v2-f405ff4cdf76fc49646095976d1a8ba8\" class=\"origin_image zh-lightbox-thumb\" width=\"3072\" data-original=\"https://pic3.zhimg.com/v2-d17b36391bea39c82868731de467d102_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"3TWRdECw\"\u003e就更细化了。\u003c/p\u003e\u003cp data-pid=\"0mh21v_r\"\u003e我们的\u003cb\u003e记忆目标是\u003c/b\u003e，理解这个知识点后，知道它属于哪个三级标题的，这个三级标题又属于哪个二级标题的，这个二级标题属于哪个一级标题……\u003c/p\u003e\u003cp data-pid=\"qdELSOPu\"\u003e实际上，由于对不同知识点的要求和需要掌握的程度不一样，有主次之分，也有重点与非重点之分，是不用把整体目录都背下来的，往往只是最重点的那些个部分。\u003c/p\u003e\u003ch2\u003e第三步，\u003cb\u003e运用\u003c/b\u003e。\u003c/h2\u003e\u003cp data-pid=\"iimZmkCA\"\u003e好了，假设我们已经理解和记忆了。\u003c/p\u003e\u003cp data-pid=\"v93av-c2\"\u003e\u003cb\u003e认识可以指导实践\u003c/b\u003e，但进行到这一步还远远不够。\u003c/p\u003e\u003cp data-pid=\"IqAuWkOc\"\u003e因为：\u003cb\u003e实践决定认识，实践还可以反作用于认识。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"yLX6xCaH\"\u003e费曼学习法最核心的结论就是要输出去倒逼输入，就是要\u003cb\u003e运用实践对认识的反作用力\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"7baKe815\"\u003e毕竟\u003cb\u003e实践是检验认识真理性的唯一标准\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"qYwdQiJs\"\u003e是骡子是马，拉出来遛遛就知道。\u003c/p\u003e\u003cp data-pid=\"SXpoVpCd\"\u003e没理解透彻or没记住，一检验便知。同时，实践可以强化理解和记忆。\u003c/p\u003e\u003cp data-pid=\"IvlHD6LP\"\u003e关于运用，\u003cb\u003e最好的办法是教给别人，用你的理解讲给一个小学生听，做到他能听懂\u003c/b\u003e。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-2a1bcf9b434eff696828f895be0f618b_1440w.jpg\" data-rawwidth=\"1200\" data-rawheight=\"1200\" data-size=\"normal\" data-original-token=\"v2-2d8dd6127ef788d78b57a1ace83fdd1e\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://picx.zhimg.com/v2-2a1bcf9b434eff696828f895be0f618b_r.jpg\"/\u003e\u003cfigcaption\u003e图源：百度百科\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"fnD334Ab\"\u003e退而求其次，\u003cb\u003e最常见的就是做题or动手，各种各样的题。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"BJafXAy6\"\u003e再次，就是和别人讨论这个知识点。\u003c/p\u003e\u003cp data-pid=\"GQCIEHVa\"\u003e关于驱动力、阻力和未来的发展变化，留着明天再接着写，先睡觉\u003c/p\u003e\u003cp data-pid=\"oClgrsWg\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1930510725215921474?share_code=PzKcmSjG4Kxx\u0026amp;utm_psn=1932913539921739926\" class=\"internal\" target=\"_blank\"\u003e战胜诱惑：手机\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"VPRdpcbC\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1929305344926020965?share_code=3FBuoldok2JH\u0026amp;utm_psn=1932917014445400160\" class=\"internal\" target=\"_blank\"\u003e专心学习\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"T8MOpeAy\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1928711991217881922?share_code=G9HcVIESFuRy\u0026amp;utm_psn=1932917220675162129\" class=\"internal\" target=\"_blank\"\u003e从“赖床废柴”到“晨型战神”：我用这套系统，驯服了早起这只猛兽！\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"H3SbFjZ6\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1925812319025370958?share_code=1eF14e3jurZxh\u0026amp;utm_psn=1932917479807641207\" class=\"internal\" target=\"_blank\"\u003e赶专线一 250708\u003c/a\u003e\u003c/p\u003e","is_labeled":false,"visited_count":111,"thumbnails":["https://pica.zhimg.com/50/v2-0dbfb088cd46012e7666dd57b89d6716_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-c93b1f550db575bf104801c1a8658bda_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-6ee85f05ccf996768f90484303576547_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-16a4e9f1cd6bee0e22a07fef8e03af40_720w.jpg?source=b6762063"],"favorite_count":12,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1929469460571457382}","attached_info":"CtoHCLOQq8rj2evBaRAHGgkyNjA1MDQyOTYg8e+YxAYoBDAAQHVKIwoYVFNfU09VUkNFX1dBUk1fVVBfQk9PU1QyEgEwGAAgADoASi8KJFRTX1NPVVJDRV9XQVJNVVBfVFdPVE9XRVJfRVhQVjJfVEVYVBIBMBgAIAA6AFoIMTM3MDc2NzZiIGQzMmM3ZDM3ODk1MDI5OWMwNDNjNDY1ZGM1Y2FjMGM2chMxOTI5NDY5NDYwNTcxNDU3MzgyigEVY18xOTI1NTg0NTMwNjE1NTQyODk5qgEJcmVjb21tZW5kwgEgM2E5MDA4MjIzNDgyNDliZTVkZmFlOTVlY2QwOTE0MGHyAQoIDBIGTm9ybWFs8gEoCAoSJDhkMmM2NDE5LTk2NDMtNGE0MC1hMDIzLTlkYjVkYzgwMTdhMPIBBggLEgIyMIICAIgCjeOzzoUzkgIgM2E5MDA4MjIzNDgyNDliZTVkZmFlOTVlY2QwOTE0MGGaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxl2gIYVFNfU09VUkNFX1dBUk1fVVBfQk9PU1Qy6AID+gILTk9STUFMX0ZMT1eKAyA2NWM5OWIwZmZkMGI0NjY5YTNhZGUyMjAxOTUxZGY1NJoDDQoCdjIQABoFb3RoZXKoA2/YAwDqAy5jb250ZW50V2FybXVwVHdvVG93ZXJUdnBUZXh0Qm9vc3RFeHBWMlJlY2FsbGVy+gOKAhIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgDEIwHGIwEIiN2Mi0wOWVkNWQ2ZTNjZGJhMzlhN2U1M2QwODE2NDRiZWE5NTotCAMQ1A0YgAoiI3YyLWYyNzgyYmQyZjIyMWZjNTJiMzY3Yjc0Mjc4NGZmOTA0Oi0IAhCAGBiAICIjdjItOWQ5YzExZjMxNDllMWMwOWI1MjEyODU2MDE0Y2UxNjU6LQgCEIAYGIAgIiN2Mi1mNDA1ZmY0Y2RmNzZmYzQ5NjQ2MDk1OTc2ZDFhOGJhODotCAMQsAkYsAkiI3YyLTJkOGRkNjEyN2VmNzg4ZDc4YjU3YTFhY2U4M2ZkZDFlgAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEAmFpwgQDNDAwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAABgrnyPP4EFAAAAAAAAAACJBcR/DucXddM/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRSQBgCgBnWoBgCSAi4KCTI2MDUwNDI5NhITMTkyOTQ2OTQ2MDU3MTQ1NzM4MhgHIgpJTUFHRV9URVhU","action_card":false},{"id":"118_1753853260.328","type":"feed","offset":118,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853260,"updated_time":1753853260,"target":{"id":"1933869672115179914","type":"answer","url":"https://api.zhihu.com/answers/1933869672115179914","author":{"id":"138b68518ef08d7effdb2023a5d86944","url":"https://api.zhihu.com/people/138b68518ef08d7effdb2023a5d86944","user_type":"people","url_token":"linxueli","name":"子林","headline":"心理咨询，情感分析，AI应用","avatar_url":"https://pic1.zhimg.com/50/v2-566ee141ffcf7fd0babd0a9b7424acd4_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"南京理工大学 兵器发射理论与技术硕士"}],"followers_count":10419,"is_following":false,"is_followed":false},"created_time":1753850459,"updated_time":1753853155,"voteup_count":0,"thanks_count":0,"comment_count":1,"is_copyable":true,"question":{"id":"3030186294","type":"question","url":"https://api.zhihu.com/questions/3030186294","author":{"id":"6ecc50ee5a431ea468d4dce1d72be038","url":"https://api.zhihu.com/people/6ecc50ee5a431ea468d4dce1d72be038","user_type":"people","url_token":"28-70-17-31","name":"绿草中的小熊","headline":"—— · ——","avatar_url":"https://picx.zhimg.com/50/v2-1abe7b115ea0ab9e5dfe334d5a1fef38_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":86,"is_following":false,"is_followed":false},"title":"AI已经这么厉害了，人类还有学习的必要吗？","created":1730639195,"answer_count":0,"follower_count":0,"comment_count":8,"bound_topic_ids":[119,350,224342,1386211],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://picx.zhimg.com/50/v2-7858f1cec347e99b92037244a9d5b3de_720w.jpg?source=b6762063","excerpt":"题主，你把因果搞反了。 AI现在之所以这么厉害，恰恰是因为人类还在不断地学习。 AI本质上是人类智慧的浓缩。它不是凭空诞生的神明，而是大量人类认知劳动的副产品。模型的训练数据、算法架构、优化思路，全都出自人类工程师和研究者的长期学习与实验。 而且目前的AI，远谈不上替代学习或自主进化。它的厉害，是依赖过去的数据堆砌，是在高维空间里模仿人类的思维路径。它能总结规律，但不能产生范式突变。你不会指望靠AI搞出相…","excerpt_new":"题主，你把因果搞反了。 AI现在之所以这么厉害，恰恰是因为人类还在不断地学习。 AI本质上是人类智慧的浓缩。它不是凭空诞生的神明，而是大量人类认知劳动的副产品。模型的训练数据、算法架构、优化思路，全都出自人类工程师和研究者的长期学习与实验。 而且目前的AI，远谈不上替代学习或自主进化。它的厉害，是依赖过去的数据堆砌，是在高维空间里模仿人类的思维路径。它能总结规律，但不能产生范式突变。你不会指望靠AI搞出相…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"iLy8v0bE\"\u003e题主，你把因果搞反了。\u003c/p\u003e\u003cp data-pid=\"F30B1ncN\"\u003eAI现在之所以这么厉害，恰恰是因为人类还在不断地学习。\u003c/p\u003e\u003cp data-pid=\"TuH7joif\"\u003eAI本质上是人类智慧的浓缩。它不是凭空诞生的神明，而是大量人类认知劳动的副产品。模型的训练数据、算法架构、优化思路，全都出自人类工程师和研究者的长期学习与实验。\u003c/p\u003e\u003cp data-pid=\"LcYbV1oY\"\u003e而且目前的AI，远谈不上替代学习或自主进化。它的厉害，是依赖过去的数据堆砌，是在高维空间里模仿人类的思维路径。它能总结规律，但不能产生范式突变。你不会指望靠AI搞出相对论2或者编写《时间简史2》吧？\u003c/p\u003e\u003cp data-pid=\"U2EBuHPD\"\u003e说到底，AI再强，也只是一个认知放大器——放大的是谁的认知？当然是人类的。\u003c/p\u003e\u003cp data-pid=\"zKk62Ao1\"\u003e真正该问的不是人类还有没有学习的必要，而是你愿不愿意成为那个\u003cb\u003e会用AI来学习、来思考、来创造\u003c/b\u003e的人，而不是被AI内容喂养、思维退化的人。\u003c/p\u003e\u003cp data-pid=\"7_W1aQkl\"\u003eAI时代的学习，不是被替代，而是升维。\u003c/p\u003e\u003ch3\u003e那么，什么是\u003cb\u003e升维学习\u003c/b\u003e？简单说，就是从学习知识升级为学习结构！\u003c/h3\u003e\u003cp data-pid=\"P34359HX\"\u003e传统的学习，是横向扩展知识面，像在平面上拼拼图：多背点公式，多懂点概念，多看点例题。但升维学习，是把你从二维的知识堆积拉到三维甚至四维的认知建模——你不只是问“这是什么”，而是开始问：“它是如何运作的”“这类问题能不能抽象成一个模型”“有哪些底层规律是跨领域通用的”。\u003c/p\u003e\u003cp data-pid=\"3DEuBX_P\"\u003e比如：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"RkHLnQOJ\"\u003e 学会写代码，是二维；\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"rVORN7sh\"\u003e 理解编程背后的逻辑、模式、系统抽象，就是升维。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"1aMOBTK6\"\u003e 会用AI写报告，是二维；\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"gGGcCp4u\"\u003e 能设计AI和人的协作流程，让AI成为你思想延伸的结构，就是升维。\u003cbr/\u003e \u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"4Ux1xUBG\"\u003e再比如我回答这个问题，如果我只是查资料、总结段子，那是二维；但我在这里做的是提炼逻辑、搭建认知框架、传递升维学习的思考方式——这是升维。\u003c/p\u003e\u003cp data-pid=\"FJbETAGG\"\u003e说白了，\u003cb\u003e升维学习的核心不是吸收内容，而是构建结构。框架搭起来，让AI帮你添砖加瓦！这才是人类在AI时代该有的姿态。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"b8TX0tkT\"\u003e未来能驾驭AI的人，一定是能驾驭结构的人。\u003c/p\u003e\u003cp data-pid=\"e8zCLNoZ\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1933868272828265109\" class=\"internal\" target=\"_blank\"\u003e为什么说AI时代，我们需要的是升维学习？什么是升维学习？具体怎么做？\u003c/a\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-303db10b67f8496e298f9e003b7d9d0b_1440w.jpg\" data-rawwidth=\"1024\" data-rawheight=\"1024\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-6756bb07653b9606fd0dabb964962715\" data-default-watermark-src=\"https://picx.zhimg.com/v2-364e18c33e098bb9802d76451e35daf7_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic2.zhimg.com/v2-303db10b67f8496e298f9e003b7d9d0b_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-399af32a58bc1466ff871c8b7c8fb8c4_1440w.jpg\" data-rawwidth=\"1024\" data-rawheight=\"1024\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-bcaa36149ce39ac0fe8720499fe7ef13\" data-default-watermark-src=\"https://pica.zhimg.com/v2-6cf6735078b0dcff608f09a2167c52e4_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic3.zhimg.com/v2-399af32a58bc1466ff871c8b7c8fb8c4_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-fbd8a3627185bbf51c6e1fde13afd16d_1440w.jpg\" data-rawwidth=\"1024\" data-rawheight=\"1024\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-e5c1706954bafb93e40ad2e05d414c12\" data-default-watermark-src=\"https://pica.zhimg.com/v2-c9373735dde8ba47473a59c83c2a7524_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic4.zhimg.com/v2-fbd8a3627185bbf51c6e1fde13afd16d_r.jpg\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":1,"thumbnails":["https://picx.zhimg.com/50/v2-7858f1cec347e99b92037244a9d5b3de_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-66dd273d925ad67010bbd3cd52bc49e7_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-b2aa379363c644ccdfbeef7c51d85f56_720w.jpg?source=b6762063"],"favorite_count":0,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1933869672115179914}","attached_info":"CpsHCLOQq8rj2evBaRAEGgk3Mzk1OTI4Njkg28SmxAYoADABQHZKJAoZVFNfU09VUkNFX1dBUk1fVVBfTk9STUFMMRIBMBgAIAA6AEoiChdUU19TT1VSQ0VfV0FSTVVQX1JVQ0VORRIBMBgAIAA6AFoJMTExNTgyNTU1YiBkMzJjN2QzNzg5NTAyOTljMDQzYzQ2NWRjNWNhYzBjNnITMTkzMzg2OTY3MjExNTE3OTkxNIoBCjMwMzAxODYyOTSqAQlyZWNvbW1lbmTCASAxMzhiNjg1MThlZjA4ZDdlZmZkYjIwMjNhNWQ4Njk0NPIBCggMEgZOb3JtYWzyASgIChIkYjMzYTYwYmYtZTIyNS00MWY0LWJjNDktMjEzNmZkMDE3ZGY28gEGCAsSAjIwggIAiAKN47POhTOSAiAxMzhiNjg1MThlZjA4ZDdlZmZkYjIwMjNhNWQ4Njk0NJoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhpDb250ZW50V2FybVVwSXNvbGF0aW9uUnVsZcoCHEJheWVzRmlyc3RMZXZlbElzb2xhdGlvblJ1bGXKAhhDb250ZW50V2FybVVwQnJlYWtJblJ1bGXaAhlUU19TT1VSQ0VfV0FSTV9VUF9OT1JNQUwx6AIE+gILTk9STUFMX0ZMT1eKAyA2NWM5OWIwZmZkMGI0NjY5YTNhZGUyMjAxOTUxZGY1NJoDDQoCdjIQABoFb3RoZXKoAwHYAwDqAwt0ZXh0X3J1Y2VuZfoDrAESDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IABCACBiACCIjdjItNjc1NmJiMDc2NTNiOTYwNmZkMGRhYmI5NjQ5NjI3MTU6LQgAEIAIGIAIIiN2Mi1iY2FhMzYxNDljZTM5YWMwZmU4NzIwNDk5ZmU3ZWYxMzotCAAQgAgYgAgiI3YyLWU1YzE3MDY5NTRiYWZiOTNlNDBhZDJlMDVkNDE0YzEygAQAiAQAkgQGTm9ybWFsmgQBNKAEAKgEALAEALoEAmFpwgQDNDAwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAACgcjylP4EFAAAAAAAAAACJBcR/DucXddM/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRSQBgCgBnaoBgGSAi4KCTczOTU5Mjg2ORITMTkzMzg2OTY3MjExNTE3OTkxNBgEIgpJTUFHRV9URVhU","action_card":false},{"id":"119_1753853260.416","type":"feed","offset":119,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853260,"updated_time":1753853260,"target":{"id":"1933826619052659367","type":"answer","url":"https://api.zhihu.com/answers/1933826619052659367","author":{"id":"80dc9b6b94ac7aa2a856d778c846b399","url":"https://api.zhihu.com/people/80dc9b6b94ac7aa2a856d778c846b399","user_type":"people","url_token":"kxs-36-1","name":"kxs","headline":"","avatar_url":"https://pica.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"created_time":1753840194,"updated_time":1753840194,"voteup_count":1,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"1926787537667596852","type":"question","url":"https://api.zhihu.com/questions/1926787537667596852","author":{"id":"dfe0968bf1e57bb0d7167dcb9634035e","url":"https://api.zhihu.com/people/dfe0968bf1e57bb0d7167dcb9634035e","user_type":"people","url_token":"19-91-39-29-76","name":"卢慧玲","headline":"自媒体创作者","avatar_url":"https://picx.zhimg.com/50/v2-4401af3a40bdef8a0f7eb521b487549f_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":0,"is_following":false,"is_followed":false},"title":"身上只有10,000块，你们觉得能创业吗？","created":1752161946,"answer_count":0,"follower_count":0,"comment_count":0,"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://pic1.zhimg.com/50/v2-a1f500ab353dad5af5995c91eb41de3f_720w.jpg?source=b6762063","excerpt":"这年头挣钱不易，请谨慎投资创业。如果是没有任何经验的小白，建议先找一个自己想干的行业，先去给别人打工几个月，积累经验。 切记盲目投资，这年头挣钱如捉鬼，赔钱就是分分钟的事。去做自己擅长喜欢的领域。不要去触碰那些高回报，自己根本什么都不懂的行业，只会被当韭菜割掉。","excerpt_new":"这年头挣钱不易，请谨慎投资创业。如果是没有任何经验的小白，建议先找一个自己想干的行业，先去给别人打工几个月，积累经验。 切记盲目投资，这年头挣钱如捉鬼，赔钱就是分分钟的事。去做自己擅长喜欢的领域。不要去触碰那些高回报，自己根本什么都不懂的行业，只会被当韭菜割掉。","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"ufoGz34S\"\u003e这年头挣钱不易，请谨慎投资创业。如果是没有任何经验的小白，建议先找一个自己想干的行业，先去给别人打工几个月，积累经验。\u003c/p\u003e\u003cp data-pid=\"Z-Qf2_Ut\"\u003e切记盲目投资，这年头挣钱如捉鬼，赔钱就是分分钟的事。去做自己擅长喜欢的领域。不要去触碰那些高回报，自己根本什么都不懂的行业，只会被当韭菜割掉。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-4366ed3fe4dc56f266e80bc93bba182d_1440w.jpg\" data-rawwidth=\"3000\" data-rawheight=\"4000\" data-size=\"normal\" data-original-token=\"v2-7b3c47aa616cd1f9cd4833820c004104\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-8545328a6ddd0438c76328f1e4b4e735_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"3000\" data-original=\"https://pic2.zhimg.com/v2-4366ed3fe4dc56f266e80bc93bba182d_r.jpg\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":true,"visited_count":9,"thumbnails":["https://picx.zhimg.com/50/v2-a1f500ab353dad5af5995c91eb41de3f_720w.jpg?source=b6762063"],"favorite_count":0,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1933826619052659367}","attached_info":"CpkGCLOQq8rj2evBaRAEGgk3Mzk1NTgzODkgwvSlxAYoATAAQHdKJAoZVFNfU09VUkNFX1dBUk1fVVBfTk9STUFMMRIBMBgAIAA6AEooCh1UU19TT1VSQ0VfV0FSTVVQX1BSRVRSQUlOX0kySRIBMBgAIAA6AFoJMTE1NzYwMjMxYiBkMzJjN2QzNzg5NTAyOTljMDQzYzQ2NWRjNWNhYzBjNnITMTkzMzgyNjYxOTA1MjY1OTM2N4oBEzE5MjY3ODc1Mzc2Njc1OTY4NTKqAQlyZWNvbW1lbmTCASA4MGRjOWI2Yjk0YWM3YWEyYTg1NmQ3NzhjODQ2YjM5OfIBCggMEgZOb3JtYWzyASgIChIkYWJkOGY4NmQtNzM2ZS00YTRhLWI2MjktYzUxMWNkNzk0OWFk8gEGCAsSAjIwggIAiAKN47POhTOSAiA4MGRjOWI2Yjk0YWM3YWEyYTg1NmQ3NzhjODQ2YjM5OZoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhpDb250ZW50V2FybVVwSXNvbGF0aW9uUnVsZdoCGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDHoAgL6AgtOT1JNQUxfRkxPV4oDIDY1Yzk5YjBmZmQwYjQ2NjlhM2FkZTIyMDE5NTFkZjU0mgMNCgJ2MhAAGgVvdGhlcqgDCdgDAOoDE3ByZXRyYWluX2kyaV9yZWNhbGz6A04SDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IBBC4FxigHyIjdjItN2IzYzQ3YWE2MTZjZDFmOWNkNDgzMzgyMGMwMDQxMDSABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAAAMZaA/gQUAAAAAAAAAAIkFxH8O5xd10z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFFJAGAKAGd6gGAJICLgoJNzM5NTU4Mzg5EhMxOTMzODI2NjE5MDUyNjU5MzY3GAQiCklNQUdFX1RFWFQ=","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=119\u0026desktop=true\u0026end_offset=119\u0026page_number=21\u0026session_token=d32c7d378950299c043c465dc5cac0c6","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=119\u0026desktop=true\u0026end_offset=119\u0026page_number=21\u0026session_token=d32c7d378950299c043c465dc5cac0c6","totals":0},"fresh_text":"推荐已更新"}
