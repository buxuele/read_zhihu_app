{"data":[{"id":"156_1753854240.117","type":"feed","offset":156,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753854240,"updated_time":1753854240,"target":{"id":"1925672038900699471","type":"answer","url":"https://api.zhihu.com/answers/1925672038900699471","author":{"id":"57bd5d18bc8be7229ee13ffae3db12fe","url":"https://api.zhihu.com/people/57bd5d18bc8be7229ee13ffae3db12fe","user_type":"people","url_token":"yi-shui-wu-chen","name":"一水无尘","headline":"一个非常业余的写作和写代码爱好者！","avatar_url":"https://picx.zhimg.com/50/v2-688379e8364d54b6742b37f51e264315_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":4033,"is_following":false,"is_followed":false},"created_time":1751895991,"updated_time":1751895991,"voteup_count":1228,"thanks_count":63,"comment_count":137,"is_copyable":true,"question":{"id":"1922905904380108960","type":"question","url":"https://api.zhihu.com/questions/1922905904380108960","author":{"id":"60e66ab83d906f3f5479670fa8493ff7","url":"https://api.zhihu.com/people/60e66ab83d906f3f5479670fa8493ff7","user_type":"people","url_token":"jian-feng-xian-sheng","name":"剑风先生","headline":"静看人生，岁月无痕","avatar_url":"https://picx.zhimg.com/50/v2-2cf5ca56936d24d1171c07f8a915f3f3_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":676,"is_following":false,"is_followed":false},"title":"《大明王朝 1566》当中，六必居的老板怎么知道严嵩要倒台的？","created":1751236493,"answer_count":0,"follower_count":0,"comment_count":2,"bound_topic_ids":[2102,107530,162815],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"严嵩：“皇上喜欢吃六心居的酱菜。每季新出的酱菜老臣都要给皇上送去一坛。今儿正月十六，应该天一亮六心居就会把春季的酱菜送来。今年看样子是不敢来了。” 徐阶蓦地想起了什么，起身走到门边，开了一扇门：“来人！” 一个书办立刻从院子里趋到门边：“回阁老，小人在。” 徐阶：“到府门外看看，六心居送酱菜的人来了没有。如果没来，立刻去传我的话，催他们把新腌的酱菜即刻送进来。” “是。”那书办答着奔了出去。 严嵩嘴…","excerpt_new":"严嵩：“皇上喜欢吃六心居的酱菜。每季新出的酱菜老臣都要给皇上送去一坛。今儿正月十六，应该天一亮六心居就会把春季的酱菜送来。今年看样子是不敢来了。” 徐阶蓦地想起了什么，起身走到门边，开了一扇门：“来人！” 一个书办立刻从院子里趋到门边：“回阁老，小人在。” 徐阶：“到府门外看看，六心居送酱菜的人来了没有。如果没来，立刻去传我的话，催他们把新腌的酱菜即刻送进来。” “是。”那书办答着奔了出去。 严嵩嘴…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"r3_gR8wA\"\u003e严嵩：“皇上喜欢吃六心居的酱菜。每季新出的酱菜老臣都要给皇上送去一坛。今儿正月十六，应该天一亮六心居就会把春季的酱菜送来。今年看样子是不敢来了。”\u003c/p\u003e\u003cp data-pid=\"4t76O_P0\"\u003e徐阶蓦地想起了什么，起身走到门边，开了一扇门：“来人！”\u003c/p\u003e\u003cp data-pid=\"A7XyrhCm\"\u003e一个书办立刻从院子里趋到门边：“回阁老，小人在。”\u003c/p\u003e\u003cp data-pid=\"v3UM5IHc\"\u003e徐阶：“到府门外看看，六心居送酱菜的人来了没有。如果没来，立刻去传我的话，催他们把新腌的酱菜即刻送进来。”\u003c/p\u003e\u003cp data-pid=\"OctmeCf0\"\u003e“是。”那书办答着奔了出去。\u003c/p\u003e\u003cp data-pid=\"KeC8rxnn\"\u003e严嵩嘴唇动了动，看着徐阶似乎想说什么，但又什么都没说。\u003c/p\u003e\u003cp data-pid=\"MnSSVEF-\"\u003e大约半个时辰，二十坛酱菜都被抬到了这里，占了好大一片院落。\u003c/p\u003e\u003cp data-pid=\"n_wA2j7I\"\u003e六心居当家的老板是个中年人，被领到这里，却不敢进去，跪在院子里大声说道：“小民拜见阁老。今年小铺腌制的各式酱菜一共二十坛，奉阁老之命，都送来了。”\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-fc5ceedafe9aa8fbfd032ff92e0b8252_1440w.jpg\" data-rawwidth=\"640\" data-rawheight=\"340\" data-size=\"normal\" data-original-token=\"v2-09b2f875c624c69706e87e4ef0099532\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-f0ebfe9f44b666529824b7da22566b43_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pica.zhimg.com/v2-fc5ceedafe9aa8fbfd032ff92e0b8252_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"u1cKrHqy\"\u003e正如严嵩所料，昨夜提刑司、镇抚司围了严世蕃几个人的府邸，不到天明已传遍了京城，如果徐阶不派人传话，这老板今天打死了也不会再送酱菜来。\u003c/p\u003e\u003cp data-pid=\"IQ2diFok\"\u003e因徐阶传唤，此时不得不来。这时遥遥望见书房里既坐着严嵩也坐着徐阶，他口称“阁老”自然不错，而平时应该说的“敬献阁老”这时改成了“奉阁老之命，都送来了”，这个“阁老”自然指的就是徐阶了，更加没错。\u003c/p\u003e\u003cp data-pid=\"2ky0ntP3\"\u003e亏他这时竟能琢磨出这几句难说的话，总算说得滴水不漏。说完，他便低头跪在那里，再也不动。\u003c/p\u003e\u003cp data-pid=\"7EzD8LHl\"\u003e这几句话严嵩也听到了，坐在那里茫茫地向门外的院子望去：“是赵老板吗？进来吧。”\u003c/p\u003e\u003cp data-pid=\"tyyRJZbe\"\u003e从这里可以看到，那赵姓老板依然跪在那里，一动不动。\u003c/p\u003e\u003cp data-pid=\"4fyEUfRr\"\u003e严嵩望向了徐阶：“他怕见我了。徐阁老，烦你叫他进来吧。”\u003c/p\u003e\u003cp data-pid=\"qiT0kYjd\"\u003e徐阶只好望向门外：“严阁老叫你，你没有听到吗？”\u003c/p\u003e\u003cp data-pid=\"Ov1sZ80G\"\u003e“是。”那赵老板这才应了一声，万般不情愿地爬了起来，走到了门边，再不肯进来，就在那里又跪下了。\u003c/p\u003e\u003cp data-pid=\"h5fn6whW\"\u003e“赵老板。”严嵩又叫了他一声。\u003c/p\u003e\u003cp data-pid=\"cNhP6TWV\"\u003e“在。”那赵老板这个“在”字答得有如蚊蝇，头却依然低在那里。\u003c/p\u003e\u003cp data-pid=\"1vcUViDY\"\u003e徐阶：“阁老叫你，抬头回话！”\u003c/p\u003e\u003cp data-pid=\"NwcJ7Y_4\"\u003e“是。”那赵老板不得不抬头了，却只望向徐阶，不看严嵩。\u003c/p\u003e\u003cp data-pid=\"hlplZ9ow\"\u003e严嵩依然唠叨着：“二十多年了，难为你每年几次给我送酱菜。记得你多次说过，想请我为你的店面题块匾，今天我就给你写。”\u003c/p\u003e\u003cp data-pid=\"E8U-LqBt\"\u003e那赵老板立刻伏下头去，慌忙答道：“小民一间小店，做的都是平常百姓的生意，怎敢烦劳官家题匾。万万不敢。阁老若无别事，小民就此拜别。”说着磕下头去。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-dd6401ff2e3f849c1624a773d5ffa5f2_1440w.jpg\" data-rawwidth=\"640\" data-rawheight=\"364\" data-size=\"normal\" data-original-token=\"v2-d41ccb03084050831f918d3000a1edb5\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-c57fbe817c4c1e8582cf028dd3fea1e9_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-dd6401ff2e3f849c1624a773d5ffa5f2_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"kbWA3p1K\"\u003e严嵩笑了，笑出了眼泪，转望向徐阶：“徐阁老你都看见了，平时，多少人千金求老夫一字而不可得。现在，老夫的字白送人，都没人敢要了。回去吧，今后老夫也不会再烦你送酱菜了。好好做生意，皇上也喜欢吃你们的酱菜呢。”\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-10d92e98e8fd616b3f558f55ba241fd5_1440w.jpg\" data-rawwidth=\"936\" data-rawheight=\"500\" data-size=\"normal\" data-original-token=\"v2-5e68f4f9df38d36a0833da8de783e0b0\" data-default-watermark-src=\"https://pica.zhimg.com/v2-f78bedf0f82185cff2d2238871377afa_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"936\" data-original=\"https://picx.zhimg.com/v2-10d92e98e8fd616b3f558f55ba241fd5_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"r7puOz8d\"\u003e那赵老板连忙磕了最后一个头，爬了起来，低头躬身退了出去。\u003c/p\u003e\u003cp data-pid=\"57Hq-U52\"\u003e“来人。”严嵩这一声竟然叫得中气十足。\u003c/p\u003e\u003cp data-pid=\"ENgs_evE\"\u003e他的一个管事进来了，望着他满脸黯然。\u003c/p\u003e\u003cp data-pid=\"4qAz7EO3\"\u003e严嵩：“挑一坛八宝酱菜，我要敬献皇上。”\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":422955,"favorite_count":153,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1925672038900699471}","attached_info":"CpgHCI+c4LWdz6m/ogEQBBoJNzM1ODkwNDgzILefr8MGKMwJMIkBQJwBSjAKG1RTX1NPVVJDRV9CQVNJQ19JTkZPX1JFQ0FMTBIBMBgAIAA6CnsicmF3IjoiIn1aCTExNTYxNTkyMGIgMGViYTFiZTM1ZGEyOWQyNzgzZWFlODIyMmQzZTVlNDlyEzE5MjU2NzIwMzg5MDA2OTk0NzGKARMxOTIyOTA1OTA0MzgwMTA4OTYwqgEJcmVjb21tZW5kwgEgNTdiZDVkMThiYzhiZTcyMjllZTEzZmZhZTNkYjEyZmXyAQoIDBIGTm9ybWFs8gEoCAoSJGFhMzdjMWZiLWYxNTItNDA0OC04NTZjLTUyMTE1M2MxODBiZfIBBggLEgIyN4ICAIgCtsnvzoUzkgIgNTdiZDVkMThiYzhiZTcyMjllZTEzZmZhZTNkYjEyZmWaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCFlJldmlzaXRWYWx1ZVdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXaAhtUU19TT1VSQ0VfQkFTSUNfSU5GT19SRUNBTEzoAgP6AgtOT1JNQUxfRkxPV4oDIGNmNjdhNTIwY2I1ZTQ1ZGM5MTUwZDFmYjBjZWIyNzM0mgMNCgJ2MhAAGgVvdGhlcqgDq+gZ2AMA6gMRYmFzaWNfaW5mb19yZWNhbGz6A6wBEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAMQgAUY1AIiI3YyLTA5YjJmODc1YzYyNGM2OTcwNmU4N2U0ZWYwMDk5NTMyOi0IAxCABRjsAiIjdjItZDQxY2NiMDMwODQwNTA4MzFmOTE4ZDMwMDBhMWVkYjU6LQgDEKgHGPQDIiN2Mi01ZTY4ZjRmOWRmMzhkMzZhMDgzM2RhOGRlNzgzZTBiMIAEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAZtYW51YWzCBAMxNzDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAKDXm7k/gQUAAAAAAAAAAIkFDTteRUxy0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFG5AGAKAGnAGoBgGSAi4KCTczNTg5MDQ4MxITMTkyNTY3MjAzODkwMDY5OTQ3MRgEIgpJTUFHRV9URVhU","action_card":false},{"id":"157_1753854240.212","type":"feed","offset":157,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753854240,"updated_time":1753854240,"target":{"id":"1931379423766085931","type":"article","url":"https://api.zhihu.com/articles/1931379423766085931","author":{"id":"7af62e4119791a452e88718cb5ccc0be","url":"https://api.zhihu.com/people/7af62e4119791a452e88718cb5ccc0be","user_type":"people","url_token":"albert-chen-4","name":"北方的郎","headline":"专注模型与代码，公众号：AI方法与实践","avatar_url":"https://picx.zhimg.com/50/v2-9bc49daad69ac6fd88a9d354c9932677_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":12867,"is_following":false,"is_followed":false},"title":"2025 年大语言模型架构全景对比：新结构、新趋势与新选择","image_url":"https://picx.zhimg.com/v2-55e8791562410adba23da75e94ace645.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1753257474,"updated":1753341329,"voteup_count":16,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"本文内容摘译自： https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html 原作写的非常好，就是有点太长了。我对内容进行了大幅的精简，想了解详细内容的，请看原文。 在过去的几年里，大语言模型（LLM）领域快速发展，从最早期的 GPT 和 BERT 演进至如今多样化的模型家族。到 2025 年，我们见证了大量开源 LLM 的发布，这些模型不仅在规模上不断突破，更在底层架构上呈现出越来越多的创新点。 本文将系统梳理和比较 DeepSeek-V3/R1、OLMo 2、Gemma 3、Mistral Sm…","excerpt_new":"本文内容摘译自： https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html 原作写的非常好，就是有点太长了。我对内容进行了大幅的精简，想了解详细内容的，请看原文。 在过去的几年里，大语言模型（LLM）领域快速发展，从最早期的 GPT 和 BERT 演进至如今多样化的模型家族。到 2025 年，我们见证了大量开源 LLM 的发布，这些模型不仅在规模上不断突破，更在底层架构上呈现出越来越多的创新点。 本文将系统梳理和比较 DeepSeek-V3/R1、OLMo 2、Gemma 3、Mistral Sm…","preview_type":"default","preview_text":"","column":{"id":"c_1195306329030553600","type":"column","url":"https://api.zhihu.com/columns/c_1195306329030553600","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://picx.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"北方的郎","imageUrl":"https://picx.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b","comment_permission":"public","intro":"专注介绍各方面的AI技术、应用攻略及方案","updated":1739485515,"is_following":false},"content":"\u003cp data-pid=\"uRpy9cZj\"\u003e本文内容摘译自：\u003ca href=\"https://link.zhihu.com/?target=https%3A//sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003esebastianraschka.com/bl\u003c/span\u003e\u003cspan class=\"invisible\"\u003eog/2025/the-big-llm-architecture-comparison.html\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"7617kQ_M\"\u003e原作写的非常好，就是有点太长了。我对内容进行了大幅的精简，想了解详细内容的，请看原文。\u003c/p\u003e\u003cp data-pid=\"DC_7MYmi\"\u003e在过去的几年里，大语言模型（LLM）领域快速发展，从最早期的 GPT 和 BERT 演进至如今多样化的模型家族。到 2025 年，我们见证了大量开源 LLM 的发布，这些模型不仅在规模上不断突破，更在底层架构上呈现出越来越多的创新点。\u003c/p\u003e\u003cp data-pid=\"8JNjqTVk\"\u003e本文将系统梳理和比较 DeepSeek-V3/R1、OLMo 2、Gemma 3、Mistral Small 3.1、LLaMA 4、Qwen 3、SmolLM3 以及 Kimi 2 等代表性模型在架构设计上的异同，并探讨这些架构改进背后的动机与技术逻辑。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-0db25a907af7d80885d869d4d122cfc1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"1116\" data-original-token=\"v2-3781c0a0e968cf04c60734bb8b0113f4\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic4.zhimg.com/v2-0db25a907af7d80885d869d4d122cfc1_r.jpg\"/\u003e\u003cfigcaption\u003e图 1：本文介绍的架构的子集。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e一、DeepSeek-V3/R1：在推理效率与模型容量中找到平衡\u003c/h2\u003e\u003cp data-pid=\"slmgoZfu\"\u003eDeepSeek 系列由国内团队主导开发，是 2025 年架构创新的代表之一。V3/R1 版本引入了两个关键特性：\u003c/p\u003e\u003ch3\u003e1. Multi-head Latent Attention（MLA）\u003c/h3\u003e\u003cp data-pid=\"mtfb9yvk\"\u003eMLA 是对传统 Attention 机制的一个突破。与 Multi-head Attention（MHA）或 Grouped-query Attention（GQA）相比，MLA 的设计理念是压缩查询过程中的 Key-Value（KV）缓存数据，从而减少显存使用。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-67d877005f50f8013fc9e362e3cb82a4_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1023\" data-rawheight=\"474\" data-original-token=\"v2-0eeee1d2443f68075ab334199aa3016c\" class=\"origin_image zh-lightbox-thumb\" width=\"1023\" data-original=\"https://pic3.zhimg.com/v2-67d877005f50f8013fc9e362e3cb82a4_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"LMjy-JZV\"\u003e具体方式是在前向传播中将 KV 向量进行压缩，存入缓存，推理阶段再通过额外的查询机制还原。这种结构既保证了性能，又能显著降低内存开销。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-65998c2534b893d52e4a6fd8ea9f3c0c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1550\" data-rawheight=\"858\" data-original-token=\"v2-a496df548329219088b84e08ec79d6fd\" class=\"origin_image zh-lightbox-thumb\" width=\"1550\" data-original=\"https://pic1.zhimg.com/v2-65998c2534b893d52e4a6fd8ea9f3c0c_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e2. 稀疏专家路由（MoE）\u003c/h3\u003e\u003cp data-pid=\"gLYl1SQM\"\u003eDeepSeek-V3 采用了一种大规模稀疏专家架构：256 个专家模块，每次只激活其中的 9 个专家，包括一个“共享专家”用于所有 token 的基础处理。总参数量达到 671B，但实际推理时激活参数仅约 37B，大大降低了推理成本，同时保持模型能力的高度可扩展性。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-90e722a1dfca28f1822ed16644e29bec_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"1009\" data-original-token=\"v2-24d91d64b946aa13e310dc66631b00ca\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic1.zhimg.com/v2-90e722a1dfca28f1822ed16644e29bec_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"PF91SFJ5\"\u003e此外，DeepSeek 还对 LayerNorm 和 KV 缓存做了定制优化，使其能更好适配高吞吐率场景，如搜索和问答等。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-69cf03708238512ea0f8c8972a28cf46_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1039\" data-rawheight=\"569\" data-original-token=\"v2-bb2af3977bf0d4d86ec2ab0052695501\" class=\"origin_image zh-lightbox-thumb\" width=\"1039\" data-original=\"https://pic1.zhimg.com/v2-69cf03708238512ea0f8c8972a28cf46_r.jpg\"/\u003e\u003c/figure\u003e\u003ch2\u003e二、OLMo 2：追求训练稳定性的精致模型\u003c/h2\u003e\u003cp data-pid=\"LsMeJBry\"\u003eOLMo 2 是由 AI2 团队开发的完全开源模型，在设计上并没有使用 MoE，而是聚焦在架构稳定性和训练鲁棒性方面。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-7e63e55df73fbdde7502b6a233da9ab4_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1027\" data-rawheight=\"823\" data-original-token=\"v2-a20619bc1b62925cad04ca9ca8e937c6\" class=\"origin_image zh-lightbox-thumb\" width=\"1027\" data-original=\"https://pic3.zhimg.com/v2-7e63e55df73fbdde7502b6a233da9ab4_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e1. Post-Norm 架构\u003c/h3\u003e\u003cp data-pid=\"MwdT2Ztk\"\u003e传统 Transformer 采用的是 Pre-Norm，即在 Attention 和 FFN 层之前使用 LayerNorm。而 OLMo 2 选择了 Post-Norm 结构，将 RMSNorm 放置在每个子层之后。这一变动虽然会加大训练初期的梯度波动，但长期看能带来更好的收敛表现和模型表现力。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-abad2c03648af447e9ee27f915cc38ab_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1444\" data-rawheight=\"789\" data-original-token=\"v2-b7e8896ea0fd0fbd3be86efa0c686477\" class=\"origin_image zh-lightbox-thumb\" width=\"1444\" data-original=\"https://picx.zhimg.com/v2-abad2c03648af447e9ee27f915cc38ab_r.jpg\"/\u003e\u003cfigcaption\u003e图 8：Post-Norm、Pre-Norm 和 OLMo 2 的 Post-Norm 风格的比较。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e2. QK-Norm 机制\u003c/h3\u003e\u003cp data-pid=\"OYhdFOV0\"\u003e这是 OLMo 2 的另一项独特设计：对 attention 模块中的 query 和 key 向量单独使用 RMSNorm，从而稳定 attention score 的分布，提高训练质量。\u003c/p\u003e\u003ch3\u003e3. 坚守密集架构\u003c/h3\u003e\u003cp data-pid=\"NQDGRvy8\"\u003eOLMo 并未采用 MoE，强调架构的简单性和可解释性，为开发者提供更清晰、易于调试的训练过程，也更方便研究型使用。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-208a54002f7aa77e853be29f0f05b390_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1179\" data-rawheight=\"624\" data-original-token=\"v2-2eea215fbf472b756731e429afa4bf40\" class=\"origin_image zh-lightbox-thumb\" width=\"1179\" data-original=\"https://pica.zhimg.com/v2-208a54002f7aa77e853be29f0f05b390_r.jpg\"/\u003e\u003cfigcaption\u003e图 10：Llama 3 和 OLMo 2 之间的架构比较。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e三、Gemma 3：为效率而设计的主流架构\u003c/h2\u003e\u003cp data-pid=\"qIyDqMhg\"\u003eGemma 由 Google DeepMind 发布，属于轻量通用 LLM，专为推理效率和部署友好性而设计，尤其适合中小规模任务和边缘部署场景。\u003c/p\u003e\u003ch3\u003e1. 滑动窗口注意力（sliding-window attention）\u003c/h3\u003e\u003cp data-pid=\"TIzWxXOP\"\u003e为了在保持长上下文能力的同时降低显存开销，Gemma 采用滑动窗口机制：模型以 5:1 的比例组合局部注意力和全局注意力，并将局部窗口大小从 4k 缩小到 1k。这一设计大幅减少了 KV 缓存的存储压力，尤其在多轮对话和流式处理场景中优势明显。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-fca954301744dffbd4c55a60c6f95b8c_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"665\" data-rawheight=\"302\" data-original-token=\"v2-78d78bf3ade0a638085f686f8a0f009c\" class=\"origin_image zh-lightbox-thumb\" width=\"665\" data-original=\"https://pic1.zhimg.com/v2-fca954301744dffbd4c55a60c6f95b8c_r.jpg\"/\u003e\u003cfigcaption\u003e图 11：Gemma 3 论文 （https://arxiv.org/abs/2503.19786） 中的注释图显示了通过滑动窗口注意力节省的 KV 缓存。\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-4ae918cd801f552426d87966d00a6ccb_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"792\" data-original-token=\"v2-ed56fd503942d851f8fa915ee6dbaeba\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic4.zhimg.com/v2-4ae918cd801f552426d87966d00a6ccb_r.jpg\"/\u003e\u003cfigcaption\u003e图 12：常规注意力（左）和滑动窗口注意力（右）之间的比较。\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-bc66ba2e695c4dccede7cdb225ffe00c_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"477\" data-original-token=\"v2-1f6757b22c60c747105b576da2410d14\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic3.zhimg.com/v2-bc66ba2e695c4dccede7cdb225ffe00c_r.jpg\"/\u003e\u003cfigcaption\u003e图 13：来自 Gemma 3 论文 （https://arxiv.org/abs/2503.19786） 的注释图显示，滑动窗口注意力对 LLM 生成的输出困惑度几乎没有影响。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e2. 双向 RMSNorm\u003c/h3\u003e\u003cp data-pid=\"6x7D0V5z\"\u003e每个模块（包括 attention 和 FFN）前后都使用 RMSNorm，是对 LayerNorm 的一种增强，提升模型训练稳定性与泛化能力。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-13288772edc6e9673e247f8166279214_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"855\" data-original-token=\"v2-538bdbf7cc391a162dc5687ec2880e50\" class=\"origin_image zh-lightbox-thumb\" width=\"1068\" data-original=\"https://pic3.zhimg.com/v2-13288772edc6e9673e247f8166279214_r.jpg\"/\u003e\u003cfigcaption\u003e图 14：OLMo2 和 Gemma 3 之间的架构比较;请注意 Gemma 3 中的其他归一化图层。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e3. Gemma 3n：适配低算力设备\u003c/h3\u003e\u003cp data-pid=\"zhBzhYaR\"\u003eGemma 还发布了面向低功耗设备的“n”系列版本，采用 PLE（按需加载嵌入）和 MatFormer（可切分 Transformer）技术，进一步降低部署门槛。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-b98dc3a367d22b71d967d3e58a44d37c_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"662\" data-rawheight=\"483\" data-original-token=\"v2-e499e73545b8b5437d2a2220f695f4b2\" class=\"origin_image zh-lightbox-thumb\" width=\"662\" data-original=\"https://pic1.zhimg.com/v2-b98dc3a367d22b71d967d3e58a44d37c_r.jpg\"/\u003e\u003cfigcaption\u003e图 15：来自 Google Gemma 3n 博客 （https://developers.googleblog.com/en/introducing-gemma-3n/） 的注释图，说明了 PLE 内存节省。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e四、Mistral Small 3.1：精巧设计下的强大性能\u003c/h2\u003e\u003cp data-pid=\"4gnmC9O1\"\u003eMistral Small 3.1 是一款参数规模约 24B 的模型，但在多项评测中性能超越 Gemma 3 27B，在数学和多轮对话任务表现优异。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-c05356bde97a8dd4d13995bd3bc439b6_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1460\" data-rawheight=\"793\" data-original-token=\"v2-6c0ca69943cc021255957208e1e7c3b5\" class=\"origin_image zh-lightbox-thumb\" width=\"1460\" data-original=\"https://pic1.zhimg.com/v2-c05356bde97a8dd4d13995bd3bc439b6_r.jpg\"/\u003e\u003cfigcaption\u003e图 16：Gemma 3 27B 和 Mistral 3.1 Small 24B 之间的架构比较。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e1. 标准 GQA 架构\u003c/h3\u003e\u003cp data-pid=\"fTZqnorS\"\u003e相比 MLA，Mistral 选择了更保守的 GQA 架构，同时配合高效实现的 FlashAttention v2，使得推理效率在同规模模型中表现出色。\u003c/p\u003e\u003ch3\u003e2. 不采用滑窗\u003c/h3\u003e\u003cp data-pid=\"1pfEOSVk\"\u003e尽管 FlashAttention 支持长上下文，Mistral 并未加入滑动窗口机制，而是坚持密集 attention，确保多轮长文本任务中的上下文一致性。\u003c/p\u003e\u003cp data-pid=\"2rGYPXK0\"\u003e这一系列设计使其成为精度和效率的良好折中方案，适合通用对话和语言任务。\u003c/p\u003e\u003ch2\u003e五、LLaMA 4：旗舰大模型的稀疏化演进\u003c/h2\u003e\u003cp data-pid=\"rtb1FJw8\"\u003eMeta 的 LLaMA 4 是目前公开参数最多的开源 LLM，规模超过 400B，采用了灵活的 MoE 路由机制。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-42c703c290838cb0126894a0162f5802_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"823\" data-original-token=\"v2-8a4bb02fd15c2b6e0fe6541d65d4b472\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic1.zhimg.com/v2-42c703c290838cb0126894a0162f5802_r.jpg\"/\u003e\u003cfigcaption\u003e图 17：DeepSeek V3（6710 亿个参数）和 Llama 4 Maverick（4000 亿个参数）之间的架构比较。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e1. 双专家激活\u003c/h3\u003e\u003cp data-pid=\"Ner7BZvM\"\u003e与 DeepSeek 每层激活 9 个专家不同，LLaMA 4 每层只激活 2 个专家（从 64 个中选择），使得计算成本与性能之间达到一个新的平衡。\u003c/p\u003e\u003ch3\u003e2. 层间专家分布不均\u003c/h3\u003e\u003cp data-pid=\"cxh-RV8a\"\u003eLLaMA 在部分层使用密集 FFN，其余层使用 MoE，使模型对低层进行稳定特征提取，对高层进行语义扩展。这种混合架构使训练更稳定，同时提高表达能力。\u003c/p\u003e\u003ch2\u003e六、Qwen 3：架构灵活、设计成熟\u003c/h2\u003e\u003cp data-pid=\"zH0qwulv\"\u003eQwen 3 是阿里推出的新一代 LLM 系列，包括密集版与 MoE 版。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-5d1e626c23cd5888e7e483009421db2d_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1331\" data-rawheight=\"807\" data-original-token=\"v2-8f2d277b277b6b3756e223897e93ac7d\" class=\"origin_image zh-lightbox-thumb\" width=\"1331\" data-original=\"https://pic2.zhimg.com/v2-5d1e626c23cd5888e7e483009421db2d_r.jpg\"/\u003e\u003cfigcaption\u003e图 18：Qwen3 0.6B 和 Llama 3 1B 的架构比较;请注意，Qwen3 是一个更深层的架构，具有更多的层，而 Llama 3 是一个更广泛的架构，具有更多的注意力头。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e1. MoE 架构演化\u003c/h3\u003e\u003cp data-pid=\"olZxhhcD\"\u003eQwen 3 的 MoE 结构与 DeepSeek 类似，也使用 256 个专家模块，但其激活策略略有不同：仅激活 8 个专家，并移除了共享专家模块。根据官方分析，这样设计在性能上无明显损失，反而简化了计算图。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-ba40feb67047dec920029b30da2884bd_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1467\" data-rawheight=\"750\" data-original-token=\"v2-62481df2c0eea5ab95678b691d0f4a59\" class=\"origin_image zh-lightbox-thumb\" width=\"1467\" data-original=\"https://picx.zhimg.com/v2-ba40feb67047dec920029b30da2884bd_r.jpg\"/\u003e\u003cfigcaption\u003e图 19：DeepSeek-V3 和 Qwen3 235B-A22B 之间的架构比较。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e2. 模型规模涵盖广泛\u003c/h3\u003e\u003cp data-pid=\"mdQcQ8WA\"\u003e从 0.5B 到 235B，Qwen 提供了多种模型规格，几乎覆盖所有常见应用场景，包括对话、编程、多模态任务等。\u003c/p\u003e\u003ch2\u003e七、SmolLM3：小模型中的效率之王\u003c/h2\u003e\u003cp data-pid=\"5p6FcoxW\"\u003eSmolLM3 是一款仅 3B 参数的小型 LLM，却能在多个任务上与大模型抗衡。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-dd11d922ef108b02622244dc7e903f4a_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"743\" data-rawheight=\"519\" data-original-token=\"v2-e57fe21207229097985eeb0568329c02\" class=\"origin_image zh-lightbox-thumb\" width=\"743\" data-original=\"https://pic3.zhimg.com/v2-dd11d922ef108b02622244dc7e903f4a_r.jpg\"/\u003e\u003cfigcaption\u003e图 20：SmolLM3 公告帖子 https://huggingface.co/blog/smollm3 的注释图，将 SmolLM3 胜率与 Qwen3 1.7B 和 4B 以及 Llama 3 3B 和 Gemma 3 4B 进行了比较。\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-63a66b753f37064d6b1333dfcb97bfcf_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1431\" data-rawheight=\"777\" data-original-token=\"v2-7f1dc1631d4af36eefb1180ecb175301\" class=\"origin_image zh-lightbox-thumb\" width=\"1431\" data-original=\"https://pic4.zhimg.com/v2-63a66b753f37064d6b1333dfcb97bfcf_r.jpg\"/\u003e\u003cfigcaption\u003e图 21：Qwen3 4B 和 SmolLM3 3B 之间的并排架构比较。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e1. NoPE（No Positional Embedding）\u003c/h3\u003e\u003cp data-pid=\"wGnpHZ1N\"\u003e不同于大多数 LLM 使用的位置编码（绝对或相对），SmolLM3 直接使用因果掩码推理 token 顺序，完全取消位置嵌入。这种极简结构有助于泛长场景的适配，并提升泛化能力。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-fb01f571f684a2a27fd71f0f1119e674_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"548\" data-original-token=\"v2-483877484f3ebd490a24a629b88ed357\" class=\"origin_image zh-lightbox-thumb\" width=\"1190\" data-original=\"https://pica.zhimg.com/v2-fb01f571f684a2a27fd71f0f1119e674_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-1c36186965860d60798a8ea6407071fc_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1364\" data-rawheight=\"800\" data-original-token=\"v2-a0fb475a002fd2fcec8d80ff7b7e8606\" class=\"origin_image zh-lightbox-thumb\" width=\"1364\" data-original=\"https://pic3.zhimg.com/v2-1c36186965860d60798a8ea6407071fc_r.jpg\"/\u003e\u003cfigcaption\u003e图 23：NoPE 论文 （https://arxiv.org/abs/2305.19466） 中的注释图显示了 NoPE 更好的长度泛化。\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e2. 极致压缩设计\u003c/h3\u003e\u003cp data-pid=\"LdTGVKqH\"\u003e通过结构优化和训练技巧，SmolLM3 成功压缩至适合边缘设备部署，代表了小模型架构的新方向。\u003c/p\u003e\u003ch2\u003e八、Kimi 2：巨型模型的实验场\u003c/h2\u003e\u003cp data-pid=\"eTuhW4Xx\"\u003e月之暗面推出的 Kimi 2 拥有超过 1 万亿参数，是目前最大的开源 LLM。\u003c/p\u003e\u003ch3\u003e1. 深度 MoE 架构\u003c/h3\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-53ce65d1df16a6752b4763932a68705a_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1599\" data-rawheight=\"816\" data-original-token=\"v2-92c91a095735848829c343ebb67e58df\" class=\"origin_image zh-lightbox-thumb\" width=\"1599\" data-original=\"https://pic1.zhimg.com/v2-53ce65d1df16a6752b4763932a68705a_r.jpg\"/\u003e\u003cfigcaption\u003e图 25：DeepSeek V3 和 Kimi K2 之间的架构比较。\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"2bWERUw_\"\u003eKimi 2 基于 DeepSeek-V3 的 MoE 架构扩展，采用更少的 attention 头、更密集的专家分布，使其在极大参数下仍能保持推理速度。\u003c/p\u003e\u003ch3\u003e2. 优化器创新：Muon\u003c/h3\u003e\u003cp data-pid=\"3Dbgvv83\"\u003eKimi 2 采用了一种名为 Muon 的优化器，替代传统 AdamW，目标是提升大模型训练收敛性与稳定性。初步实验显示训练波动更小，特别适合超大规模参数训练。\u003c/p\u003e\u003ch2\u003e结语：架构革新的六大趋势\u003c/h2\u003e\u003cp data-pid=\"yiRjJiG5\"\u003e从这 8 大代表性模型的架构设计可以归纳出如下趋势：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"QakYoLjK\"\u003e\u003cb\u003eKV 缓存压缩与优化（MLA、滑窗）\u003c/b\u003e：显存瓶颈是部署难点，结构压缩成为必选项。\u003c/li\u003e\u003cli data-pid=\"sqCE8M5T\"\u003e\u003cb\u003e稀疏专家机制（MoE）成为主流\u003c/b\u003e：几乎所有主流高性能 LLM 都转向专家路由，兼顾容量和推理效率。\u003c/li\u003e\u003cli data-pid=\"Yc-_CATt\"\u003e\u003cb\u003e新型规范与归一化方法（双 RMSNorm、QK-Norm）\u003c/b\u003e：为了解决深层模型训练不稳定问题，各模型不断优化 LayerNorm 的位置与方式。\u003c/li\u003e\u003cli data-pid=\"ui9pcd3i\"\u003e\u003cb\u003e小模型崛起（SmolLM、Gemma-n）\u003c/b\u003e：轻量模型成为边缘 AI 与嵌入式部署的主角。\u003c/li\u003e\u003cli data-pid=\"jNTemvAl\"\u003e\u003cb\u003e创新优化器应用（Muon）\u003c/b\u003e：训练稳定性与收敛速度成为新挑战。\u003c/li\u003e\u003cli data-pid=\"CNEHEmCe\"\u003e\u003cb\u003e架构透明性重视（如 OLMo）\u003c/b\u003e：研究团队和社区倾向于追求可解释、可复现、可修改的模型结构。\u003cbr/\u003e \u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"I1eSDhFc\"\u003e在未来的大模型技术演进中，架构的创新仍将是最核心的推动力。而 2025 年，是这一场架构竞争最激烈、最精彩的阶段。\u003c/p\u003e\u003cp data-pid=\"ZMbbfLyF\"\u003e——完——\u003c/p\u003e\u003cp data-pid=\"pSAGW5sV\"\u003e\u003ca href=\"https://www.zhihu.com/people/7af62e4119791a452e88718cb5ccc0be\" class=\"internal\" target=\"_blank\"\u003e@北方的郎\u003c/a\u003e · 专注模型与代码\u003c/p\u003e\u003cp data-pid=\"yDLeyBNk\"\u003e喜欢的朋友，欢迎赞同、关注、分享三连 ^O^\u003c/p\u003e","is_labeled":false,"visited_count":528,"thumbnails":["https://picx.zhimg.com/v2-55e8791562410adba23da75e94ace645.jpg?source=7e7ef6e2\u0026needBackground=1","https://pica.zhimg.com/50/v2-924b89825b10b76d5e6f235a869cd324_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-ff489bc773afbb79dbe0811fd08cd745_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-9ff49ffa0c565cbd77017d3e248d4919_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-4694aaf56f0111cacad3a462aae13f7b_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-602723dab9d4f22685a5fa0b994737ff_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-07db0c077ef7ef1087fd7df051bc4ce2_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-e4b154728aa19fda9a35cbd68327a322_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-50e08ba013fc8314870283f34d9c7350_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-e30572dd239444bbd002099e164b0ac5_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-a2a66153ca751c33d990813d969e9936_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-ad2f7f76db817bc72abd0f64bb499ef2_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-ef6ba6bf0149a15892febbcfe64eea5d_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-8c2679742e094294f57a03ea0ea210da_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-38a67653ca79b59f7f43f0ed57d459cf_720w.jpg?source=b6762063"],"favorite_count":45,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1931379423766085931}","attached_info":"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","action_card":false},{"id":"158_1753854240.476","type":"feed","offset":158,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753854240,"updated_time":1753854240,"target":{"id":"1916566825132594719","type":"answer","url":"https://api.zhihu.com/answers/1916566825132594719","author":{"id":"52916e1e7abfc6e7813cfc39a5e821aa","url":"https://api.zhihu.com/people/52916e1e7abfc6e7813cfc39a5e821aa","user_type":"people","url_token":"3-5-8-96-47","name":"耶律夷列","headline":"威廉三世一生黑","avatar_url":"https://picx.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":2313,"is_following":false,"is_followed":false},"created_time":1749725139,"updated_time":1749725139,"voteup_count":9204,"thanks_count":104,"comment_count":908,"is_copyable":true,"question":{"id":"419893789","type":"question","url":"https://api.zhihu.com/questions/419893789","author":{"id":"b89781c484c38a2a1fcd8c2eda5e044a","url":"https://api.zhihu.com/people/b89781c484c38a2a1fcd8c2eda5e044a","user_type":"people","url_token":"jin-sheng-78-60","name":"今生","headline":"for you ","avatar_url":"https://picx.zhimg.com/50/v2-3d4aa97374b77a03b72691289dc47f1a_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":101,"is_following":false,"is_followed":false},"title":"怎么看待北京大龄单身女突破80w？","created":1599468039,"answer_count":0,"follower_count":0,"comment_count":146,"bound_topic_ids":[202,922,5416],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://picx.zhimg.com/50/v2-0a19b483caf7efb2398c4f2243299fe5_720w.jpg?source=b6762063","excerpt":"很正常啊。现在女孩子的结婚要求，李鸿章都不敢答应！","excerpt_new":"很正常啊。现在女孩子的结婚要求，李鸿章都不敢答应！","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-b1bd757521c1071bbc78066b5b34bb3c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1028\" data-rawheight=\"6598\" data-qrcode-action=\"none\" data-original-token=\"v2-d25644675fbeb0118fea3ef6b3ab4619\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-f2fd0e8575929013d834b34e62b7b2f0_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1028\" data-original=\"https://pic3.zhimg.com/v2-b1bd757521c1071bbc78066b5b34bb3c_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"WH9iwMT0\"\u003e很正常啊。现在女孩子的结婚要求，李鸿章都不敢答应！\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":968509,"thumbnails":["https://picx.zhimg.com/50/v2-0a19b483caf7efb2398c4f2243299fe5_720w.jpg?source=b6762063"],"favorite_count":617,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1916566825132594719}","attached_info":"Ct4FCI+c4LWdz6m/ogEQBBoJNzMxODc4MjYzINPfqsIGKPRHMIwHQJ4BSjAKG1RTX1NPVVJDRV9CQVNJQ19JTkZPX1JFQ0FMTBIBMBgAIAA6CnsicmF3IjoiIn1aCDU1NTg3ODM4YiAwZWJhMWJlMzVkYTI5ZDI3ODNlYWU4MjIyZDNlNWU0OXITMTkxNjU2NjgyNTEzMjU5NDcxOYoBCTQxOTg5Mzc4OaoBCXJlY29tbWVuZMIBIDUyOTE2ZTFlN2FiZmM2ZTc4MTNjZmMzOWE1ZTgyMWFh8gEKCAwSBk5vcm1hbPIBKAgKEiQzMGE2OThkZS05ZDdmLTRjZjQtODE5MC0xYmEzYzU2YWE2M2XyAQYICxICMjeCAgCIArbJ786FM5ICIDUyOTE2ZTFlN2FiZmM2ZTc4MTNjZmMzOWE1ZTgyMWFhmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZdoCG1RTX1NPVVJDRV9CQVNJQ19JTkZPX1JFQ0FMTOgCAvoCC05PUk1BTF9GTE9XigMgY2Y2N2E1MjBjYjVlNDVkYzkxNTBkMWZiMGNlYjI3MzSaAw0KAnYyEAAaBW90aGVyqAO9jjvYAwDqAxFiYXNpY19pbmZvX3JlY2FsbPoDThIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgCEIQIGMYzIiN2Mi1kMjU2NDQ2NzVmYmViMDExOGZlYTNlZjZiM2FiNDYxOYAEAIgEAJIEBk5vcm1hbJoEATKgBACoBACwBAC6BAZtYW51YWzCBAMxNjDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAAAugLI/gQUAAAAAAAAAAIkFDTteRUxy0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFG5AGAKAGngGoBgCSAi4KCTczMTg3ODI2MxITMTkxNjU2NjgyNTEzMjU5NDcxORgEIgpJTUFHRV9URVhU","action_card":false},{"id":"159_1753854240.746","type":"feed","offset":159,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753854240,"updated_time":1753854240,"target":{"id":"1917922665974456797","type":"answer","url":"https://api.zhihu.com/answers/1917922665974456797","author":{"id":"667d03ff91797ea5970abd45e63970e7","url":"https://api.zhihu.com/people/667d03ff91797ea5970abd45e63970e7","user_type":"people","url_token":"zhang-shuo-66-3","name":"花儿好美啊","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-20a24709cbad579737978809076328d4_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":3489,"is_following":false,"is_followed":false},"created_time":1750048396,"updated_time":1750048396,"voteup_count":3769,"thanks_count":129,"comment_count":395,"is_copyable":true,"question":{"id":"620729078","type":"question","url":"https://api.zhihu.com/questions/620729078","author":{"id":"0f605887423f634680dfc9b9a3e9b1e6","url":"https://api.zhihu.com/people/0f605887423f634680dfc9b9a3e9b1e6","user_type":"people","url_token":"yt0bui","name":"知乎用户5LWvp4","headline":"","avatar_url":"https://pica.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":0,"is_following":false,"is_followed":false},"title":"有性瘾女朋友每天都要很多遍要不要分手?","created":1693975246,"answer_count":0,"follower_count":0,"comment_count":50,"bound_topic_ids":[2506,4758],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"你别说这问题我还真的研究过，16年的时候我毕业没多久就遇到过一例高三女生，实在没办法了她母亲带来我们中医科求治，当然我们也没有好的办法，只能告诉他们可以先吃药控制，中药开了一些疏肝解郁的，也许高中毕业压力减小就好了，后来回访确实高考完去上大学就自愈了，后来又遇到一例，一个男性说自己受不了了来我们这开药补补，已经被榨干了，后来聊天中发现他老婆全职带娃就一个孩子基本上都是他母亲带的，太闲了，后来去厂里…","excerpt_new":"你别说这问题我还真的研究过，16年的时候我毕业没多久就遇到过一例高三女生，实在没办法了她母亲带来我们中医科求治，当然我们也没有好的办法，只能告诉他们可以先吃药控制，中药开了一些疏肝解郁的，也许高中毕业压力减小就好了，后来回访确实高考完去上大学就自愈了，后来又遇到一例，一个男性说自己受不了了来我们这开药补补，已经被榨干了，后来聊天中发现他老婆全职带娃就一个孩子基本上都是他母亲带的，太闲了，后来去厂里…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"7fRgsBSP\"\u003e你别说这问题我还真的研究过，16年的时候我毕业没多久就遇到过一例高三女生，实在没办法了她母亲带来我们中医科求治，当然我们也没有好的办法，只能告诉他们可以先吃药控制，中药开了一些疏肝解郁的，也许高中毕业压力减小就好了，后来回访确实高考完去上大学就自愈了，后来又遇到一例，一个男性说自己受不了了来我们这开药补补，已经被榨干了，后来聊天中发现他老婆全职带娃就一个孩子基本上都是他母亲带的，太闲了，后来去厂里上班就好了，期间大哥是靠高科技电动工具和橡胶棍勉强活下去的。\u003c/p\u003e\u003cp data-pid=\"H6SeHXGt\"\u003e性瘾是疾病  学名性爱成瘾症/性冲动障碍症，是可以治愈的，男性患病比例大于女性，发作时得不到满足会有强烈的焦虑和痛苦感，没有能力控制性冲动，这个和普通的欲望不同，为了得到满足会做影响正常生活的事情，甚至不顾法律和道德约束，是一种心理疾病，属于心理障碍导致的行为失控，可以通过专业的心理医生引导治疗。\u003c/p\u003e\u003cp data-pid=\"zk1kvr_-\"\u003e性瘾的发病机制很复杂，遗传，多巴胺功能紊乱，情绪，原生家庭，青春期孤独，创伤应激等，遇到压力很大的事情和产生强烈负面，抑郁，烦躁，焦虑，伤心等情绪后更容易发作。目前治疗手段主要通过  认知行为疗法和5-羟色胺再摄取抑制剂 能得到很好的控制，是可以治愈的。\u003c/p\u003e\u003cp data-pid=\"1gkHBXtG\"\u003e目前题主可以通过鼓励她做一些事情分散注意力和多夸奖多赞同让她有自我和他人认同感，让她有被需要的心理暗示，带去看专业的心理医生。短时间实在吃不消就去某宝买跳蛋，筋膜枪，震动棒，硅胶丁丁之类的先保命要紧。\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":938221,"favorite_count":1149,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1917922665974456797}","attached_info":"CqoGCI+c4LWdz6m/ogEQBBoJNzMyNDQzODgzIIy9vsIGKLkdMIsDQJ8BSigKHVRTX1NPVVJDRV9ORUFSTElORV9DT05URU5UX1YyEgEwGAAgADoASisKFlRTX1NPVVJDRV9GRUVEUkVfTVNfVjISATAYACAAOgp7InJhdyI6IiJ9WgkxMDAyMTk1MTdiIDBlYmExYmUzNWRhMjlkMjc4M2VhZTgyMjJkM2U1ZTQ5chMxOTE3OTIyNjY1OTc0NDU2Nzk3igEJNjIwNzI5MDc4qgEJcmVjb21tZW5kwgEgNjY3ZDAzZmY5MTc5N2VhNTk3MGFiZDQ1ZTYzOTcwZTfyAQoIDBIGTm9ybWFs8gEoCAoSJDhkYjY1MGQ2LWYyMTUtNGU1Yi04MjdlLTM1YWJlOTM2ZTk1OPIBBggLEgIyN4ICAIgCtsnvzoUzkgIgNjY3ZDAzZmY5MTc5N2VhNTk3MGFiZDQ1ZTYzOTcwZTeaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCFlJldmlzaXRWYWx1ZVdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZdoCHVRTX1NPVVJDRV9ORUFSTElORV9DT05URU5UX1Yy6AIC+gILTk9STUFMX0ZMT1eKAyBjZjY3YTUyMGNiNWU0NWRjOTE1MGQxZmIwY2ViMjczNJoDDQoCdjIQABoFb3RoZXKoA+2hOdgDAPoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQGbWFudWFswgQDMTYwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAADAczy7P4EFAAAAAAAAAACJBQ07XkVMctM/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRuQBgCgBp8BqAYAkgIuCgk3MzI0NDM4ODMSEzE5MTc5MjI2NjU5NzQ0NTY3OTcYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"160_1753854240.79","type":"feed","offset":160,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753854240,"updated_time":1753854240,"target":{"id":"1933262215072449236","type":"article","url":"https://api.zhihu.com/articles/1933262215072449236","author":{"id":"b1c4d8a74f35bdea957199870267038c","url":"https://api.zhihu.com/people/b1c4d8a74f35bdea957199870267038c","user_type":"people","url_token":"mu-rong-qian-yu-71","name":"慕容千语","headline":"关注微信公众号：慕容千语；可领取一份精心整理的学习笔记","avatar_url":"https://pic1.zhimg.com/50/v2-51272f56655465a1ff1dd9377dcda9fe_l.jpg?source=b6762063","is_org":false,"gender":-1,"badge":[{"type":"identity_people","description":"互联网行业 员工"}],"followers_count":12152,"is_following":false,"is_followed":false},"title":"2025最新整理75道高频AI大模型面试真题（含详细解答）","comment_permission":"all","created":1753796429,"updated":1753796429,"voteup_count":0,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"最近是不是总刷到 “ 大模型岗位薪资翻倍” 的消息？打开招聘软件，满眼都是 “Transformer 原理”“RLHF 调优” 等专业术语，明明学过却讲不明白？别急！相信很多人都有过类似经历：投出大量简历后终于迎来面试，却被 “MoE 架构的理解” 问住；或是因答不出 “预训练数据清洗” 这类基础问题，与 offer 失之交臂。 并非能力不足，而是大模型领域更新太快，考点零散，网上资料要么过时要么碎片化。我花了很大功夫深度研究字节、…","excerpt_new":"最近是不是总刷到 “ 大模型岗位薪资翻倍” 的消息？打开招聘软件，满眼都是 “Transformer 原理”“RLHF 调优” 等专业术语，明明学过却讲不明白？别急！相信很多人都有过类似经历：投出大量简历后终于迎来面试，却被 “MoE 架构的理解” 问住；或是因答不出 “预训练数据清洗” 这类基础问题，与 offer 失之交臂。 并非能力不足，而是大模型领域更新太快，考点零散，网上资料要么过时要么碎片化。我花了很大功夫深度研究字节、…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"b9bcIU8Y\"\u003e最近是不是总刷到 “\u003cb\u003e大模型岗位薪资翻倍\u003c/b\u003e” 的消息？打开招聘软件，满眼都是 “\u003cb\u003eTransformer 原理\u003c/b\u003e”“\u003cb\u003eRLHF 调优\u003c/b\u003e” 等专业术语，明明学过却讲不明白？别急！相信很多人都有过类似经历：投出大量简历后终于迎来面试，却被 “MoE 架构的理解” 问住；或是因答不出 “预训练数据清洗” 这类基础问题，与 offer 失之交臂。\u003c/p\u003e\u003cp data-pid=\"eNg1UPiM\"\u003e\u003cb\u003e并非能力不足\u003c/b\u003e，而是大模型领域更新太快，考点零散，网上资料要么过时要么碎片化。我花了很大功夫深度研究字节、阿里等 50 多家企业面试题，\u003cb\u003e精选出 75 道高频题\u003c/b\u003e。从 “注意力机制实现” 等基础问题，到 “大模型部署优化”“AI 安全伦理” 等进阶与热点话题，每道题都有详尽解析，并标注了得分要点。\u003c/p\u003e\u003cp data-pid=\"HPtMZayJ\"\u003e无论你是想转行入门，还是计划跳槽，这份资料都能助你高效准备面试。毕竟，准备得越充分，面试时越有底气。现在就刷起来吧，下次面试让你从容应答，顺利拿下 offer！\u003c/p\u003e\u003cp data-pid=\"VfgLuhgr\"\u003e\u003cb\u003e接下来，我将为大家展示【文末含所有AI大模型资源获取地址】\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"yUXYS5uT\"\u003e\u003cb\u003e本篇内容目前只展现了部分面试题，更多的 AI 大模型资源及面试题可以点击下方这篇文章前往获取。\u003c/b\u003e\u003c/p\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1932105466470703631\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\"\u003e2025最全大模型全套资源免费获取：视频教程+书籍+行业报告+面试集锦+学习路线！！！\u003c/a\u003e\u003ch3\u003e\u003cb\u003e一、基础篇\u003c/b\u003e\u003c/h3\u003e\u003col\u003e\u003cli data-pid=\"Tv8THYLE\"\u003e目前主流的开源体系有哪些？\u003c/li\u003e\u003cli data-pid=\"2WxqG6Sw\"\u003eprefix Decoder 和 causal Decoder 和 Encoder-Decoder 区别是什么？\u003c/li\u003e\u003cli data-pid=\"oUT3BXxG\"\u003e大模型 LLM 的训练目标是什么？\u003c/li\u003e\u003cli data-pid=\"TsykQBVv\"\u003e涌现能力是啥原因？\u003c/li\u003e\u003cli data-pid=\"X0xyCWs7\"\u003e为何现在的大模型大部分是 Decoder-only 结构？\u003c/li\u003e\u003cli data-pid=\"hRlkq8Zk\"\u003e简单介绍一下大模型【LLMs】\u003c/li\u003e\u003cli data-pid=\"eesJYAXQ\"\u003e大模型【LLMs】后面跟的 175B、60B、540B 等指什么？\u003c/li\u003e\u003cli data-pid=\"mMZcumdA\"\u003e大模型【LLMs】具有什么优点？\u003c/li\u003e\u003cli data-pid=\"ur6mlHFV\"\u003e大模型【LLMs】具有什么缺点？\u003c/li\u003e\u003cli data-pid=\"_zyVCw2O\"\u003e大模型【LLMs】主要的应用领域有哪些？\u003c/li\u003e\u003cli data-pid=\"k_vqye1i\"\u003e大模型【LLMs】如何进行评估与优化？\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003e二、进阶篇\u003c/h3\u003e\u003col\u003e\u003cli data-pid=\"oEWUjXSo\"\u003e什么是生成式大模型？\u003c/li\u003e\u003cli data-pid=\"lB41_M69\"\u003e大模型是怎么让生成的文本丰富而不单调的呢？\u003c/li\u003e\u003cli data-pid=\"41qpchWf\"\u003e什么是 LLMs 复读机问题？\u003c/li\u003e\u003cli data-pid=\"L_stOhcO\"\u003e为什么会出现 LLMs 复读机问题？\u003c/li\u003e\u003cli data-pid=\"YaXu6ACS\"\u003e如何缓解 LLMs 复读机问题？\u003c/li\u003e\u003cli data-pid=\"sDR99jd0\"\u003ellama 输入句子长度理论上可以无限长吗？\u003c/li\u003e\u003cli data-pid=\"yXlJaW5b\"\u003e什么情况用 Bert 模型，什么情况用 LLaMA、ChatGLM 类大模型？\u003c/li\u003e\u003cli data-pid=\"3PxQe1t6\"\u003e各个专业领域是否需要各自的大模型来服务？\u003c/li\u003e\u003cli data-pid=\"EqLTYsDZ\"\u003e如何让大模型处理更长的文本？\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003e三、微调篇\u003c/h3\u003e\u003col\u003e\u003cli data-pid=\"kH8ZFi_x\"\u003e如果想要在某个模型基础上做全参数微调，究竟需要多少显存？\u003c/li\u003e\u003cli data-pid=\"8wqcWudE\"\u003e为什么SFT之后感觉LLM傻了?\u003c/li\u003e\u003cli data-pid=\"AZL_xmZW\"\u003eSFT 指令微调数据如何构建?\u003c/li\u003e\u003cli data-pid=\"yofix0RE\"\u003e领域模型Continue PreTrain 数据选取？\u003c/li\u003e\u003cli data-pid=\"xTS7WVYp\"\u003e领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？\u003c/li\u003e\u003cli data-pid=\"T7e_4U7y\"\u003e领域模型Continue PreTrain ，如何让模型在预训练过程中就学习到更多的知识？\u003c/li\u003e\u003cli data-pid=\"45nGDh14\"\u003e进行SFT操作的时候，基座模型选用Chat还是Base?\u003c/li\u003e\u003cli data-pid=\"xuf8K1Y9\"\u003e领域模型微调指令\u0026amp;数据输入格式要求？\u003c/li\u003e\u003cli data-pid=\"WzdOuVzj\"\u003e领域模型微调领域评测集构建？\u003c/li\u003e\u003cli data-pid=\"CY2muis2\"\u003e领域模型词表扩增是不是有必要的？\u003c/li\u003e\u003cli data-pid=\"zgoH0pHu\"\u003e如何训练自己的大模型？\u003c/li\u003e\u003cli data-pid=\"Bv34ceb_\"\u003e训练中文大模型有啥经验？\u003c/li\u003e\u003cli data-pid=\"hU0FkCmO\"\u003e指令微调的好处？\u003c/li\u003e\u003cli data-pid=\"z7tQnIdf\"\u003e预训练和微调哪个阶段注入知识的？\u003c/li\u003e\u003cli data-pid=\"hVFIN2l2\"\u003e想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？\u003c/li\u003e\u003cli data-pid=\"KvIGw-wf\"\u003e多轮对话任务如何微调模型？\u003c/li\u003e\u003cli data-pid=\"8U4lC9Wa\"\u003e微调后的模型出现能力劣化，灾难性遗忘是怎么回事？\u003c/li\u003e\u003cli data-pid=\"31DyJo_p\"\u003e微调模型需要多大显存？\u003c/li\u003e\u003cli data-pid=\"7b7bdy1Z\"\u003e大模型LLM进行SFT操作的时候在学习什么？\u003c/li\u003e\u003cli data-pid=\"nD0mKcDW\"\u003e预训练和SFT操作有什么不同\u003c/li\u003e\u003cli data-pid=\"En3X_D7t\"\u003e样本量规模增大，训练出现OOM错\u003c/li\u003e\u003cli data-pid=\"FJb7bKz-\"\u003e大模型LLM进行SFT 如何对样本进行优化？\u003c/li\u003e\u003cli data-pid=\"x_NJqmmf\"\u003e模型参数迭代实验\u003c/li\u003e\u003cli data-pid=\"aNEG_VXj\"\u003e微调大模型的一些建议\u003c/li\u003e\u003cli data-pid=\"p-Of9AF8\"\u003e微调大模型时，如果 batch size 设置太小会出现什么问题？\u003c/li\u003e\u003cli data-pid=\"pFSlfSPw\"\u003e微调大模型时，如果 batch size 设置太大会出现什么问题？\u003c/li\u003e\u003cli data-pid=\"i4TGvVOb\"\u003e微调大模型时, batch size 如何设置问题？\u003c/li\u003e\u003cli data-pid=\"1WqK9zNr\"\u003e微调大模型时, 优化器如何？\u003c/li\u003e\u003cli data-pid=\"HQeTLRw4\"\u003e哪些因素会影响内存使用？\u003c/li\u003e\u003cli data-pid=\"2ilk5pJ3\"\u003e进行领域大模型预训练应用哪些数据集比较好？\u003c/li\u003e\u003cli data-pid=\"wuA4XRmJ\"\u003e用于大模型微调的数据集如何构建？\u003c/li\u003e\u003cli data-pid=\"Qua7JVjP\"\u003e大模型训练loss突刺原因和解决办法\u003c/li\u003e\u003cli data-pid=\"kkZzCFqV\"\u003e大模型训练loss突刺是什么？\u003c/li\u003e\u003cli data-pid=\"0Itd2cO7\"\u003e为什么大模型训练会出现loss突刺？\u003c/li\u003e\u003cli data-pid=\"g4JT7qn1\"\u003e大模型训练loss突刺如何解决？\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003e四、RAG经验篇\u003c/h3\u003e\u003col\u003e\u003cli data-pid=\"BN6qdGDV\"\u003eLLMs 已经具备了较强能力了，存在哪些不足点? \u003c/li\u003e\u003cli data-pid=\"VsY_pvNa\"\u003e什么是 RAG?\u003c/li\u003e\u003cli data-pid=\"RMJJzqn8\"\u003e使用 RAG 的好处?\u003c/li\u003e\u003cli data-pid=\"RcnL_oqc\"\u003eRAG V.S. SFT \u003c/li\u003e\u003cli data-pid=\"bglk_UVX\"\u003e介绍一下 RAG 典型实现方法？ \u003c/li\u003e\u003cli data-pid=\"22CXbIus\"\u003e介绍一下 RAG 典型案例？\u003c/li\u003e\u003cli data-pid=\"Ab2oycXQ\"\u003eRAG 存在什么问题？\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003e五、Agent经验篇\u003c/h3\u003e\u003col\u003e\u003cli data-pid=\"vWKY6p57\"\u003e什么是大模型（LLMs）agent？ \u003c/li\u003e\u003cli data-pid=\"ZJpTjJ31\"\u003e大模型（LLMs）agent 有哪些部分组成？\u003c/li\u003e\u003cli data-pid=\"HirUYseJ\"\u003e大模型（LLMs）agent 主要利用了大模型哪些能力？\u003c/li\u003e\u003cli data-pid=\"VRwsKz4s\"\u003e结合代码讲解大模型（LLMs）agent 思路？ \u003c/li\u003e\u003cli data-pid=\"Zfquw0Bd\"\u003e如何给LLM注入领域知识？ \u003c/li\u003e\u003cli data-pid=\"6EQ-oFe0\"\u003e常见LLM Agent框架或者应用有哪些？\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003e六、多模态篇\u003c/h3\u003e\u003col\u003e\u003cli data-pid=\"yomWRR3b\"\u003e最近关注的论文，多模态视觉大模型(CLIP,DALLE)？\u003c/li\u003e\u003cli data-pid=\"RygQcyV3\"\u003eblip2的架构，优势和之前多模态模型的区别？ \u003c/li\u003e\u003cli data-pid=\"frlGLruY\"\u003e多模态融合后，怎样知道最终结果受哪种模态影响更大？\u003c/li\u003e\u003cli data-pid=\"grsDV1-Z\"\u003e多模态中常见的SOTA模型有哪些？\u003c/li\u003e\u003cli data-pid=\"XD1Zl0U8\"\u003e介绍一下stable diffusion的原理？\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"VF6j0D10\"\u003e\u003cb\u003e本篇内容目前只展现了部分面试题，更多的 AI 大模型资源及面试题可以点击下方这篇文章前往获取。\u003c/b\u003e\u003c/p\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1932105466470703631\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\"\u003e2025最全大模型全套资源免费获取：视频教程+书籍+行业报告+面试集锦+学习路线！！！\u003c/a\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e\u003cbr/\u003e \u003c/h3\u003e","is_labeled":false,"visited_count":54,"favorite_count":2,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1933262215072449236}","attached_info":"CqsGCI+c4LWdz6m/ogEQBxoJMjYwOTQwNTAyIM2eo8QGKAAwAECgAUokChlUU19TT1VSQ0VfV0FSTV9VUF9OT1JNQUwyEgEwGAAgADoASi0KIlRTX1NPVVJDRV9XQVJNX1VQX0hJR0hfSU5URVJBQ1RJT04SATAYACAAOgBiIDBlYmExYmUzNWRhMjlkMjc4M2VhZTgyMjJkM2U1ZTQ5chMxOTMzMjYyMjE1MDcyNDQ5MjM2qgEJcmVjb21tZW5kwgEgYjFjNGQ4YTc0ZjM1YmRlYTk1NzE5OTg3MDI2NzAzOGPyAQoIDBIGTm9ybWFs8gEoCAoSJGVhMDBkYjFmLWVmMzYtNDcxMy05NjQ3LWNhMjk3ZTFhMWIwY/IBBggLEgIyN4ICAIgCtsnvzoUzkgIgYjFjNGQ4YTc0ZjM1YmRlYTk1NzE5OTg3MDI2NzAzOGOaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIaQ29udGVudFdhcm1VcElzb2xhdGlvblJ1bGXKAhJUaGVtZUlzb2xhdGlvblJ1bGXKAhxCYXllc0ZpcnN0TGV2ZWxJc29sYXRpb25SdWxlygIYQ29udGVudFdhcm1VcEJyZWFrSW5SdWxl2gIZVFNfU09VUkNFX1dBUk1fVVBfTk9STUFMMugCAvoCC05PUk1BTF9GTE9XigMgY2Y2N2E1MjBjYjVlNDVkYzkxNTBkMWZiMGNlYjI3MzSaAw0KAnYyEAAaBW90aGVyqAM22AMA6gMedGV4dF9oaWdoX2ludGVyYWN0aW9uX3JlY2FsbGVy+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATKgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAoOjJkz+BBQAAAAAAAAAAiQUNO15FTHLTP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUbkAYAoAagAagGAZICLgoJMjYwOTQwNTAyEhMxOTMzMjYyMjE1MDcyNDQ5MjM2GAciCklNQUdFX1RFWFQ=","action_card":false},{"id":"161_1753854240.336","type":"feed","offset":161,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753854240,"updated_time":1753854240,"target":{"id":"1930929473118778239","type":"answer","url":"https://api.zhihu.com/answers/1930929473118778239","author":{"id":"a8ae2401ca190282f7e47a0e783a1873","url":"https://api.zhihu.com/people/a8ae2401ca190282f7e47a0e783a1873","user_type":"people","url_token":"31-47-38-49","name":"挽歌学长","headline":"985退学创业，烽狂创客创始人，全国旅居中","avatar_url":"https://pic1.zhimg.com/50/v2-3383fc7f536c034094d29cab74bdabfc_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":19,"is_following":false,"is_followed":false},"created_time":1753149461,"updated_time":1753833636,"voteup_count":23,"thanks_count":0,"comment_count":2,"is_copyable":true,"question":{"id":"50343728","type":"question","url":"https://api.zhihu.com/questions/50343728","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://picx.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"你有什么值得分享的高效学习方法？","created":1473002490,"answer_count":0,"follower_count":0,"comment_count":50,"bound_topic_ids":[119,1351,2566,3074,5361],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"1、先说一下我的学习战绩:①从小学二年级到初中毕业，几乎每一次考试都是全班第一名。 ②数学、物理、化学、生物、历史、地理，我正常发挥基本上都是满分，语文、英语、政治都是接近满分。 ③高中没有之前那么努力，但是也是班级前几名的位置，高考正常发挥，是前五高校。 ④目前我没有遇到一门学科，是我学不会，学不好的。哪怕是大学。 ⑤除了学科成绩，其他领域也是如此。我创业没有老师教我，一年时间，从纯小白，到自媒体变…","excerpt_new":"1、先说一下我的学习战绩:①从小学二年级到初中毕业，几乎每一次考试都是全班第一名。 ②数学、物理、化学、生物、历史、地理，我正常发挥基本上都是满分，语文、英语、政治都是接近满分。 ③高中没有之前那么努力，但是也是班级前几名的位置，高考正常发挥，是前五高校。 ④目前我没有遇到一门学科，是我学不会，学不好的。哪怕是大学。 ⑤除了学科成绩，其他领域也是如此。我创业没有老师教我，一年时间，从纯小白，到自媒体变…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"ZwrnjJQK\"\u003e\u003cb\u003e1、先说一下我的学习战绩\u003c/b\u003e:\u003c/p\u003e\u003cp data-pid=\"ypbITXuw\"\u003e①从小学二年级到初中毕业，几乎每一次考试都是全班第一名。\u003c/p\u003e\u003cp data-pid=\"BRqxTCgq\"\u003e②数学、物理、化学、生物、历史、地理，我正常发挥基本上都是满分，语文、英语、政治都是接近满分。\u003c/p\u003e\u003cp data-pid=\"Jqk0dAUc\"\u003e③高中没有之前那么努力，但是也是班级前几名的位置，高考正常发挥，是前五高校。\u003c/p\u003e\u003cp data-pid=\"fDxxe6jw\"\u003e④目前我没有遇到一门学科，是我学不会，学不好的。哪怕是大学。\u003c/p\u003e\u003cp data-pid=\"eJkJib-V\"\u003e⑤除了学科成绩，其他领域也是如此。我创业没有老师教我，一年时间，从纯小白，到自媒体变现百万，商业、情商、财商……我都是自己一路学过来的。\u003c/p\u003e\u003cp data-pid=\"pMb5Nm-Z\"\u003e⑥最近的案例。我自学了三天台球，在广州时候，助教说我看起来像学了几个月的。\u003c/p\u003e\u003cp data-pid=\"A5ZCky5_\"\u003e⑦包括，打游戏。我人生第一次打游戏，是连键盘都不会按的真小白，一切对我来说都是陌生的。《黑神话悟空》总共花了不到半个月时间，打到了五周目通关了，已经卸载。\u003c/p\u003e\u003cp data-pid=\"c750W9XS\"\u003e⑧还有很多其他案例，比如篮球，吉他……\u003c/p\u003e\u003cp data-pid=\"-sb0xExq\"\u003e所有我想学的技能或者领域，除非它真的有天赋限制（比如身高），否则我都能拿到我想要的结果，而且，绝对比身边百分之九十九的人快、准、狠。\u003c/p\u003e\u003cp data-pid=\"2GA3JaNm\"\u003e所以，今天分享的虽然是学习方法论，\u003cb\u003e但是不限于大家日常说的学科学习，更是我自己保持终身快速成长的底层逻辑。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"bC0lHhJh\"\u003e回到正题，具体到某一个领域的学习上，我是怎么开始的？以自媒体为例:我刚刚接触自媒体赛道的时候，我是如何实现快速晋升的？\u003c/p\u003e\u003cp data-pid=\"oJDA8F_1\"\u003e\u003cb\u003e2、我的学习方法\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"HVJJxwZ1\"\u003e①\u003cb\u003e快速探索、了解、提炼规则规律\u003c/b\u003e \u003c/p\u003e\u003cp data-pid=\"6lhRWXrV\"\u003e需要:大量吸收能接触到的所有信息，并且进行甄别、判断。 刚刚进圈的时候，打开了一个新的天地，我那个时候，废寝忘食的汲取我能接触到的所有关于这个新词汇“自媒体”相关的一切信息。 \u003c/p\u003e\u003cp data-pid=\"B1RInCTG\"\u003e没有人带，甚至还没有进圈。 我便开始疯狂的去各大自媒体平台上检索各种相关信息。\u003c/p\u003e\u003cp data-pid=\"lLgbVwBl\"\u003e比如：公众号变现的多少种模式，公众号如何开通流量主，公众号的推荐机制，排版软件，自媒体相关的提效神器。 ……\u003c/p\u003e\u003cp data-pid=\"KjMhr1Yk\"\u003e只要是不懂的，都去检索，都去研究，都去认识，一天不够？十天！十天不够，一个月，废寝忘食，直到自己对这个新的世界有了相对清晰的认识。 \u003c/p\u003e\u003cp data-pid=\"H4bM7yKX\"\u003e了解的过程中，我会有意识的根据自己过往的经历经验，去判断，去甄别，去筛选，提炼总结一些“相对正确”的规则或者规律。 这样一来，我就不再像一个“瞎子”，幸存在这个完全陌生的领域，相反，我还在这个新的世界里面，看到了现实世界里面碰不到的机遇。\u003c/p\u003e\u003cp data-pid=\"2PzNRJaF\"\u003e所以，想要提高自己的学习能力：\u003c/p\u003e\u003cp data-pid=\"1iOtJFOA\"\u003e\u003cb\u003e第一步是提高自己主动探索和获取新信息的能力，其实这也是差距产生的第一步。\u003c/b\u003e 很多人习惯性的被动接收信息，被动接受教育，但事实上，凡是被动的输入和教育，都是非常有限的，只能保证你的平庸，不能保证你的卓越。\u003c/p\u003e\u003cp data-pid=\"K3QR6tGk\"\u003e学习台球，打游戏，照样是如此。\u003c/p\u003e\u003cp data-pid=\"uJioG4Hv\"\u003e你要学打台球？能不能先把台球的规则花最少的时间，背一遍？ 你要学打游戏？能不能花一天时间，先把游戏相关的所有规则了解一遍？ 不要一上来就是莽，就是冲，就是干，你都不知道做什么是对的，你大概率就是会处处碰壁，最后失意而归。 \u003c/p\u003e\u003cp data-pid=\"HaoMxnlh\"\u003e永远不要等着社会和现实来教育你，不然你只有吃不完的苦，咽不尽的辛酸。 \u003c/p\u003e\u003cp data-pid=\"7hchcCr7\"\u003e\u003cb\u003e第二步，实操验证真假是非，取其精华，去其糟粕，把有效且高效的解决方法内化成为自己的习惯和思维\u003c/b\u003e。 光是知道是不够的，你还需要去论证它。 \u003cb\u003e学一门学科，在你刷了几十套题目之前，你是找不到节奏和规律的。 \u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Jf3QlbtK\"\u003e\u003cb\u003e“知道“到“做到”之间的鸿沟，是第二个拉开差距的地方。\u003c/b\u003e 比如，我刚刚接触自媒体的时候，我不单单是疯狂的寻找和输入相关的信息，我同时还在验证这些信息的真实性、时效性、准确性等等。\u003c/p\u003e\u003cp data-pid=\"pewdzPJH\"\u003e最简单的案例，新接触了一个新的工具，我就去用一下试试，再和其他的工具对比一下，到底是不是真好用。 \u003cb\u003e那么在学科学习里面，也是如此，新学到一个解题方案，别光听了就觉得自己懂了，要真正做一套题，验证自己是否能独立的快速的解决它们。 \u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"f3kC4szN\"\u003e当然，有优质的信息源，也有劣质的信息源，我们通过测试甄别出来之后，还需优胜劣汰，取其精华，去其糟粕。 真要没有具体方向的一通乱学，一通乱搞，出结果还是非常难的。\u003c/p\u003e\u003cp data-pid=\"Ms5eREFf\"\u003e第一个是效率问题，第二个是耐力问题。\u003c/p\u003e\u003cp data-pid=\"8qjL-_A6\"\u003e②提效省力:\u003cb\u003e对标模仿，看见差距，弥补差距\u003c/b\u003e 你需要:向比你强的人学习，复制别人的成功路径。 看清楚你和别人的差距所在，然后找到弥补差距的解决方案。 \u003c/p\u003e\u003cp data-pid=\"NKhsUGSP\"\u003e我是个台球新手，找个助教➕陪练，行不行？ 我是个自媒体新手，找个自媒体前辈手把手教我，行不行？ 另外，我是个学渣，我向学霸学习行不行？ 而且，注意，\u003cb\u003e这里的学习，不仅仅是找对方提问，教学，更多要看别人怎么做的，为什么，源头在哪？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"E0EnqNym\"\u003e并且，分析一下，差距产生的地方在哪里，然后逐个击破！ \u003cb\u003e具体到具体的差距上去，一个又一个的解决，一个又一个的弥补\u003c/b\u003e。 把比你牛逼的人的思维、技能、能力都学过来，从来不要指望对方教会你。 记住:\u003cb\u003e大神从来不是用来膜拜的，而是用来拆解学习超越的。 \u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"vznZLE7g\"\u003e\u003cb\u003e③学会给自己制造正反馈\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"c3qipiH1\"\u003e你需要：借助一种向上向善的人性，对抗另外一种向下向恶的人性。\u003c/p\u003e\u003cp data-pid=\"ljaS91Wh\"\u003e其实，很多人不是做不好，学不会，而是等不及。\u003c/p\u003e\u003cp data-pid=\"7pXBVWnN\"\u003e学英语，学了一个周，没看到什么提升，好吧，我没天赋，我没希望，要不还是躺平吧…… \u003c/p\u003e\u003cp data-pid=\"_YlE4YnE\"\u003e学自媒体，学了一个月，发了几百条朋友圈，写了几十条公众号，发了几十条短视频……\u003c/p\u003e\u003cp data-pid=\"mCLH8yea\"\u003e结果没有获得什么反馈，没有流量，也没有变现。 所以，我不适合做自媒体，我还是放弃吧…… 参加一个项目训练营，按照要求发了几条，没有看到收益，也不去研究原因，不浪费时间了，撤！ 真说实话，不是我看不起谁，而是真的，\u003cb\u003e绝大多数人，在耐力上、在执行力上、在主动性上、在努力程度上……都是垃圾！ \u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"ZUuDIswy\"\u003e\u003cb\u003e要啥啥没有，问啥啥不会，叫学学不进，吃不来一点亏，受不了一点寂寞，扛不住一点痛苦。 要么混吃等死，要么杞人忧天，要么操碎了别人的心，就是没顾得上规划自己的路。\u003c/b\u003e 这就是正常人的常态。（随机吐槽几句）\u003c/p\u003e\u003cp data-pid=\"wMzVEjDj\"\u003e回到正题，那么，怎么才能让自己“等得及”？ 其实这就是最难的一个点。 我小学没学过英语，初一不及格，初中毕业，接近满分。 我高中进的尖子班，英语听力很垃圾，基础很薄弱，比不过同班同学，高一刚进班，英语成绩，稳定在倒数几位。 到了高三，我照样英语提升到了全班前列，正常发挥140-145。 \u003c/p\u003e\u003cp data-pid=\"6mMh0uVc\"\u003e你说我成绩好才一直有正反馈，能坚持？ 不是，得反过来:是因为我一直都在给自己制造正反馈，我才能坚持到，将成绩一步步提升至我想要的结果的时候。 \u003cb\u003e所以，你们明白了吗？ 想要持久，你得学会给自己制造快乐，学会自己生产正反馈，哪怕是一件很难的事，哪怕是一件很枯燥的事情。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"vLwDsmh7\"\u003e 我记得我之前分享过:\u003c/p\u003e\u003cp data-pid=\"WQwg5zdq\"\u003e\u003cb\u003e不要强制束缚自己的人性。能对抗人性的，只有另外一种人性。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"CpE4uT_5\"\u003e是的，我也讨厌每天枯燥的学习。是的，我也向往操场上的欢声笑语。是的，我也喜欢看小说，我也喜欢看电视，在课本之外，我也有无数喜欢的事物。 \u003c/p\u003e\u003cp data-pid=\"RfFqxp-F\"\u003e但是，不好意思，我现在得选择学习，非常认真的、刻苦的学习。 我是怎么告诉自己的？\u003cb\u003e你必须要努力，否则你这辈子就只能留在贫困的农村，走不出贵州的大山，看不见更宽广的世界，也遇不到更好的人。\u003c/b\u003e 我是怎么让自己喜欢上学习的？你学习成绩好了，大家都会认可你，老师会夸奖你，同学会仰慕你，父母会以你为骄傲。 后来的后来，\u003cb\u003e摆烂的理由很多，颓废的诱惑很多，但是，我给自己努力的理由更多，我给自己精进的诱惑更多……\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"2qxSaFEs\"\u003e所以，我不是抗拒，而是内心深处在向往，发自内心的充满学习的动力、 我做了什么，其实现在回顾起来：\u003cb\u003e本质上就是调动另外一种恐惧来压制了面临孤独的恐惧，调动了另外一种欲望来压制了放飞自己的欲望。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"0HOSfLXB\"\u003e仅此而已。 支配别人一向被认为是权力的象征，但是我现在告诉你:如果你连自己都支配不了，那么你也活该被别人支配。  说得比较本质，不确定大家能否理解。 如果大家有相关问题，可以在评论区提出来，我给大家我的答案。 \u003c/p\u003e\u003cp data-pid=\"wQnd4V0D\"\u003e（码字不易，点个赞支持一下吧，收藏比点赞多这么多(ﾟ⊿ﾟ)ﾂ）\u003c/p\u003e\u003cp data-pid=\"fhoZW5bO\"\u003eover。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":1050,"favorite_count":89,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1930929473118778239}","attached_info":"CrcFCI+c4LWdz6m/ogEQBBoJNzM4MjI1Nzk0IJXg+8MGKBcwAkChAUooCh1UU19TT1VSQ0VfTkVBUkxJTkVfQ09OVEVOVF9WMhIBMBgAIAA6AEooCh1UU19TT1VSQ0VfSU5URVJFU1RfV09SRF9NRVJHRRIBMBgAIAA6AFoIMTIzNTQxNzZiIDBlYmExYmUzNWRhMjlkMjc4M2VhZTgyMjJkM2U1ZTQ5chMxOTMwOTI5NDczMTE4Nzc4MjM5igEINTAzNDM3MjiqAQlyZWNvbW1lbmTCASBhOGFlMjQwMWNhMTkwMjgyZjdlNDdhMGU3ODNhMTg3M/IBCggMEgZOb3JtYWzyASgIChIkMzgzYjMzNmUtZGI3Yi00MzNlLWFhYjAtOTg0NDA0YTE0Zjdj8gEGCAsSAjI3ggIAiAK2ye/OhTOSAiBhOGFlMjQwMWNhMTkwMjgyZjdlNDdhMGU3ODNhMTg3M5oCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXaAh1UU19TT1VSQ0VfTkVBUkxJTkVfQ09OVEVOVF9WMugCA/oCC05PUk1BTF9GTE9XigMgY2Y2N2E1MjBjYjVlNDVkYzkxNTBkMWZiMGNlYjI3MzSaAw0KAnYyEAAaBW90aGVyqAOaCNgDAPoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAEBvy6E/gQUAAAAAAAAAAIkFDTteRUxy0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFG5AGAKAGoQGoBgCSAi4KCTczODIyNTc5NBITMTkzMDkyOTQ3MzExODc3ODIzORgEIgpJTUFHRV9URVhU","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=161\u0026desktop=true\u0026end_offset=161\u0026page_number=28\u0026session_token=0eba1be35da29d2783eae8222d3e5e49","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=161\u0026desktop=true\u0026end_offset=161\u0026page_number=28\u0026session_token=0eba1be35da29d2783eae8222d3e5e49","totals":0},"fresh_text":"推荐已更新"}
