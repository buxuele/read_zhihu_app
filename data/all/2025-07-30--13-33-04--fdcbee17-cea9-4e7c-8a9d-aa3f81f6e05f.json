{"data":[{"id":"84_1753853602.799","type":"feed","offset":84,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853602,"updated_time":1753853602,"target":{"id":"1926940511417140051","type":"article","url":"https://api.zhihu.com/articles/1926940511417140051","author":{"id":"c273edb4a62f7a8d9373081847a6620d","url":"https://api.zhihu.com/people/c273edb4a62f7a8d9373081847a6620d","user_type":"people","url_token":"wuzesheng","name":"时间的朋友札记","headline":"学习与成长 =\u0026gt; 公众号: 时间的朋友札记","avatar_url":"https://pic1.zhimg.com/50/v2-840c54771ba62519376346ac6395300a_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"互联网行业 从业人员"}],"followers_count":1068,"is_following":false,"is_followed":false},"title":"Andrej Karpathy: AI  时代的软件","image_url":"https://picx.zhimg.com/v2-c729b0defc98d9529d356cc71b69c3e2.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1752198653,"updated":1752198653,"voteup_count":4,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"最近看了一个 Andrej Karpathy(AK 大神) 在 YC 创业营的分享视频，可以说是今年看到的最有洞察力的 AI 相关分享，质量非常高。今天这篇文章，主要分享一些看完这个视频后的收获。对于想深入了解的朋友，强烈建议大家看原视频，在 B 站或者 Youtube 都有，名字叫《Software in the era of AI》。 软件发展的三个阶段AK 把软件的发展分为了三个阶段： 第一个阶段，是软件 1.0 阶段，特点是\u0026#34;代码\u0026#34;，软件的主要形态是在计算机上运行…","excerpt_new":"最近看了一个 Andrej Karpathy(AK 大神) 在 YC 创业营的分享视频，可以说是今年看到的最有洞察力的 AI 相关分享，质量非常高。今天这篇文章，主要分享一些看完这个视频后的收获。对于想深入了解的朋友，强烈建议大家看原视频，在 B 站或者 Youtube 都有，名字叫《Software in the era of AI》。 软件发展的三个阶段AK 把软件的发展分为了三个阶段： 第一个阶段，是软件 1.0 阶段，特点是\u0026#34;代码\u0026#34;，软件的主要形态是在计算机上运行…","preview_type":"default","preview_text":"","column":{"id":"c_1768208688031289345","type":"column","url":"https://api.zhihu.com/columns/c_1768208688031289345","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pica.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"AI技术观察","imageUrl":"https://pic1.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b","comment_permission":"private","intro":"我们对技术的理解，对技术本身更加重要","updated":1717723466,"is_following":false},"content":"\u003cp data-pid=\"2kYmiSZ1\"\u003e最近看了一个 Andrej Karpathy(AK 大神) 在 YC 创业营的分享视频，可以说是今年看到的最有洞察力的 AI 相关分享，质量非常高。今天这篇文章，主要分享一些看完这个视频后的收获。对于想深入了解的朋友，强烈建议大家看原视频，在 B 站或者 Youtube 都有，名字叫《Software in the era of AI》。\u003c/p\u003e\u003ch2\u003e软件发展的三个阶段\u003c/h2\u003e\u003cp data-pid=\"6tAyZLIT\"\u003eAK 把软件的发展分为了三个阶段：\u003c/p\u003e\u003cp data-pid=\"SHwQdXuj\"\u003e第一个阶段，是软件 1.0 阶段，特点是\u0026#34;代码\u0026#34;，软件的主要形态是在计算机上运行的代码，对应的时间是从 20 世纪 40 年代开始。\u003c/p\u003e\u003cp data-pid=\"VMzX5NBQ\"\u003e第二个阶段，是软件 2.0 阶段，特点是\u0026#34;权重\u0026#34;、或者\u0026#34;参数\u0026#34;，软件的主要形态是各种神经网络，对应的时间是从 2012 年左右开始。\u003c/p\u003e\u003cp data-pid=\"dmNzcnR9\"\u003e第三个阶段，是软件 3.0 阶段，特点是\u0026#34;提示词\u0026#34;，软件的主要形态是大语言模型 (LLM)，对应的时间是从 2019 年左右开始。\u003c/p\u003e\u003cp data-pid=\"iJNyiFFO\"\u003e当下，我们就是处在软件 3.0 的阶段，LLM 是可编程的神经网络，而我们则通过提示词，来让 LLM 完成各种各样的任务。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-4188a8754b498747706b028eea4c500e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-original-token=\"v2-4188a8754b498747706b028eea4c500e\" class=\"content_image\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"CwYTUeeV\"\u003e上图是 AK 给的一个例子，对于给定的一组词，进行“情绪分类”。软件 1.0 时代的做法，是写代码，分词查词表计算打分；软件 2.0 时代的做法，是准备数据集训练神经网络，然后利用训练所得的参数计算分类结果；软件 3.0 时代的做法，就是直接丢给 LLM 一段自然语言的提示词，然后就得到了结果。\u003c/p\u003e\u003ch2\u003e三个理解大模型的角度\u003c/h2\u003e\u003cp data-pid=\"0OtlIoFN\"\u003eAK 在分享了三个理解大模型的角度，通过这三个角度，可以让我们对大模型有一个更全面的认识：\u003c/p\u003e\u003cp data-pid=\"X6UcaekH\"\u003e第一个角度，是从“公用事业”的角度，比如可以类比“电力”。他们共同的特点是：\u003c/p\u003e\u003cp data-pid=\"XIhL-TuE\"\u003e（1）前期都有比较大的资本投入（CapEx），电力需要通过大投入来构建基础的电网，大模型需要通过大投入来训练基础模型；\u003c/p\u003e\u003cp data-pid=\"ZXwrqU3g\"\u003e（2）他们也都需要有持续的运营投入（OpEx），电网要持续不断地发电出来，并且要通过网络传输给终端用户，供他们使用。而大模型要持续提供服务，也需要持续不断的 GPU 算力和通过网络的传输；\u003c/p\u003e\u003cp data-pid=\"nMTq-g2C\"\u003e（3）他们的边际成本都不是 0，使用上都需要按“量”计费，电是通过“度数”计费，而大模型是通过使用的\u0026#34;token\u0026#34;来计费；\u003c/p\u003e\u003cp data-pid=\"4oIzAvuf\"\u003e（4）电的使用可以通过 Transfer Switch 方便地切换，使用电网的电、太阳能发电、或者电池供电等。LLM 也可以通过 OpenRouter，方便地切换使用不同的供应商、不同的模型；\u003c/p\u003e\u003cp data-pid=\"IKYgdxJc\"\u003e（5）如果电力设施出故障了，我们的灯就会不亮了，我们会受到直接的影响；如果大模型出故障了，我们软件的智能也就“消退”了，也会受到比较大的影响。\u003c/p\u003e\u003cp data-pid=\"s2d_AjaC\"\u003e第二个角度，是从\u0026#34;高科技工厂\u0026#34;的角度，比如\u0026#34;晶圆厂\u0026#34;。他们的共同特点是：\u003c/p\u003e\u003cp data-pid=\"t91KQGyo\"\u003e（1）投资门槛都很高，都需要有比较高的资本性支出（CapEx）才能启动来搭建一座晶圆厂、或是训练一个大模型；\u003c/p\u003e\u003cp data-pid=\"FBtPhFjb\"\u003e（2） 技术门槛比较高、发展很迅速。全球顶尖的晶圆厂只有以台积电为首的屈指可数的几家，全球顶尖的大模型厂商也只有以 OpenAI 为首的屈指可数的几家；\u003c/p\u003e\u003cp data-pid=\"lWgZBzpD\"\u003e（3） 大家使用 Nvidia 的 GPU 来训练大模型，就有点像自己无晶圆厂，找台积电来代工；而像 Google 这种用自己研发的 TPU 来训练大模型，则像自有晶圆厂来为自己生产芯片的模式。\u003c/p\u003e\u003cp data-pid=\"ewNyT1_I\"\u003e第三个角度，是从\u0026#34;操作系统\u0026#34;的角度。AK 把 LLM 比作操作系统，但我理解他实际上的意思，是把 LLM 比作成了一个计算机系统，如下图所示：\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-d4a8eb0301b928b3bb9c2fcabdada9ec_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-original-token=\"v2-d4a8eb0301b928b3bb9c2fcabdada9ec\" class=\"content_image\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"VyFKyugp\"\u003e（1）LLM 本身相当于是整个系统中的\u0026#34;CPU\u0026#34;，是负责计算的核心；\u003c/p\u003e\u003cp data-pid=\"s9fYJIgk\"\u003e（2）LLM 的上下文窗口（Context Window），相当于是内存（RAM），负责存储当下的工作记忆；\u003c/p\u003e\u003cp data-pid=\"x8_9fe_z\"\u003e（3）像视频、音频这些内容，就是外围的各种各样的输入；\u003c/p\u003e\u003cp data-pid=\"rSKoQK_1\"\u003e（4）传统的软件，像计算器、Python 解释器等，就是供 LLM 调用的各种工具；\u003c/p\u003e\u003cp data-pid=\"xGM6FQRU\"\u003e（5）文件系统加上向量化的 Embedding，就是这个系统的磁盘（Disk），负责持久的存储数据；\u003c/p\u003e\u003cp data-pid=\"jkQ4zSgi\"\u003e（6）浏览器就像是\u0026#34;以太网\u0026#34;（Ethernet），负责让 LLM 获取网络上的各种信息。\u003c/p\u003e\u003cp data-pid=\"1FloQR6b\"\u003e有了上面的理解，AK 总结到，当下我们所处的时代，很像 20 世纪 5、60 年代的分时(Time-sharing)的时代，那个时代特点是：计算机是中心化的、非常昂贵的，还没有个人计算机；很多人共同使用一台大型计算机系统；计算机要批量处理很多用户的请求。\u003c/p\u003e\u003cp data-pid=\"HIIrFVH3\"\u003e而 LLM 当下的状态，就很像分时时代的计算机，AK 给了一个很精辟的总结\u0026#34;Available via time-sharing, distributed like utility\u0026#34;，大致的意思是说\u0026#34;像分时机一样使用，像公用事业(电)一样分发\u0026#34;。\u003c/p\u003e\u003ch2\u003e大模型“心理学”\u003c/h2\u003e\u003cp data-pid=\"DD_ypGWH\"\u003e大模型“心理学”，是从心智的角度来说明大模型所具有的一些先天的特点，我们只有理解好这些，才能够更好地用好大模型。AK 有一句很精辟的总结，当前的 LLM 是“Kind of a lossy simulation of a savant with cognitive issues”，翻译过来的意思是“当前的大模型某种程度上是一个\u003cb\u003e有认知缺陷的天才的有损模拟\u003c/b\u003e”。为什么会这么说呢，我们来看看大模型的一些特点：\u003c/p\u003e\u003cp data-pid=\"AcvE4z1I\"\u003e（1）大模型拥有百科全局般的知识和记忆力，这是我们人类大脑很难做到的；\u003c/p\u003e\u003cp data-pid=\"cEDJXDvl\"\u003e（2）大模型可能会有幻觉问题，尤其对于自己没见过的内容，不知道该怎么回答的时候，就容易胡编乱造；\u003c/p\u003e\u003cp data-pid=\"y0XsEUla\"\u003e（3）大模型的智力是跳跃式的，在一些问题上比人聪明，但也有一些问题上会犯低级的错误。比如大模型很可能认为 9.11 是大于 9.9 的；\u003c/p\u003e\u003cp data-pid=\"cBg3p-3r\"\u003e（4）大模型还有\u0026#34;顺行性遗忘症(Anterograde amnesia)\u0026#34;，具体的表现是它只有当下 context window 中的记忆，不会像人类一样，可以通过睡眠来加深对知识的理解，并且存储过往的记忆；\u003c/p\u003e\u003cp data-pid=\"szvp0pX-\"\u003e（5）大模型还比较容易轻信、比较容易受骗，具体的表现是人们可以通过在提示词中注入一些欺骗性的内容来欺骗大模型，从而可能会让大模型泄露一些隐私信息。\u003c/p\u003e\u003ch2\u003e当下的机会：部分自治的应用\u003c/h2\u003e\u003cp data-pid=\"fvWP2jPz\"\u003e大模型如此强大，但同时又有这么多先天性的“缺陷”，那我们究竟该如何用好大模型呢？AK 给出的答案是，要做部分自治的应用（Partially autonomous apps）。\u003c/p\u003e\u003cp data-pid=\"RExKpTJW\"\u003eAK 给了三个问题，可以帮助我们更好地去思考该如何做部分自治的应用：\u003c/p\u003e\u003cp data-pid=\"csdY9b13\"\u003e（1）第一个问题：如何让大模型看到我人类能看到的东西？\u003c/p\u003e\u003cp data-pid=\"L2TQRvfy\"\u003e（2）第二个问题：如何让大模型像人一样行动？\u003c/p\u003e\u003cp data-pid=\"qW-bXLPm\"\u003e（3）第三个问题：如何让人持续监督大模型的进展，直至最终解决问题？\u003c/p\u003e\u003cp data-pid=\"ofl6odYT\"\u003e我有一个感觉，当下各种各样的 Agent 产品，很多人都聚焦在第一、二两个问题上发力，但真正最重要的是第三个问题。这也是我在深入使用了 Cursor 和 Claude Code 之后，发现 Claude Code 比 Cursor 好用的本质原因。\u003c/p\u003e\u003cp data-pid=\"b71nYOdg\"\u003e关于构建部分自治的应用，核心是构建好完整的“生成 -\u0026gt; 验证”闭环的工作流，如下图所示：\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-fa07e2ee90f727a557f4f2c86d7b30d6_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-original-token=\"v2-fa07e2ee90f727a557f4f2c86d7b30d6\" class=\"content_image\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"9Z2tUXvq\"\u003e在上述整个工作流闭环中，有两个要点：\u003c/p\u003e\u003cp data-pid=\"kNmVuNrc\"\u003e（1）AI 负责生成，但要确保 AI 是在受人控制的情况下生成，这样才更有可能收敛出想要的结果。AK 用了一个词叫\u0026#34;on a tight leash\u0026#34;，非常形象，就是人要手中时时拽着一根绳，把 AI\u0026#34;牵住\u0026#34;，不能让 AI\u0026#34;信马由缰\u0026#34;，否则就很难收敛到靠谱的结果；\u003c/p\u003e\u003cp data-pid=\"cCWagDtJ\"\u003e（2）人负责结果的验证，这里致胜的关键是要把人验证这个环节做得体验足够好，使用起来足够简单、足够快捷。典型的例子就是 Claude Code 在 AI 执行的过程中，每一个小的修改都有 Diff 显示出来，人可以很方便地看出修改了什么，如果接受就按\u0026#34;回车\u0026#34;确认，如果不接受就按\u0026#34;Esc\u0026#34;阻止，然后再重新提要求，做优化。\u003c/p\u003e\u003cp data-pid=\"0MpDYhU9\"\u003e下图是 AI 辅助编程的一个图，非常形象，人手里牵着一根绳，让 AI 用“三头六臂”解决各种各样的问题：\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-9902ac3fbaf9ff57cad766d9c11df15f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-original-token=\"v2-9902ac3fbaf9ff57cad766d9c11df15f\" class=\"content_image\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"QZ4Ax-uM\"\u003e关于构建部分自治的应用，AK 还分享一个非常核心的概念，叫“自治滑块”（Automony Slider）。这是一种理念，一种可以由用户灵活调节 AI 在产品中的参与度的产品设计理念。\u003c/p\u003e\u003cp data-pid=\"2rN30jBE\"\u003e我们以 Cursor 为例，在 Cursor 中有三种模式，可供用户选择：\u003c/p\u003e\u003cp data-pid=\"z8kUfODV\"\u003e（1）低自治的模式：只用 AI 做代码补全，用户主导所有决策，用户通过 Tab 来确认补全的结果；\u003c/p\u003e\u003cp data-pid=\"vXaD4tAm\"\u003e（2）中等自治模式：用户选择一段代码，通过 Cmd+K 让 AI 帮忙重构或修复问题，AI 只对局部代码做修改，用户审核后决定是否采纳；\u003c/p\u003e\u003cp data-pid=\"b5dNPiEd\"\u003e（3）高等自治模式：用户通过 Cmd+L 让 AI 直接修改整个文件，或者通过 Cmd+I 让 AI 直接修改整个项目，AI 变成了\u0026#34;代理人\u0026#34;，自动执行大块任务，用户只需要做最后的审核。\u003c/p\u003e\u003cp data-pid=\"QvzgvTsu\"\u003e通过上面 Cursor 的例子，我们可以对\u0026#34;自治滑块\u0026#34;的产品理念有个具象的认知，这种理念的核心是：AI 不是只有全自动或者全手动的两种模式可选择，AI 的使用就像这个世界一样，是一道连续的光谱。我们设计产品的人，可以在产品中通过交互上的设计，让人能够方便地选择 AI 在具体场景中的参与度，这是非常关键的。\u003c/p\u003e\u003ch2\u003eAgent 友好地构建\u003c/h2\u003e\u003cp data-pid=\"RvI4Uh7R\"\u003e最后，AK 分享了当下是 AI 时代，如果我们在构建新的事物的时候，能够考虑到对 LLM 友好、对 Agent 友好，那未来 Agent 的发展会更好。关于对 Agent 友好的构建，他列举了一些场景：\u003c/p\u003e\u003cp data-pid=\"8iFyNS0M\"\u003e（1）以前我们做网站，都有一个 robots.txt 这样的文本文件，来告诉搜索引擎的爬虫如何来爬取网站的内容，这是以前的“搜索引擎”友好网站的做法。现在，我们也可以借鉴这个思路，给自己的网站放一个 llms.txt 这样的文本文件，告诉 LLM 如何使用网站中的内容，这样 LLM 就能更好地使用我们所提供的信息了。\u003c/p\u003e\u003cp data-pid=\"wRQQqEl_\"\u003e（2） 以前的产品文档，会有图片、视频等各种的富文本格式，这对人类阅读来说很友好。未来，我们是不是也可以准备面向 LLM 的产品文档，以 Markdown 格式为主，这样 LLM 也能够更好地了解我们的产品是如何使用的，进而调用我们的产品来解决具体的问题。\u003c/p\u003e\u003cp data-pid=\"5fCs0YPD\"\u003e（3）从操作层面，面向人的文档，会说\u0026#34;点击\u0026#34;这里、那里，那对于 LLM，是不是可以直接换成 cURL 的指令，这样 LLM 就可以很轻松地完成这一类的任务了。\u003c/p\u003e\u003cp data-pid=\"8KBTygxG\"\u003e（4）另外，当下 MCP 是一个很好的事情，它把 LLM 调用工具这件事给标准化了，相当于是 LLM 上有了一个\u0026#34;USB-C\u0026#34;的接口了，那我们是不是就可以把自己的产品，通过这个\u0026#34;USB-C\u0026#34;接口接入到 LLM 中去。所以，可以考虑提供自己产品的 MCP，让 LLM 来更好地调用。\u003c/p\u003e\u003cp data-pid=\"P7t_IMNm\"\u003e最后的总结，很多人说 2025 年是 AI Agent 之年，但 AK 的判断是 2025-2035 是 AI Agent 的黄金十年，我个人更认可 AK 这个判断。当下，我们能做的，就是持续不断学习，去积极地拥抱新的变革。\u003c/p\u003e\u003cblockquote data-pid=\"PIkeeFhy\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//reminds-app.com\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ereminds AI 卡片笔记\u003c/a\u003e 正在火热内测中，感兴趣的朋友，可以了解一下:)\u003c/blockquote\u003e","is_labeled":false,"visited_count":119,"thumbnails":["https://picx.zhimg.com/v2-c729b0defc98d9529d356cc71b69c3e2.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-de06fce5cd4aae4a03e7f30dbe7e54a6_720w.jpg?source=b6762063"],"favorite_count":6,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1926940511417140051}","attached_info":"CtsICJHZk76kieTJgQEQBxoJMjYwMjAxNTc1IP3bwcMGKAQwAEBUSkIKLVRTX1NPVVJDRV9UV09UT1dFUl9NVUxUSV9TQ0VORV9WMV9SRUNBTExfVEVYVBIBMBgAIAA6CnsicmF3IjoiIn1aCDEzNTA0MDAzYiBhYTdkMjI0ZTA2ZDRiMzFiNzRmNjRkMDgwZDliN2Y1NnITMTkyNjk0MDUxMTQxNzE0MDA1MYIBX2h0dHBzOi8vcGljeC56aGltZy5jb20vdjItYzcyOWIwZGVmYzk4ZDk1MjlkMzU2Y2M3MWI2OWMzZTIuanBnP3NvdXJjZT03ZTdlZjZlMiZuZWVkQmFja2dyb3VuZD0xigEVY18xNzY4MjA4Njg4MDMxMjg5MzQ1qgEJcmVjb21tZW5kwgEgYzI3M2VkYjRhNjJmN2E4ZDkzNzMwODE4NDdhNjYyMGTyAQoIDBIGTm9ybWFs8gEoCAoSJDRiNDNmOGVjLThiZTUtNDljNS1iNTY2LWY2NThmM2UyZjc0ZPIBBggLEgIxNYICAIgCzdbIzoUzkgIgYzI3M2VkYjRhNjJmN2E4ZDkzNzMwODE4NDdhNjYyMGSaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIXVGVzdGVkQW5kV29ya1dlaWdodFJ1bGXKAhdTYW1lQXV0aG9ySXNvbGF0aW9uUnVsZdoCLVRTX1NPVVJDRV9UV09UT1dFUl9NVUxUSV9TQ0VORV9WMV9SRUNBTExfVEVYVOgCA/oCC05PUk1BTF9GTE9XigMgZDVjNTExNjQxMGM3NDQyOGEzOTVmNDUwMTY0MjI3OGOaAw0KAnYyEAAaBW90aGVyqAN32AMA6gMVdGV4dEZlZWRUd29Ub3dlclYxQWxs+gOKAhIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgCELgIGLQEIiN2Mi00MTg4YTg3NTRiNDk4NzQ3NzA2YjAyOGVlYTRjNTAwZTotCAEQuAgYgAUiI3YyLWQ0YThlYjAzMDFiOTI4YjNiYjljMmZjYWJkYWRhOWVjOi0IAhC4CBjoBCIjdjItZmEwN2UyZWU5MGY3MjdhNTU3ZjRmMmM4NmQ3YjMwZDY6LQgCELgIGN4EIiN2Mi05OTAyYWMzZmJhZjlmZjU3Y2FkNzY2ZDljMTFkZjE1ZjotCAIQuAgYzAMiI3YyLWM3MjliMGRlZmM5OGQ5NTI5ZDM1NmNjNzFiNjljM2UygAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEAmFpwgQDNDAwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAAAAwpSSP4EFAAAAAAAAAACJBS6Pq0aeb9M/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQ+QBgCgBlSoBgGSAi4KCTI2MDIwMTU3NRITMTkyNjk0MDUxMTQxNzE0MDA1MRgHIgpJTUFHRV9URVhU","action_card":false},{"id":"85_1753853602.304","type":"feed","offset":85,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853602,"updated_time":1753853602,"target":{"id":"127168085011","type":"answer","url":"https://api.zhihu.com/answers/127168085011","author":{"id":"90ee5bd339cc519b6365afeeda94a85b","url":"https://api.zhihu.com/people/90ee5bd339cc519b6365afeeda94a85b","user_type":"people","url_token":"wang-guo-chao-42","name":"子任","headline":"前时政记者，有些过于犀利的回答会被误删，可关注公众号:王子任","avatar_url":"https://pica.zhimg.com/50/v2-eca5b2c6f8a0f1708327b57dacaa0398_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"中国人民大学 外交学硕士"}],"followers_count":27353,"is_following":false,"is_followed":false},"created_time":1742279482,"updated_time":1742279482,"voteup_count":19801,"thanks_count":681,"comment_count":1640,"is_copyable":false,"question":{"id":"459115062","type":"question","url":"https://api.zhihu.com/questions/459115062","author":{"id":"edb2473ec71c138a89ea5188e9289eb7","url":"https://api.zhihu.com/people/edb2473ec71c138a89ea5188e9289eb7","user_type":"people","url_token":"jiao-mo-mo-17","name":"2024390235","headline":"独上孤峰的风中舞者，迸发出强烈色彩的力量，撒入人间世俗。","avatar_url":"https://pica.zhimg.com/50/v2-533e818e960689f63cb1ed8e0c697840_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":1,"is_following":false,"is_followed":false},"title":"为什么很多人都不喜欢朱元璋？","created":1620831191,"answer_count":0,"follower_count":0,"comment_count":37,"bound_topic_ids":[248,12009,25792,56888,144850],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"我以前在 HelloTalk 上认识过一个台湾妹子，长得挺漂亮，聊得也挺愉快，后来才发现，她是个彻头彻尾的台独。 他一本正经地跟我说台湾是一个国家，仿佛我才是那个大陆被洗脑的人。 我当时愣了一下，不是不知道有这种人，而是没想到，分裂真的能把曾经的同胞变成这样。 这让我想起了乌克兰和俄罗斯，分开才三十年，就已经打得不共戴天； 台湾和大陆分开八十年，现在的年轻人已经不觉得彼此是同一个民族。 而朱元璋建立明朝的时候，…","excerpt_new":"我以前在 HelloTalk 上认识过一个台湾妹子，长得挺漂亮，聊得也挺愉快，后来才发现，她是个彻头彻尾的台独。 他一本正经地跟我说台湾是一个国家，仿佛我才是那个大陆被洗脑的人。 我当时愣了一下，不是不知道有这种人，而是没想到，分裂真的能把曾经的同胞变成这样。 这让我想起了乌克兰和俄罗斯，分开才三十年，就已经打得不共戴天； 台湾和大陆分开八十年，现在的年轻人已经不觉得彼此是同一个民族。 而朱元璋建立明朝的时候，…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"9X4XmNW5\"\u003e我以前在 HelloTalk 上认识过一个台湾妹子，长得挺漂亮，聊得也挺愉快，后来才发现，她是个彻头彻尾的台独。\u003c/p\u003e\u003cp data-pid=\"NDJPt4VP\"\u003e他一本正经地跟我说台湾是一个国家，仿佛我才是那个大陆被洗脑的人。\u003c/p\u003e\u003cp data-pid=\"1BVe-L7I\"\u003e我当时愣了一下，不是不知道有这种人，而是没想到，分裂真的能把曾经的同胞变成这样。\u003c/p\u003e\u003cp data-pid=\"_fkasWoA\"\u003e这让我想起了乌克兰和俄罗斯，分开才三十年，就已经打得不共戴天；\u003c/p\u003e\u003cp data-pid=\"9UlW1v2G\"\u003e台湾和大陆分开八十年，现在的年轻人已经不觉得彼此是同一个民族。\u003c/p\u003e\u003cp data-pid=\"R7OZnHLY\"\u003e而朱元璋建立明朝的时候，南北汉人已经分裂了四百年，四百年意味着什么？意味着北方人已经习惯了穿胡服、说胡语，南方人早就把北方人当成异族，科举考试甚至已经完全分开，北方人连南方的榜都上不了，放在今天，就是两个民族的关系了。\u003c/p\u003e\u003cp data-pid=\"UObilplR\"\u003e可朱元璋硬是把这片四百年裂痕的土地重新缝合在了一起，让华夏再次成为一个整体，让这个古老的文明活了下来。\u003c/p\u003e\u003cp data-pid=\"CSpK39G6\"\u003e然而，就是这样一个有再造中华之功的皇帝，在今天的影视剧里，却成了一个被彻底丑化的形象。\u003c/p\u003e\u003cp data-pid=\"FPvYgbAb\"\u003e他的相貌被描绘得狰狞暴戾，他的政绩被刻意淡化，他的明朝被黑得体无完肤，反倒是满清的皇帝，一个比一个“仁慈英明”，康乾盛世被吹上了天，仿佛清朝才是华夏文明的巅峰。\u003c/p\u003e\u003cp data-pid=\"kHuWKmri\"\u003e这一切，不是偶然的。\u003c/p\u003e\u003cp data-pid=\"UzDPkp3T\"\u003e清朝灭亡以后，大量八旗子弟留在北京、天津，没了俸禄，没了特权，祖上的铁饭碗砸了，只能靠唱戏、说书、写剧本讨口饭吃。\u003c/p\u003e\u003cp data-pid=\"nAfOMnGH\"\u003e过去这些行业是“下九流”，但时代变了，文化圈成了话语权的中心，他们靠着祖上流传下来的资源，在北京的影视圈扎下了根，成了一股绕不开的势力。\u003c/p\u003e\u003cp data-pid=\"iG69ntR8\"\u003e再往后，内地影视产业越做越大，“京圈”也越做越强，而“京圈”里那些祖上顶着红蓝黄旗的，自然也就成了掌舵人。\u003c/p\u003e\u003cp data-pid=\"siPINfod\"\u003e于是，清宫剧一部接一部地拍，满屏都是九龙玉玺、黄袍加身，爱新觉罗家族轮番登场，把清朝拍得又神秘又高级，仿佛那是华夏文明的高光时刻。\u003c/p\u003e\u003cp data-pid=\"ft12VBpj\"\u003e而朱元璋呢？却在这些人的笔下，成了一个满脸麻子的乡下土包子，一个“杀人如麻、心胸狭隘”的暴君。\u003c/p\u003e\u003cp data-pid=\"ZXvFqwbl\"\u003e他们为什么要这么做？因为朱元璋才是真正打碎了他们祖宗江山的人，才是他们最恨的那个人。\u003c/p\u003e\u003cp data-pid=\"jdWlIsFA\"\u003e这些人不敢吹蒙古，不敢美化元朝，因为元朝的屠杀太过血腥，铁证如山，实在是洗不白；\u003c/p\u003e\u003cp data-pid=\"DTMV3jz8\"\u003e他们也不敢否认辛亥革命，因为革命已经发生，民国早已被历史承认；\u003c/p\u003e\u003cp data-pid=\"164sGk4g\"\u003e可他们能做的，就是把明朝丑化到极致，把朱元璋踩到泥里，让人们慢慢遗忘，他曾经拯救了华夏，让这个民族得以延续。\u003c/p\u003e\u003cp data-pid=\"fOy1I1_8\"\u003e孙中山辛亥革命成功后，做的第一件事，就是去祭拜朱元璋的陵墓。\u003c/p\u003e\u003cp data-pid=\"_WVfQ3n3\"\u003e为什么？因为他知道，如果没有朱元璋，华夏文明早在元朝那一百多年里就被摧毁了，哪还有后来的明清，更不会有今天的中国。\u003c/p\u003e\u003cp data-pid=\"gd6PPv7l\"\u003e可看看今天的影视圈，满屏的清宫戏，把满清美化成“温文尔雅”的“盛世”，反倒是明朝，被描绘成一个充满内斗、腐朽不堪的王朝。\u003c/p\u003e\u003cp data-pid=\"7leUPNqz\"\u003e这是文化的侵蚀，是历史记忆的篡改，是一场没有硝烟的战争。\u003c/p\u003e\u003cp data-pid=\"lowk6KDC\"\u003e能让人团结在一起，是最难的，而要让人分裂，反而极其容易。\u003c/p\u003e\u003cp data-pid=\"tftar2ZB\"\u003e四川和重庆分开才二十几年，重庆人已经不觉得自己是四川人了；\u003c/p\u003e\u003cp data-pid=\"0aaT53Jp\"\u003e台湾和大陆分开八十年，已经形同陌路；\u003c/p\u003e\u003cp data-pid=\"CvwoEBeh\"\u003e乌克兰和俄罗斯分开三十年，已经兵戎相见。可朱元璋面对的是四百年的裂痕，他做到了让华夏再度一统。\u003c/p\u003e\u003cp data-pid=\"bYLn3kAu\"\u003e他做了千百年来无数人想做却做不到的事情，他真正拯救了华夏文明。\u003c/p\u003e\u003cp data-pid=\"Sa_cUKGT\"\u003e可他今天被抹黑得连普通人都不愿意去了解，影视剧里不见他的正面形象，反倒是那些亡国的满清皇帝，被塑造成一个个有情有义的“好君王”。\u003c/p\u003e\u003cp data-pid=\"5kBfk3aK\"\u003e这到底是谁在篡改历史？谁在引导舆论？答案已经再明显不过了。\u003c/p\u003e\u003cp data-pid=\"SNMvSt0b\"\u003e朱元璋是华夏文明的重塑者，是这个民族的救星。\u003c/p\u003e\u003cp data-pid=\"aVu7dbj6\"\u003e摸黑他的人，不是愚蠢，就是别有用心。\u003c/p\u003e\u003cp data-pid=\"Ntj1wVJ9\"\u003e而那些故意歪曲历史、篡改记忆的人，他们才是真正的民族败类。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":783238,"favorite_count":2642,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 127168085011}","attached_info":"CvMFCJHZk76kieTJgQEQBBoJNzE4NjQ4NTg2ILqm5L4GKNmaATDoDEBVSj8KKlRTX1NPVVJDRV9aUkVDQUxMX0ZFRURSRV9ORVdCSUVfSE9VUkxZX1JVTRIBMBgAIAA6CnsicmF3IjoiIn1aCDY0MzA1OTAyYiBhYTdkMjI0ZTA2ZDRiMzFiNzRmNjRkMDgwZDliN2Y1NnIMMTI3MTY4MDg1MDExigEJNDU5MTE1MDYyqgEJcmVjb21tZW5kwgEgOTBlZTViZDMzOWNjNTE5YjYzNjVhZmVlZGE5NGE4NWLyAQoIDBIGTm9ybWFs8gEoCAoSJDZhOGI5YzhiLTYxNWItNDJhYy1hMTI5LTNlZjMyMmM5MjZiM/IBBggLEgIxNYICAIgCzdbIzoUzkgIgOTBlZTViZDMzOWNjNTE5YjYzNjVhZmVlZGE5NGE4NWKaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIUQ29udGVudEFnZVdlaWdodFJ1bGXKAhVRdWVzdGlvbklzb2xhdGlvblJ1bGXaAipUU19TT1VSQ0VfWlJFQ0FMTF9GRUVEUkVfTkVXQklFX0hPVVJMWV9SVU3oAgL6AgtOT1JNQUxfRkxPV4oDIGQ1YzUxMTY0MTBjNzQ0MjhhMzk1ZjQ1MDE2NDIyNzhjmgMNCgJ2MhAAGgVvdGhlcqgDhucv2AMA6gMQbmV3YmllX2ZlZWRyZV92MvoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQGbWFudWFswgQDMTYwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAABANSa4P4EFAAAAAAAAAACJBS6Pq0aeb9M/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQ+QBgCgBlWoBgOSAicKCTcxODY0ODU4NhIMMTI3MTY4MDg1MDExGAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"86_1753853602.41","type":"feed","offset":86,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853602,"updated_time":1753853602,"target":{"id":"1928116157891744524","type":"article","url":"https://api.zhihu.com/articles/1928116157891744524","author":{"id":"9ed8837858cebbd3ddf9e835025a16b6","url":"https://api.zhihu.com/people/9ed8837858cebbd3ddf9e835025a16b6","user_type":"people","url_token":"80-49-69-43","name":"天涯卧龙","headline":"前天涯版主卧龙，创建千人老粉圈子。","avatar_url":"https://pic1.zhimg.com/50/v2-7f51dcf634f5525277891b0f9a2a48ed_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":6708,"is_following":false,"is_followed":false},"title":"坦白说，今天这篇文章价值千万，只因道破了最关键的天机！（天涯隐学）","image_url":"https://picx.zhimg.com/v2-72795d96b7484f767b940820352f39e0.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1752479624,"updated":1752479624,"voteup_count":54,"voting":0,"comment_count":2,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"今天这篇文章将从非常现实甚至略显冷酷的视角，剖析了个人（尤其男性）在中年阶段面临的核心挑战（时间碎片化、家庭责任、社会压力、平庸宿命），并提出了一个极其强调个人专注、时间守护、舍弃琐事、经济独立和情绪防御的“破局”之道。 中年以后，其实最珍贵的不是什么家庭和睦，而是你还能够有一件自己值得干的事情为之全力以赴，不被任何人骚扰，也不会被人胡乱加塞安排打断自己的计划。 如果你仔细去观察的话，绝大多数穷人…","excerpt_new":"今天这篇文章将从非常现实甚至略显冷酷的视角，剖析了个人（尤其男性）在中年阶段面临的核心挑战（时间碎片化、家庭责任、社会压力、平庸宿命），并提出了一个极其强调个人专注、时间守护、舍弃琐事、经济独立和情绪防御的“破局”之道。 中年以后，其实最珍贵的不是什么家庭和睦，而是你还能够有一件自己值得干的事情为之全力以赴，不被任何人骚扰，也不会被人胡乱加塞安排打断自己的计划。 如果你仔细去观察的话，绝大多数穷人…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"nn7VjVF9\"\u003e今天这篇文章将从非常现实甚至略显冷酷的视角，剖析了个人（尤其男性）在中年阶段面临的核心挑战（时间碎片化、家庭责任、社会压力、平庸宿命），并提出了一个极其强调个人专注、时间守护、舍弃琐事、经济独立和情绪防御的“破局”之道。\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"NwbSU7sB\"\u003e中年以后，其实最珍贵的不是什么家庭和睦，而是你还能够有一件自己值得干的事情为之全力以赴，不被任何人骚扰，也不会被人胡乱加塞安排打断自己的计划。\u003cbr/\u003e\u003cbr/\u003e如果你仔细去观察的话，绝大多数穷人最大的问题，就是他们每天都在被各种琐事占用大量的时间，然后在毫无意义的事情上反反复复地挣扎理论，最后把自己的生活和工作搞得一团乱麻，全然没有一秒钟在真正重要的事情上取得突破。\u003cbr/\u003e\u003cbr/\u003e我一直在提醒我的读者：你要趁着还没有家庭和孩子拖累的时候，全心全意地去拼那么一段时间，把所有的琐事都放掉，除了目标以外放弃一些无意义的事情。这个动作将会使你超越绝大多数的同龄人。我在二十出头的时候就是如此，那时候我为了更快地实现自己的目标，基本上就是吃住在公司，每天醒着就是在工作。\u003cbr/\u003e因为我觉得每天挤公交来回太麻烦了，甚至好几天不洗澡，蓬头垢面的，实在脏得不行了就去附近开个钟点房冲一下，又回来继续干活。而就是那一段时间，我的收入和认知都得到了前所未有的进步。\u003cbr/\u003e\u003cbr/\u003e天才与正常人的差别，其实并不是天才比正常人聪明，而是天才更懂得舍弃那些没用的东西，深度地投入到事情之中去。而绝大多数的普通人最笨的地方就在于，他们总在花大把时间维持所谓的“正常”，而这些所谓正常的行为将占用他们大量的时间，最后使得他们无所作为。年纪再大一些，有了家庭有了孩子之后，时间和精力也就更割裂了，终生也就不会有任何成就了。\u003cbr/\u003e\u003cbr/\u003e这个世界其实是很残酷的，因为绝大多数平庸之辈的命运都是趋同的。他们最终的宿命就是成为他人的“血包”，无时无刻不被各种机构吸血，直到有一天毫无价值了就被抛弃，而且在这个过程中还要忍受各种唾骂和嫌弃。我觉得对于一个男人来说，如果自己的事业做不起来，那么不要指望任何一个女人会温柔地对待他。一定要相信这个世界上不管是男人还是女人，其实都是没有爱情的，双方只不过是因为短暂的幻觉而在一起，而之后的生活能否太平，全部都取决于社会地位是否足够强大。\u003cbr/\u003e\u003cbr/\u003e贫贱夫妻百事哀，事事都会走向畸形。你会看见非常多的家庭，就是一个发疯的女人，天天咒骂她的老公不行，但又骂骂咧咧地不离婚，最后他们的孩子长大了以后外出打工，自此也就不会再回去了。不管他们用道德绑架还是以死相逼，都已经无法挽回了。很多并不相爱的，甚至没有能力抚养后代的人勉强组成了一个家庭，祸害了一代又一代，但他们从未想过自己可能根本就配不上这些东西，亦或者最终这就是他们要承受的代价。\u003cbr/\u003e\u003cbr/\u003e有时候，知道自己无能为力而选择丁克、选择不生，其实是一种善良。\u003cbr/\u003e\u003cbr/\u003e不管什么地方的女人，什么年纪的女人，底色都是慕强的。你强，她们就会仰慕你；而你弱了，她们就会骑到你的头上，把你最后的一点尊严践踏光。所以我都告诫很多兄弟，今时今日再难，也不要回家去诉苦，宁可多去打几份工，也不要多说一句话。因为钱的事是说理说不通的，你越表现得软弱，越会引来比你更弱的人试图吃掉你。\u003cbr/\u003e\u003cbr/\u003e三十岁之前，你可以什么都喜欢，什么都爱，什么都观察一番，充分地去体验这个世界，轰轰烈烈地去爱去折腾。但等你真的过了三十岁之后，还对外面的东西太过于渴望，那就是灭顶之灾了。因为这个社会对于男性和女性的宽容，仅限于年少之时。只要你年纪大了无所作为，就会被这个社会体系狠狠地鄙视。甚至施加压力的不是别人，恰巧就是你身边最亲近的人，他们往往是能够最精准地切中你最痛、最软弱的那一部分的。\u003cbr/\u003e\u003cbr/\u003e在你混得不好的时候，一定要忙起来，最好是忙到脚不沾地，千万不要让任何人轻易找到你，也不要让任何人知道你的状态和情绪。因为只要他们看到你了，知道了你的近况，一定会把你“切了瓜分干净”，不仅会带着你干各种破财的事情，还会把你的情绪搞得一团乱麻。\u003cbr/\u003e\u003cbr/\u003e要学会孤独内敛，要学会安安静静地把事情做成。三十岁之后就不要奢望任何天上掉馅饼的事情了，哪怕真的有来了也不吃。就是要自己磨自己的本事，花更长的时间在一件事上偷偷地打磨到极致，期间不管谁来找你都不搭理他。因为只要你与他人过多谈论你尚未完成的大事，那么这件事的能量一定会偷偷泄掉，最后引来各种麻烦。\u003cbr/\u003e\u003cbr/\u003e低谷时不要到处乱窜，上升时不要显摆，把一切可能被他人利用的小尾巴打扫得干干净净。你要学会构建自己经济的铜墙铁壁，学会构建自己情绪的千吨城墙。只有你把自己的状态和钱财守护得好好的，你才有机会真正地度过每一个危机。\u003cbr/\u003e\u003cbr/\u003e那些过得好的人，无一例外都是如履薄冰、战战兢兢的。因为足够的警惕、足够的小心，不去贪恋任何不属于自己的东西，才能安安稳稳地度过每一个艰难时刻。你要相信这个世界上坏人是比好人多的，而且很多好人作恶的时候，从来不会觉得自己干的是坏事。哪一天你觉得此时此刻很顺利、很安全了，那一定是你要走下坡路的时候了。\u003cbr/\u003e\u003cbr/\u003e我们要做到极致的纯粹，要时时刻刻防守，不让任何人以任何理由来骚扰你、吸取你的能量，不要让任何人来占用你的时间，把无关的琐事加给你。\u003cbr/\u003e\u003cbr/\u003e你要相信，能够逃出去的注定只有少数人，这个比例是百分之一，也可能是千分之一。所以你想要取得巨大的成功，就注定了和普通人的思维和做法无缘，因为他们之所以普通、之所以平庸，就是他们已经无法再深度地投入到一件事里面去了，他们早就被很多他们认为很重要的琐事给绑定了。\u003cbr/\u003e\u003cbr/\u003e你要问我，人如何才能够找到出路，能够实现巨大的成就？我觉得无它，只有一条路可以走，那就是大量地、大量地把时间花在那件真正重要的事情上，碾压性地投入且不被打扰。\u003cbr/\u003e\u003cbr/\u003e而你看很多人，他们并未遵守这一条准则，每天就是大量时间被无聊的琐事给占用，而且他们从不花钱去请保姆、请阿姨、请管家、请专业的人来打理，最后使得自己的生活和时间一团乱麻，错过了最佳的发展时期，再想起来要奋斗就已经晚了。\u003cbr/\u003e\u003cbr/\u003e在我们谈论财富自由之前，我们必须先拥有自由的意志。如果一个人连意志都不自由，那么也很难真正地超越财富。\u003cbr/\u003e\u003cbr/\u003e以上的话很重，并非任何人都能够听进去、听得懂，但如果你真心想要改变，那么我建议你，至少耐心读三遍以上，让这些赤裸裸的真理吃进你的骨髓里，成为你思维和决策的一部分。每当有小人、傻子、假好人向你接近时，你要知幻即离，你要立马离开那里。\u003cbr/\u003e\u003cbr/\u003e不对任何人、任何事抱有侥幸，永远遵循等价交换的原则，可以避免99%的麻烦。\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"l7ZDQ1CC\"\u003e早期的天涯卧虎藏龙，高质量、高内涵，讲道理，讲事实不偏颇，可以说能让人打开一扇新世界的大门。毕竟那时候能上网逛论坛的有绝大部分人是精英阶层的智商分子，无数的观念碰撞发酵成就出许多极品神铁，精英之所以是精英，富人之所以是富人，就是他们有一套，更加切合实际，有理念习惯和方法组成的模式在发挥着重要作用。\u003cbr/\u003e\u003cbr/\u003e这套系统的东西在世家和精英圈子当中有着一定的传承，并成为了一种常识。而普罗大众恰恰对此知之甚少，一些理念似是而非，某些习惯令人生厌，很多方法不得要领。\u003cbr/\u003e\u003cbr/\u003e隐学神帖部分观点和内容已不符合大环境与主流价值观，大家几乎没有看到的机会，因为一发出去就会被判定为违规，重则封号轻则限流。这也是此类资源迅速消亡的一大原因。\u003cbr/\u003e\u003cbr/\u003e现在真正优质的内容，正确的内容，基本都隐藏在某一角落，才具有一定的开放度与私密性。\u003c/p\u003e\u003cp data-pid=\"YuwUWmXy\"\u003e\u003cbr/\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//pan.quark.cn/s/2fd5716c3467\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ehttps://pan.quark.cn/s/2fd5716c\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"EuJY3SE4\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e\u003ci\u003e天涯隐学圈：\u003c/i\u003e\u003c/b\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/3Tj1ojxXdGxi5MfHzAaYhg\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e天涯隐学交流群（入群简介）\u003c/a\u003e\u003c/p\u003e","is_labeled":false,"visited_count":3421,"thumbnails":["https://picx.zhimg.com/v2-72795d96b7484f767b940820352f39e0.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-6c7b7f336edcee4ca11773846ad57dad_720w.jpg?source=b6762063"],"favorite_count":213,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1928116157891744524}","attached_info":"CsUGCJHZk76kieTJgQEQBxoJMjYwMzI0Mzg1IIjv0sMGKDYwAkBWSjAKBkl0ZW1DRhIgZG9jX3R5cGU6IEFydGljbGUKaWQ6IDI2MDQ3NTAzMAoYACAAOgBiIGFhN2QyMjRlMDZkNGIzMWI3NGY2NGQwODBkOWI3ZjU2chMxOTI4MTE2MTU3ODkxNzQ0NTI0ggFfaHR0cHM6Ly9waWN4LnpoaW1nLmNvbS92Mi03Mjc5NWQ5NmI3NDg0Zjc2N2I5NDA4MjAzNTJmMzllMC5qcGc/c291cmNlPTdlN2VmNmUyJm5lZWRCYWNrZ3JvdW5kPTGqAQlyZWNvbW1lbmTCASA5ZWQ4ODM3ODU4Y2ViYmQzZGRmOWU4MzUwMjVhMTZiNvIBCggMEgZOb3JtYWzyASgIChIkNDM1YmU5YTQtNjg0NS00Y2I1LTljZTUtMDMyYzQzOThiMjMz8gEGCAsSAjE1ggIAiALN1sjOhTOSAiA5ZWQ4ODM3ODU4Y2ViYmQzZGRmOWU4MzUwMjVhMTZiNpoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZcoCF1NhbWVBdXRob3JJc29sYXRpb25SdWxl2gIGSXRlbUNG6AIC+gILTk9STUFMX0ZMT1eKAyBkNWM1MTE2NDEwYzc0NDI4YTM5NWY0NTAxNjQyMjc4Y5oDDQoCdjIQABoFb3RoZXKoA90a2AMA6gMVdGV4dEFsbFNpdGVNdkl0ZW1DRlYy+gNOEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAQQ0AYYsAsiI3YyLTcyNzk1ZDk2Yjc0ODRmNzY3Yjk0MDgyMDM1MmYzOWUwgAQAiAQAkgQGTm9ybWFsmgQBMqAEAKgEALAEALoEBm1hbnVhbMIEAzE3MMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAoOJQoj+BBQAAAAAAAAAAiQUuj6tGnm/TP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUPkAYAoAZWqAYAkgIuCgkyNjAzMjQzODUSEzE5MjgxMTYxNTc4OTE3NDQ1MjQYByIKSU1BR0VfVEVYVA==","action_card":false},{"id":"87_1753853602.545","type":"feed","offset":87,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853602,"updated_time":1753853602,"target":{"id":"1933662464563742392","type":"answer","url":"https://api.zhihu.com/answers/1933662464563742392","author":{"id":"0cabf9a69e4e6a7775a57d48f1f68149","url":"https://api.zhihu.com/people/0cabf9a69e4e6a7775a57d48f1f68149","user_type":"people","url_token":"zr9558","name":"数学人生","headline":"庾信平生最萧瑟，暮年诗赋动江关。","avatar_url":"https://pica.zhimg.com/50/132f5d0d10e0c5ad4079633336b33eb8_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"best_answerer","description":"数学等 4 个话题下的优秀答主","topic_names":["数学","大学","科研","深度学习（Deep Learning）"],"topic_ids":[1291,2029,2232,89794]},{"type":"identity_people","description":"新加坡国立大学 数学博士"}],"followers_count":141811,"is_following":false,"is_followed":false},"created_time":1753801057,"updated_time":1753801057,"voteup_count":54,"thanks_count":3,"comment_count":7,"is_copyable":false,"question":{"id":"821234746","type":"question","url":"https://api.zhihu.com/questions/821234746","author":{"id":"7d2c3d7fb2fec331a747748ad3fa25e8","url":"https://api.zhihu.com/people/7d2c3d7fb2fec331a747748ad3fa25e8","user_type":"people","url_token":"fei-yuan-zhou-13","name":"飞鸳甃","headline":"目前的学习规划，小初高。","avatar_url":"https://pic1.zhimg.com/50/v2-d2a74f1a332c265972c332b6d2b1a56f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":19794,"is_following":false,"is_followed":false},"title":"你们心目中最好的高中数学教材是哪个版本?","created":1728651587,"answer_count":0,"follower_count":0,"comment_count":4,"bound_topic_ids":[1291,5830,31793,73664,685927],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://picx.zhimg.com/50/v2-8cf4b453facd286b409df4d01d5634b9_720w.jpg?source=b6762063","excerpt":"我推荐的这一本应该不能算作教材，但却是一本非常优秀的数学书籍，那就是 《数学解题学引论》。   《数学解题学引论》是罗增儒教授的代表作，其核心价值在于首次系统构建了数学解题的理论框架，突破了传统解题研究零散化的局限。   一、开创性理论体系解题学学科奠基 首次提出“数学解题学”概念，将解题从经验层面提升为独立学科。书中构建了“解题思想-策略-方法-过程分析”四级理论框架（图1-1），填补了数学方法论中系统化解题理…","excerpt_new":"我推荐的这一本应该不能算作教材，但却是一本非常优秀的数学书籍，那就是 《数学解题学引论》。   《数学解题学引论》是罗增儒教授的代表作，其核心价值在于首次系统构建了数学解题的理论框架，突破了传统解题研究零散化的局限。   一、开创性理论体系解题学学科奠基 首次提出“数学解题学”概念，将解题从经验层面提升为独立学科。书中构建了“解题思想-策略-方法-过程分析”四级理论框架（图1-1），填补了数学方法论中系统化解题理…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"pXXjhWVi\"\u003e我推荐的这一本应该不能算作教材，但却是一本非常优秀的数学书籍，那就是\u003cb\u003e《数学解题学引论》\u003c/b\u003e。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-f4e6dae85ea55e28758245f2fc4935c7_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"882\" data-rawheight=\"1268\" data-original-token=\"v2-84216fd99ad4e6018a9731331c5f8a05\" data-default-watermark-src=\"https://pic1.zhimg.com/v2-be4b33025001b9585ab6f0fe76735694_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"882\" data-original=\"https://pic4.zhimg.com/v2-f4e6dae85ea55e28758245f2fc4935c7_r.jpg\"/\u003e\u003cfigcaption\u003e《数学解题学引论》\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"4hhoYNbp\"\u003e《数学解题学引论》是罗增儒教授的代表作，其核心价值在于首次系统构建了数学解题的理论框架，突破了传统解题研究零散化的局限。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-38c9809b9f0d90cd8cb8058c3ee902e9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"692\" data-original-token=\"v2-df93d351104e08afb8db57ccb4a53dcd\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-9bac3fb7608fb626fb973f1795b16298_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1190\" data-original=\"https://pic2.zhimg.com/v2-38c9809b9f0d90cd8cb8058c3ee902e9_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"yv3oaIKB\"\u003e\u003cb\u003e一、开创性理论体系\u003c/b\u003e\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"KGWe7k-I\"\u003e\u003cb\u003e解题学学科奠基\u003c/b\u003e\u003cbr/\u003e首次提出“数学解题学”概念，将解题从经验层面提升为独立学科。书中构建了“解题思想-策略-方法-过程分析”四级理论框架（图1-1），填补了数学方法论中系统化解题理论的空白。\u003c/li\u003e\u003cli data-pid=\"EkecLHI4\"\u003e\u003cb\u003e解题过程三维模型\u003c/b\u003e\u003cbr/\u003e独创解题过程分析模型：\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli data-pid=\"jqr1FSsg\"\u003e\u003cb\u003e思维分析\u003c/b\u003e：提出“四阶段说”（审题→探索→实施→反思）与“三层次说”（一般性解决→功能性解决→操作性解决）\u003c/li\u003e\u003cli data-pid=\"hK6t4zUe\"\u003e\u003cb\u003e结构分析\u003c/b\u003e：揭示解题链的“条件-结论”逻辑网络\u003c/li\u003e\u003cli data-pid=\"rEfIUbvA\"\u003e\u003cb\u003e长度分析\u003c/b\u003e：通过30道初等数学问题（附录）展示解法优化路径\u003c/li\u003e\u003c/ul\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-c95a7968cd1f7ea29973f7c1df4fe310_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"860\" data-rawheight=\"1344\" data-original-token=\"v2-720ad16fa90dd2667aa93f2a7b098b9d\" data-default-watermark-src=\"https://pica.zhimg.com/v2-7b52dd51fc09e076b1dec38c209d12c0_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"860\" data-original=\"https://pic1.zhimg.com/v2-c95a7968cd1f7ea29973f7c1df4fe310_r.jpg\"/\u003e\u003cfigcaption\u003e附录\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"BWaOMSsw\"\u003e\u003cb\u003e二、解题基本功四维模型\u003c/b\u003e\u003c/p\u003e\u003ctable data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"\u003e\u003ctbody\u003e\u003ctr\u003e\u003cth\u003e维度\u003c/th\u003e\u003cth\u003e核心要素\u003c/th\u003e\u003cth\u003e创新点\u003c/th\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e知识结构\u003c/td\u003e\u003ctd\u003e概念系统/定理系统/符号系统\u003c/td\u003e\u003ctd\u003e强调“剖分相等”等高等数学思想对初等解题的指导（例1-4）\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e思维能力\u003c/td\u003e\u003ctd\u003e程序化思维/策略化思维\u003c/td\u003e\u003ctd\u003e对比两种思维水平（例1-7），揭示高观点解题优势\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e经验题感\u003c/td\u003e\u003ctd\u003e模式识别/审美直觉\u003c/td\u003e\u003ctd\u003e提出“解题经验是有序组合”，用复数恒等式突破几何不等式（例1-9）\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e情感态度\u003c/td\u003e\u003ctd\u003e意志品质/心理调节\u003c/td\u003e\u003ctd\u003e批判“考试目的论”，主张解题的创造体验（例1-13反例分析）\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-5a9378de341f523a9ec119d1fdc7312b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"872\" data-rawheight=\"1332\" data-original-token=\"v2-b332e75716c0fd67861a2d41d6fbe65f\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-ea8d688007ed00fb48999fe44725d36f_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"872\" data-original=\"https://pic4.zhimg.com/v2-5a9378de341f523a9ec119d1fdc7312b_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"5B66evIQ\"\u003e三、解题策略十大原理\u003c/p\u003e\u003cp data-pid=\"llTzM-fO\"\u003e书中提炼的10大策略构成方法论体系：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"ts_wCYVT\"\u003e\u003cb\u003e差异分析策略\u003c/b\u003e（§6-2-3）：通过目标与条件的差异寻找突破口（例2-16中消除函数名称/角度形式差异）\u003c/li\u003e\u003cli data-pid=\"stz4TLV5\"\u003e\u003cb\u003e映射化归策略\u003c/b\u003e（§6-2-2）：建立“解题坐标系”，将几何问题代数化（例1-9复数映射法）\u003c/li\u003e\u003cli data-pid=\"vbpCzFJR\"\u003e\u003cb\u003e数形共生策略\u003c/b\u003e（§6-2-8）：揭示笛卡尔坐标双向转换原理，用面积法解方程（例4-10）\u003c/li\u003e\u003cli data-pid=\"0SyRFlgc\"\u003e\u003cb\u003e结构迁移策略\u003c/b\u003e（§6-2-1）：通过“反应块”识别问题本质（例2-11将组合问题转化为二进制计数）\u003cbr/\u003e \u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"vqZq1st5\"\u003e四、经典理论批判创新\u003c/p\u003e\u003cp data-pid=\"d3L9c0MW\"\u003e\u003cb\u003e将波利亚解题表重构：\u003c/b\u003e肯定其启发价值，设计5张学科化解题表（如“列方程解应用题表”），强化数学思想渗透。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-b133c6c6aeaa2f17969d8574497a6d03_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"1320\" data-original-token=\"v2-feacb422d7fcb6f29775685368066b9a\" data-default-watermark-src=\"https://pica.zhimg.com/v2-01e7501ea055c7178828ad8c30989ff2_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"864\" data-original=\"https://picx.zhimg.com/v2-b133c6c6aeaa2f17969d8574497a6d03_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"HpEQDI3_\"\u003e五、教学实践启示\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"qS7QpQSL\"\u003e\u003cb\u003e解题错误分类学：\u003c/b\u003e将错误分为逻辑性/科学性/策略性三类（§7-3），如“凡偶函数无反函数”的持续性误区（P.15）\u003c/li\u003e\u003cli data-pid=\"VTNEeREI\"\u003e\u003cb\u003e命题技术体系：\u003c/b\u003e提出6种编题方法（演绎/倒推/基本量法等），揭示“题海战术”的认知陷阱（§7-5）\u003c/li\u003e\u003cli data-pid=\"o4vW_ycX\"\u003e\u003cb\u003e文化审美视角：\u003c/b\u003e首创“以美启真”策略（§6-2-10），用对称性/简洁性指导解题（例1-12不等式证明的优美变形）\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"Pz5_9jIB\"\u003e\u003cb\u003e核心价值\u003c/b\u003e：本书超越了传统解题手册的局限，首次将数学解题构建为融合认识论、方法论、教学论的学科体系。其提出的“解题坐标系”“策略矩阵”“错误分类学”等原创概念，为数学教育研究提供了新范式。附录中30道开放性问题（如“30个初等数学问题”）更彰显了学术的前瞻性与开放性。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":2152,"thumbnails":["https://picx.zhimg.com/50/v2-8cf4b453facd286b409df4d01d5634b9_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-05f0b41d523f8215e1fa96d82e5c7de2_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-1cf97e65cd9baf9a19de2cbd5d16d51c_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-f5891aeddb38291a4532a78824cdc90b_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-4f387e4e248c12e4d50174f17f3c15b8_720w.jpg?source=b6762063"],"favorite_count":174,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1933662464563742392}","attached_info":"CoMICJHZk76kieTJgQEQBBoJNzM5NTA4NzYxIOHCo8QGKDYwB0BXSi4KGVRTX1NPVVJDRV9GRUVEUkVfTVNfSFFfVjISATAYACAAOgp7InJhdyI6IiJ9SisKFlRTX1NPVVJDRV9GRUVEUkVfTVNfVjISATAYACAAOgp7InJhdyI6IiJ9SigKE1RTX1NPVVJDRV9GRUVEUkVfTVMSATAYACAAOgp7InJhdyI6IiJ9WgkxMTEwODU1MzNiIGFhN2QyMjRlMDZkNGIzMWI3NGY2NGQwODBkOWI3ZjU2chMxOTMzNjYyNDY0NTYzNzQyMzkyigEJODIxMjM0NzQ2qgEJcmVjb21tZW5kwgEgMGNhYmY5YTY5ZTRlNmE3Nzc1YTU3ZDQ4ZjFmNjgxNDnyAQoIDBIGTm9ybWFs8gEoCAoSJGEyZTE0ZTJlLTQ5MTMtNGM2NS05MTY5LWViYTlkMDg3OThmZvIBBggLEgIxNYICAIgCzdbIzoUzkgIgMGNhYmY5YTY5ZTRlNmE3Nzc1YTU3ZDQ4ZjFmNjgxNDmaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIVUXVlc3Rpb25Jc29sYXRpb25SdWxl2gIZVFNfU09VUkNFX0ZFRURSRV9NU19IUV9WMugCBPoCC05PUk1BTF9GTE9XigMgZDVjNTExNjQxMGM3NDQyOGEzOTVmNDUwMTY0MjI3OGOaAw0KAnYyEAAaBW90aGVyqAPoENgDAOoDFmZlZWRyZV9tc19nYXRlX29ubHlfaHH6A4oCEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAIQ8gYY9AkiI3YyLTg0MjE2ZmQ5OWFkNGU2MDE4YTk3MzEzMzFjNWY4YTA1Oi0IAhCmCRi0BSIjdjItZGY5M2QzNTExMDRlMDhhZmI4ZGI1N2NjYjRhNTNkY2Q6LQgCENwGGMAKIiN2Mi03MjBhZDE2ZmE5MGRkMjY2N2FhOTNmMmE3YjA5OGI5ZDotCAIQ6AYYtAoiI3YyLWIzMzJlNzU3MTZjMGZkNjc4NjFhMmQ0MWQ2ZmJlNjVmOi0IAhDgBhioCiIjdjItZmVhY2I0MjJkN2ZjYjZmMjk3NzU2ODUzNjgwNjZiOWGABACIBACSBAZOb3JtYWyaBAE0oAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAMAAwYw/gQUAAAAAAAAAAIkFLo+rRp5v0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFD5AGAKAGV6gGAJICLgoJNzM5NTA4NzYxEhMxOTMzNjYyNDY0NTYzNzQyMzkyGAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"88_1753853602.18","type":"feed","offset":88,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853602,"updated_time":1753853602,"target":{"id":"1933880895791690798","type":"answer","url":"https://api.zhihu.com/answers/1933880895791690798","author":{"id":"e6fdb1dc51a89844c16b765abe549c3f","url":"https://api.zhihu.com/people/e6fdb1dc51a89844c16b765abe549c3f","user_type":"people","url_token":"ma-lian-liang-meng-lu-51","name":"马连良梦露","headline":"","avatar_url":"https://pic1.zhimg.com/50/9db24cb665fdd67ac983f667e744fe0f_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":570,"is_following":false,"is_followed":false},"created_time":1753853135,"updated_time":1753853135,"voteup_count":0,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"1929285222261854870","type":"question","url":"https://api.zhihu.com/questions/1929285222261854870","author":{"id":"1abc4b17dc70c6aec2e11014bfea8eda","url":"https://api.zhihu.com/people/1abc4b17dc70c6aec2e11014bfea8eda","user_type":"people","url_token":"khbsqw","name":"知乎用户khbSqW","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-8b8a0b712c0144fb560dc3e369f3964a_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":319,"is_following":false,"is_followed":false},"title":"如何看待有些人学历很高反而认知极低？","created":1752757441,"answer_count":0,"follower_count":0,"comment_count":3,"bound_topic_ids":[7928,8634,334146,2153244,2240157],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"不要说认知高低了，就连一些中学生都掌握的常识，很多高学历的人都不知道。已经博士毕业但是连副省级城市的概念都没听说过的人大有人在，不知道东北地区有哪三个省会的博士也很多，不知道江苏在长江以北还是以南的博士也很多，不知道解放战争打了几年的博士也很多，不知道哪一年开始改革开放的博士也很多。","excerpt_new":"不要说认知高低了，就连一些中学生都掌握的常识，很多高学历的人都不知道。已经博士毕业但是连副省级城市的概念都没听说过的人大有人在，不知道东北地区有哪三个省会的博士也很多，不知道江苏在长江以北还是以南的博士也很多，不知道解放战争打了几年的博士也很多，不知道哪一年开始改革开放的博士也很多。","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"KFO9OJ6v\"\u003e不要说认知高低了，就连一些中学生都掌握的常识，很多高学历的人都不知道。已经博士毕业但是连副省级城市的概念都没听说过的人大有人在，不知道东北地区有哪三个省会的博士也很多，不知道江苏在长江以北还是以南的博士也很多，不知道解放战争打了几年的博士也很多，不知道哪一年开始改革开放的博士也很多。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":1,"favorite_count":0,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1933880895791690798}","attached_info":"CukFCJHZk76kieTJgQEQBBoJNzM5NTk5NzM4IM/ZpsQGKAAwAEBYSiQKGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDESATAYACAAOgBKKAodVFNfU09VUkNFX1dBUk1VUF9QUkVUUkFJTl9JMkkSATAYACAAOgBaCTExNTg0NDMxNGIgYWE3ZDIyNGUwNmQ0YjMxYjc0ZjY0ZDA4MGQ5YjdmNTZyEzE5MzM4ODA4OTU3OTE2OTA3OTiKARMxOTI5Mjg1MjIyMjYxODU0ODcwqgEJcmVjb21tZW5kwgEgZTZmZGIxZGM1MWE4OTg0NGMxNmI3NjVhYmU1NDljM2byAQoIDBIGTm9ybWFs8gEoCAoSJDg5YTA4YjA1LWU3ZjgtNDM0MC05NTdhLWNlZDFkNzNjZjE5NfIBBggLEgIxNYICAIgCzdbIzoUzkgIgZTZmZGIxZGM1MWE4OTg0NGMxNmI3NjVhYmU1NDljM2aaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIYQ29udGVudFdhcm1VcEJyZWFrSW5SdWxl2gIZVFNfU09VUkNFX1dBUk1fVVBfTk9STUFMMegCAPoCC05PUk1BTF9GTE9XigMgZDVjNTExNjQxMGM3NDQyOGEzOTVmNDUwMTY0MjI3OGOaAw0KAnYyEAAaBW90aGVyqAMB2AMA6gMTcHJldHJhaW5faTJpX3JlY2FsbPoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAOA2MJc/gQUAAAAAAAAAAIkFLo+rRp5v0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFD5AGAKAGWKgGAZICLgoJNzM5NTk5NzM4EhMxOTMzODgwODk1NzkxNjkwNzk4GAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"89_1753853602.774","type":"feed","offset":89,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853602,"updated_time":1753853602,"target":{"id":"16080518294","type":"article","url":"https://api.zhihu.com/articles/16080518294","author":{"id":"3be309138d009f7d38e0f3e1cb8866a1","url":"https://api.zhihu.com/people/3be309138d009f7d38e0f3e1cb8866a1","user_type":"people","url_token":"xiao-bai-73-37-10","name":"方鸿渐","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-403d44b45bb8166797512e5739563dd6_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":128,"is_following":false,"is_followed":false},"title":"KV Cache - 从矩阵运算的角度理解","image_url":"https://picx.zhimg.com/v2-48e3869149cbfff201c1c633d4f861aa.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1735835311,"updated":1735835311,"voteup_count":275,"voting":0,"comment_count":19,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"引言KV Cache在已经成为了LLM推理系统中必须要用到的一个优化技术。KV Cache的想法很简单，即 通过把中间结果缓存下来，避免重复计算，但是KV Cache有时候又是比较令人困惑的。学习KV Cache，搞清楚下面的三个问题就算是完全弄清楚了：缓存的是什么？为什么是KV Cache，而不是QKV Cache？KV Cache对模型的输出有任何的影响吗？如果我们从矩阵乘法的角度观察attention运算的过程，那么上面的三个问题很容易理解。 自回归模型的特…","excerpt_new":"引言KV Cache在已经成为了LLM推理系统中必须要用到的一个优化技术。KV Cache的想法很简单，即 通过把中间结果缓存下来，避免重复计算，但是KV Cache有时候又是比较令人困惑的。学习KV Cache，搞清楚下面的三个问题就算是完全弄清楚了：缓存的是什么？为什么是KV Cache，而不是QKV Cache？KV Cache对模型的输出有任何的影响吗？如果我们从矩阵乘法的角度观察attention运算的过程，那么上面的三个问题很容易理解。 自回归模型的特…","preview_type":"default","preview_text":"","content":"\u003ch2\u003e引言\u003c/h2\u003e\u003cp data-pid=\"mqhxv2QE\"\u003eKV Cache在已经成为了LLM推理系统中必须要用到的一个优化技术。KV Cache的想法很简单，即\u003cb\u003e通过把中间结果缓存下来，避免重复计算\u003c/b\u003e，但是KV Cache有时候又是比较令人困惑的。学习KV Cache，搞清楚下面的三个问题就算是完全弄清楚了：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"CUoFGzAB\"\u003e缓存的是什么？\u003c/li\u003e\u003cli data-pid=\"pEAA0tht\"\u003e为什么是\u003cb\u003eKV\u003c/b\u003e Cache，而不是\u003cb\u003eQKV\u003c/b\u003e Cache？\u003c/li\u003e\u003cli data-pid=\"DeAX030Q\"\u003eKV Cache对模型的\u003cb\u003e输出有任何的影响\u003c/b\u003e吗？\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"J5G4IC6j\"\u003e如果我们从矩阵乘法的角度观察attention运算的过程，那么上面的三个问题很容易理解。\u003c/p\u003e\u003ch2\u003e自回归模型的特点\u003c/h2\u003e\u003cp data-pid=\"Hj-RFnJC\"\u003eLLM的推理被称为是自回归的过程，也就是说模型上一步的输出会被当作是下一步的输出。如下面的动图所示，用户输入的prompt是\u0026#34;recite the first law $\u0026#34;。模型产生的第一个token是\u0026#34;A\u0026#34;，然后输出的\u0026#34;A\u0026#34;会添加到用户的prompt后面，再次输入到模型中，模型输出\u0026#34;robot\u0026#34;这个token。如此反复，直到模型输出结束词。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-771a0efbd6581bb61949bb5c21e051e1_1440w.gif\" data-size=\"normal\" data-rawwidth=\"716\" data-rawheight=\"251\" data-thumbnail=\"https://pic4.zhimg.com/v2-771a0efbd6581bb61949bb5c21e051e1_b.jpg\" data-original-token=\"v2-771a0efbd6581bb61949bb5c21e051e1\" class=\"origin_image zh-lightbox-thumb\" width=\"716\" data-original=\"https://pic4.zhimg.com/v2-771a0efbd6581bb61949bb5c21e051e1_r.jpg\"/\u003e\u003cfigcaption\u003ehttps://medium.com/@joaolages/kv-caching-explained-276520203249\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Ym75RLsI\"\u003e我们把运行一次模型成为一个\u003cb\u003estep\u003c/b\u003e，每一个\u003cb\u003estep\u003c/b\u003e生成一个新的token。\u003c/p\u003e\u003ch2\u003eKV Cache的探索过程\u003c/h2\u003e\u003cp data-pid=\"Yz1m3vmR\"\u003e下面，我们从最朴素的推理方法出发，一步一步推理出KV Cache的优化方法。假设，我们的模型有两个attention层。\u003c/p\u003e\u003ch3\u003e不考虑任何Cache的推理\u003c/h3\u003e\u003cp data-pid=\"akx43XsK\"\u003e\u003cb\u003eStep 0\u003c/b\u003e\u003cbr/\u003e我们先不考虑任何Cache，观察我们的推理情况。 如图1所示，我们以“你好”作为prompt输入到模型中。模型的embedding层会将token转化为对应的embedding(由于篇幅原因，embedding层就没有展示在图1中) ，得到一个2x4的矩阵embedding1，其中第一行是“你”的embedding表示，第二行是“好”的embedding表示。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-03a7a270498a69f27c6e46acdbd9c7e0_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"8323\" data-rawheight=\"3987\" data-original-token=\"v2-708ff29b927f9f9c662becc22a1fe3c3\" class=\"origin_image zh-lightbox-thumb\" width=\"8323\" data-original=\"https://pic1.zhimg.com/v2-03a7a270498a69f27c6e46acdbd9c7e0_r.jpg\"/\u003e\u003cfigcaption\u003e图1\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"A9gYnMyK\"\u003e之后，我们进入到第一个attention层。embedding1会与attention层中的三个权重矩阵相乘，分别得到Q、K和V矩阵，如图2所示。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-3d9d124f0cd7f6936005d2377463637b_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"4150\" data-rawheight=\"1432\" data-original-token=\"v2-93ee8a0351e233beeb9e28ed6b5a276d\" class=\"origin_image zh-lightbox-thumb\" width=\"4150\" data-original=\"https://pic4.zhimg.com/v2-3d9d124f0cd7f6936005d2377463637b_r.jpg\"/\u003e\u003cfigcaption\u003e图2\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"dZOkCWaY\"\u003e在attention layer中，我们根据attention层的计算公式对Q、K和V矩阵进行计算。首先，Q和K矩阵相乘，得到一个attention矩阵，其中包括了每两个词之间的注意力值。比如，第二行第一列表示“好”这个token对“你”这个token的注意力值。需要额外注意的是，“你”这个token对“好”这个token的注意力值被mask了，即\u003cb\u003e每一个token只计算与之前的token的注意力值\u003c/b\u003e，而不计算之后的token的注意力值。然后，attention矩阵与V矩阵相乘，再把相乘结果输入到FFN层中(这里的FFN对我们要讨论的KV Cache没有什么影响，所以可以忽略)，就得到了新的embedding2。\u003c/p\u003e\u003cp data-pid=\"jUEhTToo\"\u003e第二个attenion层也是如此类推，并输出embedding3。\u003c/p\u003e\u003cp data-pid=\"1yDie6rp\"\u003e最后，embedding3的最后一行，也就是“好”这个token对应的embedding被输入到一个分类其中，预测下一个token。得到的预测结果是“啊“。到此，我们就生成第一个token，即完成了step0。\u003c/p\u003e\u003cp data-pid=\"QtpUxdwd\"\u003e\u003cb\u003eStep 1\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Qr26gVAB\"\u003e在预测了token“啊”之后，我们将“你好啊”再次输入到模型中，如图3所示。除了我们的token数量从2变成了3之外，需要执行的运算与Step 0完全相同，输入的embedding经过两层attention层之后，得到embedding3。embedding3的最后一行被输入到分类器中，预测得到token“!”。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-e76d4153fee18f21a7dcebbd4c2e15f3_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"8445\" data-rawheight=\"3930\" data-original-token=\"v2-fb9266605b0692620a73c4eec7aca011\" class=\"origin_image zh-lightbox-thumb\" width=\"8445\" data-original=\"https://picx.zhimg.com/v2-e76d4153fee18f21a7dcebbd4c2e15f3_r.jpg\"/\u003e\u003cfigcaption\u003e图3\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Vq8lblOA\"\u003e一个细节是，Step 1和Step 0的红色的部分是相同的。比如说，Step 1的K1矩阵前两行与Step 0的K1矩阵的前两行是相同的。这是因为K1矩阵由embedding1矩阵和模型的权重矩阵相乘得到，而两个Step的权重矩阵是相同的，唯一不同的地方在于Step 1的embedding1比Step 0多了最后一行，所以两个Step的QKV矩阵的前两行是相同的，如下图所示。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-3ba47dc44323e5dfa668d90a7cd0ab11_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"5406\" data-rawheight=\"3032\" data-original-token=\"v2-1d244a3831d445725e17606a83490116\" class=\"origin_image zh-lightbox-thumb\" width=\"5406\" data-original=\"https://picx.zhimg.com/v2-3ba47dc44323e5dfa668d90a7cd0ab11_r.jpg\"/\u003e\u003cfigcaption\u003e图4\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"5F30QSRC\"\u003e\u003cb\u003e删除无效计算\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"sCVinq0t\"\u003e观察图1和图3可以发现，模型之后的Classifier只会\u003cb\u003e使用embedding3矩阵的最后一行\u003c/b\u003e作为输入，并输出预测结果。由此我们可以发现，实际上，每一个embedding矩阵只有最新输入的token的行是有效的。比如，在图3的embedding3中，只有step 0产生的新token“啊”对应的第三行是有用的，前两行都是无效的。因此，我们可以删除这些无用的行，以节约计算和存储开销。\u003c/p\u003e\u003cp data-pid=\"iaWJ96mI\"\u003e图5以Step1的第一个attention layer作为演示。我们首先删除三个embedding矩阵的红色的部分。因为embedding2和embedding3矩阵是通过attention和V1矩阵相乘得到的，所以我们也需要删除attention矩阵的重复部分。而attention矩阵是由Q矩阵和K矩阵相乘得到，所以我们还需要删除Q矩阵的重复部分。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-51f90f44361097026ab792c5c131c48f_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"7529\" data-rawheight=\"4407\" data-original-token=\"v2-3eb5810ceaec4da09d85e653b549ee82\" class=\"origin_image zh-lightbox-thumb\" width=\"7529\" data-original=\"https://pic2.zhimg.com/v2-51f90f44361097026ab792c5c131c48f_r.jpg\"/\u003e\u003cfigcaption\u003e图5\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"QykFCIiR\"\u003e所以，通过删除无效计算，我们的模型推理过程可以用图6表示。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-355d0776029cef93db0b10c0973c80f8_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"12270\" data-rawheight=\"2704\" data-original-token=\"v2-b862f5eef04f5f6cebf0f00155258c60\" class=\"origin_image zh-lightbox-thumb\" width=\"12270\" data-original=\"https://pica.zhimg.com/v2-355d0776029cef93db0b10c0973c80f8_r.jpg\"/\u003e\u003cfigcaption\u003e图6\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e缓存重复计算\u003c/h3\u003e\u003cp data-pid=\"zkTPo2CJ\"\u003e经过仔细观察我们可以发现，在图6中，Step 1和Step 2的红色部分是完全相同的，只有白色的部分是不相同的。比如说，Step 1的Q1矩阵、K1矩阵和V1矩阵和Step 0的Q1矩阵、K1矩阵和V1矩阵的前两行是完全相同的。那么我们其实完全可以在Step 0的时候把这些矩阵Cache下来，然后在Step 1的时候load进来，从而节省大量的计算。\u003c/p\u003e\u003cp data-pid=\"XDJ5NxmK\"\u003e下面是进行缓存之后的计算，如图7所示(可放大观看)。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-f8bfbc80650bd3b0d35b9d0e570ad224_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"12178\" data-rawheight=\"2725\" data-original-token=\"v2-376ed2ba1d4af2dddb12294441f0c9b4\" class=\"origin_image zh-lightbox-thumb\" width=\"12178\" data-original=\"https://pica.zhimg.com/v2-f8bfbc80650bd3b0d35b9d0e570ad224_r.jpg\"/\u003e\u003cfigcaption\u003e图7\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"r4jKtNjs\"\u003e在Step 0中，我们把红色部分的数据Cache下来，然后在Step 1的load进行计算。在图7的Step 1中，红色部分的数据都是历史缓存，只有白色部分是新产生的数据。我们将缓存数据和新产生的数据拼接起来，即可得到一个新的矩阵。当然，在Step 1中，我们也需要缓存新产生的数据，以供后面的Step使用。\u003c/p\u003e\u003ch2\u003e总结\u003c/h2\u003e\u003cp data-pid=\"W-RKzsz7\"\u003e在KV Cache中，我们把第一个Step，即Step 0，称为是Prefill阶段，因为这个阶段是在“填充”KV Cache。而之后的所有Step称为是Decode阶段。通过观察我们可以发现，Prefill阶段是是Compute bound，即计算密集型的，因为这个阶段我们的embedding矩阵都特别大，需要进行大量的矩阵乘法计算；而Decode阶段是Memory bound，即访存密集型的，因为这个阶段需要从显存乃至内存中加载KV Cache。\u003c/p\u003e\u003cp data-pid=\"344EZeO1\"\u003e大模型推理系统基本上要解决下面的问题：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"7Tred0sY\"\u003e如何加速Prefill阶段的计算，比如，现在很多工作使用预查找表加速矩阵乘法计算。\u003c/li\u003e\u003cli data-pid=\"Vxo_T4nW\"\u003e如何应对Decode阶段的访存开销，比如一些工作尝试使用CSD(Computational Storage Device)缓解推理过程的Memory bound，提高推理的吞吐率。\u003c/li\u003e\u003cli data-pid=\"aVqnAKke\"\u003e推理任务的调度问题。因为LLM的是自回归的，所以我们不知道模型究竟需要执行多久。并且，模型的输出长度越长，KV Cache占据的存储空间就越多。所以，如何在多个服务器实例之间调度推理任务是非常tricky的。之后阿里今年新出的论文Llumnix要解决的就是LLM的推理任务调度的问题。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"9233eSVK\"\u003e由此，我们可以回答开始提到的三个问题。\u003c/p\u003e\u003ch3\u003e缓存的是什么？\u003c/h3\u003e\u003cp data-pid=\"hVEVObtd\"\u003e缓存的是K和V矩阵。\u003c/p\u003e\u003ch3\u003e为什么是KV Cache，而不是QKV Cache？\u003c/h3\u003e\u003cp data-pid=\"z6uCGkjV\"\u003e如图5所示，我们只需要embedding的最后一行，所以我们也只需要attention的最后一行，因此只需要Q矩阵的最后一行。\u003c/p\u003e\u003cp data-pid=\"sFQKbI4_\"\u003e所以，不缓存Q矩阵的原因是没必要缓存，我们只需要Q矩阵的最后一行即可。\u003c/p\u003e\u003ch3\u003eKV Cache对模型的输出有任何的影响吗？\u003c/h3\u003e\u003cp data-pid=\"VQp2YXSz\"\u003e因为输入到Classifier的embedding是没有改变的，所以模型的输出也不会有任何变化。\u003c/p\u003e\u003ch2\u003eReference\u003c/h2\u003e\u003cp data-pid=\"JsDz_Opw\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//medium.com/%40joaolages/kv-caching-explained-276520203249\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003emedium.com/@joaolages/k\u003c/span\u003e\u003cspan class=\"invisible\"\u003ev-caching-explained-276520203249\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cbr/\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.omrimallis.com/posts/techniques-for-kv-cache-optimization/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://www.\u003c/span\u003e\u003cspan class=\"visible\"\u003eomrimallis.com/posts/te\u003c/span\u003e\u003cspan class=\"invisible\"\u003echniques-for-kv-cache-optimization/\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":7982,"thumbnails":["https://picx.zhimg.com/v2-48e3869149cbfff201c1c633d4f861aa.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-deeb99764dbb5d0d14c4cc6abf48758c_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-1d0c9bd902c19d9c9f206f88b5546eb9_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-cba6d06a0a5052fdcce49ba0904c5a57_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-1f8a3016e510dadc4e202172c4f4b62d_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-d34726b81d4d5e6b3ad22033d1c24b5f_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-1ba4cbefd732856bf94376391feec29b_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-4dfdabb0180172cc4a399952e53f7198_720w.jpg?source=b6762063"],"favorite_count":632,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 16080518294}","attached_info":"CqIKCJHZk76kieTJgQEQBxoJMjUyMjM4Mzc3IK/92rsGKJMCMBNAWUooCh1UU19TT1VSQ0VfTkVBUkxJTkVfQ09OVEVOVF9WMhIBMBgAIAA6AEo5Ci5UU19TT1VSQ0VfVEVYVF9BTExTSVRFX01VTFRJVklFV19HQ0ZfVVNFUkNGX1YxEgEwGAAgADoAYiBhYTdkMjI0ZTA2ZDRiMzFiNzRmNjRkMDgwZDliN2Y1NnILMTYwODA1MTgyOTSCAV9odHRwczovL3BpY3guemhpbWcuY29tL3YyLTQ4ZTM4NjkxNDljYmZmZjIwMWMxYzYzM2Q0Zjg2MWFhLmpwZz9zb3VyY2U9N2U3ZWY2ZTImbmVlZEJhY2tncm91bmQ9MaoBCXJlY29tbWVuZMIBIDNiZTMwOTEzOGQwMDlmN2QzOGUwZjNlMWNiODg2NmEx8gEKCAwSBk5vcm1hbPIBKAgKEiRmN2VhODFmOC01NTUzLTQ5NDMtOWI0Ny0wMDQ2MTQyOTMzNzXyAQYICxICMTWCAgCIAs3WyM6FM5ICIDNiZTMwOTEzOGQwMDlmN2QzOGUwZjNlMWNiODg2NmExmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxlygIXVGVzdGVkQW5kV29ya1dlaWdodFJ1bGXKAhxCYXllc0ZpcnN0TGV2ZWxJc29sYXRpb25SdWxlygIXU2FtZUF1dGhvcklzb2xhdGlvblJ1bGXaAh1UU19TT1VSQ0VfTkVBUkxJTkVfQ09OVEVOVF9WMugCA/oCC05PUk1BTF9GTE9XigMgZDVjNTExNjQxMGM3NDQyOGEzOTVmNDUwMTY0MjI3OGOaAw0KAnYyEAAaBW90aGVyqAOuPtgDAPoDxgMSDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IABDMBRj7ASIjdjItNzcxYTBlZmJkNjU4MWJiNjE5NDliYjVjMjFlMDUxZTE6LQgEEINBGJMfIiN2Mi03MDhmZjI5YjkyN2Y5ZjljNjYyYmVjYzIyYTFmZTNjMzotCAIQtiAYmAsiI3YyLTkzZWU4YTAzNTFlMjMzYmVlYjllMjhlZDZiNWEyNzZkOi0IBBD9QRjaHiIjdjItZmI5MjY2NjA1YjA2OTI2MjBhNzNjNGVlYzdhY2EwMTE6LQgEEJ4qGNgXIiN2Mi0xZDI0NGEzODMxZDQ0NTcyNWUxNzYwNmE4MzQ5MDExNjotCAQQ6ToYtyIiI3YyLTNlYjU4MTBjZWFlYzRkYTA5ZDg1ZTY1M2I1NDllZTgyOi0IBBDuXxiQFSIjdjItYjg2MmY1ZWVmMDRmNWY2Y2ViZjBmMDAxNTUyNThjNjA6LQgEEJJfGKUVIiN2Mi0zNzZlZDJiYTFkNGFmMmRkZGIxMjI5NDQ0MWYwYzliNDotCAMQrAIYqAEiI3YyLTQ4ZTM4NjkxNDljYmZmZjIwMWMxYzYzM2Q0Zjg2MWFhgAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEBm1hbnVhbMIEAzE3MMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAA4O6VkT+BBQAAAAAAAAAAiQUuj6tGnm/TP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUPkAYAoAZZqAYAkgImCgkyNTIyMzgzNzcSCzE2MDgwNTE4Mjk0GAciCklNQUdFX1RFWFQ=","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=89\u0026desktop=true\u0026end_offset=89\u0026page_number=16\u0026session_token=aa7d224e06d4b31b74f64d080d9b7f56","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=89\u0026desktop=true\u0026end_offset=89\u0026page_number=16\u0026session_token=aa7d224e06d4b31b74f64d080d9b7f56","totals":0},"fresh_text":"推荐已更新"}
