{"data":[{"id":"90_1750898778.956","type":"feed","offset":90,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750898778,"updated_time":1750898778,"target":{"id":"1897622716321878697","type":"article","url":"https://api.zhihu.com/articles/1897622716321878697","author":{"id":"c1343bb5fde909ed2292ef957184a9fa","url":"https://api.zhihu.com/people/c1343bb5fde909ed2292ef957184a9fa","user_type":"people","url_token":"zhaoqf123","name":"stardust","headline":"不忘初心、方得始终","avatar_url":"https://picx.zhimg.com/50/e90f47b9d339930f63b796f0010a919e_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"阿里巴巴 员工"}],"followers_count":75,"is_following":false,"is_followed":false},"title":"AI for Science 领军人物|团队","comment_permission":"all","created":1745209055,"updated":1745545240,"voteup_count":29,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"更新 @ 2025-04-25：后续更新都放在这个回答里 国内外有哪些不错的AI For Life Science的组或实验室？   最近在调研 AI for Science，相关的领军人物和团队，记录在此。后续计划持续更新。 1. 成名团队\u0026amp;个人1.1. Shuiwang JiGoogle-Scholar Artificial intelligence for science in quantum, atomistic, and continuum systems [2023]A comprehensive survey of scientific large language models and their applications in scienti…","excerpt_new":"更新 @ 2025-04-25：后续更新都放在这个回答里 国内外有哪些不错的AI For Life Science的组或实验室？   最近在调研 AI for Science，相关的领军人物和团队，记录在此。后续计划持续更新。 1. 成名团队\u0026amp;个人1.1. Shuiwang JiGoogle-Scholar Artificial intelligence for science in quantum, atomistic, and continuum systems [2023]A comprehensive survey of scientific large language models and their applications in scienti…","preview_type":"default","preview_text":"","column":{"id":"c_1895565193444058122","type":"column","url":"https://api.zhihu.com/columns/c_1895565193444058122","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pica.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"AI for Science","imageUrl":"https://picx.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b","comment_permission":"private","intro":"关于 AI4Sci 的调研、实践与思考","updated":1744717960,"is_following":false},"content":"\u003cp data-pid=\"gR5QM2E3\"\u003e更新 @ 2025-04-25：后续更新都放在这个回答里\u003c/p\u003e\u003cp data-pid=\"INa-VAcq\"\u003e\u003ca href=\"https://www.zhihu.com/question/9044466949/answer/1897625004386009656\" class=\"internal\" target=\"_blank\"\u003e国内外有哪些不错的AI For Life Science的组或实验室？\u003c/a\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-c448d8150e8bcc5a1bfe35a8af0fa34d_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1658\" data-rawheight=\"740\" data-original-token=\"v2-b51506c43c6650c913822cb60a76a1f4\" class=\"origin_image zh-lightbox-thumb\" width=\"1658\" data-original=\"https://pic4.zhimg.com/v2-c448d8150e8bcc5a1bfe35a8af0fa34d_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"pKFSZr3o\"\u003e最近在调研 AI for Science，相关的领军人物和团队，记录在此。后续计划持续更新。\u003c/p\u003e\u003ch2\u003e1. 成名团队\u0026amp;个人\u003c/h2\u003e\u003ch3\u003e1.1. Shuiwang Ji\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"5tDiinmr\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DBZGj6sAAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"VAvz_6OZ\"\u003eArtificial intelligence for science in quantum, atomistic, and continuum systems [2023]\u003c/li\u003e\u003cli data-pid=\"qX6HamUf\"\u003eA comprehensive survey of scientific large language models and their applications in scientific discovery [2024]\u003c/li\u003e\u003cli data-pid=\"qnR3U3NN\"\u003eGeometry Informed Tokenization of Molecules for Language Model Generation [2024-08]\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"oQyCWfm7\"\u003e貌似是把 3D 坐标转为极坐标\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003ch3\u003e1.2. Weinan E\u003c/h3\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-34091951574985edf7803e77b09768fa_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1656\" data-rawheight=\"734\" data-original-token=\"v2-f15cc554ea67c40c025d37b096987aa6\" class=\"origin_image zh-lightbox-thumb\" width=\"1656\" data-original=\"https://pica.zhimg.com/v2-34091951574985edf7803e77b09768fa_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-5f2208137634244e7b0653df1708520d_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"942\" data-original-token=\"v2-05acaa08cf16c9fdfb876ea8148ff22b\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic2.zhimg.com/v2-5f2208137634244e7b0653df1708520d_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-a4ea46123138ae09d26f73a124d1b243_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1762\" data-rawheight=\"856\" data-original-token=\"v2-47fb212267fe6864219573d3f087ddca\" class=\"origin_image zh-lightbox-thumb\" width=\"1762\" data-original=\"https://pic4.zhimg.com/v2-a4ea46123138ae09d26f73a124d1b243_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"VPI6yYaJ\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//neurips.cc/virtual/2022/workshop/50019\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eNIPS-2022 AI4Sci Workshop\u003c/a\u003e：\u003ca href=\"https://link.zhihu.com/?target=https%3A//neurips.cc/virtual/2022/64266\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eInvited Talk\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"_u_cAAwj\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//bulletinofcas.researchcommons.org/cgi/viewcontent.cgi%3Farticle%3D2437%26context%3Djournal\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eAI助力打造科学研究新范式\u003c/a\u003e：仔细阅读，与上述 invited-talk 内容相似\u003c/li\u003e\u003cli data-pid=\"rX3LKPtD\"\u003e与深势公司有紧密联系：\u003ca href=\"https://link.zhihu.com/?target=https%3A//iopscience.iop.org/article/10.1088/2752-5724/ac681d/pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eDeep potentials for materials science\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e1.3. 刘铁岩\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"miO1laIy\"\u003eMicrosoft Research AI for Science 创始人之一\u003c/li\u003e\u003cli data-pid=\"kwWOyIlb\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DNh832fgAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"XpUpfG0a\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.microsoft.com/en-us/research/articles/tie-yan-liu-ai-for-science/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eAI for Science，憧憬一个人人都可参与科学发现的未来\u003c/a\u003e [2024-05] [blog]\u003c/li\u003e\u003cli data-pid=\"qxAa9GNw\"\u003eAccelerating protein engineering with fitness landscape modeling and reinforcement learning [2023]\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-49221438920b1424b003af2ee22b8fc9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1522\" data-rawheight=\"556\" data-original-token=\"v2-284930561adfa87a0cf71dd461a8d38d\" class=\"origin_image zh-lightbox-thumb\" width=\"1522\" data-original=\"https://pic2.zhimg.com/v2-49221438920b1424b003af2ee22b8fc9_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e1.4. 宋乐\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"s3frKtA8\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DXl4E0CsAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"XGuQmv-e\"\u003e2021 年加入过百图生科，2024-12 加入 \u003ca href=\"https://link.zhihu.com/?target=https%3A//genbio.ai/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGenBio AI\u003c/a\u003e [正在招聘中]\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e1.5. 唐建\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"TYl3TbmA\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3D1ir6WUEAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"dClqsG4I\"\u003e任职加拿大 Quebec AI institute，与 Begio 一脉有些关系；蛋白质结构上有所研究\u003c/li\u003e\u003cli data-pid=\"Kj-rpOtg\"\u003eAI for Science in the Era of Large Language Models [2024][Tutorial]\u003c/li\u003e\u003cli data-pid=\"tpZ0BXn6\"\u003eDESIGN OF LIGAND-BINDING PROTEINS WITH ATOMIC FLOW MATCHING [2024-09]\u003c/li\u003e\u003cli data-pid=\"EHEFZR_q\"\u003eLearning Gradient Fields for Molecular Conformation Generation [2021-06]\u003c/li\u003e\u003c/ul\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-54c8d712d1d481cde0f204cf242e708c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1758\" data-rawheight=\"616\" data-original-token=\"v2-53127f79be89e318d142e923a238a0cb\" class=\"origin_image zh-lightbox-thumb\" width=\"1758\" data-original=\"https://pic3.zhimg.com/v2-54c8d712d1d481cde0f204cf242e708c_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e1.6. David Baker\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"6RlRPXIm\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DUKqIqRsAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"WkuqhTiC\"\u003e2024 诺贝尔化学奖获得者之一\u003c/li\u003e\u003cli data-pid=\"bvu6pxpP\"\u003eDe novo design of protein structure and function with RFdiffusion [2023-07]\u003c/li\u003e\u003cli data-pid=\"awaV1IrE\"\u003e创业公司\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"2RgQbAEf\"\u003eBaker has co-founded several biotechnology companies, including Prospect Genomics which was acquired by an Eli Lilly subsidiary in 2001,[29] Icosavax which was acquired by AstraZeneca in 2023,[30] Sana Biotechnology, Lyell Immunotherapeutics, and \u003cb\u003eXaira Therapeutics\u003c/b\u003e.\u003c/li\u003e\u003cli data-pid=\"muxPlhv_\"\u003e作为 Xaira 的 Advisory Board 成员；其学术团队为公司提供技术与人才\u003c/li\u003e\u003c/ul\u003e\u003cli data-pid=\"4uqfKrlJ\"\u003e最新成果：\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"5ngd93yE\"\u003e2025年1月：AI设计抗蛇毒蛋白质\u003c/li\u003e\u003cli data-pid=\"f4WiVjt9\"\u003e2025年2月：AI从头设计功能性丝氨酸水解酶\u003c/li\u003e\u003cli data-pid=\"cD-zw7-J\"\u003e2025年2月：设计Hsp70激活剂溶解细胞内凝聚体\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003ch3\u003e1.7. Max Welling\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"ioR7fjV5\"\u003eGoogle-Scholar\u003c/li\u003e\u003cli data-pid=\"Noq_TV7M\"\u003e2025 年新创 Cusp AI，主攻材料方向\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e2. 年轻一辈\u003c/h2\u003e\u003ch3\u003e2.1. Connor W. Coley\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"mxgjdVIi\"\u003eGoogle-Scholar\u003c/li\u003e\u003cli data-pid=\"VcZGeM64\"\u003eMIT, 很年轻，貌似 2018 年第一篇论文\u003c/li\u003e\u003cli data-pid=\"peYQ-k6M\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//youtu.be/TiO6uIeWK_8%3Fsi%3D7QhKhGEt0IHGkrCr\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003emust watch\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-a4ea0c492c7c10dd6664fa69026072de_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2944\" data-rawheight=\"1588\" data-original-token=\"v2-9ad6215f070b86c42e214999f4f81b31\" class=\"origin_image zh-lightbox-thumb\" width=\"2944\" data-original=\"https://pic1.zhimg.com/v2-a4ea0c492c7c10dd6664fa69026072de_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"71qYsTgo\"\u003e一些异议：\u003c/li\u003e\u003c/ul\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-dc1727e09403215144ddfb94a40fd809_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1262\" data-rawheight=\"542\" data-original-token=\"v2-778d71feef537c0f724df713c397865f\" class=\"origin_image zh-lightbox-thumb\" width=\"1262\" data-original=\"https://pic4.zhimg.com/v2-dc1727e09403215144ddfb94a40fd809_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cul\u003e\u003cli data-pid=\"lYvtp5Cu\"\u003eYufan Liang，在阿斯利康药企工作；B 站同名账号\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003ch3\u003e2.2. Yuanqi Du\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"kFo-8rhJ\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DfAc_zZMAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"SocqDxtn\"\u003eCornell University, 很年轻，2021 年第一篇论文；3k+ citation @ 2025-03-18\u003c/li\u003e\u003cli data-pid=\"1m4SNqm5\"\u003e主攻药物设计方向；有不少 survey 文章值得一读\u003c/li\u003e\u003cli data-pid=\"Sgwbro4P\"\u003eMachine learning-aided generative molecular design [2024-06]\u003c/li\u003e\u003cli data-pid=\"L7b5RtkX\"\u003eNavigating Chemical Space with Latent Flows [2024-11]\u003c/li\u003e\u003cli data-pid=\"jTxLyi1o\"\u003eA Systematic Survey of Chemical Pre-trained Models [2023]\u003c/li\u003e\u003cli data-pid=\"yM_lLrCg\"\u003eMolgensurvey: A systematic survey in machine learning models for molecule design [2022-03]\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.3. Tianfan Fu\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"phhVgQC9\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DKPQ49w4AAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"t8T1PFUV\"\u003e南京大学，很年轻，2018 年第一篇论文；4k+ citation @ 2025-03-18\u003c/li\u003e\u003cli data-pid=\"SjZpu1v9\"\u003e药物方向；有些 survey 可以看看\u003c/li\u003e\u003cli data-pid=\"6Nqxzrfy\"\u003eBiomedical Foundation Model: A Survey [2025-03]\u003c/li\u003e\u003cli data-pid=\"34EIsm0a\"\u003eDeepPurpose: a deep learning library for drug–target interaction prediction [2020]\u003c/li\u003e\u003cli data-pid=\"0888v0D3\"\u003eReinforced Genetic Algorithm for Structure-based Drug Design [2022]\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.4. Wenhao Gao\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"rzHed9IB\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3Ds4eywrUAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"QYz5XJ0-\"\u003eMIT, 很年轻，2020 年第一篇论文；~2.7k citations @ 2025-03-18\u003c/li\u003e\u003cli data-pid=\"1UeOcpnF\"\u003e化学工程专业；和 Coley 有较多的合作；和 Tianfan Fu 有合作\u003c/li\u003e\u003cli data-pid=\"gXe6xGl2\"\u003eThe synthesizability of molecules proposed by generative models [2020]\u003c/li\u003e\u003cli data-pid=\"bMNBZtAm\"\u003eDeep learning in protein structural modeling and design [2020]\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.5. Kexin Huang\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"uEv2yAFk\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DogEXTOgAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"_VIByFrq\"\u003e斯坦福大学，很年轻，2019 年第一篇论文；~5.5k citations @ 2025-03-18\u003c/li\u003e\u003cli data-pid=\"Qf-PQlTH\"\u003eclinical，therapeutic，生物、医疗知识库 \u0026amp; 大模型方向；与 Fu，Gao 等有一定的合作\u003c/li\u003e\u003cli data-pid=\"MBVRrIyg\"\u003eMolTrans: molecular interaction transformer for drug–target interaction prediction [2021]\u003c/li\u003e\u003cli data-pid=\"oYaBelP7\"\u003eClinicalbert: Modeling clinical notes and predicting hospital readmission [2019]\u003c/li\u003e\u003cli data-pid=\"7IkRxULq\"\u003eBuilding a knowledge graph to enable precision medicine [2023]\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.6. Shengchao Liu\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"-Alolnmw\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DF1ws3XUAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"aDGHxjxl\"\u003eUC 伯克利，很年轻，2018 年第一篇论文；~2.6k citations @ 2025-03-18\u003c/li\u003e\u003cli data-pid=\"tvTa4thN\"\u003e与 唐建 有合作\u003c/li\u003e\u003cli data-pid=\"yABypQeH\"\u003eGenAI, PhysAI, Dynamics, Physics and ML Pretraining\u003c/li\u003e\u003cli data-pid=\"0WMdFb7O\"\u003ePre-training Molecular Graph Representation with 3D Geometry [2021-10]\u003c/li\u003e\u003cli data-pid=\"mH1Ppcq_\"\u003eA QUANTUM-INSPIRED NEURAL NETWORK FOR GEOMETRIC MODELING [2024-01]\u003c/li\u003e\u003cli data-pid=\"xo4hw152\"\u003eConversational Drug Editing Using Retrieval and Domain Feedback [2023-10]\u003c/li\u003e\u003cli data-pid=\"TmvwCJDJ\"\u003eAI for molecule discovery with multi-modal knowledge [2023-09]\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.7. Ziming Liu\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"AiU5TvUs\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DQeXHxlIAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e；\u003ca href=\"https://www.zhihu.com/people/liu-zi-ming-36-64\" class=\"internal\" target=\"_blank\"\u003e知乎\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"C5N62BR4\"\u003eMIT, 很年轻，2019 年第一篇论文；物理出身；~3.3k citations @ 2025-03-18\u003c/li\u003e\u003cli data-pid=\"sMeqW_N0\"\u003e与 Hanchen Wang，Tianfan Fu 等有少量合作\u003c/li\u003e\u003cli data-pid=\"uDr89q28\"\u003eMachine Learning, Physics of AI, AI for Science, Science for AI, Generative Models\u003c/li\u003e\u003cli data-pid=\"2AauPRAZ\"\u003eKAN: Kolmogorov-Arnold Networks [2024-04]\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"rOJnsdWg\"\u003e也许对学习物理化学规律的模型有帮助\u003c/li\u003e\u003c/ul\u003e\u003cli data-pid=\"GaXk-o0i\"\u003eKAN 2.0: Kolmogorov-Arnold Networks Meet Science [2024-08]\u003c/li\u003e\u003cli data-pid=\"Dp1nxwzA\"\u003eSchrödinger principal-component analysis: On the duality between principal-component analysis and the Schrödinger equation [2021]\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.8. Hanchen Wang\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"bpRmvbQc\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DYu_0vEEAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e；\u003ca href=\"https://www.zhihu.com/people/hhhc\" class=\"internal\" target=\"_blank\"\u003e知乎\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"eUEISJPA\"\u003e斯坦福、基因泰克；2017 年第一篇论文；~2.6k citations @ 2025-03-18\u003c/li\u003e\u003cli data-pid=\"JTsROavC\"\u003eTherapeutics, AI for Science, Machine Learning\u003c/li\u003e\u003cli data-pid=\"q4tQl4n0\"\u003e3D Interaction Geometric Pre-training for Molecular Relational Learning [2024-12]\u003c/li\u003e\u003cli data-pid=\"69KMyNvX\"\u003eUnsupervised Point Cloud Pre-training via Occlusion Completion [2021]\u003c/li\u003e\u003cli data-pid=\"0w1ngz8H\"\u003e最近很关注的一个点是 \u003cb\u003ein silico data generation\u003c/b\u003e，即怎么能够去用计算机程序来生成更海量的、能用的数据，以扩大搜索空间：\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.nature.com/articles/s41597-023-02207-x\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003elink\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"W0osQtVM\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/636323921\" class=\"internal\" target=\"_blank\"\u003e「AI制药+小分子」的进展与困境\u003c/a\u003e，对 AIDD 偏悲观\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.9. Marinka Zitnik\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"CmIhH5_S\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DYtUDgPIAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"P2qjR6QL\"\u003eJure 学生，与 Hanchen 等相熟；25.7k citations @ 2025-03-19\u003c/li\u003e\u003cli data-pid=\"yTzAQGnk\"\u003eGeometric Deep Learning, Knowledge Graphs, Biomedical AI, Therapeutics\u003c/li\u003e\u003cli data-pid=\"wOghPCsE\"\u003eA foundation model for clinician-centered drug repurposing [2024]\u003c/li\u003e\u003cli data-pid=\"B9dtRADO\"\u003eEvaluating generalizability of artificial intelligence models for molecular datasets [2024] [蛋白质相关]\u003c/li\u003e\u003cli data-pid=\"m3Sfds7g\"\u003eGraph Artificial Intelligence in Medicine [2024] [Review 文章]\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.10. Noe Frank\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"oFgr1S28\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DQGiLc_cAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eGoogle-Scholar\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"t7kLJYp0\"\u003e微软 \u0026amp; Free University of Berlin; 28.7k citations @ 2025-03-19\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e3. 其他\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"n3n54QgD\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DEAv91cEAAAAJ\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eJonathan Rittle\u003c/a\u003e [1988 年出生] 2010 第一篇论文\u003c/li\u003e\u003cli data-pid=\"vWkg76CQ\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3Dh3Pia7cAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eYufan Liang\u003c/a\u003e [1988 年出生] 2010 第一篇论文\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"IA3CZLOe\"\u003e\u003ca href=\"https://www.zhihu.com/people/yufan-liang-1\" class=\"internal\" target=\"_blank\"\u003e知乎\u003c/a\u003e、B站同名账号：yfliang1988\u003c/li\u003e\u003cli data-pid=\"-d4Lym_Q\"\u003e导师 David W. C. MacMillan 获 2021 年化学诺奖\u003c/li\u003e\u003c/ul\u003e\u003cli data-pid=\"p6mwwsno\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fuser%3D2k6RIwgAAAAJ%26hl%3Den\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eYang Yang\u003c/a\u003e 2011 第一篇论文\u003c/li\u003e\u003cli data-pid=\"TbOYk69Z\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DcGT8BG4AAAAJ\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eSteven M. Banik\u003c/a\u003e 2011 第一篇论文\u003c/li\u003e\u003cli data-pid=\"6EsRCejp\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com.sg/citations%3Fhl%3Den%26user%3D9MsdbKoAAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eFaisal Mahmood\u003c/a\u003e 2014 第一篇论文 [Pathology]\u003c/li\u003e\u003cli data-pid=\"EpqQgopO\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fhl%3Den%26user%3DE0iCaa4AAAAJ%26view_op%3Dlist_works%26sortby%3Dpubdate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eSong Han\u003c/a\u003e 2013 年第一篇论文 [CV \u0026amp; 网络压缩] 71.3k citations @ 2025-03-18\u003c/li\u003e\u003c/ul\u003e","is_labeled":false,"visited_count":1634,"thumbnails":["https://picx.zhimg.com/50/v2-29413452f4f105e0d45305421e2ad236_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-f5dc36e6db6738ac759f3ffa9610c073_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-58cc4454979f70fe800aa30e9e6205ec_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-514f3eae78b648e75cad368ef6d91262_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-bdddc170211bce0ec394359e12901b93_720w.jpg?source=b6762063"],"favorite_count":59,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1897622716321878697}","attached_info":"CrUICO7Mh8P08/aupAEQBxoJMjU2NjY0ODg3IN+Nl8AGKB0wAEBaSjAKBkl0ZW1DRhIgZG9jX3R5cGU6IEFydGljbGUKaWQ6IDI1OTA3MDM3NQoYACAAOgBaCDEzNjY1OTE3YiBiMWVhMWZlOGMyOTVkOTdkZGM1MTY3YzM2ZGQxNjc2N3ITMTg5NzYyMjcxNjMyMTg3ODY5N4oBFWNfMTg5NTU2NTE5MzQ0NDA1ODEyMqoBCXJlY29tbWVuZMIBIGMxMzQzYmI1ZmRlOTA5ZWQyMjkyZWY5NTcxODRhOWZh8gEKCAwSBk5vcm1hbPIBKAgKEiRhMjFlYjhmZS05NDgzLTQyMDgtYmEyZi00ZWJhYTA4YWM3N2LyAQYICxICMTaCAgCIAoPAzM36MpICIGMxMzQzYmI1ZmRlOTA5ZWQyMjkyZWY5NTcxODRhOWZhmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZdoCBkl0ZW1DRugCA/oCC05PUk1BTF9GTE9XigMgYmNkZGIzMTY0NmE2NDQ0N2IwOGJiYWZmYzQyMTgyN2KaAw0KAnYyEAAaBW90aGVyqAPiDNgDAOoDFXRleHRBbGxTaXRlTXZJdGVtQ0ZWMvoDlwMSDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAhD6DBjkBSIjdjItYjUxNTA2YzQzYzY2NTBjOTEzODIyY2I2MGE3NmExZjQ6LQgCEPgMGN4FIiN2Mi1mMTVjYzU1NGVhNjdjNDBjMDI1ZDM3YjA5Njk4N2FhNjotCAIQwAwYrgciI3YyLTA1YWNhYTA4Y2YxNmM5ZmRmYjg3NmVhODE0OGZmMjJiOi0IAhDiDRjYBiIjdjItNDdmYjIxMjI2N2ZlNjg2NDIxOTU3M2QzZjA4N2RkY2E6LQgCEPILGKwEIiN2Mi0yODQ5MzA1NjFhZGZhODdhMGNmNzFkZDQ2MWE4ZDM4ZDotCAQQ3g0Y6AQiI3YyLTUzMTI3Zjc5YmU4OWUzMThkMTQyZTkyM2EyMzhhMGNiOi0IAhCAFxi0DCIjdjItOWFkNjIxNWYwNzBiODZjNDJlMjE0OTk5ZjRmODFiMzE6LQgCEO4JGJ4EIiN2Mi03NzhkNzFmZWVmNTM3YzBmNzI0ZGY3MTNjMzk3ODY1ZoAEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAYP/5oD+BBQAAAAAAAAAAiQVdQ1zEJZjSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUQkAYAoAZgqAYBkgIuCgkyNTY2NjQ4ODcSEzE4OTc2MjI3MTYzMjE4Nzg2OTcYByIKSU1BR0VfVEVYVA==","action_card":false},{"id":"91_1750898778.464","type":"feed","offset":91,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750898778,"updated_time":1750898778,"target":{"id":"1908646363064952324","type":"article","url":"https://api.zhihu.com/articles/1908646363064952324","author":{"id":"dfe96d25562f92b9bc80722837f60bd0","url":"https://api.zhihu.com/people/dfe96d25562f92b9bc80722837f60bd0","user_type":"people","url_token":"e-chung","name":"Justin","headline":"微软资深技术经理，商务合作：MSFTJustin","avatar_url":"https://pic1.zhimg.com/50/v2-3fe7796af1b575368b10685d68f36503_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"微软 资深研发经理"}],"followers_count":23752,"is_following":false,"is_followed":false},"title":"从一线工程师到带30人团队，我在微软学到的三堂管理课","comment_permission":"all","created":1747838253,"updated":1750152126,"voteup_count":58,"voting":0,"comment_count":2,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"疫情开始那年我刚满30岁，在微软做工程师，每天沉浸在代码和架构中，最头疼的是接口怎么拆、模块怎么划。没想到一年后，我突然被调去带一个30多人的跨职能团队，从写代码的个体贡献者，变成了“上要对齐 VP，下要稳住新人”的管理者。 那是我第一次正式带人。   团队里有人是微软老兵，有人在职级上比我还高一档，还有人半开玩笑地说我是“空降来的娃娃兵”。我表面镇定，心里其实很清楚：要带好队，不靠资历，不靠年龄，而靠真正…","excerpt_new":"疫情开始那年我刚满30岁，在微软做工程师，每天沉浸在代码和架构中，最头疼的是接口怎么拆、模块怎么划。没想到一年后，我突然被调去带一个30多人的跨职能团队，从写代码的个体贡献者，变成了“上要对齐 VP，下要稳住新人”的管理者。 那是我第一次正式带人。   团队里有人是微软老兵，有人在职级上比我还高一档，还有人半开玩笑地说我是“空降来的娃娃兵”。我表面镇定，心里其实很清楚：要带好队，不靠资历，不靠年龄，而靠真正…","preview_type":"default","preview_text":"","column":{"id":"c_1613912122999734272","type":"column","url":"https://api.zhihu.com/columns/c_1613912122999734272","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pica.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"职场达人","imageUrl":"https://picx.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b","comment_permission":"private","intro":"关于职场晋升，向上管理","updated":1677566635,"is_following":false},"content":"\u003cp\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-826635a50e969e35ca05b34fe0a2d4a0_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"1708\" data-original-token=\"v2-6dc5d91a5ef6886a1aa5c2269aed7b45\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic3.zhimg.com/v2-826635a50e969e35ca05b34fe0a2d4a0_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"oBXcpoBM\"\u003e疫情开始那年我刚满30岁，在微软做工程师，每天沉浸在代码和架构中，最头疼的是接口怎么拆、模块怎么划。没想到一年后，我突然被调去带一个30多人的跨职能团队，从写代码的个体贡献者，变成了“上要对齐 VP，下要稳住新人”的管理者。\u003c/p\u003e\u003cp data-pid=\"7JQCNQT-\"\u003e那是我第一次正式带人。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-f4da34140eedd9f756944cf6269637ec_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"619\" data-original-token=\"v2-5914b876c70d7318c417c76d2cf9b8fe\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic1.zhimg.com/v2-f4da34140eedd9f756944cf6269637ec_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"tjLEfGON\"\u003e团队里有人是微软老兵，有人在职级上比我还高一档，还有人半开玩笑地说我是“空降来的娃娃兵”。我表面镇定，心里其实很清楚：要带好队，不靠资历，不靠年龄，而靠真正能“立得住”的管理心法。\u003c/p\u003e\u003cp data-pid=\"Eq_YupLR\"\u003e从刚上手那种“啥都自己来”，到后来真正理解“管理的底层逻辑”，我在微软经历了三件事，每一件都让我明白：\u003cb\u003e管理不是升职加薪的光环，而是一场心智与格局的修行。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"q-CUK0Kg\"\u003e大家有任何的关于职业发展问题，也欢迎联系我。\u003c/p\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/496199381\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\"\u003e美国大厂求职Timeline面试官视角解读\u003c/a\u003e\u003chr/\u003e\u003ch2\u003e第一课：管理不是“事事都懂”，而是“放手让人飞”\u003c/h2\u003e\u003cp data-pid=\"jp1S7K6i\"\u003e我刚开始带队，是接手了一个面向 Azure 内部客户的平台项目。\u003c/p\u003e\u003cp data-pid=\"ojRiv1CR\"\u003e我太想“证明自己”，我凡事都亲力亲为：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"QheS30il\"\u003e 代码 review 自己来\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"Ovz5bnxL\"\u003e PRD 不满意就重写\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"FbFH2WRF\"\u003e Slide 不好看就熬夜做\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"SKaZCmi5\"\u003e我以为我是“在兜底”，直到有一天，一个资深工程师在1-on-1时直接告诉我：\u003c/p\u003e\u003cblockquote data-pid=\"jR99hJqN\"\u003e “你太忙了，是因为你不相信我们。”\u003cbr/\u003e \u003c/blockquote\u003e\u003cp data-pid=\"k9M92Qqz\"\u003e这句话说得我很羞愧。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-3444ad6c25b747f66b39db0e716fbc9b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"644\" data-original-token=\"v2-6fa8f9c24afa4b391cb1b3b6ab050661\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic2.zhimg.com/v2-3444ad6c25b747f66b39db0e716fbc9b_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"4HA5zNXU\"\u003e后来我导师告诉我一句话，我至今难忘：\u003c/p\u003e\u003cblockquote data-pid=\"o1BmDlq-\"\u003e “当你开始把成功定义为‘别人成功’，你才真正开始做管理者。”\u003cbr/\u003e \u003c/blockquote\u003e\u003cp data-pid=\"W9sjyTln\"\u003e于是我开始从“主导执行”切换到“设定边界、拆解目标、驱动协作”。\u003cbr/\u003e 项目开工前，花时间一起写目标、对齐里程碑；每周只开一次高效站会；每两周和每个人做一次1-on-1，聊状态和卡点。有人犯错了，我不再第一反应亲自改，而是问：“你复盘了吗？下一次你准备怎么做？”\u003c/p\u003e\u003cp data-pid=\"QBGZrI8H\"\u003e半年后，项目提前上线，团队士气反而更高了。有人告诉我：“现在做事更有自主感了，感觉你是我们的靠山，不是我们的大脑。”\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-2af93940054cac9d4997628279212b39_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"611\" data-rawheight=\"1270\" data-original-token=\"v2-b3d8aca91eb58ac18265355cc1202874\" class=\"origin_image zh-lightbox-thumb\" width=\"611\" data-original=\"https://picx.zhimg.com/v2-2af93940054cac9d4997628279212b39_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"NuvleswV\"\u003e那一刻我明白了——\u003cb\u003e你不需要什么都做得比下属好，而是要创造一个环境，让他们愿意变得更好。\u003c/b\u003e\u003c/p\u003e\u003chr/\u003e\u003ch2\u003e第二课：关键时刻，你必须扛下“没人愿意扛的决定”\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-983c36dd9bcdb9911938da6e5c798d8f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"1692\" data-original-token=\"v2-a4eec3807d5bf88732b2a30e195d1c99\" class=\"origin_image zh-lightbox-thumb\" width=\"1440\" data-original=\"https://picx.zhimg.com/v2-983c36dd9bcdb9911938da6e5c798d8f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"a45A5x0q\"\u003e有一次我接手了一个项目，有个团队成员E，是前项目组留下的资深员工，在微软干了七八年，技术很稳，但明显“不合群”。\u003c/p\u003e\u003cp data-pid=\"FBji_uN4\"\u003e他工作拖延，邮件不回，开会频繁打断别人，还老是说一些“这不可能做出来”的话。新员工被他怼哭过，设计师直接绕过他沟通。\u003c/p\u003e\u003cp data-pid=\"nN6DZNFs\"\u003e我找他谈过几次，但效果不大。他觉得“你不过是个刚升职的 manager，资历比我浅，凭什么说我？”\u003c/p\u003e\u003cp data-pid=\"o3c7huFb\"\u003e其实我也犹豫过。他是老员工，牵一发动全身，动了他，也许其他人会寒心。但我老板提醒了我：\u003c/p\u003e\u003cblockquote data-pid=\"xc7XgUm3\"\u003e “你的职责是保护团队前进的能力，不是照顾每个人的面子。”\u003cbr/\u003e \u003c/blockquote\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-13da186c5a5901583546e783825c2528_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1117\" data-rawheight=\"288\" data-original-token=\"v2-5c2b857a0e8e5c8953205258f202dda1\" class=\"origin_image zh-lightbox-thumb\" width=\"1117\" data-original=\"https://pic3.zhimg.com/v2-13da186c5a5901583546e783825c2528_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"qU295o1O\"\u003e于是我做了完整的 performance review，记录他的行为影响、反馈来源和对业务的影响，交叉验证后做了正式的 PIP（Performance Improvement Plan），并在HR配合下执行了流程。\u003c/p\u003e\u003cp data-pid=\"gGocPX7E\"\u003e最终我们和平结束了合作，但那周我晚上都睡不好，来公司时看见他原来的座位空着，内心五味杂陈。\u003c/p\u003e\u003cp data-pid=\"n1pT6PCX\"\u003e但几周后，有人对我说：“你动了E，整个团队的空气都轻了。”\u003c/p\u003e\u003cp data-pid=\"Ri9jwm9v\"\u003e我突然意识到，管理者要做的，不只是“搞定项目”，更是\u003cb\u003e捍卫团队的信任边界\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"XTjk5Sso\"\u003e你不是审判官，但你必须成为那个敢于做艰难决定的人。\u003c/p\u003e\u003chr/\u003e\u003ch2\u003e第三课：你不是在打造“自己的部落”，而是在推动系统演进\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-f1a3a538282e2748de935e93b024a2f6_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"1708\" data-original-token=\"v2-a931821ce4c63d567c022d86e7928040\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pica.zhimg.com/v2-f1a3a538282e2748de935e93b024a2f6_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"062gxKQF\"\u003e在微软，团队增长速度快的时候，很容易陷入“领地意识”：资源多了，头衔高了，有种“我这摊子是我一手带起来”的错觉。\u003c/p\u003e\u003cp data-pid=\"v_c4i_lg\"\u003e我一开始也有点这种“山头主义”心态——希望更多人 report 给我，更多事情绕我决策，形成一种中心辐射式的控制感。\u003c/p\u003e\u003cp data-pid=\"jzS-7Fsu\"\u003e但我 mentor 点破了我：\u003c/p\u003e\u003cblockquote data-pid=\"C3BLC2Aa\"\u003e “你是在微软，不是在初创公司。你的任务不是打造一个完美的小团队，而是让这套机制在更大的系统中流畅运转。”\u003cbr/\u003e \u003c/blockquote\u003e\u003cp data-pid=\"NwcqIEZ1\"\u003e于是我开始思考协同，而不是控制。\u003c/p\u003e\u003cp data-pid=\"2MJK7W6x\"\u003e我开始：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"VxvGDZva\"\u003e 主动对接其他 org\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"rAL4XvOL\"\u003e 推动平台共享\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"H4Q3_GNv\"\u003e 建立 cross-team sync\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"1jx4SO58\"\u003e 鼓励成员 cross review\u003c/li\u003e\u003c/ul\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-82dc88b649cf3066d71d6478c3dd3a5b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"900\" data-original-token=\"v2-9a010bc8115fd252ae9658b5833fc542\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic4.zhimg.com/v2-82dc88b649cf3066d71d6478c3dd3a5b_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-4519ee0ee5223d77a9474684365938d2_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"675\" data-original-token=\"v2-baa020567ecf46877bd32afa63caaed9\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-4519ee0ee5223d77a9474684365938d2_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"cJV3zEq4\"\u003e结果原本大家以为“我们只是在做自己的一亩三分地”，最后变成了 Azure pipeline 的一个关键节点，很多团队开始主动对接我们这套方案。\u003c/p\u003e\u003cp data-pid=\"gz1TSheB\"\u003e我终于明白，\u003cb\u003e真正有影响力的管理者，不是画地为牢，而是让影响力外溢，让协同更自然地发生\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-1f089457f2bc6651e09643149eee43b1_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"810\" data-original-token=\"v2-f4e7122c1165deb0632855cc2c17178f\" class=\"origin_image zh-lightbox-thumb\" width=\"1440\" data-original=\"https://pic4.zhimg.com/v2-1f089457f2bc6651e09643149eee43b1_r.jpg\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":7112,"thumbnails":["https://picx.zhimg.com/50/v2-b5c656ad6fde9c6c04de38c891184112_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-d7aa0786b78ddcfddbecb3c95f932603_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-a9a66d9da7b262205899973303f08bcc_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-63b96e7e6d6af3f5c5c15a05d3881f67_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-3375f94898d226a145ff81227ea91975_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-b18f34a45ffbcf8354f580aa674539ec_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-ec6b63c2f9018317c4c9bc7251f661a3_720w.jpg?source=b6762063"],"favorite_count":155,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1908646363064952324}","attached_info":"CpQKCO7Mh8P08/aupAEQBxoJMjU4MDMwMDU3IK3Kt8EGKDowAkBbSkEKLFRTX1NPVVJDRV9UV09UT1dFUl9TSE9SVElOVEVSRVNUX1JFQ0FMTF9URVhUEgEwGAAgADoKeyJyYXciOiIifUooCh1UU19TT1VSQ0VfTkVBUkxJTkVfQ09OVEVOVF9WMhIBMBgAIAA6AFoIMTMyMjEyNjhiIGIxZWExZmU4YzI5NWQ5N2RkYzUxNjdjMzZkZDE2NzY3chMxOTA4NjQ2MzYzMDY0OTUyMzI0igEVY18xNjEzOTEyMTIyOTk5NzM0MjcyqgEJcmVjb21tZW5kwgEgZGZlOTZkMjU1NjJmOTJiOWJjODA3MjI4MzdmNjBiZDDyAQoIDBIGTm9ybWFs8gEoCAoSJDhhOWMxNjdiLWExYzMtNGE5Ni04ZGRhLTY1MzY0MTgzYzE5M/IBBggLEgIxNoICAIgCg8DMzfoykgIgZGZlOTZkMjU1NjJmOTJiOWJjODA3MjI4MzdmNjBiZDCaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIYUGVyaW9kSW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIUQ29udGVudEFnZVdlaWdodFJ1bGXaAixUU19TT1VSQ0VfVFdPVE9XRVJfU0hPUlRJTlRFUkVTVF9SRUNBTExfVEVYVOgCA/oCC05PUk1BTF9GTE9XigMgYmNkZGIzMTY0NmE2NDQ0N2IwOGJiYWZmYzQyMTgyN2KaAw0KAnYyEAAaBW90aGVyqAPIN9gDAOoDGmZlZWRfYXR0bV90d290b3dlcl92Ml90ZXh0+gP1AxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgEEIAKGKwNIiN2Mi02ZGM1ZDkxYTVlZjY4ODZhMWFhNWMyMjY5YWVkN2I0NTotCAMQ4QYY6wQiI3YyLTU5MTRiODc2YzcwZDczMThjNDE3Yzc2ZDJjZjliOGZlOi0IAxDhBhiEBSIjdjItNmZhOGY5YzI0YWZhNGIzOTFjYjFiM2I2YWIwNTA2NjE6LQgCEOMEGPYJIiN2Mi1iM2Q4YWNhOTFlYjU4YWMxODI2NTM1NWNjMTIwMjg3NDotCAQQoAsYnA0iI3YyLWE0ZWVjMzgwN2Q1YmY4ODczMmIyYTMwZTE5NWQxYzk5Oi0IAhDdCBigAiIjdjItNWMyYjg1N2EwZThlNWM4OTUzMjA1MjU4ZjIwMmRkYTE6LQgEEIAKGKwNIiN2Mi1hOTMxODIxY2U0YzYzZDU2N2MwMjJkODZlNzkyODA0MDotCAQQwAwYhAciI3YyLTlhMDEwYmM4MTE1ZmQyNTJhZTk2NThiNTgzM2ZjNTQyOi0IAxCwCRijBSIjdjItYmFhMDIwNTY3ZWNmNDY4NzdiZDMyYWZhNjNjYWFlZDk6LQgFEKALGKoGIiN2Mi1mNGU3MTIyYzExNjVkZWIwNjMyODU1Y2MyYzE3MTc4ZoAEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAZtYW51YWzCBAMxNjDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAGBI7Lc/gQUAAAAAAAAAAIkFXUNcxCWY0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFEJAGAKAGYagGA5ICLgoJMjU4MDMwMDU3EhMxOTA4NjQ2MzYzMDY0OTUyMzI0GAciCklNQUdFX1RFWFQ=","action_card":false},{"id":"92_1750898778.638","type":"feed","offset":92,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898778,"updated_time":1750898778,"target":{"id":"3125945026","type":"answer","url":"https://api.zhihu.com/answers/3125945026","author":{"id":"295991027d684d2325fe6e321936cfe8","url":"https://api.zhihu.com/people/295991027d684d2325fe6e321936cfe8","user_type":"people","url_token":"zhang-jun-31-97","name":"张君","headline":"知识星球“张君的成长星球”","avatar_url":"https://picx.zhimg.com/50/v2-41b52b2dd823295d3aaced7d32d0f617_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"best_answerer","description":"时间管理等 3 个话题下的优秀答主","topic_names":["时间管理","职场","职业规划"],"topic_ids":[230,2566,3479]}],"followers_count":173444,"is_following":false,"is_followed":false},"created_time":1689763387,"updated_time":1689915362,"voteup_count":6043,"thanks_count":1244,"comment_count":189,"is_copyable":false,"question":{"id":"19979300","type":"question","url":"https://api.zhihu.com/questions/19979300","author":{"id":"834d15ac6203d3bbe1a685624ad7e7ea","url":"https://api.zhihu.com/people/834d15ac6203d3bbe1a685624ad7e7ea","user_type":"people","url_token":"cui-kai-45","name":"崔凯","headline":"","avatar_url":"https://picx.zhimg.com/50/4d729a4b7_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":11538,"is_following":false,"is_followed":false},"title":"如何提高一个人的执行力？","created":1324295245,"answer_count":0,"follower_count":0,"comment_count":18,"bound_topic_ids":[404,823,3265,16208,22329],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"不要给脑子留时间，直接开干，用肉体控制精神。我靠这事儿实在是太好了，真的觉得真的非常幸运，在年纪比较小的时候就误打误撞拥有了这个超级技能。 大家都理解什么是内耗，就是总是担心自己能力不行，准备不足，总是会懒惰，会心烦，总之干点啥事儿就是趴住不动，有喜欢的人也不敢表白，有想要的目标也从来没有开始。然后一天天的就跟温水煮青蛙似的，就这么蹉跎下来。 这就是精神控制了肉体了。 我的经验就是，不要给精神留时…","excerpt_new":"不要给脑子留时间，直接开干，用肉体控制精神。我靠这事儿实在是太好了，真的觉得真的非常幸运，在年纪比较小的时候就误打误撞拥有了这个超级技能。 大家都理解什么是内耗，就是总是担心自己能力不行，准备不足，总是会懒惰，会心烦，总之干点啥事儿就是趴住不动，有喜欢的人也不敢表白，有想要的目标也从来没有开始。然后一天天的就跟温水煮青蛙似的，就这么蹉跎下来。 这就是精神控制了肉体了。 我的经验就是，不要给精神留时…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"z4ZxSEXU\"\u003e\u003cb\u003e不要给脑子留时间，直接开干，用肉体控制精神。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"2ZiFZmWi\"\u003e我靠这事儿实在是太好了，真的觉得真的非常幸运，在年纪比较小的时候就误打误撞拥有了这个超级技能。  \u003c/p\u003e\u003cp data-pid=\"f27T18KS\"\u003e大家都理解什么是内耗，就是总是担心自己能力不行，准备不足，总是会懒惰，会心烦，总之干点啥事儿就是趴住不动，有喜欢的人也不敢表白，有想要的目标也从来没有开始。然后一天天的就跟温水煮青蛙似的，就这么蹉跎下来。  \u003c/p\u003e\u003cp data-pid=\"i1Zp8EtQ\"\u003e这就是精神控制了肉体了。  \u003c/p\u003e\u003cp data-pid=\"k9ktwd26\"\u003e我的经验就是，不要给精神留时间瞎tm琢磨。  \u003c/p\u003e\u003cp data-pid=\"i1xopkOq\"\u003e早上起床，刚有那么一点点懒惰，刚开始想，哎呀要不明天再开始，马上打断精神，直接做起来。这就是肉体控制精神。  \u003c/p\u003e\u003cp data-pid=\"_skJJSdp\"\u003e大晚上吃外卖，刚手悄悄地挪到订餐软件，马上关掉手机，自己进屋去睡觉。这就是肉体控制精神。  \u003c/p\u003e\u003cp data-pid=\"HDZSmPqe\"\u003e打算写一篇文章，正犹豫要不要写的时候，关门就打开一个文档开写。这就是肉体控制精神。  \u003c/p\u003e\u003cp data-pid=\"pOb8I27H\"\u003e总之，大原则就是，不要给精神留下内好的时间，直接开干，而经验是，只要干起来了，就很快就会觉得，我擦不过如此，我也能干得下来，也没有想象的那么痛苦。  \u003c/p\u003e\u003cp data-pid=\"2sU8leGD\"\u003e长期以往，就有了积累了。有了积累，自然会因为得到了积累带来的奖励，不管是内在的对自己及的认可，还是外在的表扬或者金钱奖励，这些都恭喜都会变成自己珍贵的心理资产，这些东西会让你更加接受延迟满足，更耐受枯燥和重复，于是一个人才能做得更好，这就是一个马太效应。  \u003c/p\u003e\u003cp data-pid=\"Q_frNMMi\"\u003e我逐渐搞清楚的事情是：\u003c/p\u003e\u003cp data-pid=\"DjJQrYvI\"\u003e第一，人的精神非常容易内耗，如果把时间交给精神，那么大概率精神就会控制身体；\u003c/p\u003e\u003cp data-pid=\"s5m14fqQ\"\u003e第二，人的精神同时也很有适应性，一旦以及身处一个特定环境，精神就会自动适应这个特定环境。  \u003c/p\u003e\u003cp data-pid=\"W0OW0EVQ\"\u003e所以，要体会和学会的东西就是，要用身体控制精神。一旦身体把精神带到一个特定的环境。比如打开的文档，翻开的书，人已经在健身房，你已经骑在自行车上，那么精神就会服帖的把事情做完。  \u003c/p\u003e\u003cp data-pid=\"R4bS_XA4\"\u003e而且精神“变态”的地方在于，一旦你做了这些有点难度的事情，精神还会因为自己做得了而别人不行，而洋洋得意。  \u003c/p\u003e\u003cp data-pid=\"goPS-_Lk\"\u003e总之，这是一个非常重要的能力，就是可以训练自己，学会让身体来控制精神，而不是反过来。直接开干，干那些复杂的事情，困难的事情，自己一直在回避的事情，只要做了，就会发现，不过如此。  \u003c/p\u003e\u003cp data-pid=\"fH3HufyG\"\u003e大家真的试试看，在做那些明显是对你有好处的事情的时候，在你犹豫的第一时间，摇摆一下头，喊两嗓子也行，骂两句也完全ok，朝天空挥几下拳也行，总之，立即马上打断自己的思路，直接开干。  \u003c/p\u003e\u003cp data-pid=\"2GjxSXGj\"\u003e很快，你就会有“用身体控制精神”的经验和感觉了。大家试试看。\u003c/p\u003e\u003cp data-pid=\"_Ydlf79h\"\u003e--------------------\u003c/p\u003e\u003cp data-pid=\"-6zG6TtN\"\u003e关于我：我是一个想活得通透点，获得财富和心态的双重自由的人。想看更多文章，或者想找我提问或学习，关注公众号：\u003cb\u003e张君学习圈\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"jAY1YkXY\"\u003e1.在这里系统整理了关于长期职业发展的知识，欢迎查看：\u003ca href=\"https://zhuanlan.zhihu.com/p/430060967\" class=\"internal\" target=\"_blank\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003ezhuanlan.zhihu.com/p/43\u003c/span\u003e\u003cspan class=\"invisible\"\u003e0060967\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"kK7cV6s5\"\u003e2.关于职业问题、人生困惑，也可在值乎发起单次咨询，点击下面的卡片就行了\u003c/p\u003e\u003ca data-draft-node=\"block\" data-draft-type=\"ad-link-card\" data-ad-id=\"fee_295991027d684d2325fe6e321936cfe8\"\u003e\u003c/a\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":209514,"favorite_count":10907,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 3125945026}","attached_info":"Cv4FCO7Mh8P08/aupAEQBBoJNTk4NDM4MzA0ILv83qUGKJsvML0BQFxKQQosVFNfU09VUkNFX1RXT1RPV0VSX1NIT1JUSU5URVJFU1RfUkVDQUxMX1RFWFQSATAYACAAOgp7InJhdyI6IiJ9WgYxNzE2ODZiIGIxZWExZmU4YzI5NWQ5N2RkYzUxNjdjMzZkZDE2NzY3cgozMTI1OTQ1MDI2igEIMTk5NzkzMDCqAQlyZWNvbW1lbmTCASAyOTU5OTEwMjdkNjg0ZDIzMjVmZTZlMzIxOTM2Y2ZlOPIBCggMEgZOb3JtYWzyASgIChIkODFiMWE2Y2EtNTgwZi00ZTE4LThlNmYtMjRkNmY1NTA0NDE18gEGCAsSAjE2ggIAiAKDwMzN+jKSAiAyOTU5OTEwMjdkNjg0ZDIzMjVmZTZlMzIxOTM2Y2ZlOJoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhhQZXJpb2RJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZdoCLFRTX1NPVVJDRV9UV09UT1dFUl9TSE9SVElOVEVSRVNUX1JFQ0FMTF9URVhU6AIF+gILTk9STUFMX0ZMT1eKAyBiY2RkYjMxNjQ2YTY0NDQ3YjA4YmJhZmZjNDIxODI3YpoDDQoCdjIQABoFb3RoZXKoA+rkDNgDAOoDGmZlZWRfYXR0bV90d290b3dlcl92Ml90ZXh0+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATWgBACoBACwBAC6BAZtYW51YWzCBAMxNjDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAMDZWrA/gQUAAAAAAAAAAIkFXUNcxCWY0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFEJAGAKAGYqgGAJICJQoJNTk4NDM4MzA0EgozMTI1OTQ1MDI2GAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"93_1750898778.944","type":"feed","offset":93,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898778,"updated_time":1750898778,"target":{"id":"1921292817675850497","type":"answer","url":"https://api.zhihu.com/answers/1921292817675850497","author":{"id":"94788d16dcd32c0336af784d14328cb0","url":"https://api.zhihu.com/people/94788d16dcd32c0336af784d14328cb0","user_type":"people","url_token":"chen6072118","name":"小旋风财进","headline":"注册会计师、风险投资","avatar_url":"https://picx.zhimg.com/50/v2-164e4c435c88dd960a0ee9457e9fd9f5_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"中央财经大学 经济学硕士"}],"followers_count":10221,"is_following":false,"is_followed":false},"created_time":1750851903,"updated_time":1750851903,"voteup_count":17,"thanks_count":1,"comment_count":2,"is_copyable":false,"question":{"id":"472135327","type":"question","url":"https://api.zhihu.com/questions/472135327","author":{"id":"7724afc69161f8a5a3ff88d349b386ec","url":"https://api.zhihu.com/people/7724afc69161f8a5a3ff88d349b386ec","user_type":"people","url_token":"qing-shan-72-78-63","name":"冰山来客","headline":"寻找独角兽！","avatar_url":"https://pic1.zhimg.com/50/v2-e653603e65232f012e6d57e2d4780be0_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":113,"is_following":false,"is_followed":false},"title":"如何评价高瓴资本的张磊？","created":1626231391,"answer_count":0,"follower_count":0,"comment_count":9,"bound_topic_ids":[66,395,10895,12475,19800],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://picx.zhimg.com/50/v2-7e268d109d7917c2ec8b4264e1050274_720w.jpg?source=b6762063","excerpt":"张磊不是传统的做题家，童年有点像巴菲特，从小就研究赚钱。是不是LD的朋友的不重要，毕竟大佬都是LD的朋友，时间的朋友也不重要，每个行业都有红利期，超长期持股容易坐过山车。 他的投资理念很接地气，不像很多投资人总是用上帝视角来看问题。 高瓴资本能成功，很多人说是吃到了时代红利，话说谁成功不是吃到了红利，只是摆到面前也不一定能吃到，张磊在美国见过很多不少新的商业模式，而国内的互联网基本都是照抄海外，那会国…","excerpt_new":"张磊不是传统的做题家，童年有点像巴菲特，从小就研究赚钱。是不是LD的朋友的不重要，毕竟大佬都是LD的朋友，时间的朋友也不重要，每个行业都有红利期，超长期持股容易坐过山车。 他的投资理念很接地气，不像很多投资人总是用上帝视角来看问题。 高瓴资本能成功，很多人说是吃到了时代红利，话说谁成功不是吃到了红利，只是摆到面前也不一定能吃到，张磊在美国见过很多不少新的商业模式，而国内的互联网基本都是照抄海外，那会国…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"HPZ5oVHd\"\u003e张磊不是传统的做题家，童年有点像巴菲特，从小就研究赚钱。是不是LD的朋友的不重要，毕竟大佬都是LD的朋友，时间的朋友也不重要，每个行业都有红利期，超长期持股容易坐过山车。\u003c/p\u003e\u003cp data-pid=\"Tmgu5NiB\"\u003e他的投资理念很接地气，不像很多投资人总是用上帝视角来看问题。\u003c/p\u003e\u003cp data-pid=\"lgCpezvT\"\u003e高瓴资本能成功，很多人说是吃到了时代红利，话说谁成功不是吃到了红利，只是摆到面前也不一定能吃到，张磊在美国见过很多不少新的商业模式，而国内的互联网基本都是照抄海外，那会国内能看懂的确实不多，张磊占了海龟优势。\u003c/p\u003e\u003cp data-pid=\"MvdpQZ6q\"\u003e高瓴资本一级市场和二级市场都做，创业企业和上市公司都能去调研，对行业的理解更好，不像很多二级市场的卖方只去上市公司调研，做做访谈走个形式，如果是去大公司调研，公司是强势方，能了解的东西有限。\u003c/p\u003e\u003cp data-pid=\"liV8VkGL\"\u003e高瓴的钱主要来自捐赠基金、慈善基金、主权财富基金、养老基金及家族基金，而国内投资机构的钱一般是来自个人LP、国资、上市公司，资金性质的区别很大。高瓴背后的金主爸爸，都是长期性质的资金，愿意等，有耐心。不像国内其他机构背后的金主，更多追求短期利益，每年都要看收益，每年都要讲排名，逼着基金投资人去追涨杀跌，去追热点。长期资本有很大的优势，尤其是互联网企业、科技企业，前期要连续亏损很多年，一般人早没耐心了。\u003c/p\u003e\u003cp data-pid=\"tlJ7hlCQ\"\u003e巴菲特的资金来自保险公司伯克希尔哈撒韦，同样具有长期属性，资金性质有很大的优势，不是说国内很难出巴菲特，而是国内基金都有存续期，也容易被赎回，没法长期持股。\u003c/p\u003e\u003cp data-pid=\"pxWlo9Ef\"\u003e张磊不是书呆子，从小就做生意。\u003c/p\u003e\u003cp data-pid=\"r76ZRioJ\"\u003e1972年，张磊出生在河南驻马店一个知识分子家庭，父亲是公务员，母亲是律师，家里有很多法律和经济类的书，耳濡目染的，长了不少姿势。那个年代吃顿肉都难，张磊虽然家境不如巴菲特，没有国会议员的爸爸，但起点也不低，家庭教育这块可以说远超常人，但不是无脑鸡娃，童年学到的知识，远超同龄人。\u003c/p\u003e\u003cp data-pid=\"AeOtq0Nh\"\u003e张磊访谈时说过，很小就在家门口的驻马店火车站做生意，那会绿皮车一停靠就是几小时，旅客无聊地蹲在广场上嗑瓜子，孩子满地乱窜。\u003cbr/\u003e张磊看到了商机，把家里的连环画搬出来，搬来小板凳往站前一坐，做起租书生意，为了吸引顾客，他还提供增值服务，租5本书送凉白开和瓜子，他的书成了候车室的香饽饽。\u003c/p\u003e\u003cp data-pid=\"Roj_uxCb\"\u003e巴菲特小时候也喜欢研究赚钱，还去街上卖报纸，段永平也是实业出身，他们这类做长期投资的，喜欢研究商业模式，研究现金流，和自身做过实业有关，做研究是比较接地气的，不像很多基金经理一毕业就做投资，经常用上帝视角看问题。\u003cbr/\u003e上大学前，为了体验生活，张磊跑到建筑工地当泥瓦工。\u003c/p\u003e\u003cp data-pid=\"aXt2nLlL\"\u003e每天干10小时赚一块二，收工时满手血泡，大汗淋漓，工头说他这状元苗子来这受什么罪。\u003c/p\u003e\u003cp data-pid=\"DW5nLEdf\"\u003e1990年，张磊成为省文科状元，被人大金融录取，河南高考一直是地狱模式，这含金量不言而喻。​​\u003cbr/\u003e大二时学校搞了个家电课题，同学扎堆去北京百货大楼做市场调研,张磊与众不同，他回老家驻马店，到乡镇供销社找农民聊天。\u003c/p\u003e\u003cp data-pid=\"BPQ0iYZc\"\u003e他发现大部分农民最看重的是“坏了能不能到村里修”，品牌和价格反而是次要的,卖的好是因为售后好，靠低价倾销不行。​\u003c/p\u003e\u003cp data-pid=\"Jp7NwxAe\"\u003e张磊靠这份报告拿了特等奖，奖品是一台29寸牡丹彩电，他捐给了老家小学。\u003c/p\u003e\u003cp data-pid=\"w1KCUUyB\"\u003e1992年，那时股市还是个新鲜玩意，张磊带头在人大搞了股市模拟大赛，对投资越发感兴趣。\u003c/p\u003e\u003cp data-pid=\"5yAF4y3D\"\u003e本科毕业后，分配到了央企五矿，觉得工作太无聊，决定出国留学，见见世面。\u003c/p\u003e\u003cp data-pid=\"Bxii2KgA\"\u003e98年拿到耶鲁MBA的offer，但奖学金只覆盖第一年，学费和生活费三年需要10万美金，那时在国内是天文数字。\u003c/p\u003e\u003cp data-pid=\"mVJAmvKi\"\u003e他只能勤工俭学，给本科生改作业，教老外学中文，晚上在实验室刷试管。\u003c/p\u003e\u003cp data-pid=\"IeIOpdKf\"\u003e他想去华尔街投行实习，被拒了，不过碰到贵人大卫史文森，来到史文森掌管的耶鲁捐赠基金实习，学到了一些非主流投资逻辑。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-e842d2aae51d544bde004e9196cea88f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"867\" data-rawheight=\"486\" data-original-token=\"v2-586e3056c42715e784765420beaf5940\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-c207a93fd3f3223d506d94fdfd5a6d2b_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"867\" data-original=\"https://pic2.zhimg.com/v2-e842d2aae51d544bde004e9196cea88f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"2nAhzWcs\"\u003e比如评估非洲金矿时，不能根据教科书算净现值，还要考虑矿工HIV死亡率，这涉及到人力重置成本问题。\u003c/p\u003e\u003cp data-pid=\"sZwYodE0\"\u003e比如研究巴西石油公司的时候，需要考虑贫民窟油罐车盗窃率，犯罪成本会影响折现率。 \u003c/p\u003e\u003cp data-pid=\"vlJw5Hfr\"\u003e史文森带他参观耶鲁图书馆古籍修复室，说羊皮卷存在了800年，而基金要运营300年，投资就是要寻找能穿越王朝更迭的生意，找能够穿越周期的生意，后来张磊常把时间的朋友挂嘴边。 \u003c/p\u003e\u003cp data-pid=\"_fOW6Ilj\"\u003e2000年休学创办中华创业网，帮中国企业赴美上市，赶上互联网泡沫破灭，纳斯达克崩盘，为了留住客户承诺上市失败就赔审计费，结果烧光了500万融资，公司破产清算时，投资人主动豁免债务，张磊却卖掉中关村公寓还债，他觉得信用是最后的资产，凡是能被火烧掉的东西都不重要，比如金钱、房子、股份，真正重要的是知识、能力和价值观。\u003c/p\u003e\u003cp data-pid=\"dupe0NTW\"\u003e2003年担任纽交所首任中国代表，当时美国投资机构对中国公司有偏见，对财报不信任。\u003c/p\u003e\u003cp data-pid=\"0us1QE6n\"\u003e张磊就带华尔街的基金经理，到中国逛菜场，调研摊主进货流程，看电子秤误差情况，证明大部分中国人还是讲信用的，不能被少数劣币迷惑。 \u003c/p\u003e\u003cp data-pid=\"vyJlVoU7\"\u003e2005年开始第二次创业，取《史记》里面的“高屋建瓴”，意思是借中国崛起之势，造资本银河战舰。  \u003c/p\u003e\u003cp data-pid=\"zdymkpWG\"\u003e高瓴资本刚成立，张磊拿到耶鲁捐赠基金给的2000万美金，梭哈了腾讯。  \u003c/p\u003e\u003cp data-pid=\"JK5qTCXB\"\u003e当时腾讯市值只有20亿美金，那时白领都在用MSN，投资人觉得QQ是三低人士用的，低年龄、低收入、低学历，网瘾少年和精神小妹居多。  \u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-2b6a3643580d78801f0d4187d93584ee_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1880\" data-rawheight=\"1253\" data-original-token=\"v2-ab4aa4f80d64ce6cfd5a65771457006e\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-4a45891377e8902b14dc51ce09d0f885_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1880\" data-original=\"https://pic1.zhimg.com/v2-2b6a3643580d78801f0d4187d93584ee_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"MJdCpU6t\"\u003e张磊去做市场调研，跑到义乌小商品城，见小老板名片上都印着QQ号，用QQ拓展客户，比打电话快多了。\u003c/p\u003e\u003cp data-pid=\"JPWIg0VK\"\u003eMSN虽然白领用的多，但是接地气的产品才更适合普罗大众，有更广阔的市场。\u003c/p\u003e\u003cp data-pid=\"V_hOG5Tq\"\u003eQQ这种熟人社交的流量，是最优质的流量，一旦用户量达到一个极点，就会迅速裂变，现有用户每拉一个朋友注册，腾讯价值就翻一倍。\u003c/p\u003e\u003cp data-pid=\"5qVEAsHX\"\u003e腾讯拥有最顶级的商业模式，获客成本会越来越低，用户粘性相当高，具有天然垄断型。\u003c/p\u003e\u003cp data-pid=\"0CPWpX2c\"\u003e掌握最优质流量后，通过游戏、广告和投资变现，从5元QQ秀到万亿游戏帝国，护城河越挖越深，现在已经看不到MSN的影子。\u003c/p\u003e\u003cp data-pid=\"imVBPVQ6\"\u003eQQ是PC时代的流量王者，等智能手机普及，移动互联网时代来了，新的流量王者微信诞生，腾讯的战略也很成功。\u003c/p\u003e\u003cp data-pid=\"5EqkxGR9\"\u003e腾讯战投的风格也有高瓴的影子，投资后给流量入口，不过度参与企业管理，腾讯自己的战投也做得不错。 \u003c/p\u003e\u003cp data-pid=\"Tron2UPm\"\u003e投资腾讯赚了200倍，高瓴靠这一单奠定江湖地位。  \u003c/p\u003e\u003cp data-pid=\"GxdIqelg\"\u003e2013年，京东被阿里压得喘不过气，淘宝流量巨大，双十一交易额几百亿，京东执着自建物流，这个模式太烧钱，并且缺乏流量入口。   \u003c/p\u003e\u003cp data-pid=\"b5TcOV7a\"\u003e张磊完成了教科书级的资源重组，腾讯和京东，一个有顶级流量但是缺变现模式，一个坐拥万亿电商市场但是缺流量入口。 \u003c/p\u003e\u003cp data-pid=\"wPN0Z8WL\"\u003e微信给京东流量入口，京东接入微信支付，腾讯投资京东股权，用流量换供应链。 \u003c/p\u003e\u003cp data-pid=\"kmoWWSTV\"\u003e微信支付借京东拿下30%电商份额，逆袭支付宝，京东靠微信入口日均引流，高瓴持有的两家股权价值暴涨，一次捆绑催生两个万亿巨头。  \u003c/p\u003e\u003cp data-pid=\"_7u_EgP8\"\u003e2017年，张磊宣布531亿港元私有化百丽时，很多人在嘲讽。  \u003c/p\u003e\u003cp data-pid=\"rsmE_Te3\"\u003e因为连续13个季度业绩下滑，实体店被电商冲击，资产包袱太重，2万家店和8万店员，年租金几十亿。  \u003c/p\u003e\u003cp data-pid=\"6apYUTkj\"\u003e张磊看中的是毛细血管价值，2万家店就是2万个前置仓，试鞋后线上复购率很高，店员记录的女性脚型数据赋能产品设计，柜台变直播间，导购转型网红直播带货。  \u003c/p\u003e\u003cp data-pid=\"XZVT-Q8Q\"\u003e拆分滔搏运动单独上市，市值一度超过收购价，相当于白捡一个百丽女鞋。 \u003c/p\u003e\u003cp data-pid=\"cDKci3cz\"\u003e传统投资逻辑认为品牌、专利是壁垒，比如可口可乐秘方，耐克的品牌，张磊认为所有静态壁垒都是纸老虎，真正的护城河是企业家持续创造价值的能力。 \u003c/p\u003e\u003cp data-pid=\"RZOWKenb\"\u003e张磊最看重一个指标，ROIC，就是投入资本回报率，长期ROIC能否超过15%很重要。 \u003c/p\u003e\u003cp data-pid=\"eYBL7b6o\"\u003eROIC = 息税前利润EBIT x (1 - 税率) /(有息负债 + 股东权益 - 现金及其等价物)\u003c/p\u003e\u003cp data-pid=\"vD0dSynv\"\u003e2021年，张磊高位卖出茅台，看好江小白，他觉得年轻人的社交货币正在切换，偏好从高度白酒向低度酒转变。不同价格带的白酒就是不同的赛道，几十块钱的白酒是消费品，几百上千块的属于商务宴请奢侈品，服从性测试的载体，酒有成瘾性是个好赛道，但酒桌文化终将消逝，白酒销量从16年见顶后，持续下滑。  \u003c/p\u003e\u003cp data-pid=\"Ieu4JcTI\"\u003e投资高手的策略是灵活的，不是刻舟求剑，不能只看财报，只搞估值模型，他们会扒某书评论区，统计关键词出现频率，跟踪李佳琦直播间，记录话术里的心理学钩子，对比饮料的配料表，找认知差、信息差。 \u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-e4a591d4b88f8e692c35a0c600f4e117_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"867\" data-rawheight=\"486\" data-original-token=\"v2-b84701e0f8a323020c9dfd1d5a970f52\" data-default-watermark-src=\"https://pic2.zhimg.com/v2-089793799816972aa8d3d9d4070fdf3f_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"867\" data-original=\"https://pic4.zhimg.com/v2-e4a591d4b88f8e692c35a0c600f4e117_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"huCLK_RV\"\u003e高瓴资本现在成了蜜雪冰城最大外部股东，因为觉得茶饮行业里，蜜雪冰城的护城河最深，当时很多投资人不好看雪王，觉得廉价，不够高大上。\u003c/p\u003e\u003cp data-pid=\"Kv37IQCt\"\u003e高瓴当年调研雪王，发现2万家门店共用一套原料体系，柠檬原浆从四川安岳直接采购，价格比竞品低40%。三四线城市学生党聚集在店门口，成了奶茶社交据点，复购率比一线城市高20%，店长必须参加培训，加盟模式资产轻，扩张快。\u003c/p\u003e\u003cp data-pid=\"T-H54FMC\"\u003e张磊看懂了雪王的本质，表面上是连锁奶茶店，实际上是原料供应商，风险转移给加盟商，下游客户很稳定，现金流很稳定。\u003c/p\u003e\u003cp data-pid=\"vRMHtuP_\"\u003e当时很多投资人说蜜雪冰城是廉价奶茶，没护城河，高瓴觉得他的毛细血管式渠道网、供应链和极致性价比就是最大的护城河，果断押注，今年IPO后市值超千亿，其他几家做中高端奶茶的纷纷破发。奶茶今年疯狂打价格战，成本上都拼不过蜜雪冰城，倒掉一片竞品后，行业出清，蜜雪冰城反而会活得更好。\u003cbr/\u003e2018年光伏取消补贴，一片哀鸿，很多公司连续跌停。高瓴逆势投资隆基股份，因为觉得隆基单晶硅片成本比多晶硅低不少，度电成本接近煤电。\u003c/p\u003e\u003cp data-pid=\"JNJt3bda\"\u003e取消补贴后，会有很多骗补贴的倒闭，龙头有成本优势能活下来，很多竞争对手倒了，竞争格局反而会变好，高瓴选择在这个时候抄底，也是很明智的。\u003c/p\u003e\u003cp data-pid=\"GFsv-OnU\"\u003e预判了上面的预判，当然也可能是LD的朋友，提前3年布局中国“双碳”战略，赌光伏将成新能源基建核心，推动隆基签署大订单，隆基股价5年涨7倍。\u003cbr/\u003e高瓴资本陪跑百济神州11年，如今业绩开始兑现，已经是国内创新药top3了，高瓴也开始兑现股权，当年百济神州的PD-1还八字没一撇，高瓴就开始调研，创新药差不多是风险最大的行业，新药研发是十年磨一剑，需要经历药物发现、临床前、临床1、2、3期，first in class的成功率在8%左右，国内以mee too为主，成功率高一些。\u003c/p\u003e\u003cp data-pid=\"tiyUi5Jc\"\u003e15年后国内创新药进入红利期，但是到20年集采和医保谈判加剧后，行业进行调整期，高瓴资本吃到了一个创新药完整的红利，PD-1大单品出来后，现在百济神州市值277亿美金，高瓴浮盈超百亿。\u003cbr/\u003e投资行业的马太效应很强，他们掌握的一手信息资源，以及投研资源，不是普通人可比的，尤其是一级市场，跟踪一些前沿赛道，说不定就是未来股市的主线。\u003c/p\u003e\u003cp data-pid=\"9R-f0LYu\"\u003e高瓴总是能赶在风口早期入场，在黑天鹅之前跑路，擅长在行业至暗时刻收集带血的筹码，在行业出清之前抄底龙头，等竞争格局改善后，龙头反而受益。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":1183,"thumbnails":["https://pic1.zhimg.com/50/v2-7e268d109d7917c2ec8b4264e1050274_720w.jpg?source=b6762063"],"favorite_count":17,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921292817675850497}","attached_info":"CpwGCO7Mh8P08/aupAEQBBoJNzMzOTkwNTI2IL/C78IGKBEwAkBdSigKE1RTX1NPVVJDRV9GRUVEUkVfVjkSATAYACAAOgp7InJhdyI6IiJ9Wgg2NzE5ODQ1NWIgYjFlYTFmZThjMjk1ZDk3ZGRjNTE2N2MzNmRkMTY3NjdyEzE5MjEyOTI4MTc2NzU4NTA0OTeKAQk0NzIxMzUzMjeqAQlyZWNvbW1lbmTCASA5NDc4OGQxNmRjZDMyYzAzMzZhZjc4NGQxNDMyOGNiMPIBCggMEgZOb3JtYWzyASgIChIkM2ZlOTZhYjMtN2MwMi00NDZmLWI3YjItMGY5NDU2ZThiMDE08gEGCAsSAjE2ggIAiAKDwMzN+jKSAiA5NDc4OGQxNmRjZDMyYzAzMzZhZjc4NGQxNDMyOGNiMJoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXaAhNUU19TT1VSQ0VfRkVFRFJFX1Y56AIE+gILTk9STUFMX0ZMT1eKAyBiY2RkYjMxNjQ2YTY0NDQ3YjA4YmJhZmZjNDIxODI3YpoDDQoCdjIQABoFb3RoZXKoA58J2AMA6gMJZmVlZHJlX3Y5+gOsARIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgDEOMGGOYDIiN2Mi01ODZlMzA1NmM0MjcxNWU3ODQ3NjU0MjBiZWFmNTk0MDotCAMQ2A4Y5QkiI3YyLWFiNGFhNGY4MGQ2NGNlNmNmZDVhNjU3NzE0NTcwMDZlOi0IAxDjBhjmAyIjdjItYjg0NzAxZTBmOGEzMjMwMjBjOWRmZDFkNWE5NzBmNTKABACIBACSBAZOb3JtYWyaBAE0oAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAACC5OKw/gQUAAAAAAAAAAIkFXUNcxCWY0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFEJAGAKAGY6gGAJICLgoJNzMzOTkwNTI2EhMxOTIxMjkyODE3Njc1ODUwNDk3GAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"94_1750898778.435","type":"feed","offset":94,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898778,"updated_time":1750898778,"target":{"id":"1921383635392657347","type":"answer","url":"https://api.zhihu.com/answers/1921383635392657347","author":{"id":"018dbdf61bfdd6679e034f9c7b7ab077","url":"https://api.zhihu.com/people/018dbdf61bfdd6679e034f9c7b7ab077","user_type":"people","url_token":"90-60-22-92-23","name":"新月","headline":"一瓶不满半瓶逛荡","avatar_url":"https://pic1.zhimg.com/50/v2-1abe7b115ea0ab9e5dfe334d5a1fef38_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":0,"is_following":false,"is_followed":false},"created_time":1750873556,"updated_time":1750873556,"voteup_count":0,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"642969050","type":"question","url":"https://api.zhihu.com/questions/642969050","author":{"id":"8f03fef40943105f134d6cd398f02dc5","url":"https://api.zhihu.com/people/8f03fef40943105f134d6cd398f02dc5","user_type":"people","url_token":"fogb","name":"一抁","headline":"","avatar_url":"https://pic1.zhimg.com/50/v2-26f825123a558c1f25c740570b77cd48_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":0,"is_following":false,"is_followed":false},"title":"可以推荐一下正在读的书吗?","created":1707065639,"answer_count":0,"follower_count":0,"comment_count":28,"bound_topic_ids":[53,1164,32280],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"一、余秋雨老师的《文化苦旅》，这本书是我高中时候读的为数不多的课外书中印象最深的一本，毕业10年后又买来一本新装订版认认真真的边读边做笔记，花费了一个多月的时间过完一遍，它是一本以游记的形式为读者展示余秋雨老师所到达的一个个地方的风土人情历史故事等等，书中大多地方都是我从没去过甚至都没有听说过的，听着余秋雨老师不疾不徐的一个个叙述下来，使我感触良多，书中很多地名典故都被我记录在笔记中，以后等孩子大…","excerpt_new":"一、余秋雨老师的《文化苦旅》，这本书是我高中时候读的为数不多的课外书中印象最深的一本，毕业10年后又买来一本新装订版认认真真的边读边做笔记，花费了一个多月的时间过完一遍，它是一本以游记的形式为读者展示余秋雨老师所到达的一个个地方的风土人情历史故事等等，书中大多地方都是我从没去过甚至都没有听说过的，听着余秋雨老师不疾不徐的一个个叙述下来，使我感触良多，书中很多地名典故都被我记录在笔记中，以后等孩子大…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"DRiE-em1\"\u003e一、余秋雨老师的《文化苦旅》，这本书是我高中时候读的为数不多的课外书中印象最深的一本，毕业10年后又买来一本新装订版认认真真的边读边做笔记，花费了一个多月的时间过完一遍，它是一本以游记的形式为读者展示余秋雨老师所到达的一个个地方的风土人情历史故事等等，书中大多地方都是我从没去过甚至都没有听说过的，听着余秋雨老师不疾不徐的一个个叙述下来，使我感触良多，书中很多地名典故都被我记录在笔记中，以后等孩子大些，准备带着他把这些地方都走一遍，当问到正在读的书，我脑子里立刻就浮现了这本书\u003c/p\u003e\u003cp data-pid=\"T6a3ipKS\"\u003e二、《千年一叹》也是余秋雨老师的书，《文化苦旅》是余秋雨老师的国内游，《千年一叹》是世界游，都说读万卷书不如行万里路，但我读完这本书，却有种不仅走了几十万里甚至跨越千年之感\u003c/p\u003e\u003cp data-pid=\"U519-RUs\"\u003e三、杜月笙，这是我知乎好书推荐中发现的一个人，目前还没有找到装订比较好的写他的书，只在网上粗略看了一部分《晚清民国风云人物杜月笙》，这是我第一次从除了电视剧以外的地方认识杜月笙，这是个小人物为了出人头地犯了很多错误，他把这些亏欠都一一记在心里，当他有能力的第一时间就跑回来双倍甚至多倍赔偿这些曾因为他犯错有过损失的人，这个话不多，没脾气，脑子好使的小青年人格形象特别吸引我，虽然还没有找到合适的纸质版书籍，但目测我会找来认真读完，可能会把他当做我家娃的启蒙人物\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":35,"favorite_count":1,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921383635392657347}","attached_info":"CoIGCO7Mh8P08/aupAEQBBoJNzM0MDI4OTg4INTr8MIGKAAwAEBeSiQKGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDISATAYACAAOgBKLwokVFNfU09VUkNFX1dBUk1VUF9UV09UT1dFUl9FWFBWMl9URVhUEgEwGAAgADoAWgkxMDUxNTk2MjNiIGIxZWExZmU4YzI5NWQ5N2RkYzUxNjdjMzZkZDE2NzY3chMxOTIxMzgzNjM1MzkyNjU3MzQ3igEJNjQyOTY5MDUwqgEJcmVjb21tZW5kwgEgMDE4ZGJkZjYxYmZkZDY2NzllMDM0ZjljN2I3YWIwNzfyAQoIDBIGTm9ybWFs8gEoCAoSJDVjODFkNGQ5LTVhMTItNDY1OS1iOTI0LTFmODM4MTZlZjMzNvIBBggLEgIxNoICAIgCg8DMzfoykgIgMDE4ZGJkZjYxYmZkZDY2NzllMDM0ZjljN2I3YWIwNzeaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIYQ29udGVudFdhcm1VcEJyZWFrSW5SdWxl2gIZVFNfU09VUkNFX1dBUk1fVVBfTk9STUFMMugCAvoCC05PUk1BTF9GTE9XigMgYmNkZGIzMTY0NmE2NDQ0N2IwOGJiYWZmYzQyMTgyN2KaAw0KAnYyEAAaBW90aGVyqAMj2AMA6gMvY29udGVudFdhcm11cFR3b1Rvd2VyVHZwVGV4dE5vcm1hbEV4cFYyUmVjYWxsZXL6Ax8SDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFgAQAiAQAkgQGTm9ybWFsmgQBMqAEAKgEALAEALoEAmFpwgQDNDAwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAABAp36ZP4EFAAAAAAAAAACJBV1DXMQlmNI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRCQBgCgBmSoBgGSAi4KCTczNDAyODk4OBITMTkyMTM4MzYzNTM5MjY1NzM0NxgEIgpJTUFHRV9URVhU","action_card":false},{"id":"95_1750898778.786","type":"feed","offset":95,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750898778,"updated_time":1750898778,"target":{"id":"677607581","type":"article","url":"https://api.zhihu.com/articles/677607581","author":{"id":"acb36951daada520a426b822c9d1c020","url":"https://api.zhihu.com/people/acb36951daada520a426b822c9d1c020","user_type":"people","url_token":"lemonround","name":"猛猿","headline":"公众号：大猿搬砖简记","avatar_url":"https://picx.zhimg.com/50/v2-6304b8f8dd717ed99eeddd211d5714d1_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":29397,"is_following":false,"is_followed":false},"title":"图解大模型RLHF系列之：人人都能看懂的PPO原理与源码解读","image_url":"https://pic1.zhimg.com/v2-da82528111d2fb57519d4ddab1cd93f1.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1705118752,"updated":1734413350,"voteup_count":3709,"voting":0,"comment_count":239,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"【20241118更新】 去年此时我写了这篇文章，当时的主要目的是，想让读者在没有RL知识的情况下，能从直觉上快速理解代码，以便上手训练和修改。由于一切从“直觉”出发，因此有很多表述不准确的地方，所以最近我写了一篇比较严谨的理论文章：猛猿：人人都能看懂的RLHF-PPO理论知识 ，大家可以从这篇文章中找到更精准的解读。大家好，最近我又读了读RLHF的相关paper和一些开源实践，有了一些心得体会，整理成这篇文章。 过去在RLHF…","excerpt_new":"【20241118更新】 去年此时我写了这篇文章，当时的主要目的是，想让读者在没有RL知识的情况下，能从直觉上快速理解代码，以便上手训练和修改。由于一切从“直觉”出发，因此有很多表述不准确的地方，所以最近我写了一篇比较严谨的理论文章：猛猿：人人都能看懂的RLHF-PPO理论知识 ，大家可以从这篇文章中找到更精准的解读。大家好，最近我又读了读RLHF的相关paper和一些开源实践，有了一些心得体会，整理成这篇文章。 过去在RLHF…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"N5JyDnbE\"\u003e\u003cb\u003e\u003ci\u003e【20241118更新】\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"N7RXpJbj\"\u003e\u003cb\u003e\u003ci\u003e去年此时我写了这篇文章，当时的主要目的是，想让读者在没有RL知识的情况下，能从直觉上快速理解代码，以便上手训练和修改。由于一切从“直觉”出发，因此有很多表述不准确的地方，所以最近我写了一篇比较严谨的理论文章：\u003ca href=\"https://zhuanlan.zhihu.com/p/7461863937\" class=\"internal\" target=\"_blank\"\u003e猛猿：人人都能看懂的RLHF-PPO理论知识\u003c/a\u003e，大家可以从这篇文章中找到更精准的解读。\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"n9_J0Wfr\"\u003e大家好，最近我又读了读RLHF的相关paper和一些开源实践，有了一些心得体会，整理成这篇文章。\u003cbr/\u003e过去在RLHF的初学阶段，\u003cb\u003e有一个问题最直接地困惑着我\u003c/b\u003e：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Yxw8FrW0\"\u003e如何在NLP语境下理解强化学习的框架？例如，我知道强化学习中有Agent、Environment、Reward、State等要素，但是在NLP语境中，它们指什么？语言模型又是如何根据奖励做更新的？\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"KNFhjsA2\"\u003e为了解答这个问题，我翻阅了很多资料，看了许多的公式推导，去研究RLHF的整体框架和loss设计。虽然吭吭哧哧地入门了，但是这个过程实在痛苦，\u003cb\u003e最主要的原因是：理论的部分太多，直观的解释太少\u003c/b\u003e。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e所以，在写这篇文章时，\u003cb\u003e我直接从一个RLHF开源项目源码入手（deepspeed-chat），根据源码的实现细节，给出尽可能丰富的训练流程图，并对所有的公式给出直观的解释。希望可以帮助大家更具象地感受RLHF的训练流程。对于没有强化学习背景的朋友，也可以无痛阅读本文\u003c/b\u003e。关于RLHF，各家的开源代码间都会有一些差异，同时也不止PPO一种RLHF方式。感兴趣的朋友，也可以读读别家的源码，做一些对比。后续有时间，这个系列也会对各种RLHF方式进行比较。\u003c/p\u003e\u003cp data-pid=\"l_NZepWG\"\u003e【\u003cb\u003e如果觉得本文有帮助，欢迎点赞收藏和喜欢～】\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"zz-CR_BH\"\u003e\u003cb\u003e与LLM相关的往期技术文章汇总可见：\u003c/b\u003e\u003c/p\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/654910335\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-4e60ef5893edb8167b9111d281fdee24_l.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\"internal\"\u003e猛猿：【必看】历史技术文章汇总导航\u003c/a\u003e\u003cp data-pid=\"lYIQTu3a\"\u003e\u003cb\u003eDPO解读可见：\u003c/b\u003e\u003c/p\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/721073733\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://picx.zhimg.com/v2-571d6fac046c58f9a1039d62edcb84cf_qhd.jpg\" data-image-width=\"1446\" data-image-height=\"536\" class=\"internal\"\u003e猛猿：人人都能看懂的DPO数学原理\u003c/a\u003e\u003chr/\u003e\u003ch2\u003e一、强化学习概述\u003cbr/\u003e\u003c/h2\u003e\u003ch3\u003e1.1 强化学习整体流程\u003cbr/\u003e\u003c/h3\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-b547384a5307e3ed0a65ae9e6ba01491_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2088\" data-rawheight=\"1334\" data-original-token=\"v2-115a06c1a638e093e36a2c2508a98785\" class=\"origin_image zh-lightbox-thumb\" width=\"2088\" data-original=\"https://pic4.zhimg.com/v2-b547384a5307e3ed0a65ae9e6ba01491_r.jpg\"/\u003e\u003c/figure\u003e\u003cul\u003e\u003cli data-pid=\"4g_aLeat\"\u003e强化学习的两个实体：\u003cb\u003e智能体（Agent）\u003c/b\u003e与\u003cb\u003e环境（Environment）\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"jPlz3ady\"\u003e强化学习中两个实体的交互：\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"c1MHheKT\"\u003e\u003cb\u003e状态空间S\u003c/b\u003e：S即为State，指环境中所有可能状态的集合\u003c/li\u003e\u003cli data-pid=\"IY-agVUu\"\u003e\u003cb\u003e动作空间A\u003c/b\u003e：A即为Action，指智能体所有可能动作的集合\u003c/li\u003e\u003cli data-pid=\"WUO_7ZL6\"\u003e\u003cb\u003e奖励R：\u003c/b\u003eR即为Reward，指智能体在环境的某一状态下所获得的奖励。\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp data-pid=\"oxUz445E\"\u003e以上图为例，智能体与环境的交互过程如下：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"ZkBYjzh-\"\u003e在 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 时刻，环境的状态为 \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%7D\" alt=\"S_{t}\" eeimg=\"1\"/\u003e ，达到这一状态所获得的奖励为 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003cli data-pid=\"rjMqyHQC\"\u003e智能体观测到 \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%7D\" alt=\"S_{t}\" eeimg=\"1\"/\u003e 与 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e ，采取相应动作 \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003cli data-pid=\"j-4wyoTu\"\u003e智能体采取 \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 后，环境状态变为 \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D\" alt=\"S_{t+1}\" eeimg=\"1\"/\u003e ，得到相应的奖励 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D\" alt=\"R_{t+1}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Fo2cyvSd\"\u003e\u003cbr/\u003e智能体在这个过程中学习，它的最终目标是：\u003cb\u003e找到一个策略，这个策略根据当前观测到的环境状态和奖励反馈，来选择最佳的动作。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e1.2 价值函数\u003c/h3\u003e\u003cp data-pid=\"sL98OVhi\"\u003e在1.1中，我们谈到了奖励值 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e ，它表示环境进入状态 \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%7D\" alt=\"S_{t}\" eeimg=\"1\"/\u003e 下的\u003cb\u003e即时奖励\u003c/b\u003e。\u003cbr/\u003e\u003cb\u003e但如果只考虑即时奖励，目光似乎太短浅了\u003c/b\u003e：当下的状态和动作会影响到未来的状态和动作，进而影响到未来的整体收益。\u003cbr/\u003e所以，一种更好的设计方式是：\u003cb\u003et时刻状态s的总收益 = 身处状态s能带来的\u003cu\u003e即时收益\u003c/u\u003e + 从状态s出发后能带来的\u003cu\u003e未来收益\u003c/u\u003e。\u003c/b\u003e写成表达式就是：\u003c/p\u003e\u003cp data-pid=\"NarzmZ9Y\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D+%3D+R_%7Bt%7D+%2B+%5Cgamma+V_%7Bt%2B1%7D\" alt=\"V_{t} = R_{t} + \\gamma V_{t+1}\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"E4YjQ9Kg\"\u003e其中：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"mEokWPUL\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e ： \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 时刻的总收益，注意这个收益蕴涵了“即时”和“未来”的概念\u003c/li\u003e\u003cli data-pid=\"nzKOPtad\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e ： \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 时刻的即时收益\u003c/li\u003e\u003cli data-pid=\"lA3VKEOz\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%2B1%7D\" alt=\"V_{t+1}\" eeimg=\"1\"/\u003e ： \u003cimg src=\"https://www.zhihu.com/equation?tex=t%2B1\" alt=\"t+1\" eeimg=\"1\"/\u003e 时刻的总收益，注意这个收益蕴涵了“即时”和“未来”的概念。而 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%2B1%7D\" alt=\"V_{t+1}\" eeimg=\"1\"/\u003e 对 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e 来说就是“未来”。\u003c/li\u003e\u003cli data-pid=\"vtnShSeJ\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/\u003e ：折扣因子。它决定了我们在多大程度上考虑将“未来收益”纳入“当下收益”。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"p4BWTcYY\"\u003e\u003cbr/\u003e\u003cbr/\u003e注：在这里，我们不展开讨论RL中关于价值函数的一系列假设与推导，而是直接给出一个便于理解的简化结果，方便没有RL背景的朋友能倾注更多在“PPO策略具体怎么做”及“对PPO的直觉理解”上。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e二、NLP中的强化学习\u003c/h2\u003e\u003cp data-pid=\"fhNxk4xp\"\u003e我们在第一部分介绍了通用强化学习的流程，那么我们要怎么把这个流程对应到NLP任务中呢？\u003cb\u003e换句话说，NLP任务中的智能体、环境、状态、动作等等，都是指什么呢？\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-eb250d428d3b9a751d4ba3aeae70e290_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1141\" data-rawheight=\"662\" data-original-token=\"v2-cf1b7d2980e856afcca6a9794f8baa09\" class=\"origin_image zh-lightbox-thumb\" width=\"1141\" data-original=\"https://pica.zhimg.com/v2-eb250d428d3b9a751d4ba3aeae70e290_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"-j3CrF2h\"\u003e回想一下我们对NLP任务做强化学习（RLHF）的目的：\u003cb\u003e我们希望给模型一个prompt，让模型能生成符合人类喜好的response\u003c/b\u003e。再回想一下gpt模型做推理的过程：\u003cb\u003e每个时刻\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e \u003cb\u003e只产生一个token，即token是一个一个蹦出来的，先有上一个token，再有下一个token。\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e复习了这两点，现在我们可以更好解读上面这张图了：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"SvLS9EUF\"\u003e我们先喂给模型一个prompt，期望它能产出符合人类喜好的response\u003c/li\u003e\u003cli data-pid=\"mMvzkFyK\"\u003e在 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 时刻，模型根据上文，产出一个token，\u003cb\u003e这个token即对应着强化学习中的动作，我们记为\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 。因此不难理解，在NLP语境下，强化学习任务的动作空间就对应着词表。\u003c/li\u003e\u003cli data-pid=\"OT-B2voM\"\u003e在 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 时刻，\u003cb\u003e模型产出token \u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e对应着的即时收益为\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e，总收益为\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e（\u003c/b\u003e复习一下， \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e 蕴含着“即时收益”与“未来收益”两个内容）。这个收益即可以理解为“\u003cb\u003e对人类喜好的衡量\u003c/b\u003e”。此刻，\u003cb\u003e模型的状态从\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%7D\" alt=\"S_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e变为\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D\" alt=\"S_{t+1}\" eeimg=\"1\"/\u003e \u003cb\u003e，也就是从“上文”变成“上文 + 新产出的token”\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"IkUR2xEZ\"\u003e在NLP语境下，智能体是语言模型本身，环境则对应着它产出的语料\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"7lXsAN3s\"\u003e这样，我们就大致解释了NLP语境下的强化学习框架，不过针对上面这张图，你可能还有以下问题：\u003c/p\u003e\u003cp data-pid=\"sqjAK-_d\"\u003e\u003cbr/\u003e\u003cb\u003e（1）问题1：图中的下标是不是写得不太对？例如根据第一部分的介绍，\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e应该对应着\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D\" alt=\"R_{t+1}\" eeimg=\"1\"/\u003e \u003cb\u003e，\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%2B1%7D\" alt=\"A_{t+1}\" eeimg=\"1\"/\u003e \u003cb\u003e应该对应着\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%2B2%7D\" alt=\"R_{t+2}\" eeimg=\"1\"/\u003e \u003cb\u003e，以此类推？\u003c/b\u003e\u003cbr/\u003e答：你说的对。但这里我们不用太纠结下标的问题，只需要记住在对应的response token位置，会产生相应的即时奖励和总收益即可。之所以用图中这样的下标，是更方便我们后续理解代码。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e（2）问题2：我知道\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e肯定是由语言模型产生的，那么\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D%EF%BC%8CV_%7Bt%7D\" alt=\"R_{t}，V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e是怎么来的呢，也是语言模型产生的吗？\u003c/b\u003e\u003cbr/\u003e答：先直接说结论， \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 是由我们的语言模型产生的， \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D%EF%BC%8CV_%7Bt%7D\" alt=\"R_{t}，V_{t}\" eeimg=\"1\"/\u003e 则分别由另外两个模型来产生，在后文中我们会细说。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e（3）问题3：语言模型的参数在什么时候更新？是观测到一个\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D%2C+V_%7Bt%7D\" alt=\"R_{t}, V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e，就更新一次参数，然后再去产生\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%2B1%7D\" alt=\"A_{t+1}\" eeimg=\"1\"/\u003e \u003cb\u003e吗？\u003c/b\u003e\u003cbr/\u003e答：当然不是。你只看到某个时刻的收益，就急着用它更新模型，这也太莽撞了。我们肯定是要等有足够的观测数据了（例如等模型把完整的response生成完），再去更新它的参数。这一点我们也放在后文细说。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e（4）问题4：再谈谈\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D%2C+V_%7Bt%7D\" alt=\"R_{t}, V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e吧，在NLP的语境下我还是不太理解它们\u003c/b\u003e\u003cbr/\u003e答：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"2IiieYux\"\u003e首先，“收益”的含义是“对人类喜好的衡量”\u003c/li\u003e\u003cli data-pid=\"50PoMG_4\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e ：即时收益，指语言模型当下产生token \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 带来的收益\u003c/li\u003e\u003cli data-pid=\"6YKOoN9L\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e ： 实际期望总收益（即时+未来），指对语言模型“当下产生token \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e ，一直到整个response生产结束”后的期收益预估。因为当下语言模型还没产出 \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 后的token，所以我们只是对它之后一系列动作的收益做了估计，因而称为“期望总收益”。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e三、RLHF中的四个重要角色\u003c/h2\u003e\u003cp data-pid=\"Kwao5ezb\"\u003e本节中，我们在第二部分的基础上更进一步：更详细理清NLP语境下RLHF的运作流程。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e我们从第二部分中已经知道：生成token \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 和对应收益 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D%2C+V_%7Bt%7D\" alt=\"R_{t}, V_{t}\" eeimg=\"1\"/\u003e 的并不是一个模型。那么在RLHF中到底有几个模型？他们是怎么配合做训练的？而我们最终要的是哪个模型？\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-22c2f6fce157dc4385a14f0de50d8136_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"1280\" data-original-token=\"v2-0b6fc7168908a86e632b2c75725d43e6\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pica.zhimg.com/v2-22c2f6fce157dc4385a14f0de50d8136_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"WEs2SC4n\"\u003e如上图，\u003cb\u003e在RLHF-PPO阶段，一共有四个主要模型\u003c/b\u003e，分别是：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"iimyE46g\"\u003e\u003cb\u003eActor Model：演员模型\u003c/b\u003e，这就是我们想要训练的目标语言模型\u003c/li\u003e\u003cli data-pid=\"JFUPGtiD\"\u003e\u003cb\u003eCritic Model：评论家模型\u003c/b\u003e，它的作用是预估总收益 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003cli data-pid=\"Lzo9XvKT\"\u003e\u003cb\u003eReward Model：奖励模型\u003c/b\u003e，它的作用是计算即时收益 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003cli data-pid=\"ahf4u0g_\"\u003e\u003cb\u003eReference Model：参考模型\u003c/b\u003e，它的作用是在RLHF阶段给语言模型增加一些“约束”，防止语言模型训歪（朝不受控制的方向更新，效果可能越来越差）\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"tbUh9Ykb\"\u003e其中:\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"FKj_HYiB\"\u003e\u003cb\u003eActor/Critic Model\u003c/b\u003e在RLHF阶段是\u003cb\u003e需要训练\u003c/b\u003e的（图中给这两个模型加了粗边，就是表示这个含义）；而\u003cb\u003eReward/Reference Model\u003c/b\u003e是\u003cb\u003e参数冻结\u003c/b\u003e的。\u003c/li\u003e\u003cli data-pid=\"mzV4z7Z3\"\u003eCritic/Reward/Reference Model共同组成了一个“奖励-loss”计算体系（我自己命名的，为了方便理解），我们综合它们的结果计算loss，用于更新Actor和Critic Model\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"dBu1ptsz\"\u003e\u003cbr/\u003e\u003cbr/\u003e我们把这四个部分展开说说。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e3.1 Actor Model (演员模型)\u003c/h3\u003e\u003cp data-pid=\"Sz4niRCb\"\u003e正如前文所说，\u003cb\u003eActor就是我们想要训练的目标语言模型。我们一般用SFT阶段产出的SFT模型来对它做初始化。\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-77843f19b61a0b903793b0158e728877_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"971\" data-rawheight=\"701\" data-original-token=\"v2-54289d2eea12291705f6b0ecafef4b5d\" class=\"origin_image zh-lightbox-thumb\" width=\"971\" data-original=\"https://picx.zhimg.com/v2-77843f19b61a0b903793b0158e728877_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"KookNcNh\"\u003e我们的最终目的是让Actor模型能产生符合人类喜好的response。所以我们的策略是，先喂给Actor一条prompt （这里假设batch_size = 1，所以是1条prompt），让它生成对应的response。然后，我们再将“prompt + response\u0026#34;送入我们的“奖励-loss”计算体系中去算得最后的loss，用于更新actor。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e3.2 Reference Model（参考模型）\u003c/h3\u003e\u003cp data-pid=\"vJm4aPez\"\u003e\u003cb\u003eReference Model（以下简称Ref模型）一般也用SFT阶段得到的SFT模型做初始化，在训练过程中，它的参数是冻结的。\u003c/b\u003eRef模型的主要作用是防止Actor”训歪”，那么它具体是怎么做到这一点的呢？\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-c4030ebb772b2619043f46b9ab5e58e8_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"531\" data-original-token=\"v2-965b5de2f2f125b488ef71af5cb9786a\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pica.zhimg.com/v2-c4030ebb772b2619043f46b9ab5e58e8_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"xwTnTGL7\"\u003e“防止模型训歪”换一个更详细的解释是：\u003cb\u003e我们希望训练出来的Actor模型既能达到符合人类喜好的目的，又尽量让它和SFT模型不要差异太大\u003c/b\u003e。简言之，\u003cb\u003e我们希望两个模型的输出分布尽量相似\u003c/b\u003e。那什么指标能用来衡量输出分布的相似度呢？我们自然而然想到了\u003cb\u003eKL散度\u003c/b\u003e。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"hGpDarjE\"\u003e如图所示：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"tJPd04m9\"\u003e\u003cb\u003e对Actor模型\u003c/b\u003e，我们喂给它一个prompt，它正常输出对应的response。那么response中每一个token肯定有它对应的log_prob结果呀，我们把这样的结果记为\u003cb\u003elog_probs\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"sI-QH-iF\"\u003e\u003cb\u003e对Ref模型\u003c/b\u003e，我们把Actor生成的\u0026#34;prompt + response\u0026#34;喂给它，那么它同样能给出每个token的log_prob结果，我们记其为\u003cb\u003eref_log_probs\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"mNtRk5Xk\"\u003e那么这两个模型的输出分布相似度就可以用\u003cb\u003e\u003ccode\u003eref_log_probs - log_probs\u003c/code\u003e\u003c/b\u003e来衡量，我们可以从两个方面来理解这个公式：\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"zWkLVdlm\"\u003e\u003cb\u003e从直觉上理解\u003c/b\u003e，ref_log_probs越高，说明Ref模型对Actor模型输出的肯定性越大。即Ref模型也认为，对于某个 \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%7D\" alt=\"S_{t}\" eeimg=\"1\"/\u003e ，输出某个 \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 的概率也很高（ \u003cimg src=\"https://www.zhihu.com/equation?tex=P%28A_%7Bt%7D+%7C+S_%7Bt%7D%29\" alt=\"P(A_{t} | S_{t})\" eeimg=\"1\"/\u003e ）。这时可以认为Actor模型较Ref模型没有训歪\u003c/li\u003e\u003cli data-pid=\"SiJ2AaE1\"\u003e\u003cb\u003e从KL散度上理解\u003c/b\u003e， \u003cimg src=\"https://www.zhihu.com/equation?tex=KL%5BActor%28X%29+%7C%7C+Ref%28X%29%5D+%3D+E_%7Bx%5Csim+Actor%28x%29%7D%5Blog%5Cfrac%7BActor%28x%29%7D%7BRef%28x%29%7D%5D+%3D+log%5C_probs+-+ref%5C_log%5C_probs\" alt=\"KL[Actor(X) || Ref(X)] = E_{x\\sim Actor(x)}[log\\frac{Actor(x)}{Ref(x)}] = log\\_probs - ref\\_log\\_probs\" eeimg=\"1\"/\u003e （当然这里不是严格的等于，只是KL散度的近似），这个值越小意味着两个分布的相似性越高。\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp data-pid=\"JGlH4VzA\"\u003e\u003cbr/\u003e\u003cbr/\u003e注：你可能已经注意到，按照KL散度的定义，这里写成\u003ccode\u003elog_probs - ref_log_probs\u003c/code\u003e更合适一些。但是如果你看过一些rlhf相关的论文的话，你可能记得在计算损失函数时，有一项 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D+-+KL%E6%95%A3%E5%BA%A6\" alt=\"R_{t} - KL散度\" eeimg=\"1\"/\u003e （对这个有疑惑不要紧，我们马上在后文细说），即KL散度前带了负号，所以这里我写成\u003ccode\u003eref_log_probs - log_probs\u003c/code\u003e这样的形式，更方便大家从直觉上理解这个公式。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"CDyU6lRd\"\u003e现在，我们已经知道怎么利用Ref模型和KL散度来防止Actor训歪了。\u003cb\u003eKL散度将在后续被用于loss的计算\u003c/b\u003e，我们在后文中会详细解释。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e3.3 Critic Model（评论家模型）\u003c/h3\u003e\u003cp data-pid=\"crlqvvil\"\u003e\u003cb\u003eCritic Model用于预测期望总收益\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e，和Actor模型一样，它需要做参数更新\u003c/b\u003e。实践中，Critic Model的设计和初始化方式也有很多种，例如和Actor共享部分参数、从RW阶段的Reward Model初始化而来等等。我们讲解时，和deepspeed-chat的实现保持一致：从RW阶段的Reward Model初始化而来。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e你可能想问：训练Actor模型我能理解，但我还是不明白，为什么要单独训练一个Critic模型用于预测收益呢？\u003c/b\u003e\u003cbr/\u003e这是因为，当我们在前文讨论总收益 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e （即时 + 未来）时，我们是站在上帝视角的，也就是这个 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e 就是客观存在的、真正的总收益。但是我们在训练模型时，就没有这个上帝视角加成了，\u003cb\u003e也就是在\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e \u003cb\u003e时刻，我们给不出客观存在的总收益\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e，我们只能训练一个模型去预测它。\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e所以总结来说，在RLHF中，我们不仅要训练模型生成符合人类喜好的内容的能力（Actor），也要提升模型对人类喜好量化判断的能力（Critic）\u003c/b\u003e。这就是Critic模型存在的意义。我们来看看它的大致架构：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-6d1497cc608b9b5fd059870c7117e381_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"921\" data-rawheight=\"711\" data-original-token=\"v2-c33716e19b50e58b0935f8d104636a20\" class=\"origin_image zh-lightbox-thumb\" width=\"921\" data-original=\"https://pic4.zhimg.com/v2-6d1497cc608b9b5fd059870c7117e381_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"ZVhV93Kc\"\u003edeepspeed-chat采用了Reward模型作为它的初始化，所以这里我们也按Reward模型的架构来简单画画它。你可以简单理解成，Reward/Critic模型和Actor模型的架构是很相似的（毕竟输入都一样），同时，它在最后一层增加了一个Value Head层，该层是个简单的线形层，用于将原始输出结果映射成单一的 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e 值。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e在图中， \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e 表示Critic模型对 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 时刻及未来（response完成）的收益预估。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e3.4 Reward Model（奖励模型）\u003c/h3\u003e\u003cp data-pid=\"DIDPtjYP\"\u003eReward Model用于计算生成token \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 的即时收益，它就是RW阶段所训练的奖励模型，在RLHF过程中，它的参数是冻结的。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e你可能想问：为什么Critic模型要参与训练，而同样是和收益相关的Reward模型的参数就可以冻结呢？\u003c/b\u003e\u003cbr/\u003e这是因为，Reward模型是站在上帝视角的。这个上帝视角有两层含义：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"KDaHHuWR\"\u003e第一点，Reward模型是经过和“估算收益”相关的训练的，因此在RLHF阶段它可以直接被当作一个能产生客观值的模型。\u003c/li\u003e\u003cli data-pid=\"PNn_CPSM\"\u003e第二点，Reward模型代表的含义就是“即时收益”，你的token \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 已经产生，因此即时收益自然可以立刻算出。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"WEq06gnx\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e你还可能想问：我已经用Critic预测出\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e了，而这个\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e包含了“即时”和“未来”的概念，那我还需要代表“即时”的\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e做什么呢？直接用\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e不就好了吗？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"xwz8gXFp\"\u003e\u003cbr/\u003e为了解答这个问题，我们先回顾下1.2部分中给出的价值函数： \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D+%3D+R_%7Bt%7D+%2B+%5Cgamma+V_%7Bt%2B1%7D\" alt=\"V_{t} = R_{t} + \\gamma V_{t+1}\" eeimg=\"1\"/\u003e \u003cbr/\u003e这个函数告诉我们，我们当前可以用两个结果来表示 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 时刻的总收益：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"GWR95fqn\"\u003e结果1：Critic模型预测的 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003cli data-pid=\"_z5AfeaN\"\u003e结果2：Reward模型预测的 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e 和critic模型预测的 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%2B1%7D\" alt=\"V_{t+1}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"PtNQCYVg\"\u003e那么哪一个结果更靠近上帝视角给出的客观值呢？当然是结果2，因为结果1全靠预测，而结果2中的 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e 是事实数据。\u003cbr/\u003e我们知道Critic模型也是参与参数更新的，我们可以用\u003ccode\u003eMSE(上帝视角的客观收益-Critic模型预测的收益)\u003c/code\u003e来衡量它的loss。\u003cb\u003e但是上帝视角的客观收益我们是不知道的，只能用已知事实数据去逼近它，所以我们就用\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D\" alt=\"R_{t} + \\gamma * V_{t+1}\" eeimg=\"1\"/\u003e \u003cb\u003e来做近似。\u003c/b\u003e这就是 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D%2C+V_%7Bt%7D\" alt=\"R_{t}, V_{t}\" eeimg=\"1\"/\u003e 同时存在的意义\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"vWFadHm5\"\u003eReward模型和critic模型非常相似，这里我们就只给出架构图，不再做过多的说明。关于Reward模型的训练过程，后续有时间也会出个原理和代码解析。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-96be8ab460ede07b5879ebc27400659f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"921\" data-rawheight=\"711\" data-original-token=\"v2-efe6b22fdd0782f066fb993a2f6c7814\" class=\"origin_image zh-lightbox-thumb\" width=\"921\" data-original=\"https://picx.zhimg.com/v2-96be8ab460ede07b5879ebc27400659f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e四、RLHF中的loss计算\u003c/h2\u003e\u003cp data-pid=\"8U3FD0oa\"\u003e到目前为止，我们已经基本了解了RLHF的训练框架，以及其中的四个重要角色（训练一个RLHF，有4个模型在硬件上跑，可想而知对存储的压力）。在本节中，我们一起来解读RLHF的loss计算方式。在解读中，我们会再一次理一遍RLHF的整体训练过程，填补相关细节。在这之后，我们就可以来看代码解析了。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e在第三部分的讲解中，我们知道Actor和Critic模型都会做参数更新，所以我们的loss也分成2个：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"KN0TY51L\"\u003e\u003cb\u003eActor loss：\u003c/b\u003e用于评估Actor是否产生了符合人类喜好的结果，将作用于Actor的BWD上。\u003c/li\u003e\u003cli data-pid=\"JE1qEscX\"\u003e\u003cb\u003eCritic loss：\u003c/b\u003e用于评估Critic是否正确预测了人类的喜好，将作用于Critic的BWD上。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"ywn9qlTY\"\u003e\u003cbr/\u003e我们详细来看这两者。\u003c/p\u003e\u003ch3\u003e4.1 Actor loss\u003c/h3\u003e\u003ch3\u003e（1）直观设计\u003c/h3\u003e\u003cp data-pid=\"ykfAuw89\"\u003e我们先来看一个直观的loss设计方式：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"e0z2QBMz\"\u003eActor接收到当前上文 \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%7D\" alt=\"S_{t}\" eeimg=\"1\"/\u003e ，产出token \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e （ \u003cimg src=\"https://www.zhihu.com/equation?tex=P%28A_%7Bt%7D+%7C+S_%7Bt%7D%29\" alt=\"P(A_{t} | S_{t})\" eeimg=\"1\"/\u003e ）\u003c/li\u003e\u003cli data-pid=\"W2ICSX66\"\u003eCritic根据 \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%7D%2C+A_%7Bt%7D\" alt=\"S_{t}, A_{t}\" eeimg=\"1\"/\u003e ，产出对总收益的预测 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003cli data-pid=\"aez1zJqj\"\u003e那么Actor loss可以设计为： \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-%7B%5Ctextstyle%5Csum_%7Bt+%5Cin+response%5C_timestep%7D%7D+V_%7Bt%7Dlog+P%28A_%7Bt%7D%7CS_%7Bt%7D%29\" alt=\"actor\\_loss = -{\\textstyle\\sum_{t \\in response\\_timestep}} V_{t}log P(A_{t}|S_{t})\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"PMOFu_T-\"\u003e求和符号表示我们只考虑response部分所有token的loss，为了表达简便，我们先把这个求和符号略去（下文也是同理），也就是说：\u003c/p\u003e\u003cp data-pid=\"3T8eSvAp\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-V_%7Bt%7Dlog+P%28A_%7Bt%7D%7CS_%7Bt%7D%29\" alt=\"actor\\_loss = -V_{t}log P(A_{t}|S_{t})\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"8zk_NsPs\"\u003e我们希望minimize这个actor_loss。\u003c/p\u003e\u003cp data-pid=\"KbiSSlU_\"\u003e\u003cbr/\u003e\u003cb\u003e这个设计的直观解释是：\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"bGnKUTMB\"\u003e当 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D%3E0\" alt=\"V_{t}\u0026gt;0\" eeimg=\"1\"/\u003e 时，意味着Critic对Actor当前采取的动作给了正向反馈，因此我们就需要在训练迭代中提高 \u003cimg src=\"https://www.zhihu.com/equation?tex=P%28A_%7Bt%7D+%7C+S_%7Bt%7D%29\" alt=\"P(A_{t} | S_{t})\" eeimg=\"1\"/\u003e ，这样就能达到减小loss的作用。\u003c/li\u003e\u003cli data-pid=\"MhWY2USd\"\u003e当 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D+%3C+0\" alt=\"V_{t} \u0026lt; 0\" eeimg=\"1\"/\u003e 时，意味着Critic对Actor当前采取的动作给了负向反馈，因此我们就需要在训练迭代中降低 \u003cimg src=\"https://www.zhihu.com/equation?tex=P%28A_%7Bt%7D+%7C+S_%7Bt%7D%29\" alt=\"P(A_{t} | S_{t})\" eeimg=\"1\"/\u003e ，这样就能到达到减小loss的作用。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Q6LceNpy\"\u003e\u003cb\u003e一句话总结：这个loss设计的含义是，对上文\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=S_%7Bt%7D\" alt=\"S_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e而言，如果token\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e产生的收益较高，那就增大它出现的概率，否则降低它出现的概率。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e（2）引入优势（Advantage）\u003c/h3\u003e\u003cp data-pid=\"Bf4DItL7\"\u003e在开始讲解之前，我们举个小例子：\u003cbr/\u003e假设在王者中，中路想支援发育路，这时中路有两种选择：1. 走自家野区。2. 走大龙路。\u003cbr/\u003e中路选择走大龙路，当她做出这个决定后，Critic告诉她可以收1个人头。结果，此刻对面打野正在自家采灵芝，对面也没有什么苟草英雄，中路一路直上，最终收割2个人头。\u003cbr/\u003e因为实际收割的人头比预期要多1个，中路尝到了甜头，所以她增大了“支援发育路走大龙路”的概率。\u003cbr/\u003e\u003cb\u003e这个多出来的“甜头”，就叫做“优势”(Advantage)。\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e对NLP任务来说，如果Critic对\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e的总收益预测为\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e，但实际执行\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e后的总收益是\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D\" alt=\"R_{t} + \\gamma * V_{t+1}\" eeimg=\"1\"/\u003e \u003cb\u003e，我们就定义优势为：\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"IHefDdza\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D+%3D+R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D+-+V_%7Bt%7D\" alt=\"Adv_{t} = R_{t} + \\gamma * V_{t+1} - V_{t}\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e我们用 \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D\" alt=\"Adv_{t}\" eeimg=\"1\"/\u003e 替换掉 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e ，则此刻actor_loss变为：\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-Adv_%7Bt%7Dlog+P%28A_%7Bt%7D%7CS_%7Bt%7D%29\" alt=\"actor\\_loss = -Adv_{t}log P(A_{t}|S_{t})\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e（3）重新设计 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e \u003c/h3\u003e\u003cp data-pid=\"DHNR_OhT\"\u003e总结一下，到目前为止，我们的actor_loss形式为：\u003c/p\u003e\u003cp data-pid=\"7YPwTQW2\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-Adv_%7Bt%7Dlog+P%28A_%7Bt%7D%7CS_%7Bt%7D%29\" alt=\"actor\\_loss = -Adv_{t}log P(A_{t}|S_{t})\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"iNyn1Zo0\"\u003e其中， \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D+%3D+R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D+-+V_%7Bt%7D\" alt=\"Adv_{t} = R_{t} + \\gamma * V_{t+1} - V_{t}\" eeimg=\"1\"/\u003e \u003cbr/\u003e同时注意，这个actor_loss应该是response的所有token loss的sum或者avg。这里为了表达方便，我们的公式略去了求和或求平均的符号。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e按照这个理解， \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e 应该表示每个Actor产出token \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e 带来的即时收益，正如下图所示（其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/\u003e 表示最后一个时刻）：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-3d8bdc68829d51508017f8b17de85050_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"921\" data-rawheight=\"711\" data-original-token=\"v2-c970c3b8b520cf88d7a42ad1f84a5231\" class=\"origin_image zh-lightbox-thumb\" width=\"921\" data-original=\"https://pic1.zhimg.com/v2-3d8bdc68829d51508017f8b17de85050_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"K8oVRIuV\"\u003e但在deepspeed-chat的RLHF实践中，对 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e 做了另一种设计：\u003c/p\u003e\u003cp data-pid=\"H4Y1gEwQ\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D+%5Cbegin%7Baligned%7D+R_%7Bt%7D+%26%3D+-kl%5C_ctl+%2A+%28log%5Cfrac%7BP%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D%7BP_%7Bref%7D%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D%29%2C+t%5Cneq+T%5C%5C+R_%7Bt%7D+%26%3D+-kl%5C_ctl+%2A+%28log%5Cfrac%7BP%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D%7BP_%7Bref%7D%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D%29%2B+R_%7Bt%7D%2C+t%3DT+%5Cend%7Baligned%7D+%5Cend%7Bmatrix%7D%5Cright.\" alt=\"\\left\\{\\begin{matrix} \\begin{aligned} R_{t} \u0026amp;= -kl\\_ctl * (log\\frac{P(A_{t}|S_{t})}{P_{ref}(A_{t}|S_{t})}), t\\neq T\\\\ R_{t} \u0026amp;= -kl\\_ctl * (log\\frac{P(A_{t}|S_{t})}{P_{ref}(A_{t}|S_{t})})+ R_{t}, t=T \\end{aligned} \\end{matrix}\\right.\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"_c1_lstk\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=kl%5C_ctl\" alt=\"kl\\_ctl\" eeimg=\"1\"/\u003e ：常量，可以理解成是一个控制比例的缩放因子，在deepspeed-chat中默认设为0.1\u003c/li\u003e\u003cli data-pid=\"qASmGBQj\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=-log%5Cfrac%7BP%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D%7BP_%7Bref%7D%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D\" alt=\"-log\\frac{P(A_{t}|S_{t})}{P_{ref}(A_{t}|S_{t})}\" eeimg=\"1\"/\u003e ：这一项你是不是非常眼熟，这就是我们在3.2部分介绍的Actor和Ref模型间的KL散度呀，写成更容易理解的形式，就是\u003ccode\u003eref_log_probs - log_probs\u003c/code\u003e。在3.2中我们说过，为了防止模型训歪，我们需要把这个KL散度加入loss计算中，所以这里我们就在做这件事\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"7LsuVgyj\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e基于这些，上面这个对\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e的设计可理解成：\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"wN0NSUH7\"\u003e\u003cb\u003e当\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=t+%5Cneq+T\" alt=\"t \\neq T\" eeimg=\"1\"/\u003e \u003cb\u003e时，我们更加关心Actor是否有在Ref的约束下生产token \u003c/b\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7Bt%7D\" alt=\"A_{t}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003cli data-pid=\"9vgr7fkO\"\u003e\u003cb\u003e当\u003c/b\u003e$ \u003cimg src=\"https://www.zhihu.com/equation?tex=t+%3DT\" alt=\"t =T\" eeimg=\"1\"/\u003e \u003cb\u003e时，我们不仅关心Actor是否遵从了Ref的约束，也关心真正的即时收益\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"LgrxcwgQ\"\u003e\u003cbr/\u003e为什么只有最后一个时刻的 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e 被纳入了考量呢？这是因为在Reward模型训练阶段，就是用这个位置的 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e 来表示对完整的prompt + response的奖励预测（但不妨碍你理解成是执行完 \u003cimg src=\"https://www.zhihu.com/equation?tex=A_%7BT%7D\" alt=\"A_{T}\" eeimg=\"1\"/\u003e 的即时奖励），然后用这个指标来做模型eval的（但是Reward训练阶段算loss时，还是考虑了response部分所有token输出的reward值）。所以到了RLHF的场景下，其余时刻的即时奖励，我们就用“Actor是否遵循了Ref的约束”来进行评价。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e需要注意的是， \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e 的设计并不只有这一种。deepspeed在自己的代码注释中也有提过，可以尝试把最后一个时刻的 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7BT%7D\" alt=\"R_{T}\" eeimg=\"1\"/\u003e 替换成所有token的即时奖励的平均值。如果站在这个角度理解的话，我们同样也可以尝试在每一个位置的奖励衡量上引入 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e 。\u003c/p\u003e\u003cp data-pid=\"_8Ego8nX\"\u003e代码实践如下：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python3\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ecompute_rewards\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eprompts\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elog_probs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eref_log_probs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ereward_score\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                        \u003cspan class=\"n\"\u003eaction_mask\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n        \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        reward_function：计算最终的reward分数\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        复习一下几个相关参数的默认值：\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        self.kl_ctl = 0.1\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        self.clip_reward_value = 5\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        对于batch中的某个prompt来说，它最终的reward分数为：\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        (1) 先计算actor和ref_model的logit相似度： -self.kl_ctl * (log_probs - ref_log_probs)\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            其实写成self.kl_ctl * (ref_log_probs - log_probs)更好理解些\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            这个值越大，说明ref_model对actor生成的结果的认可度越高（即表明rlhf没有训歪），\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            没有训歪的情况下我们也应该给模型一些奖励，这个奖励就是self.kl_ctl * (ref_log_probs - log_probs)\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        （2）由于我们只取最后一个token对应位置的分数作为reward_score，因此我们只需要：\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            self.kl_ctl * (ref_log_probs - log_probs)的最后一位 + reward_score\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e         \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e         (3) 同时我们对reward_score也做了大小限制，最大不超过self.clip_reward_value（超过统一给成self.clip_reward_value），\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e             最小不低于-self.clip_reward_value（低于统一给成-self.clip_reward_value）\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e         (4) 最后返回的rewards大小为：（batch_size, 各条数据的长度），对batch中的每条数据来说：\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e             - response的最后一位：self.kl_ctl * (ref_log_probs - log_probs)的最后一位 + reward_score\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e             - response的其余位置：self.kl_ctl * (ref_log_probs - log_probs)\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\n        \u003cspan class=\"n\"\u003ekl_divergence_estimate\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ekl_ctl\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elog_probs\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003eref_log_probs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003erewards\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ekl_divergence_estimate\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# ---------------------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# response开始的位置\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# （因为我们对prompt做过padding处理，因此batch中每个prompt长度一致，也就意味着每个response开始的位置一致）\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# （所以这里start是不加s的，只是一个int）\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# ---------------------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"n\"\u003estart\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eprompts\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# ---------------------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# response结束的位置\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# （因为一个batch中，每个response的长度不一样，所以response的结束位置也不一样）\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# （所以这里end是加s的，ends的尺寸是(batch_size,)\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# ---------------------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eends\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003estart\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003eaction_mask\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e:]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# ---------------------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# 对rewards_score做限制\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# ---------------------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"n\"\u003ereward_clip\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclamp\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ereward_score\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclip_reward_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                  \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclip_reward_value\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003ebatch_size\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elog_probs\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ej\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebatch_size\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n            \u003cspan class=\"n\"\u003erewards\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"n\"\u003eends\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"p\"\u003e]][\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"n\"\u003ereward_clip\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003ej\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"c1\"\u003e# \u003c/span\u003e\n\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003erewards\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3\u003e\u003cbr/\u003e（4）重新设计优势\u003c/h3\u003e\u003cp data-pid=\"mNr8bWko\"\u003e好，再总结一下，目前为止我们的actor_loss为：\u003c/p\u003e\u003cp data-pid=\"fwBw-7MM\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-Adv_%7Bt%7Dlog+P%28A_%7Bt%7D%7CS_%7Bt%7D%29\" alt=\"actor\\_loss = -Adv_{t}log P(A_{t}|S_{t})\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"F7LdVYt6\"\u003e\u003cbr/\u003e其中， \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D+%3D+R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D+-+V_%7Bt%7D\" alt=\"Adv_{t} = R_{t} + \\gamma * V_{t+1} - V_{t}\" eeimg=\"1\"/\u003e \u003cbr/\u003e同时，我们对 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e 进行来改造，使其能够衡量Actor模型是否遵从了Ref模型的约束。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e现在我们把改造焦点放在 \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D\" alt=\"Adv_{t}\" eeimg=\"1\"/\u003e 上，回想一下，既然对于收益而言，分为即时和未来，那么对于优势而言，是不是也能引入对未来优势的考量呢？这样，我们就可以把 \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D\" alt=\"Adv_{t}\" eeimg=\"1\"/\u003e 改写成如下形式：\u003c/p\u003e\u003cp data-pid=\"D181-Cnm\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D+%3D+%28R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D+-+V_%7Bt%7D%29+%2B+%5Cgamma+%2A+%5Clambda+%2A+Adv_%7Bt%2B1%7D\" alt=\"Adv_{t} = (R_{t} + \\gamma * V_{t+1} - V_{t}) + \\gamma * \\lambda * Adv_{t+1}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"9Rw4B44n\"\u003e\u003cbr/\u003e（熟悉强化学习的朋友应该能一眼看出这是GAE，这里我们不打算做复杂的介绍，一切都站在直觉的角度理解）\u003cbr/\u003e\u003cb\u003e其中，新引入的\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/\u003e \u003cb\u003e也是一个常量，可将其理解为权衡因子，直觉上看它控制了在计算当前优势时对未来优势的考量。（从强化学习的角度上，它控制了优势估计的方差和偏差）\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e看到这里，你可能想问：这个代表未来优势的\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%2B1%7D\" alt=\"Adv_{t+1}\" eeimg=\"1\"/\u003e \u003cb\u003e，我要怎么算呢？\u003c/b\u003e\u003cbr/\u003e注意到，对于最后一个时刻 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e ，它的未来收益（ \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7BT%2B1%7D\" alt=\"V_{T+1}\" eeimg=\"1\"/\u003e ）和未来优势（ \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7BT%2B1%7D\" alt=\"Adv_{T+1}\" eeimg=\"1\"/\u003e ）都是0，也就是 \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7BT%7D+%3D+R_%7BT%7D+-+V_%7BT%7D\" alt=\"Adv_{T} = R_{T} - V_{T}\" eeimg=\"1\"/\u003e ，这是可以直接算出来的。\u003cb\u003e而有了\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7BT%7D\" alt=\"Adv_{T}\" eeimg=\"1\"/\u003e \u003cb\u003e，我们不就能从后往前，通过动态规划的方法，把所有时刻的优势都依次算出来了吗？\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e代码实践如下（其中返回值中的returns表示实际收益，将被用于计算Critic模型的loss，可以参见4.2，其余细节都在代码注释中）：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python3\"\u003e \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_advantages_and_returns\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003erewards\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n        \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        Adopted from https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py#L134\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        没有引入GAE前的t时刻的优势值：\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        detal_t = r_t + gamma * V_t+1 - V_t\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        其中：\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            - r_t表示t时刻的即时收益\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            - V_t+1表示未来时刻的预期收益\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            - r_t + gamma * V_t+1可理解成t时刻的实际预期收益\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            - V_t可理解成t时刻的预估预期收益（是模型，例如critic model自己估算出来的）\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        引入GAE后的t时刻的优势值：\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        A_t = delta_t + gamma * lambda * A_t+1\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        粗暴理解为在t时刻时，不仅考虑当下优势，还考虑了未来的优势\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        为了知道A_t, 我们得知道A_t+1，所以在本算法中采取了从后往前做动态规划求解的方法，也即：\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        假设T是最后一个时刻，则有A_T+1 = 0, 所以有: A_T = delta_T\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        知道了A_T, 就可以依次往前倒推，把A_t-1, A_t-2之类都算出来了\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        引入GAE后t时刻的实际预期收益\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        returns_t = A_t + V_t\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e                  = delta_t + gamma * lambda * A_t+1 + V_t\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e                  = r_t + gamma * V_t+1 - V_t + gamma * lambda * A_t+1 + V_t\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e                  = r_t + gamma * (V_t+1 + lambda * A_t+1)\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        注意，这里不管是advantages还是returns，都只算response的部分\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n        \n        \u003cspan class=\"c1\"\u003e# Adopted from https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py#L134\u003c/span\u003e\n        \u003cspan class=\"n\"\u003elastgaelam\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eadvantages_reversed\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n        \u003cspan class=\"n\"\u003elength\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erewards\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e()[\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# 注意这里用了reversed，是采取从后往前倒推计算的方式\u003c/span\u003e\n        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003ereversed\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e)):\u003c/span\u003e\n            \u003cspan class=\"n\"\u003enextvalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"n\"\u003elength\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\n            \u003cspan class=\"n\"\u003edelta\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erewards\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egamma\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003enextvalues\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n            \u003cspan class=\"n\"\u003elastgaelam\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edelta\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egamma\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elam\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003elastgaelam\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eadvantages_reversed\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elastgaelam\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eadvantages\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estack\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eadvantages_reversed\u003c/span\u003e\u003cspan class=\"p\"\u003e[::\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 优势\u003c/span\u003e\n        \u003cspan class=\"n\"\u003ereturns\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eadvantages\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"n\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e:]\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 实际收益\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# values: 预期收益\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eadvantages\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edetach\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e \u003cspan class=\"n\"\u003ereturns\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e（5）PPO-epoch: 引入新约束\u003c/h3\u003e\u003cp data-pid=\"Ei1UYp10\"\u003e总结一下，目前为止我们的actor_loss为：\u003c/p\u003e\u003cp data-pid=\"b7YFMkT8\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-Adv_%7Bt%7Dlog+P%28A_%7Bt%7D%7CS_%7Bt%7D%29\" alt=\"actor\\_loss = -Adv_{t}log P(A_{t}|S_{t})\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp data-pid=\"px21h3D8\"\u003e其中， \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D+%3D+%28R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D+-+V_%7Bt%7D%29+%2B+%5Cgamma+%2A+%5Clambda+%2A+Adv_%7Bt%2B1%7D\" alt=\"Adv_{t} = (R_{t} + \\gamma * V_{t+1} - V_{t}) + \\gamma * \\lambda * Adv_{t+1}\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"alYyy7gX\"\u003e同时\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Dff9Mshh\"\u003e\u003cb\u003e我们已经对\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e进行来改造，使其能够衡量Actor模型是否遵从了Ref模型的约束。\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"QRqm81wA\"\u003e\u003cb\u003e我们已经对\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D\" alt=\"Adv_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e进行改造，使其不仅考虑了当前时刻的优势，还考虑了未来的优势\u003c/b\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Za0JnRoD\"\u003e\u003cbr/\u003e基于这些改造，我们重新理一遍RLHF-PPO的训练过程。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-5b0028cc73d9f2aa599b256df24bda83_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"614\" data-original-token=\"v2-7babbfb465c3c2fd4506b39ad54f84e4\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic4.zhimg.com/v2-5b0028cc73d9f2aa599b256df24bda83_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"-L-7i0f3\"\u003e第一步，我们准备一个batch的prompts\u003c/li\u003e\u003cli data-pid=\"VHfM-MAm\"\u003e第二步，我们将这个batch的prompts喂给Actor模型，让它生成对应的responses\u003c/li\u003e\u003cli data-pid=\"__E0BeFH\"\u003e第三步，我们把prompt+responses喂给我们的Critic/Reward/Reference模型，让它生成用于计算actor/critic loss的数据，按照强化学习的术语，我们称这些数据为经验（experiences）。critic loss我们将在后文做详细讲解，目前我们只把目光聚焦到actor loss上\u003c/li\u003e\u003cli data-pid=\"hmMgK9mz\"\u003e第四步，我们根据这些经验，实际计算出actor/critic loss，然后更新Actor和Critic模型\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"YALbCJ8I\"\u003e这些步骤都很符合直觉，但是细心的你肯定发现了，\u003cb\u003e文字描述中的第四步和图例中的第四步有差异：图中说，这一个batch的经验值将被用于n次模型更新，这是什么意思呢？\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e我们知道，\u003cb\u003e在强化学习中，收集一个batch的经验是非常耗时的。对应到我们RLHF的例子中，收集一次经验，它要等四个模型做完推理才可以\u003c/b\u003e，正是因此，一个batch的经验，只用于计算1次loss，更新1次Actor和Critic模型，好像有点太浪费了。\u003cbr/\u003e\u003cbr/\u003e所以，\u003cb\u003e我们自然而然想到，1个batch的经验，能不能用来计算ppo-epochs次loss，更新ppo-epochs次Actor和Critic模型？\u003c/b\u003e简单写一下伪代码，我们想要：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python3\"\u003e\u003cspan class=\"c1\"\u003e# --------------------------------------------------------------\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 初始化RLHF中的四个模型\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# --------------------------------------------------------------\u003c/span\u003e\n\u003cspan class=\"n\"\u003eactor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ecritic\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ereward\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eref\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003einitialize_models\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# --------------------------------------------------------------\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 训练\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# --------------------------------------------------------------\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 对于每一个batch的数据\u003c/span\u003e\n\u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003esteps\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \n    \u003cspan class=\"c1\"\u003e# 先收集经验值\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eexps\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003egenerate_experience\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eprompts\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eactor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ecritic\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ereward\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eref\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 一个batch的经验值将被用于计算ppo_epochs次loss，更新ppo_epochs次模型\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 这也意味着，当你计算一次新loss时，你用的是更新后的模型\u003c/span\u003e\n    \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ej\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eppo_epochs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eactor_loss\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecal_actor_loss\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eexps\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eactor\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003ecritic_loss\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecal_critic_loss\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eexps\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ecritic\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \n        \u003cspan class=\"n\"\u003eactor\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebackward\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eactor_loss\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eactor\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estep\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \n        \u003cspan class=\"n\"\u003ecritc\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebackward\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecritic_loss\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003ecritic\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estep\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"xtMeGo9n\"\u003e\u003cb\u003e而如果我们想让一个batch的经验值被重复使用ppo_epochs次，等价于我们想要Actor在这个过程中，模拟和环境交互ppo_epochs次。\u003c/b\u003e举个例子：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"z-J2fvwe\"\u003e如果1个batch的经验值只使用1次，那么在本次更新完后，Actor就吃新的batch，正常和环境交互，产出新的经验值\u003c/li\u003e\u003cli data-pid=\"Xu-Jo_o-\"\u003e但如果1个batch的经验值被使用ppo_epochs次，在这ppo_epochs中，Actor是不吃任何新数据，不做任何交互的，所以我们只能让Actor“模拟”一下和环境交互的过程，吐出一些新数据出来。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"F3EHPzJg\"\u003e\u003cbr/\u003e那怎么让Actor模拟呢？很简单，让它观察一下之前的数据长什么样，让它依葫芦画瓢，不就行了吗？\u003cb\u003e我们假设最开始吃batch，吐出经验的actor叫\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Actor_%7Bold%7D\" alt=\"Actor_{old}\" eeimg=\"1\"/\u003e \u003cb\u003e，而在伪代码中，每次做完ppo_epochs而更新的actor叫\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Actor_%7Bnew%7D\" alt=\"Actor_{new}\" eeimg=\"1\"/\u003e \u003cb\u003e，那么我们只要尽量保证每次更新后的\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Actor_%7Bnew%7D\" alt=\"Actor_{new}\" eeimg=\"1\"/\u003e \u003cb\u003e能模仿最开始的那个\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Actor_%7Bold%7D\" alt=\"Actor_{old}\" eeimg=\"1\"/\u003e \u003cb\u003e，不就行了吗？\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e诶！是不是很眼熟！两个分布，通过什么方法让它们相近！\u003cb\u003e那当然是KL散度\u003c/b\u003e！所以，再回到我们的actor_loss上来，它现在就可被改进成：\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-Adv_%7Bt%7Dlog+%5Cfrac%7BP%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D%7BP_%7Bold%7D%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D\" alt=\"actor\\_loss = -Adv_{t}log \\frac{P(A_{t}|S_{t})}{P_{old}(A_{t}|S_{t})}\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e我们再稍作一些改动将log去掉（这个其实不是“稍作改动去掉log”的事，是涉及到PPO中重要性采样的相关内容，大家有兴趣可以参考\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/xingzheai/p/15931681.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这篇\u003c/a\u003e）：\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-Adv_%7Bt%7D+%2A+%5Cfrac%7BP%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D%7BP_%7Bold%7D%28A_%7Bt%7D%7CS_%7Bt%7D%29%7D\" alt=\"actor\\_loss = -Adv_{t} * \\frac{P(A_{t}|S_{t})}{P_{old}(A_{t}|S_{t})}\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e其中， \u003cimg src=\"https://www.zhihu.com/equation?tex=P_%7Bold%7D\" alt=\"P_{old}\" eeimg=\"1\"/\u003e 表示真正吃了batch，产出经验值的Actor；P表示ppo_epochs中实时迭代更新的Actor，它在模仿 \u003cimg src=\"https://www.zhihu.com/equation?tex=P_%7Bold%7D\" alt=\"P_{old}\" eeimg=\"1\"/\u003e 的行为。\u003cb\u003e所以这个公式从直觉上也可以理解成：在Actor想通过模拟交互的方式，使用一个batch的经验值更新自己时，它需要收到真正吃到batch的那个时刻的Actor的约束，这样才能在有效利用batch，提升训练速度的基础上，保持训练的稳定。\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e但是，谨慎的你可能此时又有新的担心了：\u003cb\u003e虽然我们在更新Actor的过程中用\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Actor_%7Bold%7D\" alt=\"Actor_{old}\" eeimg=\"1\"/\u003e \u003cb\u003e做了约束，但如果\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Actor_%7Bold%7D\" alt=\"Actor_{old}\" eeimg=\"1\"/\u003e \u003cb\u003e的约束能力不够，比如说\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7BP%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%7BP_%7Bold%7D%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D\" alt=\"\\frac{P(A_{t} | S_{t})}{P_{old}(A_{t} | S_{t})}\" eeimg=\"1\"/\u003e \u003cb\u003e还是超出了可接受的范围，那怎么办？\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e很简单，那就\u003cb\u003e剪裁（clip）\u003c/b\u003e它吧！\u003c/p\u003e\u003cp data-pid=\"XbKUagrW\"\u003e我们给 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7BP%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%7BP_%7Bold%7D%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D\" alt=\"\\frac{P(A_{t} | S_{t})}{P_{old}(A_{t} | S_{t})}\" eeimg=\"1\"/\u003e 设置一个范围，例如\u003ccode\u003e(0.8 ,1.2)\u003c/code\u003e，也就是如果这个值一旦超过1.2，那就统一变成1.2；一旦小于0.8，那就统一变成0.8。这样就能保证 \u003cimg src=\"https://www.zhihu.com/equation?tex=Actor\" alt=\"Actor\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=Actor_%7Bold%7D\" alt=\"Actor_{old}\" eeimg=\"1\"/\u003e 的分布相似性在我们的掌控之内了。此时actor_loss变为：\u003cbr/\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-min%28Adv_%7Bt%7D+%2A%5Cfrac%7BP%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%7BP_%7Bold%7D%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%2C++Adv_%7Bt%7D+%2A+clip%28%5Cfrac%7BP%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%7BP_%7Bold%7D%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%2C+0.8%2C+1.2%29%29\" alt=\"actor\\_loss = -min(Adv_{t} *\\frac{P(A_{t} | S_{t})}{P_{old}(A_{t} | S_{t})},  Adv_{t} * clip(\\frac{P(A_{t} | S_{t})}{P_{old}(A_{t} | S_{t})}, 0.8, 1.2))\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e这时要注意，如果超过变化范围，将 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7BP%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%7BP_%7Bold%7D%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D\" alt=\"\\frac{P(A_{t} | S_{t})}{P_{old}(A_{t} | S_{t})}\" eeimg=\"1\"/\u003e 强制设定为一个常数后，就说明这一部分的loss和Actor模型无关了，而 \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D\" alt=\"Adv_{t}\" eeimg=\"1\"/\u003e 这项本身也与Actor无关。\u003cb\u003e所以相当于，在超过约束范围时，我们停止对Actor模型进行更新。\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e整体代码如下：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python3\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eactor_loss_fn\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elogprobs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eold_logprobs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eadvantages\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n        \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        logprobs: 实时计算的，response部分的prob（只有这个是随着actor实时更新而改变的）\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        old_logprobs：老策略中，response部分的prob （这个是固定的，不随actor实时更新而改变）\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        advantages： 老策略中，response部分每个token对应的优势（这个是固定的，不随actor实时更新而改变）\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        mask：老策略中，response部分对应的mask情况这个是固定的，不随actor实时更新而改变）\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        之所以要引入logprobs计算actor_loss，是因为我们不希望策略每次更新的幅度太大，防止模型训歪\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        self.cliprange: 默认值是0.2\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e## policy gradient loss\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# -------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# 计算新旧策略间的KL散度\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# -------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"n\"\u003elog_ratio\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elogprobs\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003eold_logprobs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eratio\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexp\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elog_ratio\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# -------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# 计算原始loss和截断loss\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# -------------------------------------------------------------------------------------\u003c/span\u003e\n        \u003cspan class=\"n\"\u003epg_loss1\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003eadvantages\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eratio\u003c/span\u003e\n        \u003cspan class=\"n\"\u003epg_loss2\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003eadvantages\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclamp\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eratio\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mf\"\u003e1.0\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecliprange\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mf\"\u003e1.0\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecliprange\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003epg_loss\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epg_loss1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003epg_loss2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 最后是取每个非mask的response token的平均loss作为最终loss\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003epg_loss\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e（6）Actor loss小结\u003c/h3\u003e\u003cp data-pid=\"-t37f1Au\"\u003e（1）～（5）中我们一步步树立了actor_loss的改进过程，这里我们就做一个总结吧：\u003c/p\u003e\u003cp data-pid=\"PREXWVuF\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=actor%5C_loss+%3D+-min%28Adv_%7Bt%7D+%2A%5Cfrac%7BP%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%7BP_%7Bold%7D%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%2C++Adv_%7Bt%7D+%2A+clip%28%5Cfrac%7BP%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%7BP_%7Bold%7D%28A_%7Bt%7D+%7C+S_%7Bt%7D%29%7D%2C+0.8%2C+1.2%29\" alt=\"actor\\_loss = -min(Adv_{t} *\\frac{P(A_{t} | S_{t})}{P_{old}(A_{t} | S_{t})},  Adv_{t} * clip(\\frac{P(A_{t} | S_{t})}{P_{old}(A_{t} | S_{t})}, 0.8, 1.2)\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003cbr/\u003e其中：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"5XvCbz_o\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D+%3D+%28R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D+-+V_%7Bt%7D%29+%2B+%5Cgamma+%2A+%5Clambda+%2A+Adv_%7Bt%2B1%7D\" alt=\"Adv_{t} = (R_{t} + \\gamma * V_{t+1} - V_{t}) + \\gamma * \\lambda * Adv_{t+1}\" eeimg=\"1\"/\u003e \u003c/li\u003e\u003cli data-pid=\"ZNjypNfI\"\u003e\u003cb\u003e我们已经对\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e进行来改造，使其能够衡量Actor模型是否遵从了Ref模型的约束\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"dAGGXsmJ\"\u003e\u003cb\u003e我们已经对\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D\" alt=\"Adv_{t}\" eeimg=\"1\"/\u003e \u003cb\u003e进行改造，使其不仅考虑了当前时刻的优势，还考虑了未来的优势\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"LEjkaHlO\"\u003e\u003cb\u003e我们重复利用了1个batch的数据，使本来只能被用来做1次模型更新的它现在能被用来做ppo_epochs次模型更新。我们使用真正吃了batch，产出经验值的那个时刻的Actor分布来约束ppo_epochs中更新的Actor分布\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"skVdE7QI\"\u003e\u003cb\u003e我们考虑了剪裁机制（clip），在ppo_epochs次更新中，一旦Actor的更新幅度超过我们的控制范围，则不对它进行参数更新。\u003c/b\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e4.2 Critic loss\u003c/h3\u003e\u003cp data-pid=\"4jAKZdZS\"\u003e我们知道，1个batch产出的经验值，不仅被用来更新Actor，还被用来更新Critic。对于Critic loss，我们不再像Actor loss一样给出一个“演变过程”的解读，我们直接来看它最后的设计。\u003cbr/\u003e\u003cbr/\u003e首先，在之前的解说中，你可能有这样一个印象：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"87yZRw27\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e ：Critic对t时刻的总收益的预估，这个总收益包含即时和未来的概念（预估收益）\u003c/li\u003e\u003cli data-pid=\"ApctcvFU\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D\" alt=\"R_{t} + \\gamma * V_{t+1}\" eeimg=\"1\"/\u003e ：Reward计算出的即时收益 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D\" alt=\"R_{t}\" eeimg=\"1\"/\u003e ，Critic预测出的 \u003cimg src=\"https://www.zhihu.com/equation?tex=t%2B1\" alt=\"t+1\" eeimg=\"1\"/\u003e 及之后时候的收益的折现，这是比 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e 更接近t时刻真值总收益的一个值（实际收益）\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"mEC8mf7A\"\u003e所以，我们的第一想法是：\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=Critic%5C_loss+%3D+%28R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D+-+V_%7Bt%7D%29%5E%7B2%7D\" alt=\"Critic\\_loss = (R_{t} + \\gamma * V_{t+1} - V_{t})^{2}\" eeimg=\"1\"/\u003e \u003cbr/\u003e\u003cbr/\u003e现在，我们对“实际收益”和“预估收益”都做一些优化。\u003c/p\u003e\u003ch3\u003e（1）实际收益优化\u003c/h3\u003e\u003cp data-pid=\"RGP5AQg-\"\u003e我们原始的实际收益为 \u003cimg src=\"https://www.zhihu.com/equation?tex=R_%7Bt%7D+%2B+%5Cgamma+%2A+V_%7Bt%2B1%7D\" alt=\"R_{t} + \\gamma * V_{t+1}\" eeimg=\"1\"/\u003e ，但是当我们在actor_loss中引入“优势”的概念时，“优势”中刻画了更为丰富的实时收益信息，所以，我们将实际收益优化为： \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D+%2B+V_%7Bt%7D\" alt=\"Adv_{t} + V_{t}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e（2）预估收益优化\u003c/h3\u003e\u003cp data-pid=\"29uZ-XPg\"\u003e我们原始的预估收益为 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e 。\u003cbr/\u003e类比于Actor，Critic模型在ppo_epochs的过程中也是不断更新的。所以这个 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e 可以理解成是 \u003cimg src=\"https://www.zhihu.com/equation?tex=Critic_%7Bold%7D\" alt=\"Critic_{old}\" eeimg=\"1\"/\u003e ，也就是真正吃了batch，参与产出经验的那个时候的Critic产出的收益预测结果。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e我们同样想用旧模型去约束新模型，但对于Critic我们采用的约束策略就比较简单了，我们直接看代码，从中可以看出，我们用老 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e 设计了了一个变动范围，然后用这个变动范围去约束新 \u003cimg src=\"https://www.zhihu.com/equation?tex=V_%7Bt%7D\" alt=\"V_{t}\" eeimg=\"1\"/\u003e \u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python3\"\u003e\u003cspan class=\"c1\"\u003e# self.cliprange_value是一个常量\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# old_values: 老critic的预测结果\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# values：新critic的预测结果\u003c/span\u003e\n\u003cspan class=\"n\"\u003evalues_clipped\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclamp\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n            \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eold_values\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecliprange_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eold_values\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecliprange_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"5JkRMzH7\"\u003e那么最终我们就取实际收益和预估收益的MSE做为loss就好，这里注意，计算实际收益时 \u003cimg src=\"https://www.zhihu.com/equation?tex=Adv_%7Bt%7D%2C+V_%7Bt%7D\" alt=\"Adv_{t}, V_{t}\" eeimg=\"1\"/\u003e 都是老Critic（真正吃了batch的那个）产出的结果，而预估收益是随着ppo_epochs而变动的。\u003c/p\u003e\u003cp data-pid=\"6cc9epW5\"\u003e\u003cbr/\u003e代码如下：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python3\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ecritic_loss_fn\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eold_values\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ereturns\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n        \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        values: 实时critic跑出来的预估预期收益（是变动的，随着ppo epoch迭代而改变）\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        old_values：老critic跑出来的预估预期收益（是固定值）\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        returns：实际预期收益\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        mask：response部分的mask\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        self.cliprange_value = 0.2\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e## value loss\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# 用旧的value去约束新的value\u003c/span\u003e\n        \u003cspan class=\"n\"\u003evalues_clipped\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclamp\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n            \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eold_values\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecliprange_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eold_values\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecliprange_value\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecompute_fp32_loss\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n            \u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n            \u003cspan class=\"n\"\u003evalues_clipped\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalues_clipped\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \n        \u003cspan class=\"c1\"\u003e# critic模型的loss定义为（预估预期收益-实际预期收益）**2\u003c/span\u003e\n        \u003cspan class=\"n\"\u003evf_loss1\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003ereturns\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\n        \u003cspan class=\"n\"\u003evf_loss2\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evalues_clipped\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003ereturns\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\n        \u003cspan class=\"n\"\u003evf_loss\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.5\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n            \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evf_loss1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evf_loss2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 同样，最后也是把critic loss平均到每个token上\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003evf_loss\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":185060,"thumbnails":["https://pic1.zhimg.com/v2-da82528111d2fb57519d4ddab1cd93f1.jpg?source=7e7ef6e2\u0026needBackground=1","https://pica.zhimg.com/50/v2-13c0d0cb820153bb033d24df5f8d54ab_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-79b3c8251e4a76811ee3a8b91701a650_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-720e30ab83d0162c056e44c5e7f6acc4_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-ac5a0a8e229fdc4321e701e1e1b7b8d9_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-4b9870e2bef3bc34b0785b7c4219456c_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-6218955e12906306d6333ebd05d47033_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-38017bab6a6f0db96b947832db9a2327_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-a4abf764682a0f5e894515f9e44ce05e_720w.jpg?source=b6762063"],"favorite_count":5442,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 677607581}","attached_info":"CusJCO7Mh8P08/aupAEQBxoJMjM4NzA5Njg1IKCYiK0GKP0cMO8BQF9KMAoGSXRlbUNGEiBkb2NfdHlwZTogQXJ0aWNsZQppZDogMjQ3MzM0NTA5ChgAIAA6AGIgYjFlYTFmZThjMjk1ZDk3ZGRjNTE2N2MzNmRkMTY3NjdyCTY3NzYwNzU4MYIBX2h0dHBzOi8vcGljMS56aGltZy5jb20vdjItZGE4MjUyODExMWQyZmI1NzUxOWQ0ZGRhYjFjZDkzZjEuanBnP3NvdXJjZT03ZTdlZjZlMiZuZWVkQmFja2dyb3VuZD0xqgEJcmVjb21tZW5kwgEgYWNiMzY5NTFkYWFkYTUyMGE0MjZiODIyYzlkMWMwMjDyAQoIDBIGTm9ybWFs8gEoCAoSJGQyMGIyYWUzLTQxOWEtNGY2NS04OGVhLTM1ZjAzMjQ2MWZjOfIBBggLEgIxNoICAIgCg8DMzfoykgIgYWNiMzY5NTFkYWFkYTUyMGE0MjZiODIyYzlkMWMwMjCaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIUQ29udGVudEFnZVdlaWdodFJ1bGXKAhxCYXllc0ZpcnN0TGV2ZWxJc29sYXRpb25SdWxl2gIGSXRlbUNG6AIE+gILTk9STUFMX0ZMT1eKAyBiY2RkYjMxNjQ2YTY0NDQ3YjA4YmJhZmZjNDIxODI3YpoDDQoCdjIQABoFb3RoZXKoA+SlC9gDAOoDFXRleHRBbGxTaXRlTXZJdGVtQ0ZWMvoD9QMSDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAhCoEBi2CiIjdjItMTE1YTA2YzFhNjM4ZTA5M2UzNmEyYzI1MDhhOTg3ODU6LQgCEPUIGJYFIiN2Mi1jZjFiN2QyOTgwZTg1NmFmY2NhNmE5Nzk0ZjhiYWEwOTotCAIQgAoYgAoiI3YyLTBiNmZjNzE2ODkwOGE4NmU2MzJiMmM3NTcyNWQ0M2U2Oi0IAhDLBxi9BSIjdjItNTQyODlkMmVlYTEyMjkxNzA1ZjZiMGVjYWZlZjRiNWQ6LQgEEIAKGJMEIiN2Mi05NjViNWRlMmYyZjEyNWI0ODhlZjcxYWY1Y2I5Nzg2YTotCAIQmQcYxwUiI3YyLWMzMzcxNmUxOWI1MGU1OGIwOTM1ZjhkMTA0NjM2YTIwOi0IBBCZBxjHBSIjdjItZWZlNmIyMmZkZDA3ODJmMDY2ZmI5OTNhMmY2Yzc4MTQ6LQgEEJkHGMcFIiN2Mi1jOTcwYzNiOGI1MjBjZjg4ZDdhNDJhZDFmODRhNTIzMTotCAIQgAoY5gQiI3YyLTdiYWJiZmI0NjVjM2MyZmQ0NTA2YjM5YWQ1NGY4NGU0Oi0IAhDNChiLBSIjdjItZGE4MjUyODExMWQyZmI1NzUxOWQ0ZGRhYjFjZDkzZjGABACIBACSBAZOb3JtYWyaBAE0oAQAqAQAsAQAugQGbWFudWFswgQDMTcwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAADAG86aP4EFAAAAAAAAAACJBV1DXMQlmNI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRCQBgCgBmWoBgCSAiQKCTIzODcwOTY4NRIJNjc3NjA3NTgxGAciCklNQUdFX1RFWFQ=","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=95\u0026desktop=true\u0026end_offset=101\u0026page_number=17\u0026session_token=b1ea1fe8c295d97ddc5167c36dd16767","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=95\u0026desktop=true\u0026end_offset=101\u0026page_number=17\u0026session_token=b1ea1fe8c295d97ddc5167c36dd16767","totals":0},"fresh_text":"推荐已更新"}
