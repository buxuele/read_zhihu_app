{"data":[{"id":"60_1753853942.551","type":"feed","offset":60,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853942,"updated_time":1753853942,"target":{"id":"1924196602727368021","type":"answer","url":"https://api.zhihu.com/answers/1924196602727368021","author":{"id":"d8427be0dd39a9b5bd1d8b0542ab7b94","url":"https://api.zhihu.com/people/d8427be0dd39a9b5bd1d8b0542ab7b94","user_type":"people","url_token":"jiang-you-jun-49","name":"酱油君","headline":"推荐好用的工具","avatar_url":"https://pica.zhimg.com/50/f3b8fd6dc49df4b98fd8ce01087d27ac_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":116,"is_following":false,"is_followed":false},"created_time":1751544219,"updated_time":1751602892,"voteup_count":12,"thanks_count":1,"comment_count":3,"is_copyable":true,"question":{"id":"557179943","type":"question","url":"https://api.zhihu.com/questions/557179943","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pica.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"程序员都有自己的服务器吗？","created":1664568498,"answer_count":0,"follower_count":0,"comment_count":6,"bound_topic_ids":[707,1452],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://pica.zhimg.com/50/v2-fe8da07f6c7505f317ce713f045db970_720w.jpg?source=b6762063","excerpt":"我把我积灰的Mac mini 托管到机房了，有图有真相。   虽然画质又渣又昏暗，但是！这就是实锤。 作为开发者，谁不想拥有个自己的服务器呢。但无论是云服务器还是Nas，前期投入都不少。正好之前国补低价入了台Mac Mini，差点就积灰了，这下变废为宝了。 今天，我就给你分享一下是如何把mac mini 变成服务器的 。它不仅帮我砍掉了大部分云服务器开销，还让我体验到了前所未有的性能和数据掌控感。为什么是 Mac mini？它天生就是当服务…","excerpt_new":"我把我积灰的Mac mini 托管到机房了，有图有真相。   虽然画质又渣又昏暗，但是！这就是实锤。 作为开发者，谁不想拥有个自己的服务器呢。但无论是云服务器还是Nas，前期投入都不少。正好之前国补低价入了台Mac Mini，差点就积灰了，这下变废为宝了。 今天，我就给你分享一下是如何把mac mini 变成服务器的 。它不仅帮我砍掉了大部分云服务器开销，还让我体验到了前所未有的性能和数据掌控感。为什么是 Mac mini？它天生就是当服务…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"FjJDpdSd\"\u003e我把我积灰的Mac mini 托管到机房了，有图有真相。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-cf0bb4d187ace41c15f659599db59481_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"778\" data-rawheight=\"966\" data-original-token=\"v2-f9cba05e0f0fe836a78b9fe9d58e5407\" class=\"origin_image zh-lightbox-thumb\" width=\"778\" data-original=\"https://pic4.zhimg.com/v2-cf0bb4d187ace41c15f659599db59481_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"y4Sq6yPM\"\u003e\u003cbr/\u003e虽然画质又渣又昏暗，但是！这就是实锤。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e作为开发者，谁不想拥有个自己的服务器呢。但无论是云服务器还是Nas，前期投入都不少。\u003c/p\u003e\u003cp data-pid=\"16q0Whyv\"\u003e正好之前国补低价入了台Mac Mini，差点就积灰了，这下变废为宝了。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e今天，我就给你分享一下是\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.servbay.com/features\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e如何把mac mini 变成服务器的\u003c/a\u003e。它不仅帮我砍掉了大部分云服务器开销，还让我体验到了前所未有的性能和数据掌控感。\u003c/p\u003e\u003ch2\u003e为什么是 Mac mini？它天生就是当服务器的料\u003c/h2\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"F1YINCM3\"\u003e一开始我也有顾虑，家用电脑能 7x24 小时稳定运行吗？事实证明，Apple Silicon 时代的 Mac mini 完全可以。而且M4的配置足够支持一万日活的app。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e这是一笔一次性的硬件投入，如果有闲置设备的话，四舍五入就是不花钱。托管的费用是100M，一年5000；而阿里云的服务器，8核16G，一年就需要6000+，虽然差1000+看着不多，但长年累月下来也是一笔不小的费用了。 更别提阿里云百兆带宽一年就要大几万了。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-720c4d72fdf4da7ae498b020cc3c3cad_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"688\" data-original-token=\"v2-220cc5de96ab784e40cd72f579f8a53c\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https://picx.zhimg.com/v2-720c4d72fdf4da7ae498b020cc3c3cad_r.jpg\"/\u003e\u003c/figure\u003e\u003ch2\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e核心工具：ServBay 如何让一切变得简单\u003c/h2\u003e\u003cp data-pid=\"oTHt3vfl\"\u003e可能有些同学会觉得，自己搭建个服务器，又是Docker，又是Homebrew，各种配置，麻烦死了。那你就错了。\u003cbr/\u003e使用ServBay，一切困难都迎刃而解。\u003cbr/\u003e最开始的时候，我也试过 \u003ca href=\"https://link.zhihu.com/?target=https%3A//www.servbay.com/vs/homebrew\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eHomebrew\u003c/a\u003e，但软件间的依赖和冲突很让人头疼。后来发现了 ServBay，它一个专为 macOS 设计的、现代化的图形化服务器管理面板，这就是为我准备的啊。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e它把最麻烦的事情都帮你搞定了：\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"vKOX-Tef\"\u003e\u003cb\u003e图形化管理：\u003c/b\u003e 所有操作都在一个清爽的界面里完成，启动服务、添加网站，点点鼠标就行。\u003c/li\u003e\u003cli data-pid=\"gnR4zs1w\"\u003e\u003cb\u003e多版本开发语言共存：\u003c/b\u003e 可以同时跑好几个 Python、Node.js、Java、PHP 版本，每个网站指定一个，互不影响，对我们开发者来说太友好了。\u003c/li\u003e\u003cli data-pid=\"JHP1v4a_\"\u003e\u003cb\u003e一键式操作：\u003c/b\u003e 添加网站，申请 SSL 证书，都是一键完成。\u003c/li\u003e\u003cli data-pid=\"_iLCGLXW\"\u003e\u003cb\u003e高度集成：\u003c/b\u003e Nginx, MariaDB, PostgreSQL, Redis, phpMyAdmin 等常用工具都内置了，开箱即用。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"fP-SvQvK\"\u003e\u003cbr/\u003e\u003cbr/\u003e现在，我们直接进入实战环节。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e实战指南：四步让你的 Mac mini 上线服务\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e在动手前，先给你的 Mac mini 做几个简单的设置，让它进入“服务器模式”。\u003cbr/\u003e\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"9D7iOb1j\"\u003e\u003cb\u003e硬件选择：\u003c/b\u003e 16GB 内存起步会更从容，如果你的网站文件很多，可以外接一块 SSD 硬盘。\u003c/li\u003e\u003cli data-pid=\"HLTaoCgp\"\u003e\u003cb\u003e有线网络：\u003c/b\u003e \u003cb\u003e放弃 Wi-Fi，插上网线！\u003c/b\u003e 保证服务器稳定性的第一要素。\u003c/li\u003e\u003cli data-pid=\"3Hu1prkW\"\u003e\u003cb\u003e电源设置：\u003c/b\u003e 打开 \u003ccode\u003e系统设置\u003c/code\u003e \u0026gt; \u003ccode\u003e节能\u003c/code\u003e，勾选 \u003cb\u003e“断电后自动重新启动”\u003c/b\u003e。这样万一停电，来电后它能自己醒过来。还要把防止自动睡眠给勾选上。\u003c/li\u003e\u003cli data-pid=\"ToGVfaE4\"\u003e\u003cb\u003e远程管理：\u003c/b\u003e 打开 \u003ccode\u003e系统设置\u003c/code\u003e \u0026gt; \u003ccode\u003e通用\u003c/code\u003e \u0026gt; \u003ccode\u003e共享\u003c/code\u003e，开启 \u003cb\u003e“屏幕共享”\u003c/b\u003e 和 \u003cb\u003e“远程登录(SSH)”\u003c/b\u003e。这样你就可以把显示器和键鼠拔掉，让它“无头运行”，通过你的主力 Mac 来管理它。\u003cbr/\u003e\u003c/li\u003e\u003c/ol\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-a309207c69e95fc68cf25edf6cb3b3ad_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1438\" data-rawheight=\"1256\" data-original-token=\"v2-fdbf6335db4640cdf66b25448f964887\" class=\"origin_image zh-lightbox-thumb\" width=\"1438\" data-original=\"https://pic2.zhimg.com/v2-a309207c69e95fc68cf25edf6cb3b3ad_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"a9621n2J\"\u003e\u003cbr/\u003e然后进入正题，开始使用ServBay来搭建服务器。\u003cbr/\u003e\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"cdlqRuue\"\u003e\u003cb\u003e安装 ServBay：\u003c/b\u003e 从 \u003ca href=\"https://link.zhihu.com/?target=https%3A//www.servbay.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eServBay 官网\u003c/a\u003e (\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.servbay.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://www.\u003c/span\u003e\u003cspan class=\"visible\"\u003eservbay.com\u003c/span\u003e\u003cspan class=\"invisible\"\u003e\u003c/span\u003e\u003c/a\u003e) 下载安装。初始化界面的时候就可以把需要的工具全部选择上。如果漏了哪个，没关系，后面可以继续安装。\u003cbr/\u003e\u003c/li\u003e\u003cli data-pid=\"1CrBrDpK\"\u003e\u003cb\u003e启动服务：\u003c/b\u003e 打开 ServBay，在左边「软件包」选项中，安装并启动需要的软件。\u003cbr/\u003e\u003c/li\u003e\u003c/ol\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-cb4f729bcc583e03c542556ae72d4fde_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2624\" data-rawheight=\"1624\" data-original-token=\"v2-e49748883e180cbb0bcf57cbc3a9ad36\" class=\"origin_image zh-lightbox-thumb\" width=\"2624\" data-original=\"https://pic3.zhimg.com/v2-cb4f729bcc583e03c542556ae72d4fde_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"m88vCzpU\"\u003e\u003cb\u003e添加网站：\u003c/b\u003e\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli data-pid=\"s2n4BjRe\"\u003e到「网站」标签页，点左下角的 \u003ccode\u003e+\u003c/code\u003e。\u003c/li\u003e\u003cli data-pid=\"PE7i7Rpr\"\u003e\u003cb\u003e域名：\u003c/b\u003e 填入你的域名，比如你自己个人博客、网站的域名。\u003c/li\u003e\u003cli data-pid=\"ySJ507RQ\"\u003e\u003cb\u003eSSL证书：\u003c/b\u003e选择ACME，ServBay 支持\u003ccode\u003eZeroSSL\u003c/code\u003e，\u003ccode\u003eLet\u0026#39;s Encrypt\u003c/code\u003e 和 \u003ccode\u003eGoogle Trust Services\u003c/code\u003e签发的证书，而且ServBay还能自动续期，非常省心。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-7f5901ff97dcf0d6af8663aad7cccbe9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1502\" data-rawheight=\"1472\" data-original-token=\"v2-0ee42235e4648514df0df7eaccd6e1f7\" class=\"origin_image zh-lightbox-thumb\" width=\"1502\" data-original=\"https://pic2.zhimg.com/v2-7f5901ff97dcf0d6af8663aad7cccbe9_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Wr7CYqix\"\u003e\u003cb\u003e根目录：\u003c/b\u003e 指定网站文件存放的位置。\u003c/li\u003e\u003cli data-pid=\"dRbjqdjf\"\u003e\u003cb\u003ePHP 版本：\u003c/b\u003e 为这个网站选一个 PHP 版本。\u003c/li\u003e\u003cli data-pid=\"_XpMPQiU\"\u003e点击“添加”，网站就建好了。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-b0f182f5fffa5680c778bc009df3ae9e_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2624\" data-rawheight=\"1624\" data-original-token=\"v2-1c8bbc501db7fad50ab89dc326683c8b\" class=\"origin_image zh-lightbox-thumb\" width=\"2624\" data-original=\"https://pic3.zhimg.com/v2-b0f182f5fffa5680c778bc009df3ae9e_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"smtXiLse\"\u003e\u003cbr/\u003e完成了！现在，这个mac mini变身的服务器就几乎全部部署好了。\u003c/p\u003e\u003cp data-pid=\"6ODtMwgS\"\u003e但还有一个，就是备份问题。\u003c/p\u003e\u003cp data-pid=\"nIRWxw2-\"\u003e数据是服务器的生命。以前我得自己写 \u003ccode\u003emysqldump\u003c/code\u003e 脚本，再用 \u003ccode\u003elaunchd\u003c/code\u003e 设置定时任务。而 ServBay，连这个都替你考虑到了。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"3EYsiMsV\"\u003eServBay 内置了强大的备份工具，可以让你轻松实现自动化备份。\u003cbr/\u003e\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"1gb0B4Ce\"\u003e在左边导航菜单中选择「备份」\u003c/li\u003e\u003cli data-pid=\"AnAvZU1m\"\u003e\u003cb\u003e打开自动备份：\u003c/b\u003e 勾选你要备份的内容，比如网站、设置、SSL等\u003c/li\u003e\u003cli data-pid=\"WIbuh-mT\"\u003e\u003cb\u003e设置备份计划：\u003c/b\u003e 选择执行周期，比如“每天凌晨0点”。\u003c/li\u003e\u003cli data-pid=\"yBe96zfO\"\u003e\u003cb\u003e选择备份目的地：\u003c/b\u003e 将备份文件存放到另一块外置硬盘，或者一个网络位置。\u003c/li\u003e\u003cli data-pid=\"aWMlAnbb\"\u003e保存任务。\u003c/li\u003e\u003c/ol\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-63f21df52dbdb86402d2bf39f0a8ff32_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2624\" data-rawheight=\"1624\" data-original-token=\"v2-ab9966900a59f9d1a582cbcc48d46bc0\" class=\"origin_image zh-lightbox-thumb\" width=\"2624\" data-original=\"https://pica.zhimg.com/v2-63f21df52dbdb86402d2bf39f0a8ff32_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"iM7rzBPQ\"\u003e\u003cbr/\u003e从此，ServBay 就会每天在指定时间，自动、正确地打包你的网站文件和导出数据库，并存放到安全的位置。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e你只需要偶尔检查一下备份文件就好，真正做到高枕无忧。当然，别忘了备份界的黄金法则（3-2-1 原则）：\u003cb\u003e3\u003c/b\u003e 份拷贝，\u003cb\u003e2\u003c/b\u003e 种介质，\u003cb\u003e1\u003c/b\u003e 份异地。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e如果你担心自动备份会把硬盘塞爆，那就自己写个 crontab 自动清理就好了。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e因为家里的带宽和电力波动，我就把Mac Mini托管到机房了，恒温恒湿，有稳定的带宽和电力，美滋滋。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e结语：这不仅是省钱，更是自由\u003c/h2\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"9XzC8Zlc\"\u003e把闲置的 Mac mini 变成一台全天候服务器，不仅仅省下了一笔云服务费。\u003cbr/\u003e它让我重新感受到了对技术和数据的掌控力。性能卓越、管理便捷、数据安全——这些核心优势，通过 Mac mini 和 ServBay 的组合，变得触手可及。过去那些被认为是系统管理员专属的高深技能，如今被巧妙的工具拉低了门槛。\u003cbr/\u003e推荐大家快去试试。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":1929,"thumbnails":["https://pic1.zhimg.com/50/v2-fe8da07f6c7505f317ce713f045db970_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-06677b9392541c1e59480311a300d768_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-f02a24fd1a98d5538a087f3d5cbe1b49_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-de6882bc73c6d40a45e528a6fdb9701e_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-c9e38b90c8e07920412d9e52edfe4251_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-91bcfdf96366afa066cfaa18fc4f48c2_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-c8604286e4ed56ff3401af4823d02f47_720w.jpg?source=b6762063"],"favorite_count":34,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1924196602727368021}","attached_info":"CpsICPDjne3L8syghgEQBBoJNzM1MjczMDE3IJvjmcMGKAwwA0A8SkAKGFRTX1NPVVJDRV9BVVRIT1JfR0NGX0hQVhIeZG9jX3R5cGU6IE1lbWJlcgppZDogMzM1MTA3MjgKGAAgADoAWgg4NjEwMDU0NGIgYjM3NDZhZjk2ZjQ0ZmM1MTk4MmMwNTllY2JhMTlhOWVyEzE5MjQxOTY2MDI3MjczNjgwMjGKAQk1NTcxNzk5NDOqAQlyZWNvbW1lbmTCASBkODQyN2JlMGRkMzlhOWI1YmQxZDhiMDU0MmFiN2I5NPIBCggMEgZOb3JtYWzyASgIChIkYjU2Y2NkMTQtY2M5NC00OTk4LWI1MmYtMGZmZTE4NGE0Nzkw8gEGCAsSAjExggIAiALgtd3OhTOSAiBkODQyN2JlMGRkMzlhOWI1YmQxZDhiMDU0MmFiN2I5NJoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZdoCGFRTX1NPVVJDRV9BVVRIT1JfR0NGX0hQVugCA/oCC05PUk1BTF9GTE9XigMgMDUyZWYyNDYzZWJhNDYyNThlZDE3NTAyNTNlODViMjCaAw0KAnYyEAAaBW90aGVyqAOJD9gDAOoDGGdjZktubkF1dGhvckhpZ2hQdlJlY2FsbPoD6AISDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAxCKBhjGByIjdjItZjljYmEwNWUwZjBmZTgzNmE3OGI5ZmU5ZDU4ZTU0MDc6LQgCEP4FGLAFIiN2Mi0yMjBjYzVkZTk2YWI3ODRlNDBjZDcyZjU3OWY4YTUzYzotCAIQngsY6AkiI3YyLWZkYmY2MzM1ZGI0NjQwY2RmNjZiMjU0NDhmOTY0ODg3Oi0IAhDAFBjYDCIjdjItZTQ5NzQ4ODgzZTE4MGNiYjBiY2Y1N2NiYzNhOWFkMzY6LQgCEN4LGMALIiN2Mi0wZWU0MjIzNWU0NjQ4NTE0ZGYwZGY3ZWFjY2Q2ZTFmNzotCAIQwBQY2AwiI3YyLTFjOGJiYzUwMWRiN2ZhZDUwYWI4OWRjMzI2NjgzYzhiOi0IAhDAFBjYDCIjdjItYWI5OTY2OTAwYTU5ZjlkMWE1ODJjYmNjNDhkNDZiYzCABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAGCgT6Y/gQUAAAAAAAAAAIkFuwX2DBpb0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFC5AGAKAGPKgGAZICLgoJNzM1MjczMDE3EhMxOTI0MTk2NjAyNzI3MzY4MDIxGAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"61_1753853942.951","type":"feed","offset":61,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853942,"updated_time":1753853942,"target":{"id":"1928406059606843635","type":"answer","url":"https://api.zhihu.com/answers/1928406059606843635","author":{"id":"c40b63ed3f192cc36cae6ad51823e990","url":"https://api.zhihu.com/people/c40b63ed3f192cc36cae6ad51823e990","user_type":"people","url_token":"xiong-xiong-45-36","name":"不要二分法","headline":"资源和财富分配不取决于考试分数，取决于权力和经济。","avatar_url":"https://pic1.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":27640,"is_following":false,"is_followed":false},"created_time":1752547832,"updated_time":1752547832,"voteup_count":1696,"thanks_count":44,"comment_count":289,"is_copyable":true,"question":{"id":"9232047904","type":"question","url":"https://api.zhihu.com/questions/9232047904","author":{"id":"9211ccadafc0f9260d2cc3f06dd87d50","url":"https://api.zhihu.com/people/9211ccadafc0f9260d2cc3f06dd87d50","user_type":"people","url_token":"57-8-85-15-12","name":"誓灭异族势力","headline":"皇汉万岁（极端皇汉！极右！）！烂人必死","avatar_url":"https://picx.zhimg.com/50/v2-fc3b5c666b9da7b3989500e52cec9d4b_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":108,"is_following":false,"is_followed":false},"title":"为什么中国文明发展到宋明会两次亡天下?","created":1736489234,"answer_count":0,"follower_count":0,"comment_count":2,"bound_topic_ids":[325003,332944,725794,2142146,2464930],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"鹿山：你真的想知道原因吗？可能会震碎你的三观！ 先给你一个思路做一个考虑： 为什么元，清出现在宋之后？而宋之前的少数民族政权，比如魏晋时期的五胡乱华，包括秦汉时期的匈奴等，包括南北朝的北朝，少数民族政权都无法统一天下？ 而只有汉人才能够统一，比如隋朝是来自北周-西魏-北魏（三个都是少数民族政权），他们拥有那么多“猛人”依然做不到统一。难道真的是生产力？那清朝和元朝的生产力超过了明，宋？ 取得天下从来不…","excerpt_new":"鹿山：你真的想知道原因吗？可能会震碎你的三观！ 先给你一个思路做一个考虑： 为什么元，清出现在宋之后？而宋之前的少数民族政权，比如魏晋时期的五胡乱华，包括秦汉时期的匈奴等，包括南北朝的北朝，少数民族政权都无法统一天下？ 而只有汉人才能够统一，比如隋朝是来自北周-西魏-北魏（三个都是少数民族政权），他们拥有那么多“猛人”依然做不到统一。难道真的是生产力？那清朝和元朝的生产力超过了明，宋？ 取得天下从来不…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp\u003e\u003c/p\u003e\u003cp data-pid=\"391ef9OX\"\u003e鹿山：你真的想知道原因吗？可能会震碎你的三观！\u003c/p\u003e\u003cp data-pid=\"6Ibk5DQI\"\u003e先给你一个思路做一个考虑：\u003c/p\u003e\u003cp data-pid=\"XQf3yyWL\"\u003e为什么元，清出现在宋之后？而宋之前的少数民族政权，比如魏晋时期的五胡乱华，包括秦汉时期的匈奴等，包括南北朝的北朝，少数民族政权都无法统一天下？\u003c/p\u003e\u003cp data-pid=\"WpVcdxU3\"\u003e而只有汉人才能够统一，比如隋朝是来自北周-西魏-北魏（三个都是少数民族政权），他们拥有那么多“猛人”依然做不到统一。难道真的是生产力？那清朝和元朝的生产力超过了明，宋？\u003c/p\u003e\u003cp data-pid=\"gEjfcm1z\"\u003e\u003cb\u003e取得天下从来不仅仅靠军事！更是靠利益分配！\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"YbZ2YGfu\"\u003e不要去看“史书”，而是要看史书背后的“人性和利益”。\u003c/p\u003e\u003cp data-pid=\"HBF9gBHh\"\u003e史书讲的是“忠臣良将”，是“君君臣臣”，是“君臣庆会”，是“一派祥和”。好像各个都是“道德楷模”，是“阴沟里蹦出个棉花球”，是“大仁大义”。\u003c/p\u003e\u003cp data-pid=\"mZ4G-RLW\"\u003e知道问题出在哪里吗？如果用现在的说法，这叫“英雄史观”。难道历史就是这些“伟光正们”左右的吗？封建王朝的文臣写史书难道会把农民造反称为“革命”，而不是逆贼？\u003c/p\u003e\u003cp data-pid=\"NCo3PKTz\"\u003e\u003cb\u003e道德这个东西，是表演给老百姓看的，是用来“愚民”的；若用来做事，那是百无一用！\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"WazL9Tva\"\u003e什么是永恒的，是“人性”和“利益”？\u003c/p\u003e\u003cp data-pid=\"QBLoF0kX\"\u003e那么，是人性中的光辉还是阴暗？又是谁的利益？皇帝的，百姓的，武将的，还是文臣的？\u003c/p\u003e\u003cp data-pid=\"vy2Q5vLT\"\u003e皇帝的目的是“中央集权制”，只有集权他才能够“均衡各方利益”，才能够实现“权力均衡”和“利益的再分配”。\u003c/p\u003e\u003cp data-pid=\"R4PQMYej\"\u003e集权制度下的权力均衡能够把实现完全掌控和绝对统一，利益再分配才能够保障底层的利益，才能保障社会基本的稳定。\u003c/p\u003e\u003cp data-pid=\"bLHzWx5E\"\u003e正如《大明王朝1566》中，嘉靖对未来的万历讲：任何人答应你的事都不算数，只有自己能做主的才算数。\u003c/p\u003e\u003cp data-pid=\"zF5w_6oM\"\u003e不要觉得封建王朝中，皇帝是一言九鼎，完全一人堂！做过领导的都知道，如果你下面的人都不听你的，你这个领导就是个摆设，就是个吉祥物。\u003c/p\u003e\u003cp data-pid=\"KkHqFHzi\"\u003e\u003cb\u003e这就是系统的执行能力，否则都是阳奉阴违，皇帝的诏书连紫禁城都出不了！！！\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"fnJ3jPME\"\u003e\u003cb\u003e而且历史上的皇帝都是短命鬼，即使和平年代，也是各种莫名其妙的暴毙，什么原因呢？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"OmhtytFu\"\u003e可以说至少一半以上的封建帝王是说话不算数的，这个时候是外戚，后宫，权臣，文官集团说了算，皇帝属于被架空的状态。\u003c/p\u003e\u003cp data-pid=\"v5OT2BMw\"\u003e而中国封建王朝的矛盾，从始至终其实就是两个矛盾：\u003c/p\u003e\u003cp data-pid=\"aF1YjaCM\"\u003e\u003cb\u003e1-皇权与相权的矛盾。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"C1JiBZnu\"\u003e外戚后宫宦官的权力来自皇权，所以只能是百分百依附。而文官的权力来自科举，来自对生产资料的掌控，所以个人是依附皇权获得权力，但是文官利益集团却并非依附皇权。\u003c/p\u003e\u003cp data-pid=\"UEzPNn7m\"\u003e而在不同时期有不同表现，虽然皇权从未改变，但相权一直在变。汉朝是丞相，代表的是勋贵尤其是军事贵族的利益；魏晋至唐朝是丞相-宰相，代表的是士族门阀的利益；到了宋朝变成了士大夫（科举贵族）；明朝则进一步成为了东林党，一直到清朝（江南士绅）。\u003c/p\u003e\u003cp data-pid=\"MCQdE1YI\"\u003e到了宋明清，其实本质就是皇权和江南士绅的矛盾；因为这两者的利益是不同的。皇权要的是天下，士绅要的是特权。\u003c/p\u003e\u003cp data-pid=\"I4zy2Qpo\"\u003e既然不是利益的共同体，那就不会是力量的集合体，甚至还是皇权力量的对抗体。\u003c/p\u003e\u003cp data-pid=\"2DaRtBj5\"\u003e\u003cb\u003e2-中央和地方的矛盾。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Ydx9R3lP\"\u003e这很好理解，就是利益分配问题。因为在封建王朝，最大的生产资料就是土地，谁掌握了土地，谁就拥有了财富。\u003c/p\u003e\u003cp data-pid=\"djlUbkAa\"\u003e而文官集团通过科举制度的特权，以商养官，以官促商，实现了经济和商业的垄断。通过经济特权实现经济垄断和优势，通过经济优势实现教育优势，通过教育优势实现科举优势，进而实现政治优势，最终反哺经济和仕途（荐官制度和承袭制度），这是一个闭环。但士绅们通过这样的方式仅仅只是为了维系其自身利益，而非国家利益。\u003c/p\u003e\u003cp data-pid=\"KebHuAkK\"\u003e具体表现为：土地兼并和盐铁茶酒官营。本质就是地方官僚集团在经济上与中央的斗争。当然还表现在税收制度上。（明朝的一条鞭法，清朝的摊丁入亩）\u003c/p\u003e\u003cp data-pid=\"dW3ESnkb\"\u003e也就是说，文官集团逐渐实现了对基层的经济和政治掌控（土地兼并，皇权不下县）。\u003c/p\u003e\u003cp data-pid=\"WjHDceer\"\u003e如此，中央财政势必枯竭，要知道干什么都是需要用钱的。打仗，赈灾，保障最底层生存防止其造反。底层如果没有保障，结果就是造反！\u003c/p\u003e\u003cp data-pid=\"vw5_z_bK\"\u003e\u003cb\u003e明白了这些，你就知道为什么宋明两朝亡天下的原因！\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"U9ITeBHf\"\u003e以文官集团为首的士绅阶层，通过掌握相权和地方与皇权一直进行着“斗争”，进而在政治和经济上逐渐架空皇权和中央。而当皇权想进行变法改革改变局面时，遭到文官集团的疯狂反击和报复，最终失去天下！\u003c/p\u003e\u003cp data-pid=\"S5JnUu5J\"\u003e比如宋朝时期南唐后主李煜就是被江南士大夫出卖给了赵宋；而赵宋王朝也被文官集团出卖给了金朝。王安石和范仲淹的变法和新政也都是失败收场。南宋岳飞的北伐失败也是以秦桧为首的文官集团的疯狂打击，最终被害。\u003c/p\u003e\u003cp data-pid=\"vhdl9r0o\"\u003e\u003cb\u003e所以宋高宗赵构讲：非卿不忠，非朕不明！（这就是皇帝无法实现集权的无奈）\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"V2TzPb69\"\u003e明朝的东南倭患本质就是江南士大夫支持的军事走私集团；明朝也是被江南士绅和山西土财主出卖给了满清。而张居正的改革和雍正皇帝的新政，本质还是向江南士绅下手。结果张居正给抄家，雍正的历史名声一直很差。\u003c/p\u003e\u003cp data-pid=\"klu33aDD\"\u003e\u003cb\u003e所以崇祯皇帝最后的绝望是：文臣皆可杀！（最终发现文臣才是最大的卖国贼，无君无父，有家无国）\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"s_xoVQNM\"\u003e当然崇祯皇帝看到的是“文臣”，其本质是：\u003cb\u003e江南士绅文官利益集团。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"4j0mlPp4\"\u003e这群人有家而无国，见利而忘义！\u003c/p\u003e\u003cp data-pid=\"jcmZKk9L\"\u003e\u003cb\u003e当时间来到今天，就是“个人资本利益集团和文官集团”（想想曾经某位刘姓证-监-会-主-席上台期间，多少江浙沪企业上市，你就明白了）。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"BW-8oPlh\"\u003e\u003cb\u003e但靠道德和自我约束（自我革命）能约束他们吗？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"wGz_df21\"\u003e\u003cb\u003e是制度可靠，还是道德可靠？是自我革命可靠，还是人民监督可靠？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Cis1zCXD\"\u003e\u003cb\u003e而且最近二三十年这群人又抬头了，想想为什么以房产税和遗产税，资本利得税，离境税这些就是做不到？\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"1euqugRt\"\u003e\u003cb\u003e从人民视角和中央视角，不得不防！\u003c/b\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":130105,"favorite_count":1686,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1928406059606843635}","attached_info":"CpcGCPDjne3L8syghgEQBBoJNzM3MDk0NzcwIPiD18MGKKANMKECQD1KPwoqVFNfU09VUkNFX1pSRUNBTExfRkVFRFJFX05FV0JJRV9IT1VSTFlfUlVNEgEwGAAgADoKeyJyYXciOiIifUowChtUU19TT1VSQ0VfQkFTSUNfSU5GT19SRUNBTEwSATAYACAAOgp7InJhdyI6IiJ9WgkxMTI4MjI5OTBiIGIzNzQ2YWY5NmY0NGZjNTE5ODJjMDU5ZWNiYTE5YTllchMxOTI4NDA2MDU5NjA2ODQzNjM1igEKOTIzMjA0NzkwNKoBCXJlY29tbWVuZMIBIGM0MGI2M2VkM2YxOTJjYzM2Y2FlNmFkNTE4MjNlOTkw8gEKCAwSBk5vcm1hbPIBKAgKEiQzNzhlZjRjZS1jMjA2LTRjZmEtYWYyNi1mYjBhMTY0YzE4M2byAQYICxICMTGCAgCIAuC13c6FM5ICIGM0MGI2M2VkM2YxOTJjYzM2Y2FlNmFkNTE4MjNlOTkwmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFlJldmlzaXRWYWx1ZVdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXaAipUU19TT1VSQ0VfWlJFQ0FMTF9GRUVEUkVfTkVXQklFX0hPVVJMWV9SVU3oAgP6AgtOT1JNQUxfRkxPV4oDIDA1MmVmMjQ2M2ViYTQ2MjU4ZWQxNzUwMjUzZTg1YjIwmgMNCgJ2MhAAGgVvdGhlcqgDufgH2AMA6gMQbmV3YmllX2ZlZWRyZV92MvoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQGbWFudWFswgQDMTcwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAABA9s6nP4EFAAAAAAAAAACJBbsF9gwaW9M/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQuQBgCgBj2oBgOSAi4KCTczNzA5NDc3MBITMTkyODQwNjA1OTYwNjg0MzYzNRgEIgpJTUFHRV9URVhU","action_card":false},{"id":"62_1753853942.927","type":"feed","offset":62,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853942,"updated_time":1753853942,"target":{"id":"1924037468744815613","type":"article","url":"https://api.zhihu.com/articles/1924037468744815613","author":{"id":"525112db1bd3f935b9db44fee2629632","url":"https://api.zhihu.com/people/525112db1bd3f935b9db44fee2629632","user_type":"people","url_token":"wu-qi-tian-33","name":"Qitian","headline":"Learning from Machine","avatar_url":"https://picx.zhimg.com/50/v2-160014d62cb420076fba792f8f06d6e9_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":1812,"is_following":false,"is_followed":false},"title":"ICML2025 | 对流扩散引导的Transformer设计","comment_permission":"all","created":1751506554,"updated":1751732838,"voteup_count":43,"voting":0,"comment_count":4,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"分享一下我们在ICML2025的最新的工作 AdvDIFFormer，这是在前期工作NodeFormer (NeurIPS\u0026#39;22 )，DIFFormer (ICLR\u0026#39;23 , JMLR\u0026#39;25 )，SGFormer (NeurIPS\u0026#39;23 )的基础上关于graph Transformer的进一步探索。为方便follow，下图是这四个工作解决的核心问题和内在的承接关系：   希望更多了解前期工作的读者，尤其是为什么要研究图上的Transformer，如何实现大规模图上可扩展且高效的Transformer，以及扩散过程与GNN/Transformer等模型的内在联…","excerpt_new":"分享一下我们在ICML2025的最新的工作 AdvDIFFormer，这是在前期工作NodeFormer (NeurIPS\u0026#39;22 )，DIFFormer (ICLR\u0026#39;23 , JMLR\u0026#39;25 )，SGFormer (NeurIPS\u0026#39;23 )的基础上关于graph Transformer的进一步探索。为方便follow，下图是这四个工作解决的核心问题和内在的承接关系：   希望更多了解前期工作的读者，尤其是为什么要研究图上的Transformer，如何实现大规模图上可扩展且高效的Transformer，以及扩散过程与GNN/Transformer等模型的内在联…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"Q9OWD_LF\"\u003e分享一下我们在ICML2025的最新的工作\u003cb\u003eAdvDIFFormer\u003c/b\u003e，这是在前期工作\u003cb\u003eNodeFormer (\u003c/b\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2306.08385\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eNeurIPS\u0026#39;22\u003c/a\u003e\u003cb\u003e)\u003c/b\u003e，\u003cb\u003eDIFFormer (\u003c/b\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2301.09474\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eICLR\u0026#39;23\u003c/a\u003e\u003cb\u003e, \u003c/b\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2409.09111\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eJMLR\u0026#39;25\u003c/a\u003e\u003cb\u003e)\u003c/b\u003e，\u003cb\u003eSGFormer (\u003c/b\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2306.10759\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eNeurIPS\u0026#39;23\u003c/a\u003e\u003cb\u003e)的\u003c/b\u003e基础上关于graph Transformer的进一步探索。为方便follow，下图是这四个工作解决的核心问题和内在的承接关系：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-4e46db85fc7d7fc57e6c2c4f2456bc2c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"816\" data-original-token=\"v2-b648470c64a9fe8aced52b6085cb899b\" class=\"origin_image zh-lightbox-thumb\" width=\"1582\" data-original=\"https://pica.zhimg.com/v2-4e46db85fc7d7fc57e6c2c4f2456bc2c_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"87vk7UHJ\"\u003e希望更多了解前期工作的读者，尤其是为什么要研究图上的Transformer，如何实现大规模图上可扩展且高效的Transformer，以及扩散过程与GNN/Transformer等模型的内在联系，这里是之前文章的传送门：\u003c/p\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/587086593\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\"\u003eNeurIPS22 spotlight｜大图上的线性Transformer\u003c/a\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/674548352\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\"\u003eNeurIPS2023 | 面向超大图的简化Transformer\u003c/a\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/622970740\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\"\u003eICLR2023 | DIFFormer：扩散过程启发的Transformer\u003c/a\u003e\u003ch2\u003e正文开始\u003c/h2\u003e\u003cp data-pid=\"IdzeSqJ6\"\u003e设计针对图结构数据的Transformer模型（通常简称graph Transformer）目前已成为了一个备受关注的研究方向。尽管现有方法在表达能力上取得了显著进展，但它们在面对拓扑结构变化时的泛化能力仍是一个开放性问题。具体而言，大多数现有工作主要关注特征和标签的分布偏移，却忽略了训练和测试图拓扑可能来自不同分布的“拓扑分布偏移”（topological distribution shifts）现象。这种偏移在许多现实世界的关键场景中普遍存在，例如分子结构具有不同的药物相似性，在药物发现中，即使分子结构只有微小差异（拓扑变化），其药理活性也可能截然不同。\u003c/p\u003e\u003cp data-pid=\"GzqJhd8z\"\u003e为应对这一挑战，我们提出了 \u003cb\u003eAdvective Diffusion Transformer\u003c/b\u003e（简称\u003cb\u003eAdvDIFFormer\u003c/b\u003e），一个受物理学启发的图Transformer模型。该模型源于对流扩散方程（Advective Diffusion Equation），这类方程描述了一个定义在同时包含观测和潜在拓扑结构上的连续消息传递过程，能够巧妙地结合图的局部结构信息与全局依赖关系，并提供在拓扑结构变化下可证明的泛化能力。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-d8426647dfb207f499f17c656da58951_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2028\" data-rawheight=\"378\" data-original-token=\"v2-13f57e98e7b91d88c7d168ec5a5bede7\" class=\"origin_image zh-lightbox-thumb\" width=\"2028\" data-original=\"https://pic2.zhimg.com/v2-d8426647dfb207f499f17c656da58951_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"emOIl7tq\"\u003e论文题目：Supercharging Graph Transformers with Advective Diffusion\u003c/p\u003e\u003cp data-pid=\"o79Cj6J8\"\u003e论文链接：\u003ca href=\"https://link.zhihu.com/?target=https%3A//openreview.net/pdf%3Fid%3DMaOYl3P84E\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003eopenreview.net/pdf?\u003c/span\u003e\u003cspan class=\"invisible\"\u003eid=MaOYl3P84E\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"BWXWaCwS\"\u003e代码链接：\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/qitianwu/AdvDIFFormer\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003egithub.com/qitianwu/Adv\u003c/span\u003e\u003cspan class=\"invisible\"\u003eDIFFormer\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003ch2\u003e模型介绍：物理启发的图表示学习\u003c/h2\u003e\u003cp data-pid=\"BYcKPePi\"\u003e近期相关工作[1,3]阐释了图学习模型（即图神经网络）与扩散方程的本质联系，而工作[2,3]则进一步把Transformer和扩散方程联系了起来。AdvDIFFormer的核心思想是利用对流扩散方程来启发设计新的Transformer。 对流扩散方程常用于描述复杂系统中物理量的时空变化，其中“扩散”项由浓度梯度驱动（物质从高浓度区域向低浓度区域传播），而“对流”项则由扩散物质的宏观运动引起。我们将这种物理直觉映射到图上的消息传递过程。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-53ef68312a66c3c890e85883a208f0c1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1796\" data-rawheight=\"996\" data-original-token=\"v2-890e6930d3cdefa4bbcc04227adbedf4\" class=\"origin_image zh-lightbox-thumb\" width=\"1796\" data-original=\"https://pic4.zhimg.com/v2-53ef68312a66c3c890e85883a208f0c1_r.jpg\"/\u003e\u003cfigcaption\u003e对流扩散方程描述了系统中驱动物理量变化的两种效应：扩散效应是一种内在的由浓度差导致的运动，其不会随环境而改变；对流效应则是由宏观运动导致的，其会依赖于特定的环境。受此启发，一个理想的可泛化的模型，应该能学习到“扩散效应”所对应的映射关系。\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"WikkiZmg\"\u003e在图 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BG%7D%3D%28%5Cmathcal%7BV%7D%2C%5Cmathcal%7BE%7D%29\" alt=\"\\mathcal{G}=(\\mathcal{V},\\mathcal{E})\" eeimg=\"1\"/\u003e 上，我们将每个节点 \u003cimg src=\"https://www.zhihu.com/equation?tex=u\" alt=\"u\" eeimg=\"1\"/\u003e 的特征 \u003cimg src=\"https://www.zhihu.com/equation?tex=Z_u%28t%29\" alt=\"Z_u(t)\" eeimg=\"1\"/\u003e 视为在时间 \u003cimg src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/\u003e 的“浓度”。图上的对流扩散方程可以形式化为：\u003c/p\u003e\u003cp data-pid=\"3HksmYn5\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Z%28t%29%7D%7B%5Cpartial+t%7D%3D%5BC%28Z%28t%29%29%2B%5Cbeta+V%28t%29-I%5DZ%28t%29%2C+%5Cquad+0%5Cle+t%5Cle+T+%5C%5C\" alt=\"\\frac{\\partial Z(t)}{\\partial t}=[C(Z(t))+\\beta V(t)-I]Z(t), \\quad 0\\le t\\le T \\\\\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cp data-pid=\"pwSCt8mm\"\u003e其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=Z%280%29%3D%5Cphi_%7Benc%7D%28X%29\" alt=\"Z(0)=\\phi_{enc}(X)\" eeimg=\"1\"/\u003e 是初始条件，通过一个编码器 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cphi_%7Benc%7D\" alt=\"\\phi_{enc}\" eeimg=\"1\"/\u003e 从原始节点特征 \u003cimg src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/\u003e 得到；\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cbeta%5Cin%5B0%2C1%5D\" alt=\"\\beta\\in[0,1]\" eeimg=\"1\"/\u003e 是一个超参数，用于平衡对流（局部信息）和扩散（全局信息）的影响。该方程的闭合形式解为 \u003cimg src=\"https://www.zhihu.com/equation?tex=Z%28t%29%3De%5E%7B-%28I-C-%5Cbeta+V%29t%7DZ%280%29\" alt=\"Z(t)=e^{-(I-C-\\beta V)t}Z(0)\" eeimg=\"1\"/\u003e。\u003c/p\u003e\u003cp data-pid=\"u0si2b6z\"\u003e\u003cb\u003eAdvDIFFormer 的两大核心机制\u003c/b\u003e：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"tg8oru0n\"\u003e\u003cb\u003e非局部扩散作为全局注意力 (Non-local diffusion as global attention)\u003c/b\u003e: 扩散过程由浓度梯度驱动，其扩散系数在不同环境中保持不变，反映了数据固有的、不受外部结构变化影响的潜在关联。这恰好对应了图上的全局注意力机制，它允许任意节点对之间进行即时信息流，并捕获与底层数据流形相关的潜在交互。我们将耦合矩阵 \u003cimg src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/\u003e 实例化为一个\u003cb\u003e全局注意力矩阵\u003c/b\u003e，用于计算任意节点对之间的相似性：\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=C+%3D+%5BC_%7Buv%7D%5D_%7Bu%2Cv%5Cin%5Cmathcal%7BV%7D%7D%2C+%5Cquad+c_%7Buv%7D%3D%5Cfrac%7B%5Ceta%28z_%7Bu%7D%280%29%2Cz_%7Bv%7D%280%29%29%7D%7B%5Csum_%7Bw%5Cin%5Cmathcal%7BV%7D%7D%5Ceta%28z_%7Bu%7D%280%29%2Cz_%7Bw%7D%280%29%29%7D+%5C%5C\" alt=\"C = [C_{uv}]_{u,v\\in\\mathcal{V}}, \\quad c_{uv}=\\frac{\\eta(z_{u}(0),z_{v}(0))}{\\sum_{w\\in\\mathcal{V}}\\eta(z_{u}(0),z_{w}(0))} \\\\\" eeimg=\"1\"/\u003e 其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/\u003e 是一个可学习的成对相似性函数（例如，可以是一个多层感知机或点积注意力）。这种设计使得模型能够捕获到图中节点之间的长距离依赖关系，而不受限于局部邻域。\u003cbr/\u003e \u003c/li\u003e\u003cli data-pid=\"qgZZYvVS\"\u003e\u003cb\u003e对流作为局部消息传递 (Advection as local message passing)\u003c/b\u003e: 对流过程由定向运动驱动，这是一种外部力，其速度取决于具体环境。这与环境敏感的图拓扑结构类似，后者在特定环境中对预测标签具有有用的信息。我们将速度 \u003cimg src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/\u003e 实例化为\u003cb\u003e归一化图邻接矩阵\u003c/b\u003e，即 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7BA%7D%3DD%5E%7B-1%2F2%7DAD%5E%7B-1%2F2%7D\" alt=\"\\tilde{A}=D^{-1/2}AD^{-1/2}\" eeimg=\"1\"/\u003e，这反映了观测到的局部结构信息。其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/\u003e 是邻接矩阵，\u003cimg src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/\u003e 是度矩阵。这一部分实现了传统的图神经网络（GNN）中的局部消息传递，确保模型能够有效利用图中显式的局部连接信息。\u003cbr/\u003e \u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"GBk7ykzL\"\u003e通过将非局部扩散（通过注意力实现）和对流（通过MPNN实现）巧妙地融合在一个统一的模型框架中，AdvDIFFormer 能够同时处理图的全局和局部信息，从而在面对拓扑结构变化时展现出更强的可泛化性。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-3ef90533dd6abce1b4ab60e1772279e9_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"862\" data-rawheight=\"548\" data-original-token=\"v2-4c9a884bef520a306e2bd79fc78151c2\" class=\"origin_image zh-lightbox-thumb\" width=\"862\" data-original=\"https://picx.zhimg.com/v2-3ef90533dd6abce1b4ab60e1772279e9_r.jpg\"/\u003e\u003cfigcaption\u003eAdvDIFFORMER模型架构概览。模型通过对流-扩散方程融合了全局注意力（扩散）和局部消息传递（对流）机制\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e理论分析：泛化能力与拓扑鲁棒性\u003c/h2\u003e\u003cp data-pid=\"DaxwOI4g\"\u003e下面我们深入分析 AdvDIFFormer 在拓扑结构变化下的泛化能力，这也是以往图学习模型常常忽视的关键问题。我们感兴趣的是当模型从由环境 \u003cimg src=\"https://www.zhihu.com/equation?tex=E_%7Btr%7D\" alt=\"E_{tr}\" eeimg=\"1\"/\u003e 生成的训练数据迁移到由 \u003cimg src=\"https://www.zhihu.com/equation?tex=E_%7Bte%7D\" alt=\"E_{te}\" eeimg=\"1\"/\u003e 生成的测试数据时，其泛化误差如何变化。这种数据分布偏移导致了图拓扑 \u003cimg src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/\u003e 从 \u003cimg src=\"https://www.zhihu.com/equation?tex=p%28A%7CE_%7Btr%7D%29\" alt=\"p(A|E_{tr})\" eeimg=\"1\"/\u003e 分布变为 \u003cimg src=\"https://www.zhihu.com/equation?tex=p%28A%7CE_%7Bte%7D%29\" alt=\"p(A|E_{te})\" eeimg=\"1\"/\u003e 分布。\u003c/p\u003e\u003cp data-pid=\"V5CkqA3j\"\u003e我们首先将模型在拓扑结构变化下的泛化误差分解为三个误差项：\u003c/p\u003e\u003cp data-pid=\"EoDjsp5_\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BR%7D%28%5CGamma_%7B%5Ctheta%7D%3BE_%7Bte%7D%29-%5Cmathcal%7BR%7D_%7Bemp%7D%28%5CGamma_%7B%5Ctheta%7D%3BE_%7Btr%7D%29%7C%5Cle%5Cmathcal%7BD%7D_%7Bin%7D%28%5CGamma_%7B%5Ctheta%7D%2CE_%7Btr%7D%2CN_%7Btr%7D%29+%2BO%28%5Cmathbb%7BE%7D_%7BA%5Csim+p%28A%7CE_%7Btr%7D%29%2CA%27%5Csim+p%28A%7CE_%7Bte%7D%29%7D+%5B%7C%7CZ%28T%3BA%27%29+-+Z%28T%3BA%29%7C%7C_2%5D%29+%2BO%28%5Cmathbb%7BE%7D_%7B%28A%2CY%29%5Csim+p%28A%2CY%7CE_%7Btr%7D%29%2C%28A%27%2CY%27%29%5Csim+p%28A%2CY%7CE_%7Bte%7D%29%7D+%5B%7C%7CY%27+-+Y%7C%7C_2%5D%29+%5C%5C\" alt=\"|\\mathcal{R}(\\Gamma_{\\theta};E_{te})-\\mathcal{R}_{emp}(\\Gamma_{\\theta};E_{tr})|\\le\\mathcal{D}_{in}(\\Gamma_{\\theta},E_{tr},N_{tr}) +O(\\mathbb{E}_{A\\sim p(A|E_{tr}),A\u0026#39;\\sim p(A|E_{te})} [||Z(T;A\u0026#39;) - Z(T;A)||_2]) +O(\\mathbb{E}_{(A,Y)\\sim p(A,Y|E_{tr}),(A\u0026#39;,Y\u0026#39;)\\sim p(A,Y|E_{te})} [||Y\u0026#39; - Y||_2]) \\\\\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cp data-pid=\"0BUGjxX7\"\u003e其中：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"ZG8hxxu7\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7Bin%7D\" alt=\"\\mathcal{D}_{in}\" eeimg=\"1\"/\u003e 是分布内泛化误差，与测试数据无关，主要取决于模型复杂度、训练样本量等。\u003c/li\u003e\u003cli data-pid=\"43epgXg5\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=O%28%5Cmathbb%7BE%7D_%7BA%5Csim+p%28A%7CE_%7Btr%7D%29%2CA%27%5Csim+p%28A%7CE_%7Bte%7D%29%7D+%5B%7C%7CZ%28T%3BA%27%29+-+Z%28T%3BA%29%7C%7C_2%5D%29\" alt=\"O(\\mathbb{E}_{A\\sim p(A|E_{tr}),A\u0026#39;\\sim p(A|E_{te})} [||Z(T;A\u0026#39;) - Z(T;A)||_2])\" eeimg=\"1\"/\u003e 这一项 \u003cb\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7Bood-model%7D\" alt=\"\\mathcal{D}_{ood-model}\" eeimg=\"1\"/\u003e\u003c/b\u003e 反映了模型输出的节点表示 \u003cimg src=\"https://www.zhihu.com/equation?tex=Z%28T%3BA%29\" alt=\"Z(T;A)\" eeimg=\"1\"/\u003e 随拓扑变化 \u003cimg src=\"https://www.zhihu.com/equation?tex=A+%5Cto+A%27\" alt=\"A \\to A\u0026#39;\" eeimg=\"1\"/\u003e 而变化的幅度。这是我们关注的核心项，因为它直接衡量了模型对拓扑变化的敏感度。\u003c/li\u003e\u003cli data-pid=\"RUZVxByN\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=O%28%5Cmathbb%7BE%7D_%7B%28A%2CY%29%5Csim+p%28A%2CY%7CE_%7Btr%7D%29%2C%28A%27%2CY%27%29%5Csim+p%28A%2CY%7CE_%7Bte%7D%29%7D+%5B%7C%7CY%27+-+Y%7C%7C_2%5D%29\" alt=\"O(\\mathbb{E}_{(A,Y)\\sim p(A,Y|E_{tr}),(A\u0026#39;,Y\u0026#39;)\\sim p(A,Y|E_{te})} [||Y\u0026#39; - Y||_2])\" eeimg=\"1\"/\u003e 这一项 \u003cb\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7Bood-label%7D\" alt=\"\\mathcal{D}_{ood-label}\" eeimg=\"1\"/\u003e\u003c/b\u003e 衡量了不同环境下标签 \u003cimg src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/\u003e 的固有差异。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"SEqskBpG\"\u003e我们的主要贡献在于对 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7Bood-model%7D\" alt=\"\\mathcal{D}_{ood-model}\" eeimg=\"1\"/\u003e 的分析，并证明 AdvDIFFormer 能够显著控制这一项。\u003c/p\u003e\u003cblockquote data-pid=\"B-aggux_\"\u003e\u003cb\u003e定理 3.2\u003c/b\u003e： 对于在定义的生成机制下产生的任何图数据，如果 \u003cimg src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/\u003e 是单射的（保证了节点身份的唯一性），那么AdvDIFFormer模型可以将节点表示的变化幅度 \u003cimg src=\"https://www.zhihu.com/equation?tex=%7C%7CZ%28T%3BA%5E%7B%5Cprime%7D%29-Z%28T%3BA%29%7C%7C_%7B2%7D\" alt=\"||Z(T;A^{\\prime})-Z(T;A)||_{2}\" eeimg=\"1\"/\u003e 降低到任意阶 \u003cimg src=\"https://www.zhihu.com/equation?tex=O%28%5Cpsi%28%7C%7C%5CDelta%5Ctilde%7BA%7D%7C%7C_%7B2%7D%29%29\" alt=\"O(\\psi(||\\Delta\\tilde{A}||_{2}))\" eeimg=\"1\"/\u003e，其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cpsi\" alt=\"\\psi\" eeimg=\"1\"/\u003e 是任意多项式函数，\u003cimg src=\"https://www.zhihu.com/equation?tex=%5CDelta%5Ctilde%7BA%7D%3D%5Ctilde%7BA%7D%5E%7B%5Cprime%7D-%5Ctilde%7BA%7D\" alt=\"\\Delta\\tilde{A}=\\tilde{A}^{\\prime}-\\tilde{A}\" eeimg=\"1\"/\u003e 且 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7BA%7D%3DD%5E%7B-1%2F2%7DAD%5E%7B-1%2F2%7D\" alt=\"\\tilde{A}=D^{-1/2}AD^{-1/2}\" eeimg=\"1\"/\u003e。\u003c/blockquote\u003e\u003cp data-pid=\"sh4rf9zN\"\u003e这一结论的含义是，即使图的拓扑结构发生了变化，AdvDIFFormer 产生的节点表示 \u003cimg src=\"https://www.zhihu.com/equation?tex=Z%28T%29\" alt=\"Z(T)\" eeimg=\"1\"/\u003e 也只会以拓扑变化幅度 \u003cimg src=\"https://www.zhihu.com/equation?tex=%7C%7C+%5CDelta+%5Ctilde%7BA%7D+%7C%7C_2\" alt=\"|| \\Delta \\tilde{A} ||_2\" eeimg=\"1\"/\u003e 的多项式形式发生变化，而不是指数级变化。这意味着模型在处理拓扑结构变化时，能够以任意速度控制节点表示的变化率，从而保证了所需的泛化水平。\u003c/p\u003e\u003cblockquote data-pid=\"Cp0q_5Db\"\u003e\u003cb\u003e推论 3.3\u003c/b\u003e： 在定理 3.1 和 3.2 的相同条件下，AdvDIFFormer模型的泛化误差界 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7Bood-model%7D%28%5CGamma_%7B%5Ctheta%7D%2CE_%7Btr%7D%2CE_%7Btr%7D%29\" alt=\"\\mathcal{D}_{ood-model}(\\Gamma_{\\theta},E_{tr},E_{tr})\" eeimg=\"1\"/\u003e 可以降低到任意多项式阶，即 \u003cimg src=\"https://www.zhihu.com/equation?tex=O%28%5Cmathbb%7BE%7D_%7BA%5Csim+p%28A%7CE_%7Btr%7D%29%2CA%27%5Csim+p%28A%7CE_%7Bte%7D%29%7D%5B%5Cpsi%28%7C%7C%5CDelta%5Coverline%7BA%7D%7C%7C_%7B2%7D%29%5D%29\" alt=\"O(\\mathbb{E}_{A\\sim p(A|E_{tr}),A\u0026#39;\\sim p(A|E_{te})}[\\psi(||\\Delta\\overline{A}||_{2})])\" eeimg=\"1\"/\u003e。\u003c/blockquote\u003e\u003cp data-pid=\"DveAF4sF\"\u003e这一结果表明与常见的图扩散模型（其泛化误差可能随拓扑结构变化呈指数增长）不同，AdvDIFFormer 的泛化误差可以在任意速度下被控制，从而在拓扑结构变化下具有可证明的泛化能力，为模型在现实世界复杂场景中的应用提供了理论基础。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-3882ebe4b962abee5d34b8f937390493_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1760\" data-rawheight=\"382\" data-original-token=\"v2-d33eb5db7fbdbc56c2d7c8cbd92cdf72\" class=\"origin_image zh-lightbox-thumb\" width=\"1760\" data-original=\"https://pic2.zhimg.com/v2-3882ebe4b962abee5d34b8f937390493_r.jpg\"/\u003e\u003cfigcaption\u003e在合成数据上AdvDIFFormer与其他图扩散模型的对比。随着拓扑分布偏移的加重，其他方法的泛化误差显著增加，而两种AdvDIFFormer实现的泛化误差几乎不变\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e模型实现：AdvDIFFormer 的两种实现方式\u003c/h2\u003e\u003cp data-pid=\"zKUJgm6D\"\u003e由于对流扩散方程的闭合形式解中涉及矩阵指数 \u003cimg src=\"https://www.zhihu.com/equation?tex=e%5E%7B-Lt%7D\" alt=\"e^{-Lt}\" eeimg=\"1\"/\u003e 的计算，这在实际操作中是难以处理的。因此，论文提出了两种基于数值近似的实现版本。\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"J4U-woWe\"\u003e\u003cb\u003eAdvDIFFormer-i（基于线性系统近似）\u003c/b\u003e： 该版本利用 Padé-Chebyshev 理论的扩展[4,5]，通过多个矩阵逆的组合来近似矩阵指数。具体而言，形如 \u003cimg src=\"https://www.zhihu.com/equation?tex=e%5E%7B-Lt%7D\" alt=\"e^{-Lt}\" eeimg=\"1\"/\u003e 的矩阵指数可以近似表示为：\u003cimg src=\"https://www.zhihu.com/equation?tex=e%5E%7B-Lt%7D+%5Capprox+-%5Csum_%7Bi%3D1%7D%5E%7Br%7D+%5Calpha_i%28L+%2B+%5Ctheta_i+I%29%5E%7B-1%7D+%3D+-%5Csum_%7Bi%3D1%7D%5E%7Br%7D+%5Calpha_i%28%28I+-+C+-+%5Cbeta+V%29+%2B+%5Ctheta_i+I%29%5E%7B-1%7D+%5C%5C\" alt=\"e^{-Lt} \\approx -\\sum_{i=1}^{r} \\alpha_i(L + \\theta_i I)^{-1} = -\\sum_{i=1}^{r} \\alpha_i((I - C - \\beta V) + \\theta_i I)^{-1} \\\\\" eeimg=\"1\"/\u003e 其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Calpha_i\" alt=\"\\alpha_i\" eeimg=\"1\"/\u003e 和 \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta_i\" alt=\"\\theta_i\" eeimg=\"1\"/\u003e 是预定义的参数，\u003cimg src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/\u003e 是近似的阶数。为了增强神经网络的能力，AdvDIFFormer-i 将此方案扩展到多头网络，每个头都通过独立参数化的注意力网络进行传播。矩阵逆的计算则通过深度学习工具（如PyTorch）中可用的线性系统求解器实现，并支持自动微分。具体模型的数学表达为：\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"pJkwIv8F\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+L_h+%3D+%281+%2B+%5Ctheta%29%5Cmathbf+I+-+%5Cmathbf+C_h+-+%5Cbeta+%5Cmathbf+V%2C+~~h+%3D+1%2C+%5Ccdots%2C+H%2C++%5C%5C\" alt=\"\\mathbf L_h = (1 + \\theta)\\mathbf I - \\mathbf C_h - \\beta \\mathbf V, ~~h = 1, \\cdots, H,  \\\\\" eeimg=\"1\"/\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+Z%28T%29+%5Capprox+%5Csum%5Cnolimits_%7Bh%3D1%7D%5EH++%5Cphi_%7BFC%7D%5E%7B%28h%29%7D%28+%5Cmbox%7B%5Ctexttt%7Blinsolver%7D%7D%28%5Cmathbf+L_h%2C+%5Cmathbf+Z%280%29%29+%29+%5C%5C\" alt=\"\\mathbf Z(T) \\approx \\sum\\nolimits_{h=1}^H  \\phi_{FC}^{(h)}( \\mbox{\\texttt{linsolver}}(\\mathbf L_h, \\mathbf Z(0)) ) \\\\\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"R1UM91rz\"\u003e\u003cb\u003eAdvDIFFormer-s（基于几何级数近似的线性复杂度实现）\u003c/b\u003e： 当图的规模较大时，矩阵逆的计算成本会非常高。为了提高可扩展性，AdvDIFFormer-s 采用几何级数近似来计算矩阵指数。这种方法可以将矩阵指数近似为有限的级数和，即：\u003cimg src=\"https://www.zhihu.com/equation?tex=e%5E%7B-Lt%7D+%5Capprox+-%5Csum_%7Bi%3D1%7D%5E%7Br%7D+%5Calpha_i+%5Csum_%7Bk%3D0%7D%5E%7BK%7D+%28-1%29%5Ek+%5Ctheta_i%5E%7B-%28k%2B1%29%7D+L%5Ek+%3D+-%5Csum_%7Bi%3D1%7D%5E%7Br%7D+%5Calpha_i+%5Csum_%7Bk%3D0%7D%5E%7BK%7D+%28-1%29%5Ek+%5Ctheta_i%5E%7B-%28k%2B1%29%7D+%28I+-+C+-+%5Cbeta+V%29%5Ek+%5C%5C\" alt=\"e^{-Lt} \\approx -\\sum_{i=1}^{r} \\alpha_i \\sum_{k=0}^{K} (-1)^k \\theta_i^{-(k+1)} L^k = -\\sum_{i=1}^{r} \\alpha_i \\sum_{k=0}^{K} (-1)^k \\theta_i^{-(k+1)} (I - C - \\beta V)^k \\\\\" eeimg=\"1\"/\u003e 其中 \u003cimg src=\"https://www.zhihu.com/equation?tex=K\" alt=\"K\" eeimg=\"1\"/\u003e 是级数的截断长度。在 AdvDIFFormer 中，闭合形式的 PDE 解对应于 \u003cimg src=\"https://www.zhihu.com/equation?tex=L+%3D+%28I+-+C+-+%5Cbeta+V%29\" alt=\"L = (I - C - \\beta V)\" eeimg=\"1\"/\u003e。因此，几何级数求和可以表示为 \u003cimg src=\"https://www.zhihu.com/equation?tex=P_k+%3D+%28C+%2B+%5Cbeta+V%29%5Ek\" alt=\"P_k = (C + \\beta V)^k\" eeimg=\"1\"/\u003e 在 \u003cimg src=\"https://www.zhihu.com/equation?tex=k+%3D+0%2C+%5Cldots%2C+K\" alt=\"k = 0, \\ldots, K\" eeimg=\"1\"/\u003e 上的加权和。AdvDIFFormer-s 将这种加权和推广到一个单层神经网络中实现，以进一步优化计算效率。具体模型的数学表达为：\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"w7Ooc27O\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+P_h++%3D+%5Cmathbf+C_h+%2B+%5Cbeta+%5Ctilde%7B%5Cmathbf+A%7D%2C+~~h+%3D+1%2C+%5Ccdots%2C+H+++%5C%5C\" alt=\"\\mathbf P_h  = \\mathbf C_h + \\beta \\tilde{\\mathbf A}, ~~h = 1, \\cdots, H   \\\\\" eeimg=\"1\"/\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+Z%28T%29+%5Capprox+%5Csum%5Cnolimits_%7Bh%3D1%7D%5EH+%5Cphi_%7BFC%7D%5E%7B%28h%29%7D%28+%5B%5Cmathbf+Z%280%29%2C+%5Cmathbf+P_h+%5Cmathbf+Z%280%29%2C+%5Ccdots%2C+%28%5Cmathbf+P_h%29%5E%7BK%7D+%5Cmathbf+Z%280%29%5D+%29+%5C%5C\" alt=\"\\mathbf Z(T) \\approx \\sum\\nolimits_{h=1}^H \\phi_{FC}^{(h)}( [\\mathbf Z(0), \\mathbf P_h \\mathbf Z(0), \\cdots, (\\mathbf P_h)^{K} \\mathbf Z(0)] ) \\\\\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cp data-pid=\"KlrpkhDL\"\u003e在具体实现中，我们沿用了[2]中提出的线性注意力实现方式，最终模型的计算复杂度控制在相对节点数目的线性级别。\u003c/p\u003e\u003ch2\u003e实验结果\u003c/h2\u003e\u003cp data-pid=\"-inFQrj9\"\u003e我们在多个领域（包括信息网络、分子图和蛋白质相互作用网络）的不同预测任务中验证了AdvDIFFormer的泛化性能。实验设计旨在模拟实际中可能遇到的拓扑结构变化，以全面评估模型的泛化能力。\u003c/p\u003e\u003cp data-pid=\"R0c3SAq3\"\u003e\u003cb\u003e信息网络\u003c/b\u003e 我们考虑在两个广泛使用的信息网络数据集上的节点分类任务。\u003cb\u003eArxiv数据集：\u003c/b\u003e：包含来自arXiv论文引用网络的节点（论文）和边（引用关系）。\u003cb\u003eTwitch 数据集\u003c/b\u003e：包含Twitch流媒体平台的用户和关注关系。实验结果表明，在这些拓扑结构会发生变化的数据集上，AdvDIFFormer 表现出了显著优越的泛化能力。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-0219a32dcd241ae8b4170e56e32fde6e_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"882\" data-rawheight=\"740\" data-original-token=\"v2-4aefa7ba2aaa6b693f4573381d9a505e\" class=\"origin_image zh-lightbox-thumb\" width=\"882\" data-original=\"https://pic3.zhimg.com/v2-0219a32dcd241ae8b4170e56e32fde6e_r.jpg\"/\u003e\u003cfigcaption\u003e在Arxiv和Twitch数据集上的准确率对比。AdvDIFFormer在不同的测试设置下均表现出更强的鲁棒性\u003c/figcaption\u003e\u003c/figure\u003e\u003col\u003e\u003cli data-pid=\"nr29DX8I\"\u003e\u003cb\u003e蛋白质相互作用\u003c/b\u003e 我们使用了 DPPIN (Dynamic Protein-Protein Interaction Network) 数据集，该数据集包含 12 个动态蛋白质-蛋白质相互作用网络，涵盖了酵母在不同实验条件下的基因表达和蛋白质相互作用信息。我们考虑了两种预测任务来全面评估模型：\u003cb\u003e节点回归\u003c/b\u003e：预测当前时间的基因表达值（RMSE衡量）；\u003cb\u003e边回归\u003c/b\u003e：预测蛋白质之间的共表达相关系数（RMSE衡量）。AdvDIFFormer 在这些任务中均表现出色，尤其在最差情况下的测试性能中位居前列，这进一步验证了其在复杂生物网络中处理动态拓扑的有效性。\u003c/li\u003e\u003c/ol\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-89cdce287e198db3e75a4d9c8c91b568_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1780\" data-rawheight=\"578\" data-original-token=\"v2-c43c05d7de5148e3a0ea2b17fa9d7c9c\" class=\"origin_image zh-lightbox-thumb\" width=\"1780\" data-original=\"https://pica.zhimg.com/v2-89cdce287e198db3e75a4d9c8c91b568_r.jpg\"/\u003e\u003cfigcaption\u003e在DPPIN数据上的准确率对比\u003c/figcaption\u003e\u003c/figure\u003e\u003col\u003e\u003cli data-pid=\"-6BV3kRz\"\u003e\u003cb\u003e分子映射算子生成\u003c/b\u003e 在分子映射 (HAM) 数据集上，预测任务可以被建模为图分割问题，目标是预测分子中哪些原子属于同一个粗粒度映射操作。实验设置通过分子质量进行数据划分（训练集为小分子，测试集为大分子），这要求模型从训练集中的小分子泛化到测试集中的大分子，本质上是应对分子拓扑结构的变化。AdvDIFFormer能够更准确地估计分子结构，相比其他模型展现出更理想的泛化能力。\u003c/li\u003e\u003c/ol\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-6fec480ad1d85bc89e645e6bd68a8293_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1794\" data-rawheight=\"450\" data-original-token=\"v2-55d33e2f164f29e2c3d87daec52f44cf\" class=\"origin_image zh-lightbox-thumb\" width=\"1794\" data-original=\"https://picx.zhimg.com/v2-6fec480ad1d85bc89e645e6bd68a8293_r.jpg\"/\u003e\u003cfigcaption\u003e在HAM映射算子生成的结果对比\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"fIY22O8R\"\u003e这些广泛的实验结果一致表明，AdvDIFFormer 在处理各种图学习任务中的拓扑结构变化方面，均能提供卓越的性能和强大的泛化能力。\u003c/p\u003e\u003ch2\u003e结语\u003c/h2\u003e\u003cp data-pid=\"izFFLzMN\"\u003e本文提出了 AdvDIFFormer，一个基于对流扩散方程的图Transformer模型，旨在解决图学习模型在拓扑结构变化下的泛化难题。我们不仅从理论上证明了其控制泛化误差的能力，使其能够适应不同拓扑结构的图数据，更在信息网络、分子筛选和蛋白质相互作用等多个领域的经验任务中验证了其卓越的性能。这项工作为构建更具鲁棒性和泛化能力的图学习模型提供了一条新颖且富有前景的技术路径，有望推动图表示学习在现实世界复杂动态系统中的广泛应用。\u003c/p\u003e\u003ch2\u003e参考文献\u003c/h2\u003e\u003cp data-pid=\"94udfPIB\"\u003e[1] Chamberlain, B., et al. GRAND: graph neural diffusion. In International Conference on Machine Learning (ICML), 2021.\u003c/p\u003e\u003cp data-pid=\"DaK9UaZW\"\u003e[2] Wu, Q., et al. Difformer: Scalable (graph) transformers induced by energy constrained diffusion. In International Conference on Learning Representations, 2023.\u003c/p\u003e\u003cp data-pid=\"tT_OLhs1\"\u003e[3] Wu, Q., et al. Transformers from diffusion: a unified framework for neural message passing. In Journal of Machine Learning Research, 2025.\u003c/p\u003e\u003cp data-pid=\"XGAdJ_JT\"\u003e[4] Golub, G. H, et al. Matrix computations. John Hopkins University Press, 1989.\u003c/p\u003e\u003cp data-pid=\"j9M2EHKJ\"\u003e[5] Gallopoulos, E., et al. Efficient solution of parabolic equations by krylov approximation methods. SIAM journal on scientific and statistical computing, 13(5):12361264, 1992.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":2033,"thumbnails":["https://pica.zhimg.com/50/v2-557d5aacb5dbe90b9c088b5098e64a3c_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-50195ab71c54a83f8a2a303c4341b513_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-736a712f9201c9773e8ac563b1ee62e5_720w.jpg?source=b6762063"],"favorite_count":101,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1924037468744815613}","attached_info":"CvwHCPDjne3L8syghgEQBxoJMjU5ODUwODI0IPq8l8MGKCswBEA+SjAKBkl0ZW1DRhIgZG9jX3R5cGU6IEFydGljbGUKaWQ6IDI1OTAzMjE2NgoYACAAOgBiIGIzNzQ2YWY5NmY0NGZjNTE5ODJjMDU5ZWNiYTE5YTllchMxOTI0MDM3NDY4NzQ0ODE1NjEzqgEJcmVjb21tZW5kwgEgNTI1MTEyZGIxYmQzZjkzNWI5ZGI0NGZlZTI2Mjk2MzLyAQoIDBIGTm9ybWFs8gEoCAoSJGRiZTU3MzM3LWVlNzYtNGI5Ny05YzExLTg4MWI2Y2YyN2ZjZfIBBggLEgIxMYICAIgC4LXdzoUzkgIgNTI1MTEyZGIxYmQzZjkzNWI5ZGI0NGZlZTI2Mjk2MzKaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxl2gIGSXRlbUNG6AID+gILTk9STUFMX0ZMT1eKAyAwNTJlZjI0NjNlYmE0NjI1OGVkMTc1MDI1M2U4NWIyMJoDDQoCdjIQABoFb3RoZXKoA/EP2AMA6gMVdGV4dEFsbFNpdGVNdkl0ZW1DRlYy+gOXAxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgCEK4MGLAGIiN2Mi1iNjQ4NDcwYzY0YTlmZThhY2VkNTJiNjA4NWNiODk5YjotCAIQ7A8Y+gIiI3YyLTEzZjU3ZTk4ZTdiOTFkODhjN2QxNjhlYzVhNWJlZGU3Oi0IAhCEDhjkByIjdjItODkwZTY5MzBkM2NkZWZhNGJiY2MwNDIyN2FkYmVkZjQ6LQgCEN4GGKQEIiN2Mi00YzlhODg0YmVmNTIwYTMwNmUyYmQ3OWZjNzgxNTFjMjotCAMQ4A0Y/gIiI3YyLWQzM2ViNWRiN2ZiZGJjNTZjMmQ3YzhjYmQ5MmNkZjcyOi0IAhDyBhjkBSIjdjItNGFlZmE3YmEyYWFhNmI2OTNmNDU3MzM4MWQ5YTUwNWU6LQgCEPQNGMIEIiN2Mi1jNDNjMDVkN2RlNTE0OGUzYTBlYTJiMTdmYTlkN2M5YzotCAIQgg4YwgMiI3YyLTU1ZDMzZTJmMTY0ZjI5ZTJjM2Q4N2RhZWM1MmY0NGNmgAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEBm1hbnVhbMIEAzE3MMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAA4O22mD+BBQAAAAAAAAAAiQW7BfYMGlvTP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AULkAYAoAY+qAYAkgIuCgkyNTk4NTA4MjQSEzE5MjQwMzc0Njg3NDQ4MTU2MTMYByIKSU1BR0VfVEVYVA==","action_card":false},{"id":"63_1753853942.714","type":"feed","offset":63,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853942,"updated_time":1753853942,"target":{"id":"1930305781221298827","type":"answer","url":"https://api.zhihu.com/answers/1930305781221298827","author":{"id":"4d1457c2e0812807181fe39798768388","url":"https://api.zhihu.com/people/4d1457c2e0812807181fe39798768388","user_type":"people","url_token":"ta-xue-chi-tu-6","name":"筱竹君","headline":"AIGC | 阅读写作","avatar_url":"https://picx.zhimg.com/50/v2-02fce04cbf8819f01bdba6a3b763d758_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":43,"is_following":false,"is_followed":false},"created_time":1753000761,"updated_time":1753000761,"voteup_count":5,"thanks_count":0,"comment_count":2,"is_copyable":true,"question":{"id":"1928904780610270558","type":"question","url":"https://api.zhihu.com/questions/1928904780610270558","author":{"id":"bc4b4bf3d8e9d760a27581e47c1dc172","url":"https://api.zhihu.com/people/bc4b4bf3d8e9d760a27581e47c1dc172","user_type":"people","url_token":"16mncr5","name":"李光辉","headline":"成长比成功更重要。微信：cnmcxk","avatar_url":"https://pic1.zhimg.com/50/v2-8246c15a37be03de70b65d6db035d1a7_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"一诺天承（上海）电子商务有限公司 总经理"}],"followers_count":17,"is_following":false,"is_followed":false},"title":"个人知识库应该具备哪些功能条件？","created":1752666737,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[949],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://pic1.zhimg.com/50/v2-2088c19f8696bc5c62c05b7935ceb971_720w.jpg?source=b6762063","excerpt":"最近一直在搭建和使用个人知识库，我深刻体会到，一个趁手的知识库工具绝非简单的 “资料仓库”，而是能真正提升知识管理效率的 “智能助手”。结合实操经验，我总结要有三个核心功能条件，或许能帮你少走弯路。 1. 内容先行：很多人一开始就陷入 “建体系” 的误区，忙着设计复杂的分类框架，结果反而阻碍了知识积累。我的经验是先聚焦内容本身 —— 无论是阅读笔记、灵感碎片还是工作资料，先无负担地记录下来。随着内容增多，…","excerpt_new":"最近一直在搭建和使用个人知识库，我深刻体会到，一个趁手的知识库工具绝非简单的 “资料仓库”，而是能真正提升知识管理效率的 “智能助手”。结合实操经验，我总结要有三个核心功能条件，或许能帮你少走弯路。 1. 内容先行：很多人一开始就陷入 “建体系” 的误区，忙着设计复杂的分类框架，结果反而阻碍了知识积累。我的经验是先聚焦内容本身 —— 无论是阅读笔记、灵感碎片还是工作资料，先无负担地记录下来。随着内容增多，…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"GnDU7dkB\"\u003e最近一直在搭建和使用个人知识库，我深刻体会到，一个趁手的知识库工具绝非简单的 “资料仓库”，而是能真正提升知识管理效率的 “智能助手”。结合实操经验，我总结要有三个核心功能条件，或许能帮你少走弯路。​\u003c/p\u003e\u003cp data-pid=\"3mksOIQq\"\u003e\u003cb\u003e1. 内容先行：\u003c/b\u003e很多人一开始就陷入 “建体系” 的误区，忙着设计复杂的分类框架，结果反而阻碍了知识积累。我的经验是先聚焦内容本身 —— 无论是阅读笔记、灵感碎片还是工作资料，先无负担地记录下来。随着内容增多，自然会浮现出分类逻辑，这时再逐步归纳总结，效率会高得多。\u003cb\u003e资料贵精不贵多\u003c/b\u003e，盲目囤积未经消化的信息，只会让知识库变成 “信息垃圾场”。​\u003c/p\u003e\u003cp data-pid=\"74tX89gA\"\u003e\u003cb\u003e2. 智能检索功能决定使用效率\u003c/b\u003e。当知识库积累到一定规模，快速定位信息就成了核心需求。关键词搜索是基础，但\u003cb\u003e标签系统\u003c/b\u003e是提升精准度的关键。比如给一篇行业报告同时打上 “2024 趋势”“人工智能”“应用案例” 等标签，后续搜索时通过多标签组合，能瞬间过滤掉无关内容。我曾因忽视标签管理，在几百条资料里翻找某篇笔记，现在规范打标签后，查找效率提升了至少 3 倍。​\u003c/p\u003e\u003cp data-pid=\"_armUjbx\"\u003e\u003cb\u003e3. 更新机制保障知识鲜活度\u003c/b\u003e。信息时代知识迭代太快，去年的行业数据可能今年就过时了。部分工具自带的 “过期提醒” 功能很实用，能自动标记需要复核的资料，避免用旧信息误导决策。​\u003c/p\u003e\u003cp data-pid=\"yf8SnKCW\"\u003e除了核心功能，个人知识库工具的附加能力同样影响使用体验。\u003cb\u003e多平台同步\u003c/b\u003e让知识管理打破设备限制 —— 在电脑上编辑的笔记，通勤时能用手机 APP 继续梳理，开会时还能通过小程序快速调取，无缝衔接的体验极大降低了记录门槛。\u003cb\u003e数据分析与可视化\u003c/b\u003e则像个 “隐形助理”，自动生成的脑图能帮我发现知识间的关联，术语表让专业词汇一目了然，内容概要更是复盘总结的好帮手。​至于\u003cb\u003e权限管理\u003c/b\u003e，看似小众却至关重要。无论是给同事开放部分资料的查阅权限，还是对私密笔记设置加密，分级权限都能在信息共享与安全之间找到平衡，尤其适合需要兼顾工作与个人学习的用户。​\u003c/p\u003e\u003cp data-pid=\"dFbPHJQI\"\u003e综合体验下来，个人知识库工具推荐优先使用\u003cb\u003eima，get笔记\u003c/b\u003e和\u003cb\u003e知乎直答知识库\u003c/b\u003e。这三款工具都保持了 “操作轻便” 的特点，不会让用户陷入复杂的设置中。​\u003c/p\u003e\u003cp data-pid=\"Atnj3Pvz\"\u003e每个人的知识管理习惯不同，适合的工具和方法也会有差异。你在使用个人知识库时，有哪些独特的技巧或踩过的坑？欢迎在评论区分享，让我们一起把知识库打造成真正的 “智慧宝库”。​\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-d30f409d43ef9b102cf817ade4f051b8_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1073\" data-rawheight=\"529\" data-original-token=\"v2-f43b1f4b1c0bfd3805de4ade5d5cf4d9\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-53bee74ce3e6abb7ee67626ae5c023de_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1073\" data-original=\"https://pica.zhimg.com/v2-d30f409d43ef9b102cf817ade4f051b8_r.jpg\"/\u003e\u003cfigcaption\u003eima知识库首页-腾讯出品\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-1245e47569734096911fe7bee9db3460_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1452\" data-rawheight=\"623\" data-original-token=\"v2-5dc8a1afc2994dd43b1846785b03e11e\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-c4ea79f6408af43a60c8d8f1e7856ca8_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1452\" data-original=\"https://pic1.zhimg.com/v2-1245e47569734096911fe7bee9db3460_r.jpg\"/\u003e\u003cfigcaption\u003eget笔记首页-得到出品\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-88c8f23c2d7f8319d31a920779621023_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1880\" data-rawheight=\"794\" data-original-token=\"v2-ca9d0bba1995d01943a8738eefe4499c\" data-default-watermark-src=\"https://picx.zhimg.com/v2-75e88a7ca2b8d642ecc9804bd8eecf4b_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1880\" data-original=\"https://picx.zhimg.com/v2-88c8f23c2d7f8319d31a920779621023_r.jpg\"/\u003e\u003cfigcaption\u003e知乎直答知识库\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":464,"thumbnails":["https://picx.zhimg.com/50/v2-2088c19f8696bc5c62c05b7935ceb971_720w.jpg?source=b6762063"],"favorite_count":20,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1930305781221298827}","attached_info":"CssHCPDjne3L8syghgEQBBoJNzM3OTYwNjYwILnW8sMGKAUwAkA/SkIKLVRTX1NPVVJDRV9UV09UT1dFUl9NVUxUSV9TQ0VORV9WMV9SRUNBTExfVEVYVBIBMBgAIAA6CnsicmF3IjoiIn1KQgotVFNfU09VUkNFX1RXT1RPV0VSX01VTFRJX1NDRU5FX1YxX1JFQ0FMTF9URVhUEgEwGAAgADoKeyJyYXciOiIifVoJMTE1ODMwMTMzYiBiMzc0NmFmOTZmNDRmYzUxOTgyYzA1OWVjYmExOWE5ZXITMTkzMDMwNTc4MTIyMTI5ODgyN4oBEzE5Mjg5MDQ3ODA2MTAyNzA1NTiqAQlyZWNvbW1lbmTCASA0ZDE0NTdjMmUwODEyODA3MTgxZmUzOTc5ODc2ODM4OPIBCggMEgZOb3JtYWzyASgIChIkYTc4Zjc5ZGItYjRmYy00ODRjLTllM2ItMGE5YjVlOWVjZjc38gEGCAsSAjExggIAiALgtd3OhTOSAiA0ZDE0NTdjMmUwODEyODA3MTgxZmUzOTc5ODc2ODM4OJoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhNFbWJTaW1Jc29sYXRpb25SdWxl2gItVFNfU09VUkNFX1RXT1RPV0VSX01VTFRJX1NDRU5FX1YxX1JFQ0FMTF9URVhU6AID+gILTk9STUFMX0ZMT1eKAyAwNTJlZjI0NjNlYmE0NjI1OGVkMTc1MDI1M2U4NWIyMJoDDQoCdjIQABoFb3RoZXKoA9AD2AMA6gMfdGV4dEZlZWRUd29Ub3dlcldhcm11cFN1Y2Nlc3NWMfoDrAESDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAhCxCBiRBCIjdjItZjQzYjFmNGIxYzBiZmQzODA1ZGU0YWRlNWQ1Y2Y0ZDk6LQgCEKwLGO8EIiN2Mi01ZGM4YTFhZmMyOTk0ZGQ0M2IxODQ2Nzg1YjAzZTExZTotCAIQ2A4YmgYiI3YyLWNhOWQwYmJhMTk5NWQwMTk0M2E4NzM4ZWVmZTQ0OTljgAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEAmFpwgQDNDAwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAAAg8hCZP4EFAAAAAAAAAACJBbsF9gwaW9M/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQuQBgCgBj+oBgCSAi4KCTczNzk2MDY2MBITMTkzMDMwNTc4MTIyMTI5ODgyNxgEIgpJTUFHRV9URVhU","action_card":false},{"id":"64_1753853942.891","type":"feed","offset":64,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853942,"updated_time":1753853942,"target":{"id":"1933762733121214120","type":"answer","url":"https://api.zhihu.com/answers/1933762733121214120","author":{"id":"72e3eaa38bb1fe1f4073065a03e31e95","url":"https://api.zhihu.com/people/72e3eaa38bb1fe1f4073065a03e31e95","user_type":"people","url_token":"yizhizhaojun","name":"朝菌","headline":"公众号/B站：朝菌学园ZhaoDesign","avatar_url":"https://picx.zhimg.com/50/v2-9dbbe2daf211eeed0bf0b4e64bc03c92_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"清华大学 计算机系硕士"}],"followers_count":5779,"is_following":false,"is_followed":false},"created_time":1753824963,"updated_time":1753824963,"voteup_count":0,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"5426742081","type":"question","url":"https://api.zhihu.com/questions/5426742081","author":{"id":"b98ba61ba1ca39bb94a2f5ce79d93c31","url":"https://api.zhihu.com/people/b98ba61ba1ca39bb94a2f5ce79d93c31","user_type":"people","url_token":"grapefruitsoda","name":"知乎用户VxCoar","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":1,"is_following":false,"is_followed":false},"title":"30岁想转行交互设计，求建议？","created":1732828363,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[1990],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"转行路径： 1、跟住某个业内高手学习，6-12个月积累知识和作品项目。 2、放低姿态，参加实习或中小企业实践，可低薪，1年左右快速积累实战经验，正式进入交互设计/产品经理职场。 3、跳槽，3年时间大概有1-2次跳槽，成为资深职员。 接下来就是瓶颈期，企业规模、业务类型、市场环境、人才供给等影响因素就很多了，是后面要处理的事情。 可以联系【朝菌学园ZhaoDesign】，能帮你找到非常资深优秀的业内资深师傅，带你入行。","excerpt_new":"转行路径： 1、跟住某个业内高手学习，6-12个月积累知识和作品项目。 2、放低姿态，参加实习或中小企业实践，可低薪，1年左右快速积累实战经验，正式进入交互设计/产品经理职场。 3、跳槽，3年时间大概有1-2次跳槽，成为资深职员。 接下来就是瓶颈期，企业规模、业务类型、市场环境、人才供给等影响因素就很多了，是后面要处理的事情。 可以联系【朝菌学园ZhaoDesign】，能帮你找到非常资深优秀的业内资深师傅，带你入行。","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"fLushLkH\"\u003e转行路径：\u003c/p\u003e\u003cp data-pid=\"y_6C1gFK\"\u003e1、跟住某个业内高手学习，6-12个月积累知识和作品项目。\u003c/p\u003e\u003cp data-pid=\"8xU4zNTq\"\u003e2、放低姿态，参加实习或中小企业实践，可低薪，1年左右快速积累实战经验，正式进入交互设计/产品经理职场。\u003c/p\u003e\u003cp data-pid=\"nRTRSzob\"\u003e3、跳槽，3年时间大概有1-2次跳槽，成为资深职员。\u003c/p\u003e\u003cp data-pid=\"PcBMqS_J\"\u003e接下来就是瓶颈期，企业规模、业务类型、市场环境、人才供给等影响因素就很多了，是后面要处理的事情。\u003c/p\u003e\u003cp data-pid=\"UExUmzQa\"\u003e可以联系【朝菌学园ZhaoDesign】，能帮你找到非常资深优秀的业内资深师傅，带你入行。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":true,"visited_count":22,"favorite_count":1,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1933762733121214120}","attached_info":"Cv0FCPDjne3L8syghgEQBBoJNzM5NTMxMDEyIMP9pMQGKAAwAEBASiQKGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDISATAYACAAOgBKKAodVFNfU09VUkNFX1dBUk1VUF9QUkVUUkFJTl9JMkkSATAYACAAOgBaCTExMjA2MTkwOWIgYjM3NDZhZjk2ZjQ0ZmM1MTk4MmMwNTllY2JhMTlhOWVyEzE5MzM3NjI3MzMxMjEyMTQxMjCKAQo1NDI2NzQyMDgxqgEJcmVjb21tZW5kwgEgNzJlM2VhYTM4YmIxZmUxZjQwNzMwNjVhMDNlMzFlOTXyAQoIDBIGTm9ybWFs8gEoCAoSJDM5MTZlYTE4LTU0ZDYtNGQ3MC05MTI3LWE4MTFiNWM5MDM2MvIBBggLEgIxMYICAIgC4LXdzoUzkgIgNzJlM2VhYTM4YmIxZmUxZjQwNzMwNjVhMDNlMzFlOTWaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIaQ29udGVudFdhcm1VcElzb2xhdGlvblJ1bGXKAhhDb250ZW50V2FybVVwQnJlYWtJblJ1bGXaAhlUU19TT1VSQ0VfV0FSTV9VUF9OT1JNQUwy6AIC+gILTk9STUFMX0ZMT1eKAyAwNTJlZjI0NjNlYmE0NjI1OGVkMTc1MDI1M2U4NWIyMJoDDQoCdjIQABoFb3RoZXKoAxbYAwDqAxNwcmV0cmFpbl9pMmlfcmVjYWxs+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATKgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAADWblj+BBQAAAAAAAAAAiQW7BfYMGlvTP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AULkAYAoAZAqAYBkgIuCgk3Mzk1MzEwMTISEzE5MzM3NjI3MzMxMjEyMTQxMjAYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"65_1753853942.780","type":"feed","offset":65,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853942,"updated_time":1753853942,"target":{"id":"1931673285894058510","type":"article","url":"https://api.zhihu.com/articles/1931673285894058510","author":{"id":"fc3950a47a431d001b8192da2a545484","url":"https://api.zhihu.com/people/fc3950a47a431d001b8192da2a545484","user_type":"people","url_token":"dopamine-13","name":"Dopamine","headline":"","avatar_url":"https://pica.zhimg.com/50/v2-8ab5daa69e0c61cbc504bd9a669ee362_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":23,"is_following":false,"is_followed":false},"title":"2025推荐搜索系统极长/终身行为序列建模","comment_permission":"all","created":1753327999,"updated":1753328978,"voteup_count":5,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"主要论文list： https://arxiv.org/pdf/2506.00450 DV365: Extremely Long User History Modeling at Instagram. Meta压缩方法，并行于HSTU作为长期兴趣 https://arxiv.org/pdf/2502.08309v1 LUM：Unlocking Scaling Law in Industrial Recommendation Systems with a Three-step Paradigm based Large User Model 阿里，预训练压缩序列到embed https://arxiv.org/pdf/2505.04421 LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders抖音商业化 …","excerpt_new":"主要论文list： https://arxiv.org/pdf/2506.00450 DV365: Extremely Long User History Modeling at Instagram. Meta压缩方法，并行于HSTU作为长期兴趣 https://arxiv.org/pdf/2502.08309v1 LUM：Unlocking Scaling Law in Industrial Recommendation Systems with a Three-step Paradigm based Large User Model 阿里，预训练压缩序列到embed https://arxiv.org/pdf/2505.04421 LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders抖音商业化 …","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"d7NtBw-B\"\u003e主要论文list：\u003c/p\u003e\u003cp data-pid=\"oJFVTIUS\"\u003e\u003cu\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2506.00450\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2506.0045\u003c/span\u003e\u003cspan class=\"invisible\"\u003e0\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/u\u003e DV365: Extremely Long User History Modeling at Instagram. Meta压缩方法，并行于HSTU作为长期兴趣\u003c/p\u003e\u003cp data-pid=\"sSF9yVH7\"\u003e\u003cu\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2502.08309v1\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2502.0830\u003c/span\u003e\u003cspan class=\"invisible\"\u003e9v1\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/u\u003e LUM：Unlocking Scaling Law in Industrial Recommendation Systems with a Three-step Paradigm based Large User Model 阿里，预训练压缩序列到embed\u003c/p\u003e\u003cp data-pid=\"y4iPbdLu\"\u003e\u003cu\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2505.04421\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2505.0442\u003c/span\u003e\u003cspan class=\"invisible\"\u003e1\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/u\u003e  LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders抖音商业化\u003c/p\u003e\u003cp data-pid=\"GjYnLsdQ\"\u003e\u003cu\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2411.09425\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/abs/2411.0942\u003c/span\u003e\u003cspan class=\"invisible\"\u003e5\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/u\u003e MARM: Unlocking the Future of Recommendation Systems through Memory Augmentation and Scalable Complexity\u003c/p\u003e\u003cp data-pid=\"i0DZAHgH\"\u003e\u003cu\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2507.12704\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2507.1270\u003c/span\u003e\u003cspan class=\"invisible\"\u003e4\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/u\u003e PinFM: Foundation Model for User Activity Sequences at a Billion-scale Visual Discovery Platform. Pinterest论文，生成式训练压缩序列的embed\u003c/p\u003e\u003cp data-pid=\"Zip2X93w\"\u003e\u003cu\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2506.02267\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2506.0226\u003c/span\u003e\u003cspan class=\"invisible\"\u003e7\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/u\u003e  TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation。GSU范式 + NTP loss\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e\u003cb\u003e1. DV365：Instagram 的极长用户历史建模方法\u003c/b\u003e\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-3b89e33eba49477d375dbd2fc1d15fe7_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2030\" data-rawheight=\"940\" data-original-token=\"v2-fcb69fac410d43e2db6acdf4fab63404\" class=\"origin_image zh-lightbox-thumb\" width=\"2030\" data-original=\"https://pic4.zhimg.com/v2-3b89e33eba49477d375dbd2fc1d15fe7_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e\u003cb\u003e1.1 核心思想：离线嵌入与多切片总结 (MSS)\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"EBV8WkFo\"\u003e\u003cb\u003eDV365\u003c/b\u003e 是 Meta Platforms, Inc. (原 Facebook) 为其旗下社交媒体 Instagram 开发的一种用户嵌入方法，旨在高效且经济地对极长的用户历史行为序列进行建模 。该方法的核心思想在于采用\u003cb\u003e离线嵌入（offline embedding）\u003c/b\u003e策略，而非传统的端到端（end-to-end）序列长度优化方法，以应对处理超长序列时带来的巨大计算和存储开销 。Instagram 团队认为，直接扩展现有模型的序列长度在成本和投资回报率（ROI）方面已达到极限，特别是对于像 HSTU (History Transformer模型) 这样先进的注意力序列编码器，其训练和服务 GPU 成本已成为瓶颈 。因此，DV365 选择将用户历史行为序列的编码过程离线化，预先计算用户嵌入，然后将这些嵌入作为特征输入到下游的推荐模型中。这种策略不仅降低了对在线推理系统的压力，还使得模型能够处理远超以往长度的用户历史。DV365 能够处理的用户历史长度最高可达 \u003cb\u003e70,000条\u003c/b\u003e，平均长度也达到了 \u003cb\u003e40,000条\u003c/b\u003e，这远超 Instagram 现有推荐模型中通常处理的 2,000 条（受限于特征存储、提取和处理成本）或 HSTU 模型处理的 500 条（受限于 GPU 成本）。\u003c/p\u003e\u003cp data-pid=\"4AOJzPEX\"\u003e为了实现这一目标，DV365 提出了一种名为“\u003cb\u003e多切片与总结”（Multi-slicing and Summarization, MSS）\u003c/b\u003e的用户嵌入学习策略 。该策略旨在从极长的用户行为序列中提取用户长期且稳定的兴趣表征。其基本假设是，用户兴趣可以分为新兴兴趣和稳定兴趣，而超长用户历史的主要价值在于捕捉用户的稳定兴趣，这种兴趣不会随时间快速变化，因此不需要像在线训练的生产模型那样高频率地更新 。通过离线处理和 MSS 策略，DV365 能够生成高度泛化的用户表示，这些表示可以跨 Instagram 和 Threads 的多个不同产品表面（如 Reels、Feed、Explore、Stories）和推荐阶段（包括检索、早期排序 ESR、晚期排序 LSR）共享，从而显著提高 ROI 。这种一次计算、多处共享的特性，以及避免了在每次推荐请求中实时检索和处理极长序列所带来的高昂成本，是 DV365 选择离线嵌入方法的关键原因 。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e1.2 技术细节：序列切片、池化与漏斗式总结网络\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"GUcrzPxA\"\u003eDV365 的“多切片与总结”（MSS）策略包含几个关键的技术步骤，用于将极长的用户历史序列（最长 70,000 条，平均 40,000 条）编码为一个紧凑的用户嵌入 。首先，面对如此长的序列，直接应用注意力机制进行建模在计算上是不现实的。因此，DV365 采用了“\u003cb\u003e多切片”（Multi-slicing）\u003c/b\u003e的方法。具体来说，它将整个长序列分割成多个较短的子序列。这些子序列的划分方式可以根据具体业务场景和用户行为特性进行设计，例如按时间窗口、按行为类型或按内容主题等进行切片。通过这种方式，可以将一个难以处理的超长序列转化为一组相对较短的、更易于处理的子序列集合。\u003c/p\u003e\u003cp data-pid=\"P2qAbrIO\"\u003e接下来，对每个子序列应用\u003cb\u003e池化（Pooling）\u003c/b\u003e操作。池化是一种常见的降维和特征提取方法，可以从每个子序列中提取关键信息并生成一个固定长度的向量表示。常用的池化方法包括平均池化（Average Pooling）、最大池化（Max Pooling）或更复杂的基于注意力的池化。在 DV365 中，经过多切片和池化操作后，会生成大约 \u003cb\u003e200 个池化后的嵌入向量\u003c/b\u003e 。这些池化嵌入向量可以看作是原始长序列在不同片段或不同维度上的摘要信息。\u003c/p\u003e\u003cp data-pid=\"MasenTnX\"\u003e最后，DV365 设计了一个“\u003cb\u003e漏斗式总结网络”（Funnel Summarization Architecture）\u003c/b\u003e作为用户编码器（user encoder）。这个网络的目标是将前一步生成的 200 个池化嵌入向量进一步压缩和融合，最终生成一个高度凝练的、低维度的用户嵌入。这个用户嵌入旨在捕捉用户长期且稳定的兴趣。为了确保生成的用户嵌入能够与下游生产模型更好地协同工作，DV365 将用户编码器模块嵌入到一个模拟 Instagram 生产 LSR（Late Stage Ranking）模型的骨干网络中进行训练 。这种模拟训练的方式有助于用户编码器学习生成与生产模型期望更为一致的嵌入表示，从而实现知识从上游基础模型到下游应用模型的有效迁移。整个流程体现了从细粒度子序列信息到整体用户表征的逐步抽象和总结过程。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e1.3 应用与效果：Instagram 的部署与 A/B 测试提升\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"fjciJamk\"\u003eDV365 用户嵌入方法自首次发布以来，已在 Instagram 的生产环境中稳定运行超过一年，并被证明是一种高效且具有显著增益的解决方案 。该嵌入由一个单一的上游基础模型生成，并已成功部署到 Instagram 和 Threads 两个平台的 \u003cb\u003e15 个不同模型\u003c/b\u003e中，覆盖了包括 Reels、Feed、Explore、Stories 在内的多种产品界面，以及推荐系统的不同阶段，如检索（Retrieval）、早期排序（Early Stage Ranking, ESR）和晚期排序（Late Stage Ranking, LSR）。这种广泛的部署范围充分证明了 DV365 嵌入的通用性和可迁移性，能够有效地将用户长期稳定兴趣的知识迁移到各种不同的推荐任务和场景中。\u003c/p\u003e\u003cp data-pid=\"4EvJatBI\"\u003e在生产环境的 A/B 测试中，DV365 嵌入被证明在 Instagram 已经部署的先进注意力用户序列模型（如 HSTU）的基础上，仍然能够带来\u003cb\u003e显著的增量提升\u003c/b\u003e 。这意味着即使下游模型已经具备了较强的用户序列建模能力，DV365 所代表的极长用户历史信息依然能够提供额外的价值，帮助模型更全面地理解用户兴趣，从而做出更精准的推荐。论文中提到，DV365 嵌入的设计目标是捕捉用户的长期稳定兴趣，这使得它对嵌入更新的延迟和陈旧性具有固有的弹性 。通过实验验证，即使使用固定的、非更新的“陈旧嵌入”（stale embeddings），其带来的归一化熵（Normalized Entropy, NE）增益与每日更新的“新鲜嵌入”（fresh embeddings）相比，依然能够保持约 \u003cb\u003e0.35% 的稳定提升\u003c/b\u003e，并且这种提升在训练数据量增加后趋于稳定 。此外，对不同版本 DV365 嵌入之间的余弦相似度分析显示，即使两个版本相隔七天，其相似度仍然超过 90%，进一步证实了该嵌入方法在长时间跨度上的稳定性 。这些结果共同表明，DV365 不仅能够有效地从极长用户历史中提取有价值的信号，而且其离线计算和更新的特性也使其成为一个成本效益高且鲁棒的解决方案。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e2. LUM：阿里巴巴基于三步范式的大型用户模型\u003c/b\u003e\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-2c83e9afc1b1163d112dd0e3499c57e6_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2400\" data-rawheight=\"938\" data-original-token=\"v2-1c653cd504e2ba66d3a4082be9130eca\" class=\"origin_image zh-lightbox-thumb\" width=\"2400\" data-original=\"https://pic3.zhimg.com/v2-2c83e9afc1b1163d112dd0e3499c57e6_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e\u003cb\u003e2.1 核心思想：三步范式与大型用户模型 (LUM)\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"3xbstb0V\"\u003e阿里巴巴提出的 \u003cb\u003eLUM（Large User Model）\u003c/b\u003e旨在通过一个创新的\u003cb\u003e三步范式\u003c/b\u003e，解决工业级推荐系统中传统深度学习方法（DLRM）难以有效扩展以及端到端生成式推荐（E2E-GR）方法在实际应用中面临的挑战 。LUM 的核心思想是借鉴大型语言模型（LLM）中的\u003cb\u003e扩展定律（scaling law）\u003c/b\u003e，将其应用于用户建模，从而在工业推荐系统中实现性能随模型规模增大而提升的目标。传统的 DLRM 在计算资源增加时性能提升有限，而 E2E-GR 方法则存在生成式训练与判别式应用之间的不一致性、实时环境下的效率问题、对业务需求变化的灵活性不足以及与现有工业知识兼容性有限等问题 。LUM 通过将用户行为序列（UBS）的建模分解为\u003cb\u003e知识构建、知识查询和知识利用\u003c/b\u003e三个步骤，有效地将生成式预训练的优势与判别式任务的需求相结合，从而克服了这些局限性。这种方法不仅提升了模型性能，还保证了在工业环境中的实用性和可部署性，成功解锁了推荐系统中的扩展定律 。\u003c/p\u003e\u003cp data-pid=\"ftqB14f7\"\u003eLUM 的三步范式设计巧妙地平衡了模型能力与工业实践需求。第一步，\u003cb\u003e知识构建\u003c/b\u003e，通过生成式学习预训练一个基于 Transformer 的大型用户模型，旨在捕捉用户兴趣和物品关系。第二步，\u003cb\u003e知识查询\u003c/b\u003e，利用预定义的“提示”（prompts）或条件从预训练的 LUM 中提取特定用户的相关信息，这类似于一种“提示工程”。第三步，\u003cb\u003e知识利用\u003c/b\u003e，将前两步获得的输出作为补充特征整合到传统的推荐模型中，从而增强下游任务的性能 。这种解耦的设计使得 LUM 能够独立进行大规模预训练，而下游的 DLRM 则可以专注于实时学习和快速迭代，同时通过引入 LUM 提取的知识来提升自身表现。LUM 还引入了一种新颖的标记化策略，将用户行为序列中的每个物品分解为\u003cb\u003e条件标记\u003c/b\u003e和\u003cb\u003e物品标记\u003c/b\u003e，并将自回归学习过程从“下一物品预测”重新定义为“\u003cb\u003e下一条件-物品预测\u003c/b\u003e”，这有助于更好地将预训练阶段学到的知识迁移到下游的判别式任务中 。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e2.2 技术细节：知识构建、知识查询与知识利用\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"gI40qbW0\"\u003eLUM 的三步范式具体技术细节如下：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"njs74U5A\"\u003e\u003cb\u003e知识构建 (Knowledge Construction)\u003c/b\u003e：此阶段的核心是预训练一个基于 Transformer 架构的\u003cb\u003e大型用户模型 (LUM)\u003c/b\u003e。与传统的“下一物品预测”不同，LUM 引入了一种创新的“\u003cb\u003e下一条件-物品预测\u003c/b\u003e”任务。具体而言，它将用户行为序列 (UBS) 中的每个物品分解为两个独立的标记：一个\u003cb\u003e条件标记 (condition token)\u003c/b\u003e 和一个\u003cb\u003e物品标记 (item token)\u003c/b\u003e。条件标记可以表示场景、上下文等信息。因此，一个用户行为序列被表示为交替的条件标记和物品标记，例如 `[条件1, 物品1, 条件2, 物品2, ...]`。这种标记化策略丰富了用户交互的上下文信息，使得模型能够学习到更细粒度的用户兴趣和物品关系。通过这种方式，LUM 能够捕捉用户兴趣和物品关系的联合概率分布 P(物品, 条件 | 用户历史)。为了提高训练效率，LUM 还采用了诸如\u003cb\u003e打包 (packing)\u003c/b\u003e（将多个用户行为序列组合成一个训练样本）和\u003cb\u003e组查询 (group querying)\u003c/b\u003e（通过掩码机制同时处理多个查询）等技术 。这种生成式学习使得 LUM 能够探索模型规模的扩展规律，这对于提升模型性能至关重要 。\u003c/li\u003e\u003cli data-pid=\"R2LNw0K0\"\u003e\u003cb\u003e知识查询 (Knowledge Querying)\u003c/b\u003e：在 LUM 模型预训练完成后，这一阶段的目标是从模型中提取与特定用户相关的知识。通过向预训练的 LUM 输入不同的条件（例如，特定场景、搜索查询或物品类别），模型能够推断出用户在这些条件下的兴趣偏好。这个过程可以看作是一种“\u003cb\u003e提示工程 (prompt engineering)\u003c/b\u003e”，通过精心设计的查询来引导模型生成相关的用户表征或兴趣分布。为了实现更有效的响应生成，LUM 可能采用了自注意力机制来管理上下文信息，确保查询的准确性和相关性 。知识查询的输出是用户兴趣的稠密表示或与特定条件相关的物品列表，这些输出将作为下游任务的输入。\u003c/li\u003e\u003cli data-pid=\"XsGbF6kH\"\u003e\u003cb\u003e知识利用 (Knowledge Utilization)\u003c/b\u003e：最后一步是将从 LUM 查询到的知识有效地整合到下游的推荐任务中。这些知识通常以特征嵌入的形式出现，被用作传统深度学习推荐模型 (DLRM) 的补充特征。例如，可以将 LUM 生成的用户兴趣向量与用户画像特征、物品特征等拼接，然后输入到 DLRM 中进行最终的排序或召回。另一种方式是利用 LUM 提取的用户兴趣来匹配物品嵌入，从而增强基线模型在检索和排序任务中的性能 。这种设计确保了 DLRM 仍然能够满足实时学习、灵活性和兼容性的要求，同时通过引入 LUM 提供的深层用户理解来提升其性能。这种解耦的设计也便于实施缓存策略，例如缓存 LUM 的输出，从而减轻效率限制 。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"0ul_1t8E\"\u003eLUM 通过这种三步范式，不仅有效地将生成式预训练的强大表征能力与判别式推荐任务的实际需求相结合，还通过创新的标记化和预测任务设计，确保了预训练知识能够有效地迁移到下游应用，从而在工业推荐系统中实现了显著的性能提升和可扩展性。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e2.3 应用与效果：解锁扩展规律与工业部署\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"22lp778V\"\u003eLUM 模型在阿里巴巴的工业推荐系统中得到了成功部署，并通过 A/B 测试验证了其有效性。实验结果表明，LUM 的性能优于当前最先进的 DLRM 和 E2E-GR 方法 。更重要的是，LUM 展现出了与大型语言模型相似的\u003cb\u003e扩展定律 (scaling law)\u003c/b\u003e，即随着模型参数规模的增大（最高可达 \u003cb\u003e70 亿参数\u003c/b\u003e），其性能持续提升 。这一发现对于推荐系统领域具有重要意义，因为它证明了通过增大模型规模来提升推荐效果是可行的，为未来推荐模型的发展方向提供了新的思路。在阿里巴巴的实际工业应用中，部署 LUM 带来了显著的指标提升，进一步验证了其在实际业务场景中的有效性和实用性 。LUM 的成功不仅在于其优越的性能，还在于其设计充分考虑了工业系统的需求，例如通过解耦的三步范式实现了训练与服务的分离，便于缓存和实时学习，从而在保证效果的同时，也兼顾了系统的效率和灵活性 。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e3. LONGER：抖音商业化超长序列建模\u003c/b\u003e\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-7e1b25341b0c4260af627ed80a19ee8b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1654\" data-rawheight=\"1028\" data-original-token=\"v2-8788dde1df12ec5c60ad47e7ece8d965\" class=\"origin_image zh-lightbox-thumb\" width=\"1654\" data-original=\"https://picx.zhimg.com/v2-7e1b25341b0c4260af627ed80a19ee8b_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e\u003cb\u003e3.1 核心思想：优化的 Transformer 架构\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"jdNPUE_X\"\u003e\u003cb\u003eLONGER\u003c/b\u003e 是抖音商业化团队提出的一种用于工业级推荐系统中超长用户行为序列建模的 Transformer 框架。其核心思想在于通过一系列创新的模型结构设计和系统级优化，实现对超长序列（长度远超传统方法的 10^2 - 10^3 范围）的高效且有效的\u003cb\u003e端到端建模\u003c/b\u003e 。与传统的两阶段检索方法（如 SIM, TWIN）或预训练用户嵌入方法不同，LONGER 旨在直接处理完整的超长用户历史，以更全面地捕捉用户的长期和短期兴趣，从而提升推荐的准确性和多样性，并缓解信息茧房现象 。LONGER 框架集成了输入生成、Token 合并、混合注意力机制以及训练和服务优化等多个关键技术组件，以应对超长序列带来的计算复杂度和内存消耗挑战，同时保证模型在工业规模下的可部署性和效率 。该框架的设计目标是能够在实际的工业约束下，对用户长达数年、包含数万甚至更多交互行为的序列进行建模，从而充分挖掘用户历史中蕴含的丰富信息。\u003c/p\u003e\u003cp data-pid=\"bvAxuDI_\"\u003eLONGER 的架构设计充分考虑了超长序列建模的特殊性。它通过引入\u003cb\u003e全局标记 (Global Tokens)\u003c/b\u003e 来聚合锚点表示（如目标物品表示、用户 ID 嵌入），以促进全局信息融合并稳定注意力分布，缓解“注意力下沉”问题 。为了降低计算复杂度，LONGER 采用了 \u003cb\u003eToken Merge\u003c/b\u003e 技术来压缩长行为序列，同时通过 \u003cb\u003eInnerTrans\u003c/b\u003e（一种轻量级内部 Transformer）保留合并 Token 段内的依赖关系 。模型的核心是一个\u003cb\u003e混合注意力 (Hybrid Attention)\u003c/b\u003e 设计，结合了交叉因果注意力（用于突出序列的关键部分）和堆叠的自因果注意力层（用于捕捉序列间的高阶依赖关系）。此外，LONGER 还包含了一系列工程系统级优化，如完全同步的训练和服务、统一的稠密和稀疏参数存储、混合精度训练和重计算以优化内存和计算效率，以及在推理时采用 KV 缓存服务策略来减少冗余计算 。这些设计共同构成了一个能够高效处理超长序列并保持高表达能力的系统。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e3.2 技术细节：Token Merge、混合注意力机制与工程优化\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"BO6J1hkG\"\u003eLONGER 框架的技术细节主要体现在其对 Transformer 架构的改进和系统层面的优化，具体包括：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"A5ZG5yBh\"\u003e\u003cb\u003e输入生成与全局标记 (Input Generation and Global Tokens)\u003c/b\u003e： LONGER 的输入由两部分组成：全局标记和序列标记。全局标记代表上下文信息，如目标物品特征和用户标识符 (UID) 嵌入，它们被拼接在序列标记之前，作为聚合的锚点表示，有助于全局信息融合和稳定注意力分布 。为了捕捉用户行为序列中的时间动态，序列标记通过两种位置编码进行增强：(1) 绝对时间差特征，量化每个用户交互与目标物品之间的时间距离，作为边信息与每个物品嵌入拼接；(2) 可学习的绝对位置嵌入，编码每个标记在序列中的位置，并添加到物品嵌入中。处理后的标记通过一个多层感知机 (MLP) 生成输入表示 `R = [G; H]`，其中 `G` 是全局标记表示，`H` 是序列标记表示 。\u003c/li\u003e\u003cli data-pid=\"66_6hXF_\"\u003e\u003cb\u003eToken Merge 与 InnerTrans\u003c/b\u003e： 为了应对长序列带来的二次方计算复杂度 O(L^2 * d)，LONGER 采用了 \u003cb\u003eToken Merge\u003c/b\u003e 策略。该方法将相邻的 `r` 个 Token 分组并压缩，从而将序列长度减少 `r` 倍 。例如，给定一个包含 `L` 个 Token 的序列，合并后长度变为 `L/r`。为了在合并后保留组内的依赖关系，LONGER 在每个合并后的 Token 段内应用一个轻量级的 \u003cb\u003eInnerTrans\u003c/b\u003e（内部 Transformer）模块。InnerTrans 负责学习组内 Token 的交互，其计算复杂度相对较低。通过这种方式，Token Merge 显著降低了注意力计算的开销，而 InnerTrans 则弥补了因合并可能造成的信息损失。注意力计算复杂度在 Token Merge 前后的比例约为 `(L/r)^2 / L^2 = 1/r^2`，显示了其有效性 。\u003c/li\u003e\u003cli data-pid=\"31RVD1Xb\"\u003e\u003cb\u003e混合注意力机制 (Hybrid Attention Mechanism)\u003c/b\u003e： LONGER 采用了一种混合注意力设计，结合了交叉因果注意力和堆叠的自因果注意力层 。\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli data-pid=\"eyXq6H9w\"\u003e\u003cb\u003e第一层：交叉因果注意力 (Cross Causal Attention)\u003c/b\u003e： 查询矩阵 `O` 由 `m` 个全局标记 `G` 和从完整序列标记 `H` 中根据预定义采样策略（例如，最近 `k` 个或均匀采样 `k` 个 Token，实验表明最近 `k` 个效果最好）选择的 `k` 个采样序列标记 `H_S` 拼接而成，即 `O = [G; H_S]`。键 (Key) 和值 (Value) 矩阵则来自完整的序列表示 `R = [G; H]`。交叉注意力机制允许全局标记和采样的关键序列标记与整个序列进行交互，从而突出序列中与当前上下文最相关的部分，并初步压缩长序列信息。因果掩码 (Causal Mask) 确保当前位置只能关注到历史信息，保持时间相关性。\u003c/li\u003e\u003cli data-pid=\"uW1Q6bwv\"\u003e\u003cb\u003e后续层：堆叠的自因果注意力 (Stacked Self Causal Attention)\u003c/b\u003e： 在交叉注意力层之后，是 `N` 个堆叠的自因果注意力块。这些层专注于学习采样后的 Token 序列内部的依赖关系和模式。每个自注意力层的查询 (Q)、键 (K)、值 (V) 都来自前一层的输出。同样采用因果掩码。这些层通过迭代 refine 输入序列的表示，捕捉高阶交互。最终输出一个压缩的表示，用于下游预测任务 。公式表示为：`CrossAttn(O, R) -\u0026gt; SelfAttn(·) x N`，前者压缩长序列，后者学习高阶交互 。\u003c/li\u003e\u003cli data-pid=\"EtmhBSaJ\"\u003e\u003cb\u003e工程系统级优化 (Engineering System-Level Optimizations)\u003c/b\u003e： 为了确保 LONGER 在工业规模下的可行性和效率，框架包含以下关键优化：\u003c/li\u003e\u003cli data-pid=\"ZLBhfVIg\"\u003e\u003cb\u003eGPU 同步框架\u003c/b\u003e：支持在超大规模 GPU 集群上进行完全同步的训练和服务，并统一管理稠密和稀疏参数。\u003c/li\u003e\u003cli data-pid=\"PI7VSqlh\"\u003e\u003cb\u003e混合精度训练与重计算 (Mixed Precision Training and Recompute)\u003c/b\u003e：通过使用较低精度的数值格式（如 FP16）进行训练，减少内存占用和加速计算。激活重计算则用于在反向传播时重新计算某些层的激活值，以节省显存。\u003c/li\u003e\u003cli data-pid=\"wIPLtWuW\"\u003e\u003cb\u003eKV 缓存服务 (KV Cache Serving)\u003c/b\u003e：在推理阶段，缓存用户序列的 Key 和 Value 表示，并在对多个候选物品进行评分时重用这些缓存，显著减少冗余计算，降低推理延迟 。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"cJTNqxO4\"\u003e这些技术细节共同构成了 LONGER 框架的核心，使其能够有效地对工业级推荐系统中的超长用户行为序列进行建模，并在保持高表达能力的同时，满足严格的效率和可部署性要求。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e3.3 应用与效果：快手广告与电商服务的性能提升\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"7Xx8JZ6a\"\u003eLONGER 框架在抖音的商业化场景中，特别是在广告和电商领域，得到了广泛的应用和验证。通过在工业级的十亿规模数据集上进行大量实验以及在线 A/B 测试，LONGER 证明了其在处理超长用户行为序列方面的鲁棒性和泛化能力 。实验结果表明，LONGER 能够在保持具有竞争力的推荐准确性的同时，显著降低计算开销。这对于需要在延迟敏感的生成环境中部署的推荐系统至关重要。具体来说，LONGER 通过其高效的序列建模技术和系统级优化，实现了对用户长达数年历史的端到端建模，从而更全面地理解用户兴趣，提升了推荐的精准度和多样性。在快手（Kwai）的海外业务中，类似的长期序列建模方法（如 MARM，虽然具体细节不同，但目标相似）也取得了显著成效，例如在平均用户观看时长等核心指标上带来了超过 \u003cb\u003e2% 的提升\u003c/b\u003e 。LONGER 的成功部署，标志着在工业级推荐系统中应用超长序列建模技术的一个重要进展，为解决信息茧房、提升用户体验和商业价值提供了有力的技术支撑。未来的工作方向包括研究更高效的序列建模技术以及改进工业场景中的跨领域行为建模 。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e4. MARM：快手的内存增强检索模型\u003c/b\u003e\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-d7d6e70853f1757c3d3967cbb0a70263_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2142\" data-rawheight=\"1176\" data-original-token=\"v2-2db9a2d248b7f0f0dd753fde9ab1cdf0\" class=\"origin_image zh-lightbox-thumb\" width=\"2142\" data-original=\"https://pic4.zhimg.com/v2-d7d6e70853f1757c3d3967cbb0a70263_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e\u003cb\u003e4.1 核心思想：缓存增强与计算加速\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"rtcVf8Fw\"\u003e\u003cb\u003eMARM (Memory Augmented Recommendation Model)\u003c/b\u003e 是快手提出的一种用于推荐系统的内存增强模型，其核心思想是通过引入\u003cb\u003e缓存（Cache）\u003c/b\u003e技术来管理推荐模型的计算复杂性，从而实现计算加速和性能提升 。与自然语言处理（NLP）领域的扩展定律（scaling law）不同，推荐系统的瓶颈通常不在于模型参数量或训练数据量（工业推荐系统可以轻松生成每日数百亿的用户样本，模型参数量也能远超百亿），而在于\u003cb\u003e计算复杂度 (FLOPs)\u003c/b\u003e。为了保证推荐系统的稳定性和鲁棒性，尤其是在需要每日处理海量推荐样本的训练阶段和毫秒级响应的在线推理阶段，必须仔细控制计算复杂度 。MARM 通过缓存复杂模块（如多层自注意力）的计算结果，将原本计算复杂度为 O(n^2 * d) 的模块（n 为序列长度，d 为嵌入维度）降低到 O(n * d)，从而显著克服了计算瓶颈，使得对用户终身历史行为序列进行建模成为可能 。这种“空间换时间”的策略，使得模型能够以较小的推理复杂度代价，将单层注意力机制扩展到多层，从而更深入地理解用户兴趣。\u003c/p\u003e\u003cp data-pid=\"HKMkR4Um\"\u003eMARM 的设计旨在无缝赋能所有用户序列兴趣提取模块，甚至其他类型的模型。它探索了一种新的\u003cb\u003e缓存与性能之间的扩展定律\u003c/b\u003e，即通过增加缓存空间来提升推荐模型的性能 。该方法的一个关键创新在于，它不仅仅是在推理阶段使用缓存（如 NLP 中的 KV 缓存），而是在\u003cb\u003e训练和推理阶段都利用缓存\u003c/b\u003e。MARM 在训练过程中生成缓存结果，目的是保存用户与物品之间的正向行为模式信息；而在推理阶段，则共享并使用这些训练产生的缓存结果来进行更高效的预测 。此外，MAR M 的缓存具有\u003cb\u003e长期生命周期\u003c/b\u003e，因为用户行为是持续产生的，不像句子有明确的终点，所以需要长期存储用户兴趣以保证推荐质量。MARM 缓存的不是传统的 Key 和 Value 结果，而是最终计算得到的 Query 输出，这有助于减少总体存储需求 。为了支持 MARM，快手构建了一个高达 \u003cb\u003e60TB 的缓存存储中心\u003c/b\u003e，用于离线和在线服务 。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e4.2 技术细节：注意力计算结果的缓存与检索优化\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"ByiYHoxS\"\u003eMARM 的技术细节主要围绕其独特的缓存机制和对推荐模型计算流程的优化展开。其核心在于如何有效地生成、存储和利用缓存来加速复杂的注意力计算，特别是针对用户行为序列建模模块。\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"w-pZJBTO\"\u003e\u003cb\u003e缓存目标与内容\u003c/b\u003e： MARM 主要针对推荐模型中用于提取用户历史物品重要性的模块进行优化。传统上，这类模块（如 DIN, SIM, SDIM, TWIN）多采用单层目标注意力（target-attention）机制，其时间复杂度为 O(n\u003ci\u003ed) 。直觉上，通过在最终的目标注意力层之前堆叠多层自注意力（self-attention）机制（时间复杂度为 O(n^2\u003c/i\u003ed)），可以进一步增强模型的表达能力，捕捉更复杂的序列内依赖关系。然而，对于长序列（例如 n \u0026gt; 1000），多层自注意力的计算开销巨大。MAR M 的核心思想是\u003cb\u003e缓存这些多层自注意力模块的计算结果\u003c/b\u003e。具体来说，它缓存的是经过多层 masked self-attention 计算后得到的序列表示，而不是 NLP 中常见的 Key (K) 和 Value (V) 的投影结果。这样做的好处是，在后续的目标注意力计算中，可以直接利用这些缓存的、已经过复杂变换的序列表示，从而将多层自注意力的计算简化为类似单层目标注意力的计算 。\u003c/li\u003e\u003cli data-pid=\"5_LsNrVu\"\u003e\u003cb\u003e缓存生成与使用\u003c/b\u003e：\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli data-pid=\"IhyFrDbx\"\u003e\u003cb\u003e训练阶段\u003c/b\u003e：在训练过程中，MAR M 会预先计算并存储用户行为序列经过多层 masked self-attention 模块后的输出。这些输出代表了用户在不同历史片段下的兴趣表征。由于训练数据量巨大，这个过程可以充分挖掘用户的行为模式。\u003c/li\u003e\u003cli data-pid=\"VCblF0a2\"\u003e\u003cb\u003e推理阶段\u003c/b\u003e：在线推理时，当需要对一个候选物品进行评分时，模型可以直接从缓存中检索与该用户相关的、预先计算好的序列表示。然后，这些缓存的表示将与候选物品的嵌入一起，输入到目标注意力层或其他后续模块中，以计算用户对候选物品的兴趣度。由于避免了在线重复计算复杂的多层自注意力，推理效率得到显著提升。\u003c/li\u003e\u003cli data-pid=\"DFDXSbbU\"\u003e\u003cb\u003e检索优化\u003c/b\u003e： 为了高效地从庞大的缓存中检索相关信息，MAR M 需要高效的检索机制。这可能涉及到对缓存键（Cache Key）的设计，例如使用用户 ID、会话 ID 或行为序列的哈希值作为键。同时，缓存数据的组织和管理（如索引结构、数据分区）也对检索效率至关重要。快手构建的 60TB 缓存存储中心 暗示了其缓存系统需要处理海量数据，因此高效的存储和检索算法是 MARM 成功的关键。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"lqnsS5_D\"\u003e通过这些技术细节的实现，MARM 能够在保证推荐效果的前提下，显著提升长序列建模的效率，特别是在训练和推理阶段都利用缓存来加速计算。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e4.3 应用与效果：快手短视频推荐系统的部署\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"95wbBzDf\"\u003eMARM 模型已经在快手的短视频平台上成功部署，每天为数千万用户提供服务，并取得了显著的线上效果提升 。综合实验结果表明，MARM 带来了离线 \u003cb\u003eGAUC (Generalized Area Under the ROC Curve) 0.43% 的提升\u003c/b\u003e，以及线上\u003cb\u003e用户平均播放时长 (play-time per user) 2.079% 的增长\u003c/b\u003e 。这些指标的提升对于短视频平台而言至关重要，GAUC 的提升意味着模型在排序和区分正负样本方面的能力有所增强，而用户平均播放时长的增加则直接反映了用户对推荐内容的满意度和参与度的提高，这对于平台的用户留存和商业变现都具有积极意义。\u003c/p\u003e\u003cp data-pid=\"iO5ZLTe1\"\u003e在快手这样的短视频平台，用户行为序列通常非常长且动态变化迅速。用户每分钟都可能产生大量的滑动、点击、点赞、评论等交互行为。MAR M 通过其缓存和计算加速机制，使得模型能够更有效地处理这些长序列，捕捉用户的即时兴趣和长期偏好。例如，通过缓存用户近期观看过的视频的关键信息，模型可以更快地判断用户当前可能对哪些新视频感兴趣，从而减少推荐延迟，提升用户体验。同时，对更长历史行为的有效建模，也有助于发现用户潜在的兴趣点，推荐更具多样性和惊喜感的内容，避免用户陷入信息茧房。MARM 的成功部署，证明了其在处理大规模工业级推荐系统长序列问题上的有效性和实用性，为其他面临类似挑战的平台提供了宝贵的经验。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e5. TransAct V2：Pinterest 的终身用户行为序列建模\u003c/b\u003e\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-7c65e63423dc8bea0c0709441c120ea9_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1050\" data-rawheight=\"788\" data-original-token=\"v2-085d47c4c3bf144dc3b2b555953030f1\" class=\"origin_image zh-lightbox-thumb\" width=\"1050\" data-original=\"https://picx.zhimg.com/v2-7c65e63423dc8bea0c0709441c120ea9_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e\u003cb\u003e5.1 核心思想：多序列建模与 Next Action Loss\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"cOpLArPB\"\u003e\u003cb\u003eTransAct V2\u003c/b\u003e 是 Pinterest 为其首页推荐系统开发的一种生产模型，旨在通过建模用户极长的行为序列来提升点击率（CTR）预测的准确性和用户参与度 。其核心思想主要体现在两个方面：一是整合\u003cb\u003e多种类型的用户行为序列\u003c/b\u003e，特别是\u003cb\u003e终身（lifelong）用户序列\u003c/b\u003e，以更全面地捕捉用户兴趣；二是引入一个新颖的\u003cb\u003e下一动作损失（Next Action Loss, NAL）\u003c/b\u003e函数作为辅助任务，以增强模型对用户行为的预测能力 。传统的工业级 CTR 模型通常依赖于较短的、近期的用户序列，这限制了它们捕捉用户长期、稳定兴趣的能力，并且容易导致推荐内容同质化，形成“信息茧房”效应 。TransAct V2 通过引入长度可达 \u003cb\u003eO(10^4)\u003c/b\u003e 的终身用户序列，能够更好地平衡用户的即时偏好和历史行为模式，从而提升推荐的多样性和用户的长期参与度 。\u003c/p\u003e\u003cp data-pid=\"cfolnTJ1\"\u003eTransAct V2 不仅仅关注用户的历史点击或互动，还引入了\u003cb\u003e展示序列（impression sequence）\u003c/b\u003e，即用户浏览过但未发生交互的内容，这为模型提供了负反馈信号，有助于更准确地理解用户的真实偏好 。同时，通过引入 NAL 辅助任务，模型被鼓励去预测用户在下一个时间步可能执行的动作类型，这进一步增强了模型对用户行为模式的理解和预测能力 。这种多任务学习的框架，结合了对长期、近期以及展示行为序列的综合利用，使得 TransAct V2 能够构建出更丰富、更动态的用户兴趣表征。此外，TransAct V2 还特别关注了高效服务和部署大规模序列模型所面临的基础设施挑战，并提出了相应的解决方案，以确保模型能够在生产环境中低延迟、高效率地运行 。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e5.2 技术细节：长期/近期/展示序列处理、Top-K 搜索与辅助任务\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"3yvQ7GS4\"\u003eTransAct V2 在技术细节上针对多序列建模和辅助任务进行了精心设计。\u003c/p\u003e\u003cp data-pid=\"qf1_NDU0\"\u003e\u003cb\u003e序列捕获与特征化\u003c/b\u003e： 模型整合了三种类型的用户行为序列：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"IBmmkn1r\"\u003e\u003cb\u003e终身序列 (Lifelong Sequence, LL)\u003c/b\u003e：捕获用户多年来的互动历史，长度可达 O(10^4)，包含如 repins (收藏)、clicks (点击)、hides (隐藏) 等显式用户行为。每个 LL 序列的 token 包含四种特征：行为时间戳、行为类型（多热向量，如用户对同一 Pin 有多种交互）、行为发生界面（如首页、搜索页）以及一个 32 维的 PinSage embedding（表征 Pin 的内容）。为了优化存储，PinSage embedding 采用仿射量化（affine quantization）从 fp16 转换为 int8 向量 。\u003c/li\u003e\u003cli data-pid=\"lp2KPw7t\"\u003e\u003cb\u003e实时序列 (Real-Time Sequence, RT)\u003c/b\u003e：捕获用户近期的行为，长度约为 O(10^2)，特征与 LL 序列类似 。\u003c/li\u003e\u003cli data-pid=\"S9xToSCb\"\u003e\u003cb\u003e展示序列 (Impression Sequence)\u003c/b\u003e：记录用户近期浏览过但未发生交互的 Pin，长度也约为 O(10^2)，同样包含 PinSage embedding 等特征 。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"VyzR16fu\"\u003e\u003cb\u003e序列处理与编码\u003c/b\u003e： TransAct V2 使用同一个 Transformer Encoder 来处理 RT 和 LL 序列 。\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"t07xmOJN\"\u003e\u003cb\u003eTop-K 最近邻搜索 (Nearest Neighbor Search)\u003c/b\u003e：对于 LL 序列，模型以候选 Pin (c) 的 PinSage embedding 为锚点，在 LL 序列中进行最近邻搜索，选择与候选 Pin 在 PinSage embedding 空间中最相似的 Top-K 个元素，形成一个子序列。这个子序列代表了与当前候选 Pin 相关的长期兴趣点 。\u003c/li\u003e\u003cli data-pid=\"hmACRxCE\"\u003e\u003cb\u003e保留最近行为\u003c/b\u003e：为了确保模型能够获取用户最新的行为，无论这些行为与候选 Pin 的相似度如何，模型总是包含 RT 序列中最靠前的 r 个行为 。\u003c/li\u003e\u003cli data-pid=\"lYOGUBNa\"\u003e\u003cb\u003e序列拼接与长度限制\u003c/b\u003e：最终输入 Transformer Encoder 的序列是上述 Top-K 子序列、RT 序列的前 r 个行为以及展示序列的拼接。这个拼接后的序列长度被限制在几百个元素 (O(10^2))，以控制计算成本 。\u003c/li\u003e\u003cli data-pid=\"cKz5vs6y\"\u003e\u003cb\u003e特征编码与 Early Fusion\u003c/b\u003e：每个 token 的元数据（行为类型、发生界面）被嵌入为低维向量，并加入可学习的位置编码。通过将候选 Pin 的 PinSage embedding 与序列中每个 token 的 PinSage embedding 进行拼接，实现了 early fusion，帮助模型更好地理解候选 Pin 与历史行为的关联 。最终编码序列 F 是这些成分的加和 。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"pUQ9ErTN\"\u003e\u003cb\u003eTransformer Encoder\u003c/b\u003e： 编码后的序列 F 输入到一个包含两层 Transformer Encoder 的模块，每层使用单个注意力头，模型维度为 64，前馈网络 (FFN) 维度为 32。在 Transformer 中应用了因果掩码 (causal mask)，确保预测 t 时刻的行动只依赖于 0 到 t-1 时刻的行动 。Transformer 的输出 U 用于多头部预测（通过线性层和最大池化得到一个向量，再拼接回 U，作为最终的用户兴趣表征 ）和下一动作预测任务 。\u003c/p\u003e\u003cp data-pid=\"Ryitf1ic\"\u003e\u003cb\u003eNext Action Loss (NAL) 辅助任务\u003c/b\u003e： NAL 旨在预测用户在时间 t 之后（基于 Transformer 编码的用户表示 U_t）对第 (t+1) 个 Pin 的行动类型 。\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"uHxMWhRB\"\u003e\u003cb\u003e损失函数\u003c/b\u003e：NAL 采用采样 Softmax (Sampled Softmax, SSM) 损失，这是一种对比损失。给定用户 u，目标是使得预测表示 U_t 与正样本 (t+1 时刻的真实交互 Pin) 的点积高于与负样本点积的概率最大化 。\u003c/li\u003e\u003cli data-pid=\"AGlqjhji\"\u003e\u003cb\u003e样本选择\u003c/b\u003e：\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli data-pid=\"CJ72NwvK\"\u003e\u003cb\u003e正样本\u003c/b\u003e：来自用户历史序列中有正向互动（如点击、收藏）的 token 。\u003c/li\u003e\u003cli data-pid=\"CGpBmpEK\"\u003e\u003cb\u003e负样本\u003c/b\u003e：探索了批次内随机采样和基于展示的采样。实验发现，基于展示的负样本（即用户在展示序列中看到但未互动的 Pin）比批次内随机负采样更有效，因为它能提供更具挑战性的负样本，从而提升模型预测负面结果的能力 。\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e\u003cb\u003e5.3 应用与效果：Pinterest 推荐系统的 A/B 测试提升\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"zrpWfHMZ\"\u003eTransAct V2 模型在 Pinterest 的推荐系统中进行了 A/B 测试，并取得了显著的业务效果提升 。这些测试是在 Pinterest 的核心产品之一——Homefeed（信息流）上进行的，覆盖了 1.5% 的流量。基线模型是 TransAct（RT 序列版本），这是 Pinterest 之前使用的一种序列建模方法。\u003c/p\u003e\u003cp data-pid=\"ut7Odncc\"\u003eA/B 测试的结果显示，与基线相比，TransAct V2 带来了多方面的改进 ：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"cG-AgePB\"\u003e\u003cb\u003eRepin 量提升 6.35%\u003c/b\u003e：Repin 是 Pinterest 平台上用户保存喜欢内容的核心行为，其数量的提升直接反映了推荐内容对用户吸引力的增强。\u003c/li\u003e\u003cli data-pid=\"BieG2Q_I\"\u003e\u003cb\u003eHide 量降低 12.80%\u003c/b\u003e：Hide 是用户对不感兴趣内容的负面反馈，其数量的降低表明推荐系统推送不相关内容的情况减少了，用户体验得到改善。\u003c/li\u003e\u003cli data-pid=\"50QRAo4l\"\u003e\u003cb\u003e展示多样性提升 0.45%\u003c/b\u003e：推荐内容的多样性对于维持用户的新鲜感和探索欲望非常重要，多样性的提升有助于避免信息茧房，提升用户长期满意度。\u003c/li\u003e\u003cli data-pid=\"8b7itkVX\"\u003e\u003cb\u003e用户使用时长增加 1.41%\u003c/b\u003e：用户使用时长的增加是衡量推荐系统整体吸引力和用户参与度的一个综合指标。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"V3xtUP3t\"\u003e这些积极的 A/B 测试结果表明，TransAct V2 通过其多序列建模、Top-K 搜索以及 Next Action Loss 等关键技术，有效地提升了 Pinterest 推荐系统的性能。该模型能够更准确地捕捉用户的长期和短期兴趣，理解用户对展示内容的潜在反馈，从而做出更精准的个性化推荐。TransAct V2 的成功部署，进一步证明了在工业级推荐系统中，精细化的用户行为序列建模对于提升用户体验和业务指标的重要性。该模型已被证明在生产环境中具有实践价值 。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e6. PinFM：Pinterest 的用户行为序列基础模型\u003c/b\u003e\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-7da9b05195e716208f57f01e5dda4434_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"634\" data-original-token=\"v2-1a6221d27ffba1ed38508d1e800ad3a2\" class=\"origin_image zh-lightbox-thumb\" width=\"1440\" data-original=\"https://pica.zhimg.com/v2-7da9b05195e716208f57f01e5dda4434_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e\u003cb\u003e6.1 核心思想：超大规模 Transformer 预训练与生成式训练\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"rvFQMXH3\"\u003e\u003cb\u003ePinFM (Pinterest Foundation Model)\u003c/b\u003e 的核心思想在于构建一个能够处理Pinterest平台上数十亿规模用户活动序列的基础模型，其关键在于利用\u003cb\u003e超大规模的Transformer架构\u003c/b\u003e，并通过\u003cb\u003e预训练和生成式训练策略\u003c/b\u003e来学习用户行为序列的通用表示。与针对特定任务进行训练的模型不同，PinFM旨在成为一个通用的、可迁移的表示学习器，能够从海量的、多样化的用户交互数据（如pin、click、repin等）中捕捉深层次的用户兴趣和意图模式。预训练阶段通常使用无监督或自监督的方式，例如通过掩码语言模型（Masked Language Model, MLM）类似的任务，让模型学习预测序列中被掩盖的行为，从而使其理解行为之间的上下文关系和依赖。生成式训练则进一步强化模型对序列动态的理解和预测能力，例如通过让模型生成用户未来的行为序列或补全不完整的序列。这种结合了预训练和生成式训练的方法，使得PinFM能够学习到高度抽象和富有表现力的用户嵌入（embeddings），这些嵌入可以有效地用于下游的各种推荐任务，如候选物品检索、排序等，从而提升整个推荐系统的性能和效率。其目标是创建一个强大的、可扩展的用户行为理解基础，以支持Pinterest复杂且动态的推荐需求。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e6.2 技术细节：去重交叉注意力 (DCAT) 与模型量化\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"2BE3tK_F\"\u003ePinFM 的技术细节中，为了处理超大规模的用户行为序列并保证模型的效率和表达能力，引入了一些关键的创新。其中，\u003cb\u003e去重交叉注意力（Deduplicated Cross-Attention, DCAT）\u003c/b\u003e 机制是一个重要的组成部分。在推荐场景中，用户可能会与同一个物品（例如，同一个Pin）发生多次交互。传统的注意力机制在处理这种重复交互时，可能会不必要地重复计算相似的信息，导致计算资源的浪费和潜在的信息冗余。DCAT机制旨在解决这个问题，通过对序列中的重复项进行识别和处理，避免在注意力计算中对重复内容赋予过高的权重或进行冗余的信息聚合。这有助于模型更聚焦于序列中独特且有信息量的部分，从而提升注意力计算的效率和效果。\u003c/p\u003e\u003cp data-pid=\"JifPxR56\"\u003e另一个关键技术是\u003cb\u003e模型量化（Model Quantization）\u003c/b\u003e。由于PinFM是一个超大规模的Transformer模型，其参数量巨大，对存储和计算资源的需求也非常高。为了将这样的模型部署到实际的工业级推荐系统中，并满足低延迟和高吞吐量的要求，模型量化是必不可少的步骤。模型量化是指将模型中的权重和/或激活值从高精度浮点数（如FP32）转换为低精度表示（如INT8甚至INT4）。通过降低数值表示的精度，可以显著减少模型的内存占用，加速计算过程，并降低功耗。在PinFM中应用模型量化，能够使其在保持可接受的精度损失的前提下，实现更高效的在线推理，从而支持Pinterest数十亿用户规模的实时推荐需求。这些技术细节的运用，体现了PinFM在追求模型强大表达能力的同时，对工业部署实际约束的充分考虑。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e6.3 应用与效果：Pinterest 大规模视觉发现平台的部署\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"HMN-G6Ts\"\u003ePinFM 作为Pinterest的用户行为序列基础模型，其设计和优化目标直指在该平台数十亿用户规模和大规模视觉发现场景下的实际部署与应用。通过预训练和生成式训练得到的通用用户行为嵌入，可以直接或间接地应用于Pinterest推荐系统的各个环节。例如，这些高质量的嵌入可以作为用户特征输入到下游的排序模型中，帮助模型更准确地理解用户当前的兴趣和意图，从而提升推荐物品的相关性。它们也可以用于改进候选物品的检索过程，通过计算用户嵌入与物品嵌入之间的相似度，快速从海量物品库中筛选出潜在的推荐候选。\u003c/p\u003e\u003cp data-pid=\"MIE5H_Hx\"\u003e虽然具体的在线A/B测试指标提升数据在提供的摘要中未直接给出，但可以预期，PinFM的部署将为Pinterest的推荐系统带来多方面的益处。首先，由于基础模型已经在大规模数据上进行了预训练，学习到了通用的用户行为模式，因此可以加速下游任务的模型收敛，并减少对特定任务标注数据的依赖。其次，生成式训练使得模型具备一定的序列预测能力，这有助于发现用户潜在的兴趣点，并进行更前瞻性的推荐。再者，通过技术如去重交叉注意力和模型量化，PinFM能够在保证模型性能的同时，满足工业级系统对效率和资源消耗的严格要求。这些因素共同作用，将使得Pinterest能够为其用户提供更个性化、更精准的视觉内容发现体验，从而提升用户参与度和平台粘性。PinFM的成功部署将标志着Pinterest在利用基础模型技术提升推荐系统能力方面迈出了重要一步。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e7. 方法比较与关系分析\u003c/b\u003e\u003c/h2\u003e\u003ctable data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e方法\u003c/td\u003e\u003ctd\u003e提出方\u003c/td\u003e\u003ctd\u003e核心策略\u003c/td\u003e\u003ctd\u003e序列处理方式\u003c/td\u003e\u003ctd\u003e关键技术点\u003c/td\u003e\u003ctd\u003e工业应用与效果\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eDV365\u003c/td\u003e\u003ctd\u003eMeta\u003c/td\u003e\u003ctd\u003e离线嵌入，多切片与总结 (MSS)\u003c/td\u003e\u003ctd\u003e平均40,000条，最长70,000条\u003c/td\u003e\u003ctd\u003e序列切片，池化，漏斗式总结网络\u003c/td\u003e\u003ctd\u003eInstagram 和 Threads 超过15个模型部署，作为长期兴趣并行于HSTU，带来显著增量效果，稳定运行\u0026gt;1年\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eLUM\u003c/td\u003e\u003ctd\u003e阿里巴巴\u003c/td\u003e\u003ctd\u003e三步范式，大型用户模型 (LUM)\u003c/td\u003e\u003ctd\u003e超长用户行为序列 (UBS)\u003c/td\u003e\u003ctd\u003e知识构建 (下一条件-物品预测)，知识查询 (提示工程)，知识利用 (增强DLRM)\u003c/td\u003e\u003ctd\u003e淘宝搜索CTR提升2.9%，RPM提升1.2%；解锁扩展规律 (最高70亿参数)\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eLONGER\u003c/td\u003e\u003ctd\u003e抖音商业化\u003c/td\u003e\u003ctd\u003e优化的 Transformer 架构，端到端建模\u003c/td\u003e\u003ctd\u003e超长序列 (可达5000条)\u003c/td\u003e\u003ctd\u003eToken Merge, InnerTrans, 混合注意力 (交叉因果+自因果), 全局标记, 工程优化 (混合精度, KV缓存)\u003c/td\u003e\u003ctd\u003e抖音广告与电商服务部署，AUC相对提升1.57%，影响亿级用户\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eMARM\u003c/td\u003e\u003ctd\u003e快手\u003c/td\u003e\u003ctd\u003e内存增强，缓存加速\u003c/td\u003e\u003ctd\u003e用户终身历史行为序列\u003c/td\u003e\u003ctd\u003e缓存多层自注意力计算结果 (非KV)，训练推理均使用缓存，降低复杂度 (O(n^2d) -\u0026gt; O(nd))\u003c/td\u003e\u003ctd\u003e快手短视频推荐系统部署，GAUC提升0.43%，用户平均播放时长提升2.079%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eTransAct V2\u003c/td\u003e\u003ctd\u003ePinterest\u003c/td\u003e\u003ctd\u003e多序列建模 (长期/近期/展示)，Next Action Loss\u003c/td\u003e\u003ctd\u003e终身序列 (O(10^4))，实时序列 (O(10^2))，展示序列 (O(10^2))\u003c/td\u003e\u003ctd\u003eTop-K最近邻搜索，Early Fusion，Transformer Encoder，NAL辅助任务 (基于展示的负采样)\u003c/td\u003e\u003ctd\u003ePinterest Homefeed推荐，Repin量提升6.35%，Hide量降低12.80%，用户使用时长增加1.41%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003ePinFM\u003c/td\u003e\u003ctd\u003ePinterest\u003c/td\u003e\u003ctd\u003e超大规模 Transformer 预训练，生成式训练\u003c/td\u003e\u003ctd\u003e数十亿规模用户活动序列\u003c/td\u003e\u003ctd\u003e去重交叉注意力 (DCAT)，模型量化 [^user query]\u003c/td\u003e\u003ctd\u003ePinterest大规模视觉发现平台部署，为上层推荐任务提供通用用户表示 [^user query]\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cp data-pid=\"v7G_zUmv\"\u003e\u003ci\u003eTable 1: 极长用户行为序列建模方法对比\u003c/i\u003e\u003c/p\u003e\u003cp data-pid=\"BBslHQzR\"\u003e一些共同的趋势：第一，\u003cb\u003eTransformer 架构的主导地位\u003c/b\u003e得到了进一步巩固。几乎所有的方法都以 Transformer 或其变体作为核心的序列建模工具，这凸显了 Transformer 在处理序列数据、捕捉复杂依赖关系方面的强大能力和普适性 。第二，\u003cb\u003e预训练和生成式方法的兴起\u003c/b\u003e是一个值得关注的方向。LUM、TransAct V2 和 PinFM 等方法都体现了利用大规模数据进行预训练，或者采用生成式架构来学习用户行为序列表示的趋势，这有助于模型学习到更通用和鲁棒的用户兴趣表征 。第三，\u003cb\u003e压缩策略和优化技术\u003c/b\u003e是模型能否落地的关键，如何在大规模Transformer下降低了计算和推理成本，决定了工业界的可用性。各种方法并非孤立存在，而是相互借鉴和融合，例如将离线嵌入与在线缓存结合，或者在 Transformer 基础上引入更复杂的注意力机制和训练策略。这种多样性和融合性为解决超长序列建模这一复杂问题提供了更丰富的思路和工具。未来，随着数据量的持续增长和计算能力的提升，如何更高效、更智能地利用用户的终身行为序列，实现更深层次的个性化理解和预测，仍将是推荐系统领域持续探索的重要方向。\u003c/p\u003e\u003cp data-pid=\"rlxb3zyC\"\u003eCredit to KIMI K2：大部分文字由KIMI K2的deep research功能总结\u003c/p\u003e\u003cp data-pid=\"-NmueDdQ\"\u003eTODO：Onemodel \u0026amp; 生成式建模等方向论文正在陆续总结，欢迎关注。\u003c/p\u003e","is_labeled":false,"visited_count":92,"thumbnails":["https://pic1.zhimg.com/50/v2-4c342a6442ce4868cbe9afd57fd28b59_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-ea52be3f6dabf12ba4401e86f5c94934_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-b296e248edfe539220eff7e121e36e65_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-909d38390b040db805cdf2fd22c3ef90_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-5ce324d2d7b65d7a5b42e7021f6b8731_720w.jpg?source=b6762063"],"favorite_count":10,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1931673285894058510}","attached_info":"CvsHCPDjne3L8syghgEQBxoJMjYwNzYyNzgzIP/ShsQGKAUwAEBBSkIKLVRTX1NPVVJDRV9UV09UT1dFUl9NVUxUSV9TQ0VORV9WMV9SRUNBTExfVEVYVBIBMBgAIAA6CnsicmF3IjoiIn1iIGIzNzQ2YWY5NmY0NGZjNTE5ODJjMDU5ZWNiYTE5YTllchMxOTMxNjczMjg1ODk0MDU4NTEwqgEJcmVjb21tZW5kwgEgZmMzOTUwYTQ3YTQzMWQwMDFiODE5MmRhMmE1NDU0ODTyAQoIDBIGTm9ybWFs8gEoCAoSJDFkN2QwZTMzLTRjMjItNGI3Yi1hMDU5LWM1OTU2YmE3NWU1OfIBBggLEgIxMYICAIgC4LXdzoUzkgIgZmMzOTUwYTQ3YTQzMWQwMDFiODE5MmRhMmE1NDU0ODSaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIcQmF5ZXNGaXJzdExldmVsSXNvbGF0aW9uUnVsZdoCLVRTX1NPVVJDRV9UV09UT1dFUl9NVUxUSV9TQ0VORV9WMV9SRUNBTExfVEVYVOgCA/oCC05PUk1BTF9GTE9XigMgMDUyZWYyNDYzZWJhNDYyNThlZDE3NTAyNTNlODViMjCaAw0KAnYyEAAaBW90aGVyqANc2AMA6gMfdGV4dEZlZWRUd29Ub3dlcldhcm11cFN1Y2Nlc3NWMfoDuQISDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAhDuDxisByIjdjItZmNiNjlmYWM0MTBkNDNlMmRiNmFjZGY0ZmFiNjM0MDQ6LQgCEOASGKoHIiN2Mi0xYzY1M2NkNTA0ZTJiYTY2ZDNhNDA4MmJlOTEzMGVjYTotCAIQ9gwYhAgiI3YyLTg3ODhkZGUxZGYxMmVjNWM2MGFkNDdlN2VjZThkOTY1Oi0IAhDeEBiYCSIjdjItMmRiOWEyZDI0OGI3ZjBmMGRkNzUzZmRlOWFiMWNkZjA6LQgDEJoIGJQGIiN2Mi0wODVkNDdjNGMzYmYxNDRkYzNiMmI1NTU5NTMwMzBmMTotCAQQoAsY+gQiI3YyLTFhNjIyMWQyN2ZmYmExZWQzODUwOGQxZTgwMGFkM2EygAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEAmFpwgQDNDAwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAABAFu2FP4EFAAAAAAAAAACJBbsF9gwaW9M/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQuQBgCgBkGoBgCSAi4KCTI2MDc2Mjc4MxITMTkzMTY3MzI4NTg5NDA1ODUxMBgHIgpJTUFHRV9URVhU","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=65\u0026desktop=true\u0026end_offset=65\u0026page_number=12\u0026session_token=b3746af96f44fc51982c059ecba19a9e","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=65\u0026desktop=true\u0026end_offset=65\u0026page_number=12\u0026session_token=b3746af96f44fc51982c059ecba19a9e","totals":0},"fresh_text":"推荐已更新"}
