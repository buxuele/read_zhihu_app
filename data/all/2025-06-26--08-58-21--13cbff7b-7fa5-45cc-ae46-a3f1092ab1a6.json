{"data":[{"id":"102_1750899546.630","type":"feed","offset":102,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899546,"updated_time":1750899546,"target":{"id":"3478599811","type":"answer","url":"https://api.zhihu.com/answers/3478599811","author":{"id":"ed7a0de767d87df854c0fec0248800e4","url":"https://api.zhihu.com/people/ed7a0de767d87df854c0fec0248800e4","user_type":"people","url_token":"ambition-95-7","name":"山竹运营笔记","headline":"博观而约取，厚积而薄发。公众号“山竹运营笔记”","avatar_url":"https://pica.zhimg.com/50/v2-490cad2934e372fa94c372281100a08c_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":460,"is_following":false,"is_followed":false},"created_time":1714048992,"updated_time":1714048992,"voteup_count":123,"thanks_count":14,"comment_count":6,"is_copyable":true,"question":{"id":"284895698","type":"question","url":"https://api.zhihu.com/questions/284895698","author":{"id":"f94e34a3a39718e35b0b0dffef758887","url":"https://api.zhihu.com/people/f94e34a3a39718e35b0b0dffef758887","user_type":"people","url_token":"a-wen-96-49","name":"阿文","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"在闲鱼倒卖什么物品赚钱?","created":1531364352,"answer_count":0,"follower_count":0,"comment_count":3,"bound_topic_ids":[110963],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"今天我就告诉你，闲鱼卖什么东西挣钱！！ 1️⃣运动户外 天幕、帐篷，露营装备户外麻将现在都是非常热的 这个几年形成了全名皆露营的风气，新可入局选择 露营娱乐，游戏，更小众的产品，这个类目销量暴涨露营周边产品可以放心冲！ 货源：1688以及拼夕夕 2️⃣玩具类目 早教益智玩具、泡泡机，这类产品的利润都是非常高的，而且不久就会出很多新奇的玩具，一条短视 频带货几+W,小学生不一定会喜欢，但大学生一定会。 货源：玩貝巴…","excerpt_new":"今天我就告诉你，闲鱼卖什么东西挣钱！！ 1️⃣运动户外 天幕、帐篷，露营装备户外麻将现在都是非常热的 这个几年形成了全名皆露营的风气，新可入局选择 露营娱乐，游戏，更小众的产品，这个类目销量暴涨露营周边产品可以放心冲！ 货源：1688以及拼夕夕 2️⃣玩具类目 早教益智玩具、泡泡机，这类产品的利润都是非常高的，而且不久就会出很多新奇的玩具，一条短视 频带货几+W,小学生不一定会喜欢，但大学生一定会。 货源：玩貝巴…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"AVYQo83G\"\u003e今天我就告诉你，闲鱼卖什么东西挣钱！！\u003c/p\u003e\u003cp data-pid=\"o9QZVq5T\"\u003e1️⃣运动户外\u003c/p\u003e\u003cp data-pid=\"6DFEr9qt\"\u003e天幕、帐篷，露营装备户外麻将现在都是非常热的\u003c/p\u003e\u003cp data-pid=\"ZyA8dFHD\"\u003e这个几年形成了全名皆露营的风气，新可入局选择\u003c/p\u003e\u003cp data-pid=\"Z6cyC1KH\"\u003e露营娱乐，游戏，更小众的产品，这个类目销量暴涨露营周边产品可以放心冲！\u003c/p\u003e\u003cp data-pid=\"ji4u6rmO\"\u003e货源：1688以及拼夕夕\u003c/p\u003e\u003cp data-pid=\"13weM2ps\"\u003e2️⃣玩具类目\u003c/p\u003e\u003cp data-pid=\"wbl8S1cr\"\u003e早教益智玩具、泡泡机，这类产品的利润都是非常高的，而且不久就会出很多新奇的玩具，一条短视\u003c/p\u003e\u003cp data-pid=\"Tka2dGge\"\u003e频带货几+W,小学生不一定会喜欢，但大学生一定会。\u003c/p\u003e\u003cp data-pid=\"EIZmZHZV\"\u003e货源：玩貝巴巴\u003c/p\u003e\u003cp data-pid=\"gPNUubEs\"\u003e3️⃣宠物类目\u003c/p\u003e\u003cp data-pid=\"ILsXNIaF\"\u003e智能喂食器，智能宠物喂食器的价格有高有低，毛利率也能在50%以上。\u003c/p\u003e\u003cp data-pid=\"fql7OlkZ\"\u003e货源：波奇网\u003c/p\u003e\u003cp data-pid=\"COLw702O\"\u003e4️⃣图书类目\u003c/p\u003e\u003cp data-pid=\"FPx-62xC\"\u003e畅销书(例如年初卖高启强同款孙子兵法），珍藏\u003c/p\u003e\u003cp data-pid=\"6VdHJOuK\"\u003e书，二手书，工具书等等利润可达50%以上，而且退货低，售后少。\u003c/p\u003e\u003cp data-pid=\"u6GESQyo\"\u003e货源：孔夫子，甲虎网\u003c/p\u003e\u003cp data-pid=\"Im9LLF9o\"\u003e5️⃣五金配件\u003c/p\u003e\u003cp data-pid=\"_gBTJeC7\"\u003e看起来只有几快钱的客单价，但是实际上买家都是成千上万的买啊，利润难以想象，而且都是消耗品.复购高，一年四季都可以卖。\u003c/p\u003e\u003cp data-pid=\"_VaH4wS8\"\u003e货源：制造网\u003c/p\u003e\u003cp data-pid=\"wL-4p52c\"\u003e6️⃣潮玩\u003c/p\u003e\u003cp data-pid=\"b23XkQn4\"\u003e网红暴力熊摆件，潮玩手办，玛特龙泡包盲盒大网\u003c/p\u003e\u003cp data-pid=\"_u2Z0RUf\"\u003e红，售价上百的暴力熊，白胚成本低至5元，利润非常可观！\u003c/p\u003e\u003cp data-pid=\"lPkYrfGi\"\u003e货源：1688，拼夕夕\u003c/p\u003e\u003cp data-pid=\"H-y22U6x\"\u003e7️⃣电子产品\u003c/p\u003e\u003cp data-pid=\"joJXKjBe\"\u003e汽车配件，机械设备，只要花心思把产品做上去，做好基础销量和评价，后期只需要维护好就可以的\u003c/p\u003e\u003cp data-pid=\"jNKUj6S0\"\u003e都是非常好卖利润又高。\u003c/p\u003e\u003cp data-pid=\"P7Jfpzup\"\u003e货源：五三货源网\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":16031,"favorite_count":606,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 3478599811}","attached_info":"Co4GCPTbuv3BoPvOvgEQBBoJNjYyNTQ1NTMzIOCfqbEGKHswBkBmSi8KBkl0ZW1DRhIfZG9jX3R5cGU6IEFuc3dlcgppZDogNjgyNzg1NjAwChgAIAA6AFoIMjU1ODcyMDJiIDFkYTlkMTM0NTA1NTM5OWYyOGFhNDc0MGFkMDcwMmUycgozNDc4NTk5ODExigEJMjg0ODk1Njk4qgEJcmVjb21tZW5kwgEgZWQ3YTBkZTc2N2Q4N2RmODU0YzBmZWMwMjQ4ODAwZTTyAQoIDBIGTm9ybWFs8gEoCAoSJDljNzU0NGVhLTUyZjgtNDhhOC04YzA2LWM4NTMwZjgzMmExZvIBBggLEgIxOIICAIgCt7H7zfoykgIgZWQ3YTBkZTc2N2Q4N2RmODU0YzBmZWMwMjQ4ODAwZTSaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxlygIXVGVzdGVkQW5kV29ya1dlaWdodFJ1bGXaAgZJdGVtQ0boAgL6AgtOT1JNQUxfRkxPV4oDIGE4ZTMyODNiYzRlODRmNzE5Y2Y3YjlkOGQzYzNhNTNjmgMNCgJ2MhAAGgVvdGhlcqgDn33YAwDqAxV0ZXh0QWxsU2l0ZU12SXRlbUNGVjL6Ax8SDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFgAQAiAQAkgQGTm9ybWFsmgQBMqAEAKgEALAEALoEAmFpwgQDNDAwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAACg02C/P4EFAAAAAAAAAACJBQMWTF4Ig9I/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRKQBgCgBmaoBgGSAiUKCTY2MjU0NTUzMxIKMzQ3ODU5OTgxMRgEIgpJTUFHRV9URVhU","action_card":false},{"id":"103_1750899546.466","type":"feed","offset":103,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750899546,"updated_time":1750899546,"target":{"id":"1908551388432148164","type":"article","url":"https://api.zhihu.com/articles/1908551388432148164","author":{"id":"4b9fed92c094997309190410b63c59cb","url":"https://api.zhihu.com/people/4b9fed92c094997309190410b63c59cb","user_type":"people","url_token":"scorbin","name":"秋水黑刀","headline":"算法工程师 算法技术分享","avatar_url":"https://picx.zhimg.com/50/v2-95b1649394e74592fca05e24006d2f73_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":56,"is_following":false,"is_followed":false},"title":"VA-VAE","image_url":"https://picx.zhimg.com/v2-50e903d039ddcc6992f9934ee69246b7.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1747817342,"updated":1749977671,"voteup_count":101,"voting":0,"comment_count":2,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"今天来讲一篇2025 CVPR Best Paper Award Candiate： Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models Paper:https://arxiv.org/abs/2501.01423 github:https://github.com/hustvl/LightningDiT 动机Diffusion Model当前存在一个问题，就是当latent 维度越高的时候，重构效果会变好，但是生成效果会变差     作者将latent distribution可视化出来之后得到上图(使用t-SNE可视化），可以看到latent维度越高，latent distribu…","excerpt_new":"今天来讲一篇2025 CVPR Best Paper Award Candiate： Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models Paper:https://arxiv.org/abs/2501.01423 github:https://github.com/hustvl/LightningDiT 动机Diffusion Model当前存在一个问题，就是当latent 维度越高的时候，重构效果会变好，但是生成效果会变差     作者将latent distribution可视化出来之后得到上图(使用t-SNE可视化），可以看到latent维度越高，latent distribu…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"FaBbYn-b\"\u003e今天来讲一篇2025 CVPR Best Paper Award Candiate：\u003cbr/\u003eReconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models\u003cbr/\u003ePaper:\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2501.01423\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/abs/2501.0142\u003c/span\u003e\u003cspan class=\"invisible\"\u003e3\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cbr/\u003egithub:\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/hustvl/LightningDiT\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003egithub.com/hustvl/Light\u003c/span\u003e\u003cspan class=\"invisible\"\u003eningDiT\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003ch3\u003e动机\u003c/h3\u003e\u003cp data-pid=\"Y1J8vUuQ\"\u003eDiffusion Model当前存在一个问题，就是当latent 维度越高的时候，重构效果会变好，但是生成效果会变差\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-df54a4680488de3bceac48e942936e03_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"780\" data-rawheight=\"1182\" data-qrcode-action=\"none\" data-original-token=\"v2-8e0c9c810476d7bc9057f71f7ff2c062\" class=\"origin_image zh-lightbox-thumb\" width=\"780\" data-original=\"https://picx.zhimg.com/v2-df54a4680488de3bceac48e942936e03_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-29dae02e34bff23c1573412ebe5e06b2_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1603\" data-rawheight=\"656\" data-original-token=\"v2-98af0f17a9969240d0f79202dce2d44a\" class=\"origin_image zh-lightbox-thumb\" width=\"1603\" data-original=\"https://pic1.zhimg.com/v2-29dae02e34bff23c1573412ebe5e06b2_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"jthZCT3X\"\u003e\u003cbr/\u003e作者将latent distribution可视化出来之后得到上图(使用t-SNE可视化），可以看到latent维度越高，latent distribution就越集中，越分散的分布，生成的图像多样性就越高，也就是FID值就越低。\u003c/p\u003e\u003ch3\u003e方法\u003c/h3\u003e\u003cp data-pid=\"lNU3QUqc\"\u003e作者的做法是使用一组VF Loss来约束latent的分布：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-ddc14982a00de42b34a033b7fb99be7a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"565\" data-original-token=\"v2-577a3d6a15da75b7b688b8b77b6432d3\" class=\"origin_image zh-lightbox-thumb\" width=\"792\" data-original=\"https://pic1.zhimg.com/v2-ddc14982a00de42b34a033b7fb99be7a_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"TkHEmSMf\"\u003e\u003cbr/\u003elatent与foundation model feature的对齐\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-2aa0e8384f19a422563980819afe65ae_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"118\" data-original-token=\"v2-2aa0e8384f19a422563980819afe65ae\" class=\"origin_image zh-lightbox-thumb\" width=\"741\" data-original=\"https://pica.zhimg.com/v2-2aa0e8384f19a422563980819afe65ae_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"BY_mXEQ6\"\u003e\u003cbr/\u003e这里的 \u003cimg src=\"https://www.zhihu.com/equation?tex=%24%24z%27_%7Bij%7D%24%24\" alt=\"$$z\u0026#39;_{ij}$$\" eeimg=\"1\"/\u003e 是类似DINOv2 MAE这样的图像编码器输出的特征，当然是做过维度转换的，和f对齐维度：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-ee9c8945ba6285735b9f355a7db5949d_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"313\" data-original-token=\"v2-7dd014d79325b8e36d3fbc4b55408392\" class=\"origin_image zh-lightbox-thumb\" width=\"768\" data-original=\"https://pic2.zhimg.com/v2-ee9c8945ba6285735b9f355a7db5949d_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"OnxTWK6k\"\u003e\u003cbr/\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=%24%24f_%7Bij%7D%24%24\" alt=\"$$f_{ij}$$\" eeimg=\"1\"/\u003e 是图像经过SD的VAE输出的latent，m1,是一个容忍参数。ReLU的含义是，仅当其特征差异达到一定值(1-m1)的时候，才对损失有贡献。\u003cbr/\u003e相对latent对齐\u003cbr/\u003e除了前面的绝对对齐，还需要做一个相对对齐，为什么要做这个，实际上个人认为这和前面的latent distribution有关，这样能够让latent的分布呈现的分散程度和foundation Model尽可能一致\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-0ef29be579a844ffe3024732e4d08fce_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"759\" data-rawheight=\"118\" data-original-token=\"v2-0ef29be579a844ffe3024732e4d08fce\" class=\"origin_image zh-lightbox-thumb\" width=\"759\" data-original=\"https://pica.zhimg.com/v2-0ef29be579a844ffe3024732e4d08fce_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e总损失\u003cbr/\u003e\u003c/h3\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-0f096dcec1b2f1a74b1e420f544d4b0d_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"776\" data-rawheight=\"978\" data-original-token=\"v2-322e50a65b73b8175a523eeb6b363643\" class=\"origin_image zh-lightbox-thumb\" width=\"776\" data-original=\"https://picx.zhimg.com/v2-0f096dcec1b2f1a74b1e420f544d4b0d_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e实验结果\u003cbr/\u003e\u003c/h3\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-f03498a00acb8d9413f007b105700837_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1611\" data-rawheight=\"626\" data-original-token=\"v2-fe9804023b5e4d70826ea29d33e24682\" class=\"origin_image zh-lightbox-thumb\" width=\"1611\" data-original=\"https://pic4.zhimg.com/v2-f03498a00acb8d9413f007b105700837_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"bDdHFNXZ\"\u003e\u003cbr/\u003e此外，VF Loss还能加快收敛：\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-09cd4d526c1d957ba0aff0b0fc73ee78_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1587\" data-rawheight=\"627\" data-original-token=\"v2-2b936dd5e3b0ca28675c89fa040eb2b6\" class=\"origin_image zh-lightbox-thumb\" width=\"1587\" data-original=\"https://pica.zhimg.com/v2-09cd4d526c1d957ba0aff0b0fc73ee78_r.jpg\"/\u003e\u003c/figure\u003e\u003ch3\u003e问题\u003c/h3\u003e\u003cp data-pid=\"H_tINaZc\"\u003e问题1：这里的VAE是否是可训练的？\u003cbr/\u003e是的，不过这里从代码看，似乎是单独在第一阶段去训练VAE的\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-fc5521038cec5514f93e25224118894d_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"527\" data-rawheight=\"383\" data-original-token=\"v2-dd0875a8932ccaea93e496b5e3508e6b\" class=\"origin_image zh-lightbox-thumb\" width=\"527\" data-original=\"https://pic4.zhimg.com/v2-fc5521038cec5514f93e25224118894d_r.jpg\"/\u003e\u003c/figure\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python3\"\u003e\u003cspan class=\"c1\"\u003e# vf loss\u003c/span\u003e\n\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ez\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003eaux_feature\u003c/span\u003e \u003cspan class=\"ow\"\u003eis\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ez_flat\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erearrange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;b c h w -\u0026gt; b c (h w)\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eaux_feature_flat\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erearrange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eaux_feature\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;b c h w -\u0026gt; b c (h w)\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ez_norm\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efunctional\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enormalize\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ez_flat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eaux_feature_norm\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efunctional\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enormalize\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eaux_feature_flat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ez_cos_sim\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eeinsum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;bci,bcj-\u0026gt;bij\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ez_norm\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ez_norm\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eaux_feature_cos_sim\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eeinsum\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;bci,bcj-\u0026gt;bij\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eaux_feature_norm\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eaux_feature_norm\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ediff\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eabs\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ez_cos_sim\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003eaux_feature_cos_sim\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evf_loss_1\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efunctional\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erelu\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ediff\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edistmat_margin\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emean\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evf_loss_2\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efunctional\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erelu\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecos_margin\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efunctional\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecosine_similarity\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eaux_feature\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emean\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evf_loss\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evf_loss_1\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edistmat_weight\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003evf_loss_2\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecos_weight\u003c/span\u003e\n\u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evf_loss\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003eNone\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"YTkj69d9\"\u003e\u003cbr/\u003e问题2：latent维度不一样，是如何映射到相同的2维可视化distribution？\u003cbr/\u003e这里使用tsne来做分布可视化：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003etsne_results, metrics = plot_tsne_visualization(\n    [],\n    output_path=\u0026#34;tools/latent_demos/latent_tsne_f16d32.png\u0026#34;,\n    cache_file=\u0026#34;tools/latent_demos/latents_cache_f16d32.pt\u0026#34;\n)\n\ntsne_results, metrics = plot_tsne_visualization(\n    [],\n    output_path=\u0026#34;tools/latent_demos/latent_tsne_f16d32_vfdinov2.png\u0026#34;,\n    cache_file=\u0026#34;tools/latent_demos/latents_cache_f16d32_vfdinov2.pt\u0026#34;\n)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"TGMsTh9z\"\u003e关于t-sne的原理可以参考：\u003ca href=\"https://zhuanlan.zhihu.com/p/327699974\" class=\"internal\" target=\"_blank\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003ezhuanlan.zhihu.com/p/32\u003c/span\u003e\u003cspan class=\"invisible\"\u003e7699974\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e， \u003cbr/\u003e这里还有一个问题目前我也没完全弄清：为什么latent的维度越大，latent distribution就越集中？\u003cbr/\u003e作者的说法是，这是源自前人观察到的一个现象：对于discreted-valued的VAE，codebook  size越大，那么其利用率就会越低。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-3a72c95af1f11bc5bafd0c06a8948700_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"617\" data-rawheight=\"292\" data-original-token=\"v2-f70efbcb3275eae913a6d8e8550c70b8\" class=\"origin_image zh-lightbox-thumb\" width=\"617\" data-original=\"https://pica.zhimg.com/v2-3a72c95af1f11bc5bafd0c06a8948700_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"-Go6ubLp\"\u003e\u003cbr/\u003e但这个codebook size和latent dimension有何内在关联，不得而知\u003c/p\u003e","is_labeled":false,"visited_count":3453,"thumbnails":["https://picx.zhimg.com/v2-50e903d039ddcc6992f9934ee69246b7.jpg?source=7e7ef6e2\u0026needBackground=1","https://pic1.zhimg.com/50/v2-7f1e4656dfec474e5bdb0f44609e2a46_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-3c5b34219508a94011888bf2954dbe27_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-f714dc09cf03d8a26019b4a9073b1ae0_720w.jpg?source=b6762063"],"favorite_count":136,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1908551388432148164}","attached_info":"CukJCPTbuv3BoPvOvgEQBxoJMjU4MDE0NjAxIP6mtsEGKGUwAkBnSjAKBkl0ZW1DRhIgZG9jX3R5cGU6IEFydGljbGUKaWQ6IDI1OTA2MjA0MwoYACAAOgBiIDFkYTlkMTM0NTA1NTM5OWYyOGFhNDc0MGFkMDcwMmUychMxOTA4NTUxMzg4NDMyMTQ4MTY0ggFfaHR0cHM6Ly9waWN4LnpoaW1nLmNvbS92Mi01MGU5MDNkMDM5ZGRjYzY5OTJmOTkzNGVlNjkyNDZiNy5qcGc/c291cmNlPTdlN2VmNmUyJm5lZWRCYWNrZ3JvdW5kPTGqAQlyZWNvbW1lbmTCASA0YjlmZWQ5MmMwOTQ5OTczMDkxOTA0MTBiNjNjNTljYvIBCggMEgZOb3JtYWzyASgIChIkMzJhYTcwODgtZDFkMC00NjY5LTgxOTktZDhjN2ExMTgzNGEy8gEGCAsSAjE4ggIAiAK3sfvN+jKSAiA0YjlmZWQ5MmMwOTQ5OTczMDkxOTA0MTBiNjNjNTljYpoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXaAgZJdGVtQ0boAgP6AgtOT1JNQUxfRkxPV4oDIGE4ZTMyODNiYzRlODRmNzE5Y2Y3YjlkOGQzYzNhNTNjmgMNCgJ2MhAAGgVvdGhlcqgD/RrYAwDqAxV0ZXh0QWxsU2l0ZU12SXRlbUNGVjL6A6IEEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAIQwwwYkAUiI3YyLTk4YWYwZjE3YTk5NjkyNDBkMGY3OTIwMmRjZTJkNDRhOi0IAhCYBhi1BCIjdjItNTc3YTNkNmExNWRhNzViN2I2ODhiOGI3N2I2NDMyZDM6LAgCEOUFGHYiI3YyLTJhYTBlODM4NGYxOWE0MjI1NjM5ODA4MTlhZmU2NWFlOi0IAhCABhi5AiIjdjItN2RkMDE0ZDc5MzI1YjhlMzZkM2ZiYzRiNTU0MDgzOTI6LAgCEPcFGHYiI3YyLTBlZjI5YmU1NzlhODQ0ZmZlMzAyNDczMmU0ZDA4ZmNlOi0IAhCIBhjSByIjdjItMzIyZTUwYTY1YjczYjgxNzVhNTIzZWViNmIzNjM2NDM6LQgCEMsMGPIEIiN2Mi1mZTk4MDQwMjNiNWU0ZDcwODI2ZWEyOWQzM2UyNDY4MjotCAIQswwY8wQiI3YyLTJiOTM2ZGQ1ZTNiMGNhMjg2NzVjODlmYTA0MGViMmI2Oi0IAhCPBBj/AiIjdjItZGQwODc1YTg5MzJjY2FlYTkzZTQ5NmI1ZTM1MDhlNmI6LQgCEOkEGKQCIiN2Mi1mNzBlZmJjYjMyNzVlYWU5MTNhNmQ4ZTg1NTBjNzBiODotCAMQnCkYvRsiI3YyLTUwZTkwM2QwMzlkZGNjNjk5MmY5OTM0ZWU2OTI0NmI3gAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEBm1hbnVhbMIEAzE3MMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAwDFqpD+BBQAAAAAAAAAAiQUDFkxeCIPSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUSkAYAoAZnqAYDkgIuCgkyNTgwMTQ2MDESEzE5MDg1NTEzODg0MzIxNDgxNjQYByIKSU1BR0VfVEVYVA==","action_card":false},{"id":"104_1750899546.875","type":"feed","offset":104,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750899546,"updated_time":1750899546,"target":{"id":"1907949654739513685","type":"article","url":"https://api.zhihu.com/articles/1907949654739513685","author":{"id":"94d294e72b5058a06aa8f95f797f8fad","url":"https://api.zhihu.com/people/94d294e72b5058a06aa8f95f797f8fad","user_type":"people","url_token":"wan-wan-mei-xiang-dao-71-71","name":"akaihaoshuai","headline":"喜欢就去做，不喜欢的才需要理由","avatar_url":"https://picx.zhimg.com/50/v2-cd1eb6e21383332c7e8120bebfa23137_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":2791,"is_following":false,"is_followed":false},"title":"基于Qwen3的DPO/KTO/ORPO/Simpo经验总结","image_url":"https://pic1.zhimg.com/v2-a1a50c004e50c9ecae1d8afbabc5f839.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1749371646,"updated":1749483298,"voteup_count":187,"voting":0,"comment_count":6,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"总结一下RLHF的部分经验，基于Qwen3系列模型，框架使用Llama-Factory。 DPO=优化chosen-rejectedDPO的计算过程大概如下   在DPO优化过程中, 对于chosen优化的方向是有不确定性的，DPO优化只保证整体的margin增大 ，而不是单一的让chosen prob增大。 loss和chosen、rejected公式如下。   这里采用了llama-factory自带的dpo_en_demo数据集进行DPO训练，可以看出rejected和chosen都是上升的，margin也在增加。   理想情况是 chosen增大、r…","excerpt_new":"总结一下RLHF的部分经验，基于Qwen3系列模型，框架使用Llama-Factory。 DPO=优化chosen-rejectedDPO的计算过程大概如下   在DPO优化过程中, 对于chosen优化的方向是有不确定性的，DPO优化只保证整体的margin增大 ，而不是单一的让chosen prob增大。 loss和chosen、rejected公式如下。   这里采用了llama-factory自带的dpo_en_demo数据集进行DPO训练，可以看出rejected和chosen都是上升的，margin也在增加。   理想情况是 chosen增大、r…","preview_type":"default","preview_text":"","column":{"id":"c_1662098877871296512","type":"column","url":"https://api.zhihu.com/columns/c_1662098877871296512","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://picx.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"LLM(大语言模型)学习","imageUrl":"https://picx.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b","comment_permission":"private","intro":"LLM相关学习记录","updated":1689517510,"is_following":false},"content":"\u003cp data-pid=\"BR8ASajT\"\u003e总结一下RLHF的部分经验，基于Qwen3系列模型，框架使用Llama-Factory。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003eDPO=优化chosen-rejected\u003c/h2\u003e\u003cp data-pid=\"WawRJq5u\"\u003eDPO的计算过程大概如下\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-d163b7f8a6f1299add709a78f99a4c1c_1440w.jpg\" data-rawwidth=\"1235\" data-rawheight=\"639\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-1efedc17edbf1416cf2ffd59664e0566\" class=\"origin_image zh-lightbox-thumb\" width=\"1235\" data-original=\"https://pic1.zhimg.com/v2-d163b7f8a6f1299add709a78f99a4c1c_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"BOPuRd2u\"\u003e在DPO优化过程中, 对于chosen优化的方向是有不确定性的，DPO优化只保证整体的margin增大 ，而不是单一的让chosen prob增大。\u003c/p\u003e\u003cp data-pid=\"k7I71KPr\"\u003eloss和chosen、rejected公式如下。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-d3376883fd7b8d38967f0af37ec08de5_1440w.jpg\" data-rawwidth=\"881\" data-rawheight=\"381\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-cb02fca75f668f85eac9dfcfd4cb85d5\" class=\"origin_image zh-lightbox-thumb\" width=\"881\" data-original=\"https://pic2.zhimg.com/v2-d3376883fd7b8d38967f0af37ec08de5_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"ZJRhnHjG\"\u003e这里采用了llama-factory自带的dpo_en_demo数据集进行DPO训练，可以看出rejected和chosen都是上升的，margin也在增加。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-7a362fa142cf170492fd3794c3fa7acd_1440w.jpg\" data-rawwidth=\"1394\" data-rawheight=\"607\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-5830e6f7bde21142bc2d8abb56642ad8\" class=\"origin_image zh-lightbox-thumb\" width=\"1394\" data-original=\"https://pic2.zhimg.com/v2-7a362fa142cf170492fd3794c3fa7acd_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"X445i_OL\"\u003e理想情况是\u003cb\u003echosen增大、rejected降低\u003c/b\u003e。但在数据不够好/参数设置不合理的时候，就会出现chosen/rejected同时上升或者下降的情况。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"n1Fu8Qib\"\u003e下面更换Qwen2.5模型进行DPO训练，看到chosen和rejected都在降低。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-b5f571e2843ec272ccfb4303efd9ff74_1440w.jpg\" data-rawwidth=\"1507\" data-rawheight=\"598\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-2c8b43593202f10e73ef5b3c40bb9fcf\" class=\"origin_image zh-lightbox-thumb\" width=\"1507\" data-original=\"https://pic3.zhimg.com/v2-b5f571e2843ec272ccfb4303efd9ff74_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"zlIojzSM\"\u003e下面是采用实际业务数据得到的pair对，训练SFT的Qwen2.5，并加权了部分SFT的loss，以使得模型权重结果不偏离原始模型太多。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-f9fe27b6bbee89aba647f7729ca89565_1440w.jpg\" data-rawwidth=\"1302\" data-rawheight=\"613\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-3710b42c502965004e3c68ff5612ae4e\" class=\"origin_image zh-lightbox-thumb\" width=\"1302\" data-original=\"https://pic4.zhimg.com/v2-f9fe27b6bbee89aba647f7729ca89565_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"UtfFpUEJ\"\u003e其中chosen和rejected同时上升or下降的原因分析可参考：\u003c/p\u003e\u003cblockquote data-pid=\"YtbRGUmK\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/6327313416\" class=\"internal\" target=\"_blank\"\u003e小冬瓜AIGC：为什么DPO里Chosen和Rejected概率会同时下降???\u003c/a\u003e\u003cbr/\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/694381064\" class=\"internal\" target=\"_blank\"\u003eChenShawn：【不靠谱】有关DPO训练时，为什么chosen和rejected的reward一起下降的猜想\u003c/a\u003e\u003cbr/\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/30274484125\" class=\"internal\" target=\"_blank\"\u003eSonsii：DPO及其衍生算法XX-O\u003c/a\u003e\u003cbr/\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1892320522487959674\" class=\"internal\" target=\"_blank\"\u003eDPO训练过程中出现chosen reward下降\u003c/a\u003e\u003c/blockquote\u003e\u003cp data-pid=\"-oa7rJVR\"\u003e从理论上分析，chosen_reward是model_chosen_logp - ref_chosen_logp，又因logp是每个next token的logits概率。chosen_reward变大就表示，相比于原始模型来说，更新后的模型输出next token=chosen token的概率变大，也就意味着模型的输出正在接近chosen样本。rejected增大同理。\u003c/p\u003e\u003cp data-pid=\"usZSSmw6\"\u003e那么反过来讲，chosen_reward减小，表明模型更新后，离chosen样本输出越远。rejected也同理。\u003c/p\u003e\u003cp data-pid=\"4kdHKg66\"\u003e\u003cb\u003e因此好的训练效果是chosen_reward增大（输出logits更接近），rejected_rewards变小（输出logits离bad更远）。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"uzCKudRF\"\u003e\u003cb\u003e除此之外的情况，表明了训练数据pair对模型来说没有足够好的区分度，从而造成同步上升或者下降。（\u003c/b\u003e总结来说，xxx_reward分数的正负就是表明当前样本下，update_model相比ref_model，是在接近还是远离xxx样本）\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"JLLuK2GO\"\u003eloss分数是update_model好坏之差和ref_model好坏之差的变化量。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-3cee442d94eaa7670457f75f8e1b6cf6_1440w.jpg\" data-rawwidth=\"759\" data-rawheight=\"360\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-54ddeeec5caf175310b1083f5bac8115\" class=\"origin_image zh-lightbox-thumb\" width=\"759\" data-original=\"https://pic3.zhimg.com/v2-3cee442d94eaa7670457f75f8e1b6cf6_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"G7TiF1Po\"\u003eloss变小意味着logits变大，若ref_logratios不变（ref模型没变），则意味着update_model的好坏之差变大，那么当policy_rejected_logps不变时，意味着模型输出更加接近chosen的policy_chosen_logps。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"HguN3Dwq\"\u003e通常dpo训练时，rejected样本是模型输出的，chosen是better模型优化的。那么这里就有一个问题，\u003cb\u003e\u003ci\u003e数据是不是当前模型输出的，对训练效果有什么影响？\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"fTddBDB5\"\u003e基于上面分析，在认定ref_logratios不变的情况下，loss更新后，会导致update_model更接近chosen或者远离rejected或者both。\u003c/p\u003e\u003cp data-pid=\"cMoy_qa1\"\u003e如果rejected不是sft模型产生的，那么远离一个本来就不会输出的rejected（如同降低一个发生概率本身就为0的概率）就没有多大意义，这部分效果就丢失了，因此整个DPO的效果就打折了。\u003c/p\u003e\u003cblockquote data-pid=\"6QoO3s1J\"\u003e\u003cb\u003e更直观的说明就是：如同margin=chosen-rejected=0.5-（-0.3）=0.8，现在这-0.3的学习没啥意义，0.8中只有0.5是真实有效的。\u003c/b\u003e\u003c/blockquote\u003e\u003cp data-pid=\"HDh9hCW3\"\u003e对于chosen，虽然也是高质量的数据，但如果是ref model自己产生的，那么训练后会增大输出概率。若不是ref model产生的，数据分布可能会有较大差别（loss较大），那么一个问题是完全学会这样的输出比较难，另一个是大loss对权重改变较大，可能会严重影响模型原本的输出内容。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003eKTO：DPO-\u0026gt;单边学习\u003c/h2\u003e\u003cblockquote data-pid=\"GL46Ozm9\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.01306\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eKTO: Model Alignment as Prospect Theoretic Optimization\u003c/a\u003e\u003cbr/\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/696331582\" class=\"internal\" target=\"_blank\"\u003eLLM RLHF 2024论文（八）KTO\u003c/a\u003e\u003c/blockquote\u003e\u003cp data-pid=\"jthURhKO\"\u003e考虑到dpo的loss是优化margin，也就是chosen-rejected的差。可能会存在两边同时上升/下降，但margin仍然增加的情况。那么考虑固定一边，就可以优化另一部分的效果。比如rejected为0，那么优化margin就变成了优化chosen，固定chosen同理。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-5b73429944915afc4785851870bb4433_1440w.jpg\" data-rawwidth=\"654\" data-rawheight=\"316\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-b6f30a7db8e18e9da7696e48206fbdaf\" class=\"origin_image zh-lightbox-thumb\" width=\"654\" data-original=\"https://pic2.zhimg.com/v2-5b73429944915afc4785851870bb4433_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"dunA1-mk\"\u003e此时从reward优化方式变成了类似sft的优化方式（不再需要偏好数据），因为接近SFT方式，所以可以在base模型上直接KTO训练，但相比SFT来说，KTO需要有ref_model。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"KPWA2yxU\"\u003e设置kto_tag，若为true，则chosen数据保持不变，rejected_logits设置为0，反过来则一样。从而使得独立处理两种数据均有SFT效果，可以对正负样本不均衡的数据进行训练。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-bf8539934a1ba5fc0a02be2f2890aaba_1440w.jpg\" data-rawwidth=\"877\" data-rawheight=\"324\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-c10898888832b54b95a92259b83be73c\" class=\"origin_image zh-lightbox-thumb\" width=\"877\" data-original=\"https://pic3.zhimg.com/v2-bf8539934a1ba5fc0a02be2f2890aaba_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"bepIUR_9\"\u003e论文中说的HALO方法，其实可以不用管。\u003c/p\u003e\u003cp data-pid=\"fvmSrnoQ\"\u003e\u003cb\u003eKTO和SFT对比：\u003c/b\u003eSFT的数据只能是good case，而KTO的数据可以是bad case。KTO需要ref_model。\u003c/p\u003e\u003cp data-pid=\"56R5vvBC\"\u003e\u003cb\u003eDPO和KTO对比：\u003c/b\u003e且DPO可能会出现chosen_reward降低导致效果差的问题，而KTO则能保证模型一定更接近chosen的效果。DPO适合于chosen明显都比rejected更好的pair数据。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-721faa58ebc033921d611e845b72e66e_1440w.jpg\" data-rawwidth=\"657\" data-rawheight=\"343\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-4c4853d8e8c3a0d71495876dd9fd6b3b\" class=\"origin_image zh-lightbox-thumb\" width=\"657\" data-original=\"https://pic1.zhimg.com/v2-721faa58ebc033921d611e845b72e66e_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"m1PtiuEB\"\u003e论文中说KTO的效果均比DPO要强。实际效果还是要看数据质量（目前个人实测是DPO要更好一点）。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-cd7de7f9989805f7c85758d641b8260d_1440w.jpg\" data-rawwidth=\"1256\" data-rawheight=\"615\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-872fd0d786d87cc7527b8e919ce1a5ae\" class=\"origin_image zh-lightbox-thumb\" width=\"1256\" data-original=\"https://picx.zhimg.com/v2-cd7de7f9989805f7c85758d641b8260d_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"UGCKpT3j\"\u003e实际测试，chosen还是会略有下降。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003eORPO：SFT+0.1*RL\u003c/h2\u003e\u003cblockquote data-pid=\"UpAnUKJF\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2403.07691\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eORPO: Monolithic Preference Optimization without Reference Model\u003c/a\u003e\u003cbr/\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/688583797\" class=\"internal\" target=\"_blank\"\u003e大模型的PPO、DPO偏好优化算法玩不起？那建议你看一下ORPO（更有性价比！）\u003c/a\u003e\u003c/blockquote\u003e\u003cp data-pid=\"pwUXofVo\"\u003eORPO在DPO的基础上，去掉了ref_model，增加了赔率比（odds ratio）的loss。又结合了SFT的loss，将SFT和RLHF的过程合并成一个。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-1a3f250428ba04286ec8b74318ef61f5_1440w.jpg\" data-rawwidth=\"1214\" data-rawheight=\"481\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-dbabe023bab39fadc587509e4ae4d330\" class=\"origin_image zh-lightbox-thumb\" width=\"1214\" data-original=\"https://pic4.zhimg.com/v2-1a3f250428ba04286ec8b74318ef61f5_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"tdta7EQS\"\u003eloss如下，首先定义odds，就是将原本的概率放的更大（加速模型更新）。然后定义OR loss，最终叠加SFT loss。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-768b7579b3f46ec46266948692444b1c_1440w.jpg\" data-rawwidth=\"538\" data-rawheight=\"262\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-6cd7279a590b559109c437e7ff396037\" class=\"origin_image zh-lightbox-thumb\" width=\"538\" data-original=\"https://pic3.zhimg.com/v2-768b7579b3f46ec46266948692444b1c_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-1460d6ba6b4097fdd0a8efe0800b04b3_1440w.jpg\" data-rawwidth=\"854\" data-rawheight=\"211\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-cbd34c76d6f851ba12d2474609164b33\" class=\"origin_image zh-lightbox-thumb\" width=\"854\" data-original=\"https://picx.zhimg.com/v2-1460d6ba6b4097fdd0a8efe0800b04b3_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"xPxRkBQY\"\u003eORPO从本质上来说和DPO在解决的是同一个问题：拉远正负样本的距离，让模型更好地区分正负样本。\u003c/p\u003e\u003cp data-pid=\"Nu5dGv72\"\u003e这里有个加权系数，实验中通常取0.1，那么实际上主要起作用的还是SFT。也就是在SFT上多引入了一点对比loss。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"w7sspC0a\"\u003e\u003cb\u003eORPO和DPO的区别：\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"tw0D8Hkc\"\u003e1、ORPO没有ref_model，DPO有。\u003c/li\u003e\u003cli data-pid=\"uL4W3deI\"\u003e2、ORPO的加权通常是pair_loss*0.1，可以看作是SFT训练中增加一点对比学习效果。而DPO是sft_loss*0.1，是对比学习中引入一点sft效果，让模型不至于跑太偏。如果ORPO去掉sft_loss，那么就是一个去掉ref_model的激进RL方法，更容易跑偏。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"6NVEiZEX\"\u003e\u003cb\u003eORPO的缺点：通常来说SFT阶段需要几万~几十万的数据集，RL阶段需要几千~几万条pair数据。众所周知，pair数据比sft数据难获取。而ORPO是SFT+RL阶段二合一，需要大量的pair数据。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003eSimpo：DPO简化\u003c/h2\u003e\u003cblockquote data-pid=\"6zTCHQ7N\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2405.14734\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eSimPO: Simple Preference Optimization with a Reference-Free Reward\u003c/a\u003e\u003c/blockquote\u003e\u003cp data-pid=\"egAGJ-xX\"\u003eDPO的loss是最大化 \u003cimg src=\"https://www.zhihu.com/equation?tex=margin%3Dr%28x%2Cy_w%29-r%28x%2Cy_l%29%3D%5Cbeta+log%5Cfrac%7B%5Cpi_%5Ctheta%28y_w%7Cx%29%7D%7B%5Cpi_%7Bref%7D%28y_w%7Cx%29%7D-%5Cbeta+log%5Cfrac%7B%5Cpi_%5Ctheta%28y_l%7Cx%29%7D%7B%5Cpi_%7Bref%7D%28y_l%7Cx%29%7D\" alt=\"margin=r(x,y_w)-r(x,y_l)=\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}-\\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\" eeimg=\"1\"/\u003e ，但这个训练目标和推理目标 \u003cimg src=\"https://www.zhihu.com/equation?tex=log+%5Cpi_%5Ctheta%28y_w%7Cx%29\" alt=\"log \\pi_\\theta(y_w|x)\" eeimg=\"1\"/\u003e 并不相同（这是SFT的训练目标）。\u003c/p\u003e\u003cp data-pid=\"b2oVarcF\"\u003eSimpo论文中统计了 \u003cimg src=\"https://www.zhihu.com/equation?tex=r%28x%2Cy_w%29%3Er%28x%2Cy_l%29%EF%BC%8C+%E4%B8%94log+%5Cpi_%5Ctheta%28y_w%7Cx%29%3Elog+%5Cpi_%5Ctheta%28y_l%7Cx%29\" alt=\"r(x,y_w)\u0026gt;r(x,y_l)， 且log \\pi_\\theta(y_w|x)\u0026gt;log \\pi_\\theta(y_l|x)\" eeimg=\"1\"/\u003e 的占比。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-2212d56003071cece676f73422ddf58d_1440w.jpg\" data-rawwidth=\"1106\" data-rawheight=\"388\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-2212d56003071cece676f73422ddf58d\" class=\"origin_image zh-lightbox-thumb\" width=\"1106\" data-original=\"https://pic4.zhimg.com/v2-2212d56003071cece676f73422ddf58d_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"n6XklKx3\"\u003e可以看出DPO的训练结果中，有一半是不符合推理目标的要求的。\u003c/p\u003e\u003cp data-pid=\"rCbqUREn\"\u003e这种情况下必然是\u003c/p\u003e\u003cp data-pid=\"5sMI1QMJ\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=log+%5Cpi_%7Bref%7D%28y_w%7Cx%29%3Clog+%5Cpi_%7Bref%7D%28y_l%7Cx%29%3C0++%5C%5C\" alt=\"log \\pi_{ref}(y_w|x)\u0026lt;log \\pi_{ref}(y_l|x)\u0026lt;0  \\\\\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cp data-pid=\"fEwVR_SG\"\u003e或者\u003c/p\u003e\u003cp data-pid=\"KkIUFsW5\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=log+%5Cpi_%7Bref%7D%28y_w%7Cx%29%3C0%3Clog+%5Cpi_%7Bref%7D%28y_l%7Cx%29+%5C%5C\" alt=\"log \\pi_{ref}(y_w|x)\u0026lt;0\u0026lt;log \\pi_{ref}(y_l|x) \\\\\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cp data-pid=\"_Y692O2e\"\u003e也就是说原始sft模型很难生成pair数据中的chosen数据，相比来说，更容易生成rejected。\u003c/p\u003e\u003cp data-pid=\"WrUXcO8q\"\u003e\u003cb\u003e实际项目中的DPO训练数据通常就是这个样子，rejected是sft模型生成的bad case（生成概率更大），而chosen是其他better model生成的（sft模型生成概率很低）。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"YaeHarYC\"\u003eSimPO对此做了优化。从奖励函数可以看出相比于DPO，SimPO少了ref model。训练显存占用更少，并增加了长度归一化项，解决chosen通常比rejected更长导致的rl model输出长度增长的问题。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-cf2aa5ce07641d4b59efe4ac3e9be802_1440w.jpg\" data-rawwidth=\"521\" data-rawheight=\"259\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-84970c5fc58bc826a427b0c97a318237\" class=\"origin_image zh-lightbox-thumb\" width=\"521\" data-original=\"https://pic3.zhimg.com/v2-cf2aa5ce07641d4b59efe4ac3e9be802_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"DcFUeH-L\"\u003e简单推理过程可以理解为\u003c/p\u003e\u003cp data-pid=\"1ajarz2g\"\u003e\u003cimg src=\"https://www.zhihu.com/equation?tex=loss%3D-E%5B+log+%5Csigma+%28%5Cbeta+log%5Cfrac%7B%5Cpi_%5Ctheta%28y_w%7Cx%29%7D%7B%5Cpi_%7Bref%7D%28y_w%7Cx%29%7D-%5Cbeta+log%5Cfrac%7B%5Cpi_%5Ctheta%28y_l%7Cx%29%7D%7B%5Cpi_%7Bref%7D%28y_l%7Cx%29%7D%29%5D+%5C%5C++++++++%3D-E%5B+log+%5Csigma+%28%5Cbeta+log+%5Cpi_%5Ctheta%28y_w%7Cx%29+-+%5Cbeta+log%7B%5Cpi_%7Bref%7D%28y_w%7Cx%29%7D-%28%5Cbeta+log+%5Cpi_%5Ctheta%28y_l%7Cx%29+-+%5Cbeta+log+%7B%5Cpi_%7Bref%7D%28y_l%7Cx%29%7D%29%29%5D++%5C%5C++++++%3D-E%5B+log+%5Csigma+%28%5Cbeta+log+%5Cpi_%5Ctheta%28y_w%7Cx%29+-+%5Cbeta+log+%5Cpi_%5Ctheta%28y_l%7Cx%29+-%28%5Cbeta+log%7B%5Cpi_%7Bref%7D%28y_w%7Cx%29%7D+-+%5Cbeta+log+%7B%5Cpi_%7Bref%7D%28y_l%7Cx%29%7D%29%29%5D+%5C%5C++++++%3D-E%5B+log+%5Csigma+%28%5Cbeta+log+%5Cpi_%5Ctheta%28y_w%7Cx%29+-+%5Cbeta+log+%5Cpi_%5Ctheta%28y_l%7Cx%29-%5Clambda%29%5D++%5C%5C++%E5%85%B6%E4%B8%AD%5Clambda%3D%28%5Cbeta+log%7B%5Cpi_%7Bref%7D%28y_w%7Cx%29%7D+-+%5Cbeta+log+%7B%5Cpi_%7Bref%7D%28y_l%7Cx%29%7D%29\" alt=\"loss=-E[ log \\sigma (\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}-\\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})] \\\\        =-E[ log \\sigma (\\beta log \\pi_\\theta(y_w|x) - \\beta log{\\pi_{ref}(y_w|x)}-(\\beta log \\pi_\\theta(y_l|x) - \\beta log {\\pi_{ref}(y_l|x)}))]  \\\\      =-E[ log \\sigma (\\beta log \\pi_\\theta(y_w|x) - \\beta log \\pi_\\theta(y_l|x) -(\\beta log{\\pi_{ref}(y_w|x)} - \\beta log {\\pi_{ref}(y_l|x)}))] \\\\      =-E[ log \\sigma (\\beta log \\pi_\\theta(y_w|x) - \\beta log \\pi_\\theta(y_l|x)-\\lambda)]  \\\\  其中\\lambda=(\\beta log{\\pi_{ref}(y_w|x)} - \\beta log {\\pi_{ref}(y_l|x)})\" eeimg=\"1\"/\u003e\u003c/p\u003e\u003cp data-pid=\"Bx8OCG-L\"\u003eSimpo中没有KL散度约束，通过降低lr进行简单约束模型偏差。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-e7d0ab674f473390278da729f5648796_1440w.jpg\" data-rawwidth=\"819\" data-rawheight=\"140\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-e7d0ab674f473390278da729f5648796\" class=\"origin_image zh-lightbox-thumb\" width=\"819\" data-original=\"https://pica.zhimg.com/v2-e7d0ab674f473390278da729f5648796_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"UCA2uy9_\"\u003e\u003cb\u003eSimPO vs DPO：\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"daPAbXyJ\"\u003e\u003cb\u003e总结就是，DPO是优化（\u003c/b\u003e当前model比ref model更接近chosen\u003cb\u003e - \u003c/b\u003e当前model比ref model更远离rejected\u003cb\u003e）\u003c/b\u003e，目标太复杂，优化的结果不一定是最佳推理结果。\u003c/p\u003e\u003cp data-pid=\"t89dw1Af\"\u003e\u003cb\u003eSimpo是优化（\u003c/b\u003e当前model生成chosen-当前model生成rejected\u003cb\u003e）\u003c/b\u003e，目标简单明了\u003cb\u003e。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"OJiA76Q7\"\u003e\u003cb\u003e缺点：\u003c/b\u003e没有ref模型，没加KL散度，容易巡飞。。。实测效果一般。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e总结\u003c/h2\u003e\u003cp data-pid=\"reKMSw02\"\u003e1、DPO是PPO的简化版，目的是优化max(chosen-rejected)，对于不是自己生成的数据，容易产生chosen和rejected同时上升和下降的情况，效果不够稳定（虽然比PPO已经好很多了）。\u003c/p\u003e\u003cp data-pid=\"fGdm5Cso\"\u003e2、KTO把DPO的优化目标从max(margin)变成了max(chosen) or max(-rejected)，保证了RL的效果，数据集质量一般的情况下，效果比DPO好。但方法的能力上限比DPO低。\u003c/p\u003e\u003cp data-pid=\"Etdfrihc\"\u003e3、ORPO的核心还是SFT loss，同时对pair数据的需求变大。\u003c/p\u003e\u003cp data-pid=\"VbJ9WDgn\"\u003e4、Simpo相比DPO，去掉了ref model，没加KL散度，容易巡飞。\u003c/p\u003e\u003cp data-pid=\"gFd6_APV\"\u003e\u003cb\u003e总的来说，在当前所做对话项目中还是DPO用的较多，常用于针对某类具体问题纠正模型回复内容，其他任务类型不确定，完全需要具体任务具体分析。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"wrhtX0xF\"\u003e\u003cb\u003e提升对话质量方面，整体还是比较难搞的，毕竟没办法准确定义什么样的对话质量高，调prompt洗出“高质量”数据是最核心的工作内容。数据质量对结果的影响比RL方法的影响要大的多。\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e参考文章\u003c/h2\u003e\u003cp data-pid=\"Nrm7ggrG\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.18290\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eDirect Preference Optimization: Your Language Model is Secretly a Reward Model\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"MB5Q1Vl2\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/6327313416\" class=\"internal\" target=\"_blank\"\u003e小冬瓜AIGC：为什么DPO里Chosen和Rejected概率会同时下降???\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"GzYT-I73\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1082394115\" class=\"internal\" target=\"_blank\"\u003edpo 的局限性\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"GlwNVFhB\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/694381064\" class=\"internal\" target=\"_blank\"\u003eChenShawn：【不靠谱】有关DPO训练时，为什么chosen和rejected的reward一起下降的猜想\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"gxgx6rKm\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/30274484125\" class=\"internal\" target=\"_blank\"\u003eSonsii：DPO及其衍生算法XX-O\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"xjycU8-w\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/721073733?utm_psn=1820487557647036418\" class=\"internal\" target=\"_blank\"\u003e人人都能看懂的DPO数学原理\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"AVj7LUkF\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.01306\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eKTO: Model Alignment as Prospect Theoretic Optimization\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"t7iSvwSm\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/696331582\" class=\"internal\" target=\"_blank\"\u003eLLM RLHF 2024论文（八）KTO\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"r48sUswt\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/pE_sSlaGUfKNM9EaBLR-cg\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e大模型对齐技术，各种什么O：PPO,DPO, SimPO,KTO,Step-DPO, MCTS-DPO,SPO\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"C10VEzXC\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/710021282?utm_psn=1815820870922416128\" class=\"internal\" target=\"_blank\"\u003e2024年大模型Alignment偏好优化技术PPO,DPO, SimPO,KTO,Step-DPO, MCTS-DPO,SPO\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"5txvHQan\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2403.07691\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eORPO: Monolithic Preference Optimization without Reference Model\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"EOUTSVpq\"\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/688583797\" class=\"internal\" target=\"_blank\"\u003e大模型的PPO、DPO偏好优化算法玩不起？那建议你看一下ORPO（更有性价比！）\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"3AD6-ctX\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2405.14734\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eSimPO: Simple Preference Optimization with a Reference-Free Reward\u003c/a\u003e\u003c/p\u003e","is_labeled":false,"visited_count":4453,"thumbnails":["https://pic1.zhimg.com/v2-a1a50c004e50c9ecae1d8afbabc5f839.jpg?source=7e7ef6e2\u0026needBackground=1"],"favorite_count":399,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1907949654739513685}","attached_info":"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","action_card":false},{"id":"105_1750899546.932","type":"feed","offset":105,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750899546,"updated_time":1750899546,"target":{"id":"1908556956945322353","type":"article","url":"https://api.zhihu.com/articles/1908556956945322353","author":{"id":"dbf6fa2cd16c6091a654dd29f6750998","url":"https://api.zhihu.com/people/dbf6fa2cd16c6091a654dd29f6750998","user_type":"people","url_token":"29-33-3-55-32","name":"沐沐妈","headline":"","avatar_url":"https://pic1.zhimg.com/50/v2-407b8c150fad538be1a1c3ad2af0fd8c_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":1063,"is_following":false,"is_followed":false},"title":"多亏了，这8个小项目，6个月还了18w","comment_permission":"all","created":1748399943,"updated":1748399943,"voteup_count":25,"voting":0,"comment_count":1,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"去年因实体店亏损严重，负债近30W，找了很多项目都没有做起来。这8个小项目是我这一年时间亲自测试过，每个都可以做起来的，目前已经把负债解决了，希望能够帮助到大家! 1、卖word文档 我有个朋友，每天去搬人家的文章，然后整理成word，发布到某度文库，并设置价格为1-5块不等，发布了2000多篇，目前每天挣到手里的马内能吃一顿海底捞不成问题。 2、做知识变现 小红书上面有很多人卖小吃教程、高考资料、乐器课程等，虽然不起眼…","excerpt_new":"去年因实体店亏损严重，负债近30W，找了很多项目都没有做起来。这8个小项目是我这一年时间亲自测试过，每个都可以做起来的，目前已经把负债解决了，希望能够帮助到大家! 1、卖word文档 我有个朋友，每天去搬人家的文章，然后整理成word，发布到某度文库，并设置价格为1-5块不等，发布了2000多篇，目前每天挣到手里的马内能吃一顿海底捞不成问题。 2、做知识变现 小红书上面有很多人卖小吃教程、高考资料、乐器课程等，虽然不起眼…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"9iQvNLeO\"\u003e去年因实体店亏损严重，负债近30W，找了很多项目都没有做起来。这8个小项目是我这一年时间亲自测试过，每个都可以做起来的，目前已经把负债解决了，希望能够帮助到大家!\u003c/p\u003e\u003cp data-pid=\"-d8e1eil\"\u003e1、卖word文档\u003c/p\u003e\u003cp data-pid=\"Ugomt1x0\"\u003e我有个朋友，每天去搬人家的文章，然后整理成word，发布到某度文库，并设置价格为1-5块不等，发布了2000多篇，目前每天挣到手里的马内能吃一顿海底捞不成问题。\u003c/p\u003e\u003cp data-pid=\"2vvGS8SI\"\u003e2、做知识变现\u003c/p\u003e\u003cp data-pid=\"nZj89Ynx\"\u003e小红书上面有很多人卖小吃教程、高考资料、乐器课程等，虽然不起眼，但是人家一个月也有好几个W的挣，而且做知识变现可以说几乎没有成本，卖出去都是利润，不会推广的，可以去借鉴一些做知识变现的公众号，就比如说“乐一笔记”，上面会讲到很多这方面的干货。\u003c/p\u003e\u003cp data-pid=\"I-IYZbH9\"\u003e3、卖乐器课程\u003c/p\u003e\u003cp data-pid=\"RAIDq1q5\"\u003e有个上海的小学老师，用下班的时间去小红书上面卖卖钢琴课程，虽说一单只有300，但人家一天也能搞五六单，而且没有任何的成本，够不够香?\u003c/p\u003e\u003cp data-pid=\"8tW_04Kn\"\u003e4、写民间故事\u003c/p\u003e\u003cp data-pid=\"3Ip2Ir0L\"\u003e有很多自媒体上面，民间故事都非常受欢迎，通常阅读量都是几万十几万，文章收益也就比较高了，不会写故事可以借助一些AI工具去写，什么类型的故事都能帮你写出来，现在很多公众号也都自带了AI创作工具，就比如:易课魔方、问东风AIGC、91文案等，上面都带了很多AI模型，能帮你写出各种类型的文案。\u003c/p\u003e\u003cp data-pid=\"WAn_BYTf\"\u003e5、喜马拉雅讲故事\u003c/p\u003e\u003cp data-pid=\"mEM7OsMZ\"\u003e很多人都在喜马拉雅上面听相声，但是你知道吗，很多宝妈也在听上面的睡前小故事，因此，有很多人在喜马拉雅上讲故事，并且设置付费专辑，别看都是9.9，但一年能卖几十万单，你说香不香?\u003c/p\u003e\u003cp data-pid=\"kNvCZP_Y\"\u003e6、搬表情包\u003c/p\u003e\u003cp data-pid=\"eRHsBOkD\"\u003e我不知道大家有没有见过那种公众号，每天可以收藏到很多表情包，那种公众号的文章阅读都非常高，通常都是大几万以上，而表情包类公众号的变现能力也就非常不错，基本都是每个月几千上万打底。\u003c/p\u003e\u003cp data-pid=\"Pwu78MHA\"\u003e7、上传图片\u003c/p\u003e\u003cp data-pid=\"Kdxl1TpO\"\u003e别听有些人说什么千图网、昵图网上传图片能有收益这类误人子弟的方法，那些地方对作品的要求是非常高的，除非你真的是设计师，所以，我们可以选择一些低门槛的地方，比如图虫，但凡你拍的原创照片都可以发布到上面，有人下载你就可以获得相对应的分成。\u003c/p\u003e\u003cp data-pid=\"i0T-ZLlP\"\u003e8、知识星球\u003c/p\u003e\u003cp data-pid=\"l3dvt8R8\"\u003e知识星球上面可以创建付费圈子，你在上面创建付费圈子以后，自己就是圈主了，可以在里面发布一些资源、资料，比如发布小吃资料，发的多了，就会有很多人付费进入你的圈子，每个月也能搞不少。\u003c/p\u003e","is_labeled":false,"visited_count":2341,"favorite_count":97,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1908556956945322353}","attached_info":"CugFCPTbuv3BoPvOvgEQBxoJMjU4MDE2MjgxIMfu2cEGKBkwAUBpSjAKBkl0ZW1DRhIgZG9jX3R5cGU6IEFydGljbGUKaWQ6IDI1NzkxNDc4NQoYACAAOgBiIDFkYTlkMTM0NTA1NTM5OWYyOGFhNDc0MGFkMDcwMmUychMxOTA4NTU2OTU2OTQ1MzIyMzUzqgEJcmVjb21tZW5kwgEgZGJmNmZhMmNkMTZjNjA5MWE2NTRkZDI5ZjY3NTA5OTjyAQoIDBIGTm9ybWFs8gEoCAoSJGYwYjRjMTYwLTAzZTItNDUyYS05MmJlLWUwZjdjNjI0NTI3MfIBBggLEgIxOIICAIgCt7H7zfoykgIgZGJmNmZhMmNkMTZjNjA5MWE2NTRkZDI5ZjY3NTA5OTiaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxl2gIGSXRlbUNG6AIC+gILTk9STUFMX0ZMT1eKAyBhOGUzMjgzYmM0ZTg0ZjcxOWNmN2I5ZDhkM2MzYTUzY5oDDQoCdjIQABoFb3RoZXKoA6US2AMA6gMVdGV4dEFsbFNpdGVNdkl0ZW1DRlYy+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATKgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAA4IAMwT+BBQAAAAAAAAAAiQUDFkxeCIPSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUSkAYAoAZpqAYAkgIuCgkyNTgwMTYyODESEzE5MDg1NTY5NTY5NDUzMjIzNTMYByIKSU1BR0VfVEVYVA==","action_card":false},{"id":"106_1750899546.539","type":"feed","offset":106,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899546,"updated_time":1750899546,"target":{"id":"1921259881727530986","type":"answer","url":"https://api.zhihu.com/answers/1921259881727530986","author":{"id":"af4109af6391f1f856d3d90c223794e2","url":"https://api.zhihu.com/people/af4109af6391f1f856d3d90c223794e2","user_type":"people","url_token":"suan-liao-ba-68-50","name":"琳儿英语分享","headline":"分享实用英语口语，英语阅读，语法，技巧等","avatar_url":"https://pic1.zhimg.com/50/v2-ba4c325a1f9dd0b2a02d21af7551a2fe_l.jpg?source=b6762063","is_org":false,"gender":0,"badge":[{"type":"identity_people","description":"技能培训行业 从业人员"}],"followers_count":651,"is_following":false,"is_followed":false},"created_time":1750844051,"updated_time":1750844051,"voteup_count":0,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"40388473","type":"question","url":"https://api.zhihu.com/questions/40388473","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://picx.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"晨读英文的时候读什么内容好？","created":1455499988,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[919],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://pic1.zhimg.com/50/v2-420c6c1ad2d960ddf7156bb19fb7c3af_720w.jpg?source=b6762063","excerpt":"英语晨读美文4篇 美文阅读1 学会拒绝，你才能走得更远If you don't want to do something for somebody, you should manage to say \"No.\" in the first place. 如果你不想为一个人做什么事情，你一开始就应该说“不。” The moment you give in, you've given them the impression that they can use your time as long as they try hard enough. 你一旦让步，就会让他们以为只要多做些努力就能占用你的时间。 No \"Let me see.\" …","excerpt_new":"英语晨读美文4篇 美文阅读1 学会拒绝，你才能走得更远If you don't want to do something for somebody, you should manage to say \"No.\" in the first place. 如果你不想为一个人做什么事情，你一开始就应该说“不。” The moment you give in, you've given them the impression that they can use your time as long as they try hard enough. 你一旦让步，就会让他们以为只要多做些努力就能占用你的时间。 No \"Let me see.\" …","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"kzEwbt-n\"\u003e\u003cb\u003e英语晨读美文4篇\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"GBsaB5D0\"\u003e\u003cb\u003e美文阅读1 学会拒绝，你才能走得更远\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"74eODb8N\"\u003eIf you don\u0026#39;t want to do something for somebody, you should manage to say \u0026#34;No.\u0026#34; in the first place.\u003c/p\u003e\u003cp data-pid=\"NmcmvJbW\"\u003e如果你不想为一个人做什么事情，你一开始就应该说“不。”\u003c/p\u003e\u003cp data-pid=\"6Mpz5VcA\"\u003eThe moment you give in, you\u0026#39;ve given them the impression that they can use your time as long as they try hard enough.\u003c/p\u003e\u003cp data-pid=\"7XJmloaY\"\u003e你一旦让步，就会让他们以为只要多做些努力就能占用你的时间。\u003c/p\u003e\u003cp data-pid=\"ml68OSDe\"\u003eNo \u0026#34;Let me see.\u0026#34;\u003c/p\u003e\u003cp data-pid=\"AUhz30eD\"\u003e不要说“我看看。”\u003c/p\u003e\u003cp data-pid=\"2FaYdjlR\"\u003eNo \u0026#34;Maybe.\u0026#34;\u003c/p\u003e\u003cp data-pid=\"vJfcoR0z\"\u003e不要说“可能吧。”\u003c/p\u003e\u003cp data-pid=\"5m7_iMpj\"\u003eJust \u0026#34;No.\u0026#34; and a \u0026#34;Sorry.\u0026#34; if you want to be polite.\u003c/p\u003e\u003cp data-pid=\"o7l8Kr_9\"\u003e直接说“不。”，如果你想表现得礼貌点就加一个“抱歉。”\u003c/p\u003e\u003cp data-pid=\"Vb4a73J4\"\u003eOr you can ask \u0026#34;How much?\u0026#34; if it suits you.\u003c/p\u003e\u003cp data-pid=\"lMjxFb5O\"\u003e或者，如果你觉得可行，也可以问问“你愿意出多少钱？”\u003c/p\u003e\u003cp data-pid=\"TbycFSR8\"\u003eTime is the most precious thing that we have.\u003c/p\u003e\u003cp data-pid=\"DZu_PU0j\"\u003e时间是我们所拥有的最珍贵的东西。\u003c/p\u003e\u003cp data-pid=\"v6Yusjdk\"\u003eOnce you lose a piece of it, you can never get it back.\u003c/p\u003e\u003cp data-pid=\"tC4ApJAq\"\u003e一旦你失去了一段时间，你永远不可能重新得到它。\u003c/p\u003e\u003cp data-pid=\"ZhfIUJPU\"\u003eAnd you have the freedom to decide how to use each piece of it.\u003c/p\u003e\u003cp data-pid=\"RpqmiuXz\"\u003e而你有权利自由支配自己的每一份时间。\u003c/p\u003e\u003cp data-pid=\"-xQvOYZl\"\u003eSo if someone want a piece of your time, they have to be worth it, either by being someone you truly care or by providing enough payment.\u003c/p\u003e\u003cp data-pid=\"-D0w_8s9\"\u003e所以，如果有人想动用你的时间，那他们最好配得上它：要么，他们是你真正关心的人；要么，他们愿意付足够多的报酬。\u003c/p\u003e\u003cp data-pid=\"xJP-2xp2\"\u003eLet me say it once again.\u003c/p\u003e\u003cp data-pid=\"5MGEblJ_\"\u003e让我再说一遍。\u003c/p\u003e\u003cp data-pid=\"Z12UuCb1\"\u003eTime is the most precious thing that we have. Learn to refuse when people ask for your time.\u003c/p\u003e\u003cp data-pid=\"BwAPqubT\"\u003e时间是我们所拥有的最珍贵的东西。当有人想要你的时间的时候，学会拒绝。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-3ed2b3dff49058c6c3cb520b4d5c8e49_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1732\" data-rawheight=\"972\" data-original-token=\"v2-0b25c3aa18b80a0a631644a7605aaa78\" class=\"origin_image zh-lightbox-thumb\" width=\"1732\" data-original=\"https://picx.zhimg.com/v2-3ed2b3dff49058c6c3cb520b4d5c8e49_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"fDmvPLDr\"\u003e\u003cb\u003e美文阅读2 写给自己的一封信\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"ScSoRKSr\"\u003eDear Myself,\u003c/p\u003e\u003cp data-pid=\"IdtKVjLv\"\u003e亲爱的自己，\u003c/p\u003e\u003cp data-pid=\"_1vv7f6j\"\u003eLife is scary. One day you wake up feeling like you can take over the world, and the next day you wake up feeling like all you want to do is to lay in bed and hide from everything.\u003c/p\u003e\u003cp data-pid=\"Ns1Vz3Xv\"\u003e生活让人胆寒。一天醒来你还觉得自己能够接管世界，隔天起来你就只想在床上躺着，不理世事。\u003c/p\u003e\u003cp data-pid=\"jlMHiDpY\"\u003ePeople walk into your life, grab your hand, and lead you the most beautiful path you\u0026#39;ve known, but sometimes the same people let go of your hand without warning, and you become stranded at a place where you never thought you\u0026#39;ld feel lost.\u003c/p\u003e\u003cp data-pid=\"d--Sv12t\"\u003e有人走入你的生活，抓着你的手，向你展示已知的美好人生之路。但同样是这个人，在毫无预警的情况下放开你的手，你被困在当下，之前从未想过人生会如此迷惘。\u003c/p\u003e\u003cp data-pid=\"T-rlwMiH\"\u003eLet\u0026#39;s be honest, sometimes everything is going so great and it seems like nothing could go wrong , but right when you begin to think that, something so horrible comes crashing down and all of a sudden more problems come ricocheting around you and you just feel so hopeless cause it\u0026#39;s so bad...\u003c/p\u003e\u003cp data-pid=\"2DNfyWR4\"\u003e诚然，有时候一切看起来顺风顺水，当你这样认为的时候，一些不好的事情就会接踵而至（福兮祸所伏），一下让你很难接受，万念俱灰。\u003c/p\u003e\u003cp data-pid=\"qIdmv8ZS\"\u003eIt\u0026#39;s so hard to understand why such things happen in life,and I personally wish I had an answer to that \u0026#34;why?\u0026#34; you always ask yourself , but all I can say that is no matter how hard life gets,you have to keep going. The life around you will never stop going on.\u003c/p\u003e\u003cp data-pid=\"KyYgQ2O3\"\u003e生活真是让人费解，一念天堂，一念地狱。我真心希望自己能领悟生活为什么会这样。但无论生活怎样艰难，你也只能熬着。生活一直在继续。\u003c/p\u003e\u003cp data-pid=\"6Zxf5ehw\"\u003eI\u0026#39;ll be honest and say that sometimes I feel a little bit worried and all I can think is \u0026#34;will I be able to keep up? What if everything goes too fast?\u0026#34; But I realized that being scared and living with that burden of running away from problems only slow me down even more.\u003c/p\u003e\u003cp data-pid=\"BymCAAu_\"\u003e老实说我总是有点担心，我所想的就是“我还能不能坚持？如果一切都这样飞速发展着?”但我意识到，老是这样战战兢兢，回避问题，反而让自己更加落后。\u003c/p\u003e\u003cp data-pid=\"ciTaGN19\"\u003eAnd I\u0026#39;ve come to the point where I believe that because life never stops, I shouldn\u0026#39;t stop either. It\u0026#39;s okay to take break and to give yourself time to heal, but you cannot give up and you cannot quit.\u003c/p\u003e\u003cp data-pid=\"AWlwJmTb\"\u003e而且关键在于生活不止，追求不息。停下来休息一会儿，或是抽点时间自愈下没有问题，但是你不能放弃，一定不要放弃。\u003c/p\u003e\u003cp data-pid=\"mhPKUhvj\"\u003eKeep positive, fill you heart with gratitude for what you already have, and always remind to humble and true to who you are！\u003c/p\u003e\u003cp data-pid=\"Oaqrgu8V\"\u003e你能做的是，保持积极乐观，常怀感恩之心，保持谦卑，活出真我！\u003c/p\u003e\u003cp data-pid=\"mw5btf8g\"\u003eWith Love,Your Soul\u003c/p\u003e\u003cp data-pid=\"mmKLXRqt\"\u003e爱你的,你的灵魂\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-9583b823d1515a8d820fb548fb73b8fd_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1880\" data-rawheight=\"1055\" data-original-token=\"v2-d14943f11606bab0cf5705e41c19c6fc\" class=\"origin_image zh-lightbox-thumb\" width=\"1880\" data-original=\"https://picx.zhimg.com/v2-9583b823d1515a8d820fb548fb73b8fd_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"QYQWa0Xa\"\u003e\u003cb\u003e美文阅读3  艰难，才是生活的常态\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"ga7024Hv\"\u003eThrough the cold winter wasteland a man trudged, leaning into the harsh wind which spitefully tried to force him back. He was covered from head to toe in layers of thick clothing, layers of protection against the harsh environment. On he fought, searching, searching. He was working so hard. This must be the way.\u003c/p\u003e\u003cp data-pid=\"fbhXLg-g\"\u003e在寒冷的冬季，以为男子在荒地中的前行举步维艰，狂风肆虐几乎要将他吹倒。他从头到脚包裹着厚厚的衣物，用来抵御这恶劣的环境。他不遗余力的寻找着，寻找着，一定有办法。\u003c/p\u003e\u003cp data-pid=\"2PiUEANV\"\u003eIn the distance he saw what looked like steam rising out of the ground. It rose a few feet and then was quickly whipped away by the biting wind. He altered his course and turned towards the steam, gaining some blessed relief as he turned his chapped face out of the gale.\u003c/p\u003e\u003cp data-pid=\"6Ht2GhwH\"\u003e远远地他看见有蒸汽从地下往上冒，只有短短几英尺。接着狂风拖曳着他往回，他改变了方向超蒸汽那里出发，感谢上天他开裂的脸蛋不再曝露在狂风之中。\u003c/p\u003e\u003cp data-pid=\"FO1BK_aB\"\u003eAs he got closer, he thought he could make out voices. Their tone was unfamiliar to him - musical, relaxed and warm - their melody enticed him closer. Finally he got close enough to peer through the mist.\u003c/p\u003e\u003cp data-pid=\"Zd1TvWii\"\u003e他渐渐走近，发现自己可以说话了。对方的声音听起来很陌生，但是如同音乐一般让人感觉到放松，温暖。他们的旋律吸引着他不断靠近。最后，在迷雾中他靠近了他们。\u003c/p\u003e\u003cp data-pid=\"RCEIpmsF\"\u003eThere before him was a remarkable sight. In the midst of the frozen wasteland, cut into the ground was a large pool. Several people were in the pool, they seemed to be floating easily without any effort. As they saw him approach, a woman called out to him.\u003c/p\u003e\u003cp data-pid=\"XLcEk-Ml\"\u003e在旅人面前呈现出一个奇妙的场景。在刺骨寒冷的湿地中，地面上有这么大一个池塘。有不少人在其中，毫不费力地在池中飘来飘去。在旅人靠近之时，一位女士叫住了他。\u003c/p\u003e\u003cp data-pid=\"HjNCNNSX\"\u003e\u0026#34;Come in here. It is lovely and warm. You can just lie back and relax,\u0026#34; said the woman.\u003c/p\u003e\u003cp data-pid=\"rleI_1Xb\"\u003e“进来吧，这里温暖舒适。你可以躺下放松。”女士说道。\u003c/p\u003e\u003cp data-pid=\"BdJexYnD\"\u003e\u0026#34;I can\u0026#39;t. There are no steps.\u0026#34; The man replied.\u003c/p\u003e\u003cp data-pid=\"4MfBUF-4\"\u003e“我不能，这里都没有台阶。”旅人答道。\u003c/p\u003e\u003cp data-pid=\"SrShavtT\"\u003e\u0026#34;Just jump in. It really is lovely in here. Come on. Theoure\u0026#39;s plenty of room for another.\u0026#34; Another of the floaters joined in the persuasion.\u003c/p\u003e\u003cp data-pid=\"a-KdXNgu\"\u003e“只要跳进来就好了，这里很棒，快点，还有不少位子呢。”池中其他人也附和说道。\u003c/p\u003e\u003cp data-pid=\"Rutf71-6\"\u003e\u0026#34;But what if I don\u0026#39;t like it, how will I get out? The sides are too high to reach up to.\u0026#34;\u003c/p\u003e\u003cp data-pid=\"IJPs2ZjA\"\u003e“但是如果我不喜欢这里了，怎么出来？边缘太高很难出来。”\u003c/p\u003e\u003cp data-pid=\"J534ZUxf\"\u003e\u0026#34;Believe me, you won\u0026#39;t want to get out. Come on. It is so good in here.\u0026#34;\u003c/p\u003e\u003cp data-pid=\"24RkK5Lp\"\u003e“相信我，你不会想要出来的，快点进来，这里真的很好。”\u003c/p\u003e\u003cp data-pid=\"b_MVTmLC\"\u003eBut he decided not to jump in. And that was a wise decision.\u003c/p\u003e\u003cp data-pid=\"xGNJdBqf\"\u003e但他没有往下跳。而这是个明治的选择。\u003c/p\u003e\u003cp data-pid=\"a0nyxeh3\"\u003eThere are many things and people in life like this pool. They promise you life can be easy.\u003c/p\u003e\u003cp data-pid=\"QwXhgWXA\"\u003e生活中有很多人和事像这个水池一样，他们像你承诺无忧无虑。\u003c/p\u003e\u003cp data-pid=\"4hfAv7xZ\"\u003eBut in fact, they are traps that you can\u0026#39;t come back from.\u003c/p\u003e\u003cp data-pid=\"Jwz8sbme\"\u003e但实际上，他们都是能让你万劫不复的陷阱。\u003c/p\u003e\u003cp data-pid=\"X76VKkCs\"\u003eLife is hard. Stay wise. And fight.\u003c/p\u003e\u003cp data-pid=\"prx7yz5g\"\u003e生活是艰难的，请保持睿智，并战斗下去。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-f90cd24b982d14c1ba5802dbff1065d4_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1880\" data-rawheight=\"1253\" data-original-token=\"v2-d90dff49ed67eabf38e923802dbc33fb\" class=\"origin_image zh-lightbox-thumb\" width=\"1880\" data-original=\"https://pica.zhimg.com/v2-f90cd24b982d14c1ba5802dbff1065d4_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"Xet0BbuY\"\u003e\u003cb\u003e美文阅读4 磨难让我成为了更好的自己\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"HaQ_QDqM\"\u003eMother Teresa of Calcutta was often quoted as saying: \u0026#34;I know God will not give me anything I can\u0026#39;t handle. I just wish He didn\u0026#39;t trust me so much.\u0026#34; Over the years I have found myself often feeling the same way. I have had a blessed life, but it has never been an easy one.\u003c/p\u003e\u003cp data-pid=\"a5lelqUA\"\u003e加尔各答的特蕾莎修女有句经常被引用的话：“我知道上帝不会给我无法完成的事情，我只希望他不要那么信任我。”在过去的几十年中我也有同样的感受，生活虽然蒙福，但却不易。\u003c/p\u003e\u003cp data-pid=\"Ck-dhoU0\"\u003eIn my 45 years I have had to deal with sickness, injuries, accidents, and pain. I have had to deal with years of poverty and financial struggles. I have had to deal with my sons\u0026#39; mental handicaps, the death of loved ones, and being separated from those I love. I have had to deal with anger, betrayal, confusion, depression and at times even despair.\u003c/p\u003e\u003cp data-pid=\"QCI_YaRq\"\u003e在45年的岁月中我经历了疾病、伤害、事故、痛苦。除了连年的贫困和经济上的窘迫，我还要只顾上智力有障碍的儿子，与所爱之人天人永隔、被迫分离。还要面对悲愤、背叛、困惑、压抑甚至还有时不时的绝望。\u003c/p\u003e\u003cp data-pid=\"gNMMvrmH\"\u003eLike so many before me I have asked the question: Why do bad things happen to good people? It is a question that has been asked throughout the ages. Books have even been written about it.\u003c/p\u003e\u003cp data-pid=\"7fpRvp8l\"\u003e像许多前人一样，我也不禁要问：“为什么好人要遭受这么多的劫难？”这个问题被追问已久，书上也没有答案。\u003c/p\u003e\u003cp data-pid=\"t06nB3eG\"\u003eWhen I remember all that I have faced and gone through in this life, I realize that God has used all of it to eventually make me better. With pain came empathy. With grief came healing. With frustration came patience.\u003c/p\u003e\u003cp data-pid=\"pKgSexry\"\u003e当我回忆起此生所经历的那些事情，我顿悟到原来上帝这么做是为了最终让我们成为更好的人。痛苦带来同理之心，悲哀带来救助之心，挫折带来静候之心。\u003c/p\u003e\u003cp data-pid=\"P4ArqfL5\"\u003eWith struggles came strength. With sorrow came joy. With anger came love. And with despair came trust in God. Every test, challenge, and tragedy eventually led me to greater goodness, greater love, and greater Oneness with our Heavenly Father.\u003c/p\u003e\u003cp data-pid=\"SfkALMYq\"\u003e化悲愤为力量，苦尽甘来，因恨生爱。绝望中相信上帝的救赎。每一关、每一个挑战，每一个悲剧在上帝的指引下让我更仁慈、博爱，成为更好的人。\u003c/p\u003e\u003cp data-pid=\"U7VlWcte\"\u003eNone of us likes the negative experiences in life. None of us enjoys pain. None of us wants to go through sorrow. May each day here then help you to grow better and more loving.\u003c/p\u003e\u003cp data-pid=\"6B_vqNj2\"\u003e没有人喜欢负面经历，也没有人喜欢痛苦，跟没有人喜欢历经悲伤。愿每天你都能成为更好的自己，更懂得爱。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":17,"thumbnails":["https://pic1.zhimg.com/50/v2-420c6c1ad2d960ddf7156bb19fb7c3af_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-c23b8d64b62120fa0c7db39237474d44_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-2995df69b1852023e52d4feca3c625ce_720w.jpg?source=b6762063"],"favorite_count":0,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921259881727530986}","attached_info":"CucGCPTbuv3BoPvOvgEQBBoJNzMzOTczMzAxIJOF78IGKAAwAEBqSiQKGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDESATAYACAAOgBaBzgzNjI2NTJiIDFkYTlkMTM0NTA1NTM5OWYyOGFhNDc0MGFkMDcwMmUychMxOTIxMjU5ODgxNzI3NTMwOTg2igEINDAzODg0NzOqAQlyZWNvbW1lbmTCASBhZjQxMDlhZjYzOTFmMWY4NTZkM2Q5MGMyMjM3OTRlMvIBCggMEgZOb3JtYWzyASgIChIkMGFjYzI1MzMtZGUxYi00OThmLWFlMDUtYzhlYzRmNDllMGIz8gEGCAsSAjE4ggIAiAK3sfvN+jKSAiBhZjQxMDlhZjYzOTFmMWY4NTZkM2Q5MGMyMjM3OTRlMpoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhhQZXJpb2RJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhhDb250ZW50V2FybVVwQnJlYWtJblJ1bGXaAhlUU19TT1VSQ0VfV0FSTV9VUF9OT1JNQUwx6AIC+gILTk9STUFMX0ZMT1eKAyBhOGUzMjgzYmM0ZTg0ZjcxOWNmN2I5ZDhkM2MzYTUzY5oDDQoCdjIQABoFb3RoZXKoAxHYAwDqAx90ZXh0XzEyaG91cl91bmlmaW5zaGVkX3JlY2FsbGVy+gOsARIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgEEMQNGMwHIiN2Mi0wYjI1YzNhYTE4YjgwYTBhNjMxNjQ0YTc2MDVhYWE3ODotCAMQ2A4YnwgiI3YyLWQxNDk0M2YxMTYwNmJhYjBjZjU3MDVlNDFjMTljNmZjOi0IBBDYDhjlCSIjdjItZDkwZGZmNDllZDY3ZWFiZjM4ZTkyMzgwMmRiYzMzZmKABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAKCm64I/gQUAAAAAAAAAAIkFAxZMXgiD0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFEpAGAKAGaqgGAZICLgoJNzMzOTczMzAxEhMxOTIxMjU5ODgxNzI3NTMwOTg2GAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"107_1750899546.620","type":"feed","offset":107,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750899546,"updated_time":1750899546,"target":{"id":"2986627991","type":"answer","url":"https://api.zhihu.com/answers/2986627991","author":{"id":"3a0b48a5ffd7d24d60fe592c5b6974a9","url":"https://api.zhihu.com/people/3a0b48a5ffd7d24d60fe592c5b6974a9","user_type":"people","url_token":"mbcww","name":"有鸡大米","headline":"微信公众号：有鸡大米","avatar_url":"https://picx.zhimg.com/50/v2-ad41e6240cc06eea974c2d992684959b_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":54929,"is_following":false,"is_followed":false},"created_time":1681640323,"updated_time":1681640323,"voteup_count":66781,"thanks_count":12050,"comment_count":1556,"is_copyable":false,"question":{"id":"22489306","type":"question","url":"https://api.zhihu.com/questions/22489306","author":{"id":"d983dff5085728d0ad43fc96fdc1471a","url":"https://api.zhihu.com/people/d983dff5085728d0ad43fc96fdc1471a","user_type":"people","url_token":"du-bo-wen-58","name":"知乎用户XuoCia","headline":"","avatar_url":"https://pic1.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":14,"is_following":false,"is_followed":false},"title":"我国中央和地方的财政是怎样分配的？","created":1389271037,"answer_count":0,"follower_count":0,"comment_count":2,"bound_topic_ids":[400,10754,31423,37964,40235],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"计划经济没有转移支付这一说。那时候全国上下一本账，年初计委召集各方大佬开个闭门会，把中央和地方当年收支算清楚，填好表格，盖章下发，各级政府负责执行就好，钱不够用就往上打报告，中央统一解决。 但是这种模式本质是吃大锅饭，地方对于干工作是没什么积极性的，干好了给国家，干不好中央给兜底，那还努力啥？ 所以1978年改开以后，中央开始放权，让地方自己搞。当时的政策是财政包干制，年初给你定个指标，超额完成部分按…","excerpt_new":"计划经济没有转移支付这一说。那时候全国上下一本账，年初计委召集各方大佬开个闭门会，把中央和地方当年收支算清楚，填好表格，盖章下发，各级政府负责执行就好，钱不够用就往上打报告，中央统一解决。 但是这种模式本质是吃大锅饭，地方对于干工作是没什么积极性的，干好了给国家，干不好中央给兜底，那还努力啥？ 所以1978年改开以后，中央开始放权，让地方自己搞。当时的政策是财政包干制，年初给你定个指标，超额完成部分按…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"7CO9J2U4\"\u003e计划经济没有转移支付这一说。那时候全国上下一本账，年初计委召集各方大佬开个闭门会，把中央和地方当年收支算清楚，填好表格，盖章下发，各级政府负责执行就好，钱不够用就往上打报告，中央统一解决。\u003c/p\u003e\u003cp data-pid=\"H5hEafx3\"\u003e但是这种模式本质是吃大锅饭，地方对于干工作是没什么积极性的，干好了给国家，干不好中央给兜底，那还努力啥？\u003c/p\u003e\u003cp data-pid=\"KRuF-PLu\"\u003e所以1978年改开以后，中央开始放权，让地方自己搞。当时的政策是财政包干制，年初给你定个指标，超额完成部分按比例上交，剩下的，地方政府可以自行处置。\u003c/p\u003e\u003cp data-pid=\"BHLhq6o9\"\u003e有了激励，地方的劲头一下就上来了，特别是沿海几个省份，依靠地理优势，加上海外老乡积极回国投资，很快就干的风生水起。\u003c/p\u003e\u003cp data-pid=\"3fUH4lkp\"\u003e内陆省份日子过得也不错，当时的国企和银行都是地方政府的下属机构，银行给国企打钱足够多，企业有了钱就扩大生产，地方GDP也就这么上去了。\u003c/p\u003e\u003cp data-pid=\"Yn99Fegp\"\u003e唯一不爽的是中央政府，蛋糕就那么大，地方拿的多，中央手里就捉襟见肘了。\u003c/p\u003e\u003cp data-pid=\"bhb66e_r\"\u003e而改开刚开始百废待兴，铁路、公路、车站，水电气这些基础设施排队等上马，还有水利、救灾、国防，哪哪儿都得花钱，中央没钱只能去找地方政府打秋风，人家还躲着不愿意给，老子追着儿子借钱，也算是很魔幻了。所以当时领导人很不爽，经常感慨，手里没把米，小鸡都不来。\u003c/p\u003e\u003cp data-pid=\"I3rBXroY\"\u003e于是1994年，朱总理主导了分税制改革。\u003c/p\u003e\u003cp data-pid=\"VElxQnHM\"\u003e分税制看起来是税务改革，实际是中央政府集权的改革，核心就是中央拿税收大头，财权集中管理，把事权下放给地方，说白了就是钱我拿，事你干，详细内容有兴趣的同学可以自行百度。 \u003c/p\u003e\u003cp data-pid=\"gbNOwq7W\"\u003e当时甩给地方政府的事权主要是公共事务，类似教育、行政、养老保险、桥梁公路、水电气等市政工程。这都是关系民生的大事，一样都不能马虎。可是这些东西光听名字就知道是吞金兽，留给地方政府那些税收肯定不够，缺的窟窿从哪儿补呢？\u003c/p\u003e\u003cp data-pid=\"IovjaOaY\"\u003e一种办法是老百姓那儿收，那些年地方政府充分发挥想象力，发明了很多税费和摊派，特别是对农村。农村是集体经济，农民不像城镇职工，理论上国家是没有兜底义务的，但是农民的教育、医疗又不能不管，这笔开支只能是羊毛出在羊身上，从农民身上收取。那些年各种税费，像三提五统什么的，压得农民喘不过气来。\u003c/p\u003e\u003cp data-pid=\"rhSRufsB\"\u003e这个明显不是国家的初衷，所以后来中央发文件明令禁止。\u003c/p\u003e\u003cp data-pid=\"i6EZPcRH\"\u003e然后就是土地财政。土地是重要生产要素，而城市土地属于国有，地方政府有支配权。对于常年缺钱的地方政府来说，这颗摇钱树肯定不能放过。所以土地出让金就成了地方政府收入的大头。\u003c/p\u003e\u003cp data-pid=\"v0-5PknV\"\u003e但是土地属于不可再生资源，而城市可用土地本来就不多，卖一块少一块（理论上土地出让有年限，到期可以回收利用，不过也是几十年后的事了）。地方政府为了扩大财源，只能是打农村土地的主意，大量农村集体土地国有化，损害农村利益不说，还侵占了一部分耕地。而且出让土地这事，属于典型的寅吃卯粮，没有可持续性，就跟中东挖石油，山西挖煤炭一样，总量固定，肉眼就能看到天花板，你挖了子孙们就没得挖。\u003c/p\u003e\u003cp data-pid=\"ysMl7qkN\"\u003e偏偏这地方政府卖地上瘾，土地收入比重一路飙升，到后来土地出让收入占GDP的比重常年在5%左右徘徊，要知道，税收占GDP的比重也不过才15%。地方政府如果形成路径依赖，将来继任者无地可卖，地方经济可要断崖式下跌了。所以土地财政也不是长久之计。\u003c/p\u003e\u003cp data-pid=\"OHkg6804\"\u003e还有个办法就是借债。早先地方政府和银行关系好，贷款就是左手倒右手的事，所以上世纪90年代，地方政府从银行借贷搞建设属于常规操作。但是也正因为都在一个锅里吃饭，银行没啥风险意识，有领导条子，顺手就把贷款批了。这就导致政府项目搞砸后，经常会把银行也拖下水，储户取不出去钱就会上街溜达，最后还得中央出面兜底。\u003c/p\u003e\u003cp data-pid=\"GRTjSnM_\"\u003e背了几次锅的中央实在受不了，为了防止地方政府管不住自己的小手，干脆来了个釜底抽薪，出台法律禁止地方政府借债，从源头解决问题。\u003c/p\u003e\u003cp data-pid=\"2u9hRTBv\"\u003e但是经济要发展，城市建设不能停，于是城投公司出场了。\u003c/p\u003e\u003cp data-pid=\"h1MvdyFq\"\u003e城投公司理论上是独立法人，实际是地方政府的代理人。因为有政府背景，可以拿地方公共设施做抵押，所以借款非常容易。\u003c/p\u003e\u003cp data-pid=\"JybktuBH\"\u003e这事实际就是地方政府曲线救国，用城投公司这个马甲变相借债。早先这个操作也没啥问题，城投公司比较规矩，贷款主要是救急。比如咱们常说的土地出让，这事没大家想象那么简单，地方政府收地以后没法直接用，还得拆迁、平整，通水通电，这些都需要启动大笔资金，需要通过城投公司融资后，把生地加工成熟地，具备条件后再卖给开发商，或者开发成旅游景点，后续持续产生收益后再还款，这种贷款一般没什么问题。\u003c/p\u003e\u003cp data-pid=\"rIhJ4JnY\"\u003e但是后来很多城投公司放飞自我，负债经营后，把钱投进了一些不挣钱的项目，收益根本覆盖不了成本。沿海城市还好，经济发达，基础设施的投入很快就能回本。中西部城市就不行了，项目前期缺乏论证，只知道跟风，很多面子工程根本没法产生收益，投资打了水漂，城投公司的贷款也就成了坏账。\u003c/p\u003e\u003cp data-pid=\"09iY7u4u\"\u003e比如今年年初，贵州遵义路桥就被曝出有156亿的贷款还不上，大家都吃了一惊。更惊的是，后来发现全国加起来这部分隐形债务有60多万亿。财政部坐不住了，表态说城投公司的债中央不会管，谁的孩子谁抱走，国家不会给公司兜底，地方政府要自己擦屁股。\u003c/p\u003e\u003cp data-pid=\"eGvz0Nzb\"\u003e中央政府明确表示不负责，以后这城投公司想借钱就难了。\u003c/p\u003e\u003cp data-pid=\"fQBJcNF7\"\u003e但是城市基建缺启动资金也是事实，中央也知道像公路桥梁这些基建工程，前期需要大笔启动资金，而地方政府手里的收入维持当地正常运转都费劲，更没有余粮搞发展了。\u003c/p\u003e\u003cp data-pid=\"b0h5mEef\"\u003e与其让地方政府弄些皮包公司胡搞瞎搞，还不如统一管控起来，所以2014年以后，中央又下文件，把地方政府借贷的口子打开了，不过也不是完全放开，只有基建这种后续能产生收益的项目才能贷款，而且得中央审批。言外之意，借钱搞建设可以，给公务员发工资不行。这种债务我们叫专项债，在美国有个专有名词叫市政债，也就是你得有项目才能贷款，不能空手套白狼。\u003c/p\u003e\u003cp data-pid=\"s0R4GU9F\"\u003e大家应该能看出来，专项债和城投债作用差不多，未来城投公司的债可能会慢慢和专项债合并，中央统一审核，地方操盘，慢慢消化。\u003c/p\u003e\u003cp data-pid=\"sT-HWmyV\"\u003e基建可以举债，那跟民生相关的项目咋办呢？比如教育、医疗和养老，这些短期都不会产生收益，如果地方政府举债将来肯定是烂账还不上，这部分就需要中央政府的转移支付了。\u003c/p\u003e\u003cp data-pid=\"rjcCmONu\"\u003e分税制以后政府的收入主要来源于税收、土地收入、社保基金、国企利润这几部分，而改革以后，中央收入能站到全部政府收入的一半左右，而中央支出只占全部支出的15%左右，多出来的35%基本上都要以转移支付的形式返还给地方政府，主要应用于民生项目。\u003c/p\u003e\u003cp data-pid=\"_OKFWnWk\"\u003e这个返还并不是雨露均沾，一般富裕省份给的少，或者干脆就不给，经济不发达，人口多的省份，那就多补贴一点。本质上是国家从富裕省份多收点钱补贴给穷省。\u003c/p\u003e\u003cp data-pid=\"7-1ZT5wV\"\u003e所以我们看今年的转移支付大头还是四川、河南、湖南这几个人口大省，而主要的费用也投在教育，养老、医疗这些基本保障上。\u003c/p\u003e\u003cp data-pid=\"vlbq-mpI\"\u003e而且今年转移支付金额超过10万亿，创下历史新高。这个一方面是因为疫情，部分省份经济下滑厉害，需要财政支持。另外一方面，投入社保也是为了让大家对未来有稳定预期，毕竟疫情闹了这么一出，大家吓的不敢花钱，钱存在银行趴着不动，经济也就运行不起来了。\u003c/p\u003e\u003cp data-pid=\"XeZ7GGzJ\"\u003e储蓄率太高这事原因有很多，不过主要是大家缺乏安全感，如果社保能跟上，大家不用担心养老和医疗问题，可能就把钱其出来消费了，对经济也是个刺激作用。给大家稳定预期这事，对提振消费来说是指标，比一个劲降低存款利率要有效的多，所以一部分转移支付的钱要补贴给社保。\u003c/p\u003e\u003cp data-pid=\"qRDceoUs\"\u003e除了中央拨款，还有一些非常规操作可以转移支付，比如建立全国统一的土地市场，可以把西部地区农业用地指标变相卖给东部，价钱甚至可以达到房地产用地或者工业用地的价格水平，我在上一篇文章《\u003ca href=\"https://www.zhihu.com/question/22489306#rd\" class=\"internal\" target=\"_blank\"\u003e从狂飙看土地制度变迁\u003c/a\u003e》里说过这事，这个其实也是东部对西部的转移支付，像这种用市场的方式实现财富转移，也是未来共同富裕的一个大趋势。\u003c/p\u003e\u003cp data-pid=\"i7NVc1mg\"\u003e有小伙伴可能要问了，富人补贴穷人，这个算不算劫富济贫？是不是不太公平？\u003c/p\u003e\u003cp data-pid=\"IS71fhF2\"\u003e表面上看，好像是东部在义务帮助西部，其实抛开共同富裕这个大目标不说，东部能够发展少不了西部的支持，从这个意义上说，拿出点钱来补贴西部义不容辞。\u003c/p\u003e\u003cp data-pid=\"82qU6zkO\"\u003e比如沿海能够发达靠的是外贸和加工业，而我们的产品能在国际上有竞争力，最重要的一个原因就是便宜的劳动力，也就是咱们常说人口红利。\u003c/p\u003e\u003cp data-pid=\"roS6vb2T\"\u003e这些农民工从哪儿来的？还不是内部这些人口大省，农民工拿着极低的工资在发达地区打工，创造的受益都归了当地，甚至连社保都不用管。等这些人四五十岁干不动，回村里养老，那时候动用的可是老家的社保基金，相当于东部免费用了劳力，而把福利负担甩给西部，你说这补偿一点是不是合情合理？\u003c/p\u003e\u003cp data-pid=\"-sLKrLE_\"\u003e还有些边疆省份，主要任务是国防，干的都是幕后工作，但有他们的奉献才有安定团结，后方稳固前方才能安心发展，这个从GDP上看不出来，但是中央心里有一本账。\u003c/p\u003e\u003cp data-pid=\"kvH3JUgM\"\u003e还有类似山西这种煤炭大省，这么多年给国家贡献了多少能源？自己可是落了一身病，地底都被挖空了，环境也污染的不成样子。\u003c/p\u003e\u003cp data-pid=\"JRfoQI8F\"\u003e可能大家还不知道，我们国家现在主要能源还是煤炭，占全部能源消费量的一半还多，也正是有山西的付出，发达地区工业用电和民用电才有保障，基础设施扎实经济才能腾飞。你说像山西这种老黄牛，是不是也该在功劳簿上记上一笔？\u003c/p\u003e\u003cp data-pid=\"1uHWteCk\"\u003e还有个事之前不怎么提，我们国家的汇率一直被低估，这是刻意为之，目的是为了保护沿海的外贸企业，毕竟低汇率相当于拉低成本，成本低了产品在国际上才有竞争优势。但是维持低汇率要付出代价，国家需要不断的印发人民币对冲这部分因为外贸盈余进来的美元，货币超发这个通胀的代价可是全国人民承担了。相当于用全国的力量补贴沿海的出口企业，你说这笔帐是不是也得算一算？\u003c/p\u003e\u003cp data-pid=\"qJbj38DK\"\u003e所以发达地区的成绩是全国支援出来的，咱们讨论各省的成绩也得放在全国这个大环境下说，广东如果没有后方的支援，最多也就是个亚洲四小虎的水平，站在风口看起来欣欣向荣，等人口红利耗尽，产业已转移马上就会打回原形，这点可以参考98年金融危机那几个东南亚国家的惨样。\u003c/p\u003e\u003cp data-pid=\"HWVZwcqQ\"\u003e而且没有后方支援，那就只能是给美国当小弟，需要堵抢眼的时候你上，人家吃肉的时候，能不能给你块骨头还得看心情。像日韩，跟着老大混了这么多年，钱倒是没少挣，看起来光鲜亮丽，实际还是个拎包的角色，老大一发飙就跪了。\u003c/p\u003e\u003cp data-pid=\"mtcfkiEf\"\u003e也只有把中国看成一个整体，依靠地理纵深，齐全的产业门类，统一大市场，说话才有底气，个体的存在才能更有意义，从这个意义上来说先富起来那部分人是踩了别人的肩膀，拿出来财富回馈大家也算是羊毛出在羊身上，责无旁贷。\u003c/p\u003e\u003cp data-pid=\"DyrOporA\"\u003e当然了，我们的目标是要缩小地区差异，实现共同富裕，光靠中央转移支付也没法从根本上解决问题。\u003c/p\u003e\u003cp data-pid=\"AruURhcN\"\u003e比如中央统筹容易让一些地方养成等、靠、要的习惯。看过电影《私人定制》的小伙伴应该还记得，老乡找领导走后门，目的就是为了保留贫困县的帽子，有这个证书国家就会包养，各种补贴可能比你努力工作还要挣得多。这就是转移支付的副产品之一，大家都倾向于哭穷，会哭的孩子有奶吃嘛，可能有的地方就会把精力主要放在哭上，反而没了自我奋斗的动力。\u003c/p\u003e\u003cp data-pid=\"8gXaw35y\"\u003e而且如果不从根本上解决产生差距的原因，中央转移支付的费用，有一部分还会流向富省，比如山西拿了教育补贴，花一大笔钱培养出大学生，可能有一大部分都跑发达地区了，相当于转移支付的钱绕了一圈回到原点。\u003c/p\u003e\u003cp data-pid=\"8050ZsLQ\"\u003e老人拿了养老金，可能去大城市跟儿子住，或者把养老金和工资拿出来给儿子付了房子首付，这钱变相给大城市发展做了贡献。\u003c/p\u003e\u003cp data-pid=\"Uo8lvHz8\"\u003e还有医保的支出，老百姓在北京的医院看了病，费用却拿回当地报销，是不是山西的医保给北京贡献了GDP？\u003c/p\u003e\u003cp data-pid=\"ZIaKAkxp\"\u003e这就有点像借钱给穷人，他不会挣钱，四处买买买以后，钱最后还是流向富人的口袋。\u003c/p\u003e\u003cp data-pid=\"moaIKOVs\"\u003e所以如果只靠中央强制转移，发达地区有虹吸效应，迟早会把高端人口和财富都吸引过去，这个转移的效率就要打个折扣。\u003c/p\u003e\u003cp data-pid=\"PJgTv2Mb\"\u003e未来的解决办法，解铃还须系铃人 既然由头是中央和地方权责不匹配，解决办法肯定是往平衡的方向走，适当给地方下放权力，调动积极性。\u003c/p\u003e\u003cp data-pid=\"FiNEa7sS\"\u003e我们国家这两年一直在酝酿税费改革，核心就是用所得税和财产税这种直接税，慢慢替换现在以交易税费为主的间接税，相应的税费会在中央和地方之间重新分配，多向地方倾斜，地方政府手里有稳定收入，也可以好好规划一下自己的发展，这也是调动地方积极性的一种手段。 \u003c/p\u003e\u003cp data-pid=\"E6CvScMI\"\u003e至于有小伙伴担心的“一放就乱“，大概率不会再出现。乱的本质上还是市场化程度不够，各地都有自己的门道，所以有钱就开始瞎折腾。如果全国统一大市场，大家共用一个规则，捞偏门的空间就没了，大家按照市场规律行事，没准也能各自找到突破。\u003c/p\u003e\u003cp data-pid=\"CdydR7Fp\"\u003e此外中央可能会回收一些事权，比如社保和教育。事实上，几乎所有的发达国家，社保和教育都是国家统一负担，这是消除地方差异的根本，这两事如果全国统一，至少能减少人口往大城市聚集的趋势，落后地区留住人才，才有发展的希望。\u003c/p\u003e\u003cp data-pid=\"ZjlMIPsO\"\u003e统一社保和大市场相当于是底座，在这个底座上提供公平公开的竞争环境，通过市场配置资源，在这个基础上给地方政府一个相对宽松的空间，大家各自发掘自己的比较优势，通过市场完成转移支付，应该能达到一个效率和公平的平衡。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":3009164,"favorite_count":71287,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 2986627991}","attached_info":"CoUGCPTbuv3BoPvOvgEQBBoJNTczMTA4Nzk1IIOX76EGKN2JBDCUDEBrSjAKG1RTX1NPVVJDRV9CQVNJQ19JTkZPX1JFQ0FMTBIBMBgAIAA6CnsicmF3IjoiIn1KLwoGSXRlbUNGEh9kb2NfdHlwZTogQW5zd2VyCmlkOiA2MjcyMjUxMjcKGAAgADoAWgcxMTc2NTI1YiAxZGE5ZDEzNDUwNTUzOTlmMjhhYTQ3NDBhZDA3MDJlMnIKMjk4NjYyNzk5MYoBCDIyNDg5MzA2qgEJcmVjb21tZW5kwgEgM2EwYjQ4YTVmZmQ3ZDI0ZDYwZmU1OTJjNWI2OTc0YTnyAQoIDBIGTm9ybWFs8gEoCAoSJDVlNGU4NWRhLTZlOTYtNDZlOC05YTE0LWZjNDlhYTE4NTkzOPIBBggLEgIxOIICAIgCt7H7zfoykgIgM2EwYjQ4YTVmZmQ3ZDI0ZDYwZmU1OTJjNWI2OTc0YTmaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWUmV2aXNpdFZhbHVlV2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxl2gIbVFNfU09VUkNFX0JBU0lDX0lORk9fUkVDQUxM6AIF+gILTk9STUFMX0ZMT1eKAyBhOGUzMjgzYmM0ZTg0ZjcxOWNmN2I5ZDhkM2MzYTUzY5oDDQoCdjIQABoFb3RoZXKoA4zVtwHYAwDqAxFiYXNpY19pbmZvX3JlY2FsbPoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAE1oAQAqAQAsAQAugQGbWFudWFswgQDMTEwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAADAGamtP4EFAAAAAAAAAACJBQMWTF4Ig9I/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRKQBgCgBmuoBgCSAiUKCTU3MzEwODc5NRIKMjk4NjYyNzk5MRgEIgpJTUFHRV9URVhU","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=107\u0026desktop=true\u0026end_offset=107\u0026page_number=19\u0026session_token=1da9d1345055399f28aa4740ad0702e2","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=107\u0026desktop=true\u0026end_offset=107\u0026page_number=19\u0026session_token=1da9d1345055399f28aa4740ad0702e2","totals":0},"fresh_text":"推荐已更新"}
