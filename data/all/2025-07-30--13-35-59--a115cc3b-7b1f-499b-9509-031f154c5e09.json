{"data":[{"id":"66_1753853777.193","type":"feed","offset":66,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853777,"updated_time":1753853777,"target":{"id":"1930366515322019969","type":"article","url":"https://api.zhihu.com/articles/1930366515322019969","author":{"id":"0ba9de7ade5dbc85e1916296fc4692ed","url":"https://api.zhihu.com/people/0ba9de7ade5dbc85e1916296fc4692ed","user_type":"people","url_token":"TianYa.com","name":"陈康成","headline":"聚焦具身智能机器人@公众号：机器觉醒时代","avatar_url":"https://pic1.zhimg.com/50/v2-a340bd7db9d889520bbd960d6caf77df_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"东北大学 工学硕士"}],"followers_count":10633,"is_following":false,"is_followed":false},"title":"2022~2025：2万字讲清谷歌在具身智能基础模型领域的关键布局","image_url":"https://pica.zhimg.com/v2-c5d2af4fa285f936873fdcdaf93ad5e2.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1753016065,"updated":1753016065,"voteup_count":4,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"谷歌在具身智能领域的布局以具身智能大模型为技术基座，通过 端云协同推理、开源工具链下沉、跨形态硬件泛化适配及渐进式场景验证，构建从感知到执行的协同框架，探索具身智能机器人从单一任务工具向通用具身智能体的演进路径。   2022年：谷歌具身智能基础模型关键布局1. 2022年4月，谷歌推出具身智能模型 SayCan尽管大型语言模型（LLMs）能从海量文本中习得丰富知识，但它们缺乏对物理世界的具象认知，也无法观测自身生成内容对物…","excerpt_new":"谷歌在具身智能领域的布局以具身智能大模型为技术基座，通过 端云协同推理、开源工具链下沉、跨形态硬件泛化适配及渐进式场景验证，构建从感知到执行的协同框架，探索具身智能机器人从单一任务工具向通用具身智能体的演进路径。   2022年：谷歌具身智能基础模型关键布局1. 2022年4月，谷歌推出具身智能模型 SayCan尽管大型语言模型（LLMs）能从海量文本中习得丰富知识，但它们缺乏对物理世界的具象认知，也无法观测自身生成内容对物…","preview_type":"default","preview_text":"","column":{"id":"c_1894075213697758199","type":"column","url":"https://api.zhihu.com/columns/c_1894075213697758199","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://picx.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"具身智能机器人","imageUrl":"https://pic1.zhimg.com/v2-8a91b399a563b96cde85da20cbaf69d0_720w.jpg?source=d16d100b","comment_permission":"private","intro":"具身智能机器人前沿技术与产业信息分享","updated":1746622360,"is_following":false},"content":"\u003cp data-pid=\"AytN1uVz\"\u003e谷歌在具身智能领域的布局以具身智能大模型为技术基座，通过\u003cb\u003e\u003cu\u003e端云协同推理、开源工具链下沉、跨形态硬件泛化适配及渐进式场景验证\u003c/u\u003e\u003c/b\u003e，构建从感知到执行的协同框架，探索具身智能机器人从单一任务工具向通用具身智能体的演进路径。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-467c4681eca1ae733d00a92304f26166_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"948\" data-original-token=\"v2-467c4681eca1ae733d00a92304f26166\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-467c4681eca1ae733d00a92304f26166_r.jpg\"/\u003e\u003cfigcaption\u003e谷歌具身智能基础模型领域关键布局（机器觉醒时代制表）\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e\u003cb\u003e2022年：谷歌具身智能基础模型关键布局\u003c/b\u003e\u003c/h2\u003e\u003ch3\u003e\u003cb\u003e1. 2022年4月，谷歌推出具身智能模型 SayCan\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"Ijt9utnE\"\u003e尽管大型语言模型（LLMs）能从海量文本中习得丰富知识，但它们缺乏对物理世界的具象认知，也无法观测自身生成内容对物理过程的实际影响，难以支持在特定具身化实体中进行决策。\u003c/p\u003e\u003cp data-pid=\"pYc8dRir\"\u003e这使得其在将高级指令拆解为机器人可执行的低级指令时，不仅可能产生人类眼中荒谬甚至滑稽的错误，更可能在特定物理场景中生成逻辑混乱或存在安全风险的指令解析。\u003c/p\u003e\u003cp data-pid=\"SEBWRDSQ\"\u003e如下图所示：当具备\u0026#34;抓取海绵\u0026#34;、\u0026#34;移动到餐桌\u0026#34;等技能的厨房机器人收到\u0026#34;饮料洒了，能否帮忙清理？\u0026#34;的请求时，语言模型可能给出看似合理却无法执行的方案——例如建议\u0026#34;使用吸尘器清理\u0026#34;，但该方案在场景中无吸尘器，或具身智能体不具备操作吸尘器的能力时，会完全失效，因为具身智能体的本体能力范围通常是固定且有限的。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-e7af45ea8975bb4d2a27982f7ed88884_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"319\" data-original-token=\"v2-e7af45ea8975bb4d2a27982f7ed88884\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-e7af45ea8975bb4d2a27982f7ed88884_r.jpg\"/\u003e\u003cfigcaption\u003eSayCan模型通过预训练技能的价值函数实现语言模型现实落地\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"GzmZX7Hp\"\u003e\u003cb\u003e1）具身智能模型—— SayCan\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"QGWNJt-N\"\u003e通过提示词工程，大型语言模型或许能够将高级指令分解为子任务，但如果脱离机器人自身能力以及机器人与环境当前状态的语境，它就无法完成这种任务的分解。\u003c/p\u003e\u003cp data-pid=\"hKgR6ytI\"\u003e因此，需要明确将高级指令分解为一系列可用的低级技能。其中，一种方法是精心设计提示词，这是一种引导语言模型产生特定响应结构的技术。提示词工程会在模型的上下文文本（即“提示词”）中提供示例，明确指定任务以及模型需要模仿的响应结构。\u003c/p\u003e\u003cp data-pid=\"_v7FYrDV\"\u003e然而，这并不足以将输出完全限制在具身智能体可执行的基本技能范围内，实际上，它有时会生成不可执行的动作，或生成的语言格式难以解析为独立步骤。\u003c/p\u003e\u003cp data-pid=\"4bsX61j9\"\u003e基于此，2022年4月，谷歌提出SayCan模型，旨在从大型语言模型（LLMs）中提取知识并将其应用到实际物理任务的落地执行中。其中，大型语言模型（Say）提供任务层面的落地能力，用于确定有助于实现高级目标的有效动作；而习得的可供性函数（Can）提供世界层面的落地能力，用于判断计划中哪些动作（或计划）是可执行的。\u003c/p\u003e\u003cp data-pid=\"aE8hZSfC\"\u003e具体来说，采用强化学习来为各个技能学习价值函数，该函数通过可供性表征物理世界的可行操作空间；随后，再将这些技能的文本标签作为潜在响应，由语言模型对其进行评分。\u003c/p\u003e\u003cp data-pid=\"hZd5DJ6E\"\u003e\u003cb\u003e因此，技能和语言模型相结合进而形成了一种共生关系：\u003c/b\u003e技能及其价值函数可充当语言模型的“手和眼”，而语言模型则提供关于如何完成任务的高层级语义知识。\u003c/p\u003e\u003cp data-pid=\"mj0RpvLY\"\u003e通过预训练技能实现现实世界具身化 —— 这些技能将约束模型仅生成既可行又符合场景的文本动作指令，从而让机器人等具身智能体能够遵循高级文本指令 —— 大语言模型负责提供执行复杂时序扩展指令的流程知识，而技能关联的价值函数则构建起连接知识与物理环境的关键具身化桥梁。\u003c/p\u003e\u003cp data-pid=\"57px-v8w\"\u003e除了让大型语言模型单纯解读指令外，还可以借助它对“单个技能在推进高级指令完成过程中所起作用的可能性” 进行评分 —— 若每个技能都有一个可供性函数（如习得的价值函数），用于量化其从当前状态成功执行的可能性，那么该函数的数值便可用于对技能的可能性进行加权。\u003c/p\u003e\u003cp data-pid=\"2hjUmctx\"\u003e通过这种方式，大型语言模型会给出“每个技能对完成指令的贡献概率”，而可供性函数会给出 “每个技能成功执行的概率”—— 将两者结合，就能得到 “每个技能成功完成指令的概率”。\u003c/p\u003e\u003cp data-pid=\"_AckoIUG\"\u003e\u003cb\u003e另外，SayCan 还具有可解释性特点：\u003c/b\u003e模型不仅输出生成式响应，还会给出多种可能响应的概率分布。下图展示了其核心机制：任务集（定义机器人低级策略可执行的技能）与提示词工程（提供计划示例及人机对话格式约束）共同作用，将大型语言模型（LLM）的输出约束为可执行的技能序列。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-8a64e6bb33d94eb40307e4e81528ef67_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"656\" data-original-token=\"v2-8a64e6bb33d94eb40307e4e81528ef67\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-8a64e6bb33d94eb40307e4e81528ef67_r.jpg\"/\u003e\u003cfigcaption\u003eSayCan模型执行过程示意图\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"je41Ydb4\"\u003e\u003cb\u003e2）在机器人系统中执行SayCan\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"_uHeBrWy\"\u003e\u003cb\u003ea. 语言条件型机器人控制策略\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"E1D6qxb7\"\u003e为实例化SayCan框架，需提供一组技能组件，每个组件包含\u003cb\u003e\u003cu\u003e策略模块、价值函数及简短语言描述\u003c/u\u003e\u003c/b\u003e（例如\u0026#34;拾取易拉罐\u0026#34;）。\u003c/p\u003e\u003cp data-pid=\"sLTZejBU\"\u003e在谷歌的实施方案中，各独立技能的训练采用两种方法：\u003cb\u003e遵循BC-Z方法的图像行为克隆，或采用MT-Opt的强化学习。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"GBiXY0x_\"\u003e无论技能策略如何获取，均使用基于时序差分备份（TD backups）训练的价值函数作为该技能的功能可供性模型。虽然当前数据收集阶段显示行为克隆策略成功率更高，但强化学习策略提供的价值函数作为关键抽象层，能将控制能力转化为场景语义理解。\u003c/p\u003e\u003cp data-pid=\"7Er449t-\"\u003e为降低多技能训练成本，谷歌分别采用\u003cb\u003e多任务行为克隆\u003c/b\u003e与\u003cb\u003e多任务强化学习方案\u003c/b\u003e：通过以语言描述为输入条件的多任务策略模型，替代为每个技能单独训练策略和价值函数的传统模式。\u003c/p\u003e\u003cp data-pid=\"yE66b535\"\u003e为了让策略以语言为条件，谷歌采用了一个预训练的大型句子编码器语言模型。在训练过程中，冻结该语言模型的参数，并使用通过输入每个技能的文本描述生成的嵌入向量。这些文本嵌入向量被用作策略和价值函数的输入，用于指定应执行的技能。\u003c/p\u003e\u003cp data-pid=\"UQ0-kYC7\"\u003e由于用于生成文本嵌入向量的语言模型与用于规划的语言模型不一定相同，因此SayCan 能够灵活运用不同的语言模型 —— 这些模型分别适用于不同的抽象层级，比如有的擅长理解涉及多个技能的规划，有的则更适合细致地表达特定技能。\u003c/p\u003e\u003cp data-pid=\"UVlOz00K\"\u003e\u003cb\u003eb. 训练低级技能\u003c/b\u003e \u003c/p\u003e\u003cp data-pid=\"fUtC0xXL\"\u003e谷歌分别采用\u003cb\u003e\u003cu\u003e行为克隆（BC）和强化学习（RL）\u003c/u\u003e\u003c/b\u003e的策略训练流程，以获得受语言条件约束的策略和价值函数。\u003c/p\u003e\u003cp data-pid=\"M5jFFCDl\"\u003e\u003cb\u003e行为克隆（BC）策略：\u003c/b\u003e谷歌以BC-Z为基础，采用了类似的策略网络架构。该架构的训练中，连续动作组件采用均方误差（MSE）损失函数，离散动作组件采用交叉熵损失函数，且每个动作组件的权重相同。训练过程中使用了标准的图像增强手段（随机调整亮度和对比度）以及随机裁剪。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-9c9ae48dee2aaf80db89a54deed97041_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"570\" data-original-token=\"v2-9c9ae48dee2aaf80db89a54deed97041\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-9c9ae48dee2aaf80db89a54deed97041_r.jpg\"/\u003e\u003cfigcaption\u003eBC策略中的网络架构\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"fWExOSyF\"\u003e\u003cb\u003e强化学习（RL）策略：\u003c/b\u003e谷歌在日常机器人模拟器中使用MT-Opt ，并结合 RetinaGAN 的模拟到现实迁移技术。通过利用模拟演示提供初始成功案例来提升模拟策略的性能，然后通过在线数据收集，持续改进强化学习效果。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-513af3512b0fb97df62a49478a44f66b_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"633\" data-original-token=\"v2-513af3512b0fb97df62a49478a44f66b\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-513af3512b0fb97df62a49478a44f66b_r.jpg\"/\u003e\u003cfigcaption\u003eRL策略中的网络架构\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"PKsFggue\"\u003e\u003cb\u003e备注：\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"-dPWSRoJ\"\u003e\u003cb\u003e1）采用稀疏奖励函数：\u003c/b\u003e若语言指令在一个回合结束时被成功执行，奖励值为1.0；否则为0。语言指令的执行成功与否由人工评定 —— 评定者会观看机器人执行技能的视频以及给定的指令。如果三名评定者中有两名认为技能已成功完成，则该回合被标记为正向奖励。\u003c/p\u003e\u003cp data-pid=\"Y_MuAaYy\"\u003e\u003cb\u003e2）策略动作空间包括：\u003c/b\u003e末端执行器姿态的六个自由度、夹爪的开合指令、机器人移动基座的x-y 位置和偏航角增量，以及终止动作。\u003c/p\u003e\u003cp data-pid=\"WdpJk_-K\"\u003e\u003cb\u003e3) SayCan的局限性\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Zr469Ukh\"\u003e尽管SayCan 方法为语言模型与智能体行动能力的结合提供了可行路径，但它仍存在诸多局限性。\u003c/p\u003e\u003cp data-pid=\"U3ErqoeS\"\u003e\u003cb\u003e首先，\u003c/b\u003e该方法继承了大型语言模型（LLMs）的固有局限，尤其体现在对训练数据分布的强依赖上 —— 若训练数据中缺乏特定场景、指令或技能的样本，语言模型可能无法生成合理的技能规划，甚至会输出与实际需求脱节的决策建议。\u003c/p\u003e\u003cp data-pid=\"Ev9zWRHp\"\u003e\u003cb\u003e其次，\u003c/b\u003e尽管SayCan 支持用户通过自然语言指令与智能体交互，但其性能的核心瓶颈在于底层技能库的覆盖范围与执行精度：若技能库未包含完成任务所需的关键动作（如特定场景下的抓取姿势、精细操作步骤），即便语言模型规划逻辑严密，智能体也无法将规划转化为有效行动。\u003c/p\u003e\u003cp data-pid=\"kcB0SbLs\"\u003e\u003cb\u003e此外，\u003c/b\u003e当前系统缺乏对技能执行过程的实时反馈与动态调整机制：当个别技能在语言模型评估为“高价值” 的情况下实际执行失效时（如预期抓取物体却未成功），系统难以快速修正执行路径。尽管通过优化语言模型的提示策略可能在一定程度上缓解这一问题，但复杂场景下的鲁棒性仍待提升。\u003c/p\u003e\u003cp data-pid=\"v6czIr9V\"\u003e\u003cb\u003e同时，\u003c/b\u003eSayCan在复杂任务的长时序规划上存在短板。对于需要多步骤协同的任务（如 “整理桌面并将文件放入抽屉”），语言模型可能难以将自然语言指令分解为连贯的技能序列，容易出现步骤遗漏或逻辑冲突（如先关闭抽屉再试图放入文件），导致任务执行中断。\u003c/p\u003e\u003cp data-pid=\"2nR1wbPW\"\u003e\u003cb\u003e另外，\u003c/b\u003e系统对环境动态变化的适应性较弱。若任务执行过程中环境发生未预期的改变（如物体位置移动、突发障碍物出现），预先规划的技能序列可能不再适用，而语言模型难以基于实时环境反馈快速更新规划，导致智能体陷入“规划 - 执行” 脱节的困境。\u003c/p\u003e\u003cp data-pid=\"_m6Kxt0J\"\u003e\u003cb\u003e最后，\u003c/b\u003e安全性校验机制的缺失也是重要局限。语言模型可能推荐存在潜在风险的技能（如抓取易碎品时采用不当力度），但系统缺乏对技能安全性的前置评估，可能引发物体损坏或环境干扰等问题。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e\u003cb\u003e2. 2022年5月：谷歌发布通用具身智能体Gato\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"22gJ8QtG\"\u003e2022年5月，DeepMind发布通用具身智能体 Gato。它是一个集多模态、多任务、多具身特性于一体的通用智能体，其核心架构采用了包含 11.8 亿参数的Transformer序列模型。\u003c/p\u003e\u003cp data-pid=\"iyqz9rdz\"\u003e\u003cb\u003e核心设计：\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"LThw22dc\"\u003e继承LLM的Transformer序列建模范式，通过将跨模态数据序列化（图像分块、动作离散化）扩展至物理交互领域；基于广泛的多模态数据训练（涵盖图像、文本、本体状态感知、关节扭矩、按钮操作等），赋予模型对离散/连续观测与动作的泛化处理能力。\u003c/li\u003e\u003cli data-pid=\"HOADVGPD\"\u003e通过使用一组具有相同权重的单一神经网络，可处理不同具身形态（如机械臂、仿生机器人）的多源传感数据，实现跨场景感知与动作生成。\u003c/li\u003e\u003c/ul\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-dff000d4d0275e4d58afa407e144a149_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"716\" data-original-token=\"v2-dff000d4d0275e4d58afa407e144a149\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-dff000d4d0275e4d58afa407e144a149_r.jpg\"/\u003e\u003cfigcaption\u003e通用智能体Gato可适配不同具身形态\u003c/figcaption\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"yCcRc-EA\"\u003e\u003cb\u003e1）基础模型训练数据\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"q24jHmLL\"\u003e\u003cb\u003ea. 控制任务数据（占比 85.3%）：\u003c/b\u003e包含游戏交互（如Atari游戏按键序列）、机器人操作（真实机械臂关节力矩、本体状态感知数据）以及导航与规划任务（如Meta-World中的机械臂操控、BabyAI中的3D导航）。这些数据主要来自模拟环境（如MuJoCo、DM Control Suite）和真实机器人平台（如Sawyer机械臂）的轨迹记录，总计覆盖596项任务，占训练数据总量的85.3%。\u003c/p\u003e\u003cp data-pid=\"LDaSIAd7\"\u003e\u003cb\u003eb. 视觉与语言数据（占比 14.7%）：\u003c/b\u003e整合了纯文本语料（对话、网页文本）、图像数据（如ImageNet）及图文配对信息（图像描述任务）。此类数据用于支持图像字幕生成、文本对话等能力，但其占比显著低于控制任务。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-31023e2939ce1af1b5865032b0e883e0_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"532\" data-original-token=\"v2-31023e2939ce1af1b5865032b0e883e0\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-31023e2939ce1af1b5865032b0e883e0_r.jpg\"/\u003e\u003cfigcaption\u003eGato模型训练所使用数据集\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"n1MkBrhH\"\u003e\u003cb\u003e2）模型的训练与部署\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"BNSMoxXH\"\u003e\u003cb\u003ea. 训练阶段\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"q1xL_5Fc\"\u003e来自不同任务和模态的数据被序列化为一个扁平的 Token 序列，分批处理后由 Transformer 神经网络处理。通过掩码机制，损失函数仅应用于目标输出（即文本和各种动作）。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-f8559af554cd116184ca13699dbdfe06_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"522\" data-original-token=\"v2-f8559af554cd116184ca13699dbdfe06\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-f8559af554cd116184ca13699dbdfe06_r.jpg\"/\u003e\u003cfigcaption\u003eGato训练阶段示意图\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"cQlis8W-\"\u003e\u003cb\u003eb. 部署阶段\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"3oJ5ro4i\"\u003e采样的 Tokens 会根据上下文组合成对话回复、图像字幕、按钮操作或其他动作。Gato 使用自回归生成控制策略，预测 t+1 的编码并反解码为动作，与环境交互。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-8e38fe777fb4a1730dec2b599859608e_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"464\" data-original-token=\"v2-8e38fe777fb4a1730dec2b599859608e\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-8e38fe777fb4a1730dec2b599859608e_r.jpg\"/\u003e\u003cfigcaption\u003e将Gato部署为策略的过程示意图\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"D2wiHL_S\"\u003e\u003cb\u003e3）Gato —— 迈向AGI的关键一步\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"8bK5Vsw5\"\u003e\u003cb\u003eGato 首次提出 “通才智能体（Generalist Agent）”概念，将 AI研究从“任务特定优化”转向“跨多任务统一建模”。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"6d7HamDy\"\u003e\u003cb\u003ea. 方法论革新：\u003c/b\u003e扩展LLM的“预训练+微调”范式至物理交互场景，通过数据序列化（图像分块、动作离散化等）实现多模态统一处理，首次验证Transformer在低维连续控制任务（如机械臂操作）中的潜力，但未突破其短期记忆瓶颈。例如，Gato在物理任务中依赖专家演示数据，且未解决长期记忆问题（上下文窗口仅1024 tokens）。\u003c/p\u003e\u003cp data-pid=\"40toH0Gw\"\u003e\u003cb\u003eb. AGI路径探索：\u003c/b\u003e通过参数缩放实验（79M→364M→1.18B）验证模型规模、数据多样性与多任务泛化能力的正相关性，为通用模型研发提供实证依据；\u003cb\u003e但后续研究表明，单纯扩大规模难以提升专业化能力，需结合“通专融合”架构解决任务可持续性问题。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"_tinVN7f\"\u003e\u003cb\u003eGato验证并实现了跨模态统一建模，\u003c/b\u003e将计算机视觉（CV）、自然语言处理（NLP）和机器人控制等不同模态数据（如图像、文本、传感器信号、关节力矩）通过统一的Transformer 序列模型进行处理。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"27Ip0rca\"\u003e\u003cb\u003e\u003cu\u003e数据序列化：\u003c/u\u003e\u003c/b\u003e所有模态数据被转化为Token序列（如文本通过SentencePiece编码，图像分割为 16x16 图块，连续状态和动作通过标量离散化），形成统一输入空间；\u003c/li\u003e\u003cli data-pid=\"FwL9sp5Q\"\u003e\u003cb\u003e\u003cu\u003e模型参数共享：\u003c/u\u003e\u003c/b\u003e同一套11.8 亿参数的模型可同时处理视觉识别、语言对话、机器人操作等任务，避免了传统方法为每个任务单独设计模型的冗余；\u003c/li\u003e\u003cli data-pid=\"w0xj8271\"\u003e\u003cb\u003e\u003cu\u003e动态决策：\u003c/u\u003e\u003c/b\u003e模型基于输入序列的上下文自回归地预测下一个Token，该Token可以自动对应到不同的输出模态（如生成文本回复、游戏按键或机械臂控制指令），实现跨模态无缝交互。\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"cpvvTJbu\"\u003e总而言之，Gato在跨模态整合、任务通用性上的突破，被学界视为迈向通用人工智能的关键一步。\u003c/p\u003e\u003cp data-pid=\"25MiVh7H\"\u003e\u003cb\u003e4）Gato的局限性\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"60EiQMif\"\u003e有业内相关专家指出，Gato模型的最大价值之一是将强化学习、计算机视觉与自然语言处理三大领域深度融合。尽管技术路径上借鉴了既有框架，但能将图像、文本与机器控制等不同模态数据映射至同一表征空间，并用同一套模型参数实现统一表达，已实属难得。\u003c/p\u003e\u003cp data-pid=\"B6QsKZsB\"\u003e但是，Gato总体上依然是数据驱动的方式，且并没有在训练分布外的任务上获得较好效果。同时，训练数据总体上偏向游戏和机器人控制任务，采用有监督的离线训练方式，依赖专家数据，未充分利用强化学习的核心机制——奖励信号和在线交互。例如，其机器人控制任务的成功依赖预训练的专家轨迹，而非通过实时奖励优化策略。\u003c/p\u003e\u003cp data-pid=\"UOQpvQKd\"\u003e这一局限性在后续模型RoboCat中通过自我改进循环（Self-Improvement Loop）得到部分解决。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e3. 2022年10月：谷歌发布机器人Transformer模型RT-1\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"0Gjb_XWj\"\u003e2022 年 10 月，谷歌 DeepMind 发布 RT-1 模型，其训练数据源自 13 台机器人持续 17 个月采集的超 13 万条任务片段。该研究开创性地将Transformer的应用向前推进 —— 将语言和视觉观测到机器人动作的映射视为一个序列建模问题，并利用Transformer学习这一映射。\u003c/p\u003e\u003cp data-pid=\"VLl-gxhB\"\u003e能否借助多样化的机器人任务数据，训练出统一且强大的多任务骨干模型，使其具备对\u003cb\u003e\u003cu\u003e新任务、操作环境及物体的零样本泛化能力\u003c/u\u003e\u003c/b\u003e？这一目标面临两大核心挑战：\u003cb\u003e\u003cu\u003e数据集构建与模型设计。\u003c/u\u003e\u003c/b\u003e谷歌RT-1模型正是针对该命题的突破性探索。在RT-1模型的研究探索中，谷歌DeepMind 研究人员发现：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"bOcJ1IbV\"\u003e数据模型要实现优质泛化，必须构建兼具规模与广度的数据集，覆盖多样化任务与场景。同时，\u003cb\u003e\u003cu\u003e数据集中的任务需具备强关联性以支撑泛化，使模型能够发现结构相似任务间的内在规律，并通过创新性组合完成新任务。\u003c/u\u003e\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"h9jmZrBd\"\u003e模型架构设计的挑战在于—— \u003cb\u003e\u003cu\u003e高效的多任务机器人学习需要的高容量模型。\u003c/u\u003e\u003c/b\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"FRb1t7eq\"\u003e\u003cb\u003e1）RT-1模型工作原理\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"1hw6GngY\"\u003eRT-1执行闭环控制，并以3Hz的频率持续输出动作指令，直至触发\u0026#34;终止\u0026#34;动作或达到预设时间步上限。\u003c/p\u003e\u003cp data-pid=\"fTh3yBnF\"\u003e首先通过ImageNet预训练的卷积网络EfficientNet处理图像，该网络通过FiLM模块与指令的预训练嵌入向量进行条件调节；随后采用令牌学习器（tokenLearner）生成紧凑令牌集( set of tokens)；最终由Transformer对这些令牌执行注意力计算，输出离散化动作令牌(action token)。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-ce8fc8c78f326b1843ddf7a650c903d2_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"314\" data-original-token=\"v2-ce8fc8c78f326b1843ddf7a650c903d2\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-ce8fc8c78f326b1843ddf7a650c903d2_r.jpg\"/\u003e\u003cfigcaption\u003eRT-1工作流程图\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"BWnu1Nip\"\u003e\u003cb\u003eRT-1架构包含以下核心内容：\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-16705cf7aae1b85cad0a94f36220750b_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"986\" data-rawheight=\"1614\" data-original-token=\"v2-16705cf7aae1b85cad0a94f36220750b\" class=\"origin_image zh-lightbox-thumb\" width=\"986\" data-original=\"https://picx.zhimg.com/v2-16705cf7aae1b85cad0a94f36220750b_r.jpg\"/\u003e\u003cfigcaption\u003eRT-1架构图\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"nk78AqJ3\"\u003e\u003cb\u003ea. EfficientNet网络\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"OsQXM-bf\"\u003eRT-1通过将6 幅图像的历史记录输入一个基于ImageNet 预训练的 EfficientNet-B3模型来对其进行令牌化。\u003c/p\u003e\u003cp data-pid=\"e5HEebci\"\u003e该模型接收分辨率为 300×300 的6幅图像作为输入，并从最终的卷积层输出一个形状为 9×9×512 的空间特征图。\u003c/p\u003e\u003cp data-pid=\"Rih5bcC1\"\u003e为了包含语言指令，研究人员使用预训练的语言嵌入形式，让图像令牌化器以自然语言指令为条件，从而能够在早期提取任务相关的图像特征，并提升 RT-1 的性能。\u003c/p\u003e\u003cp data-pid=\"YrASuD9V\"\u003eEfficientNet是一种卷积神经网络（CNN），是高效的 “图像特征提取器”。它好比是提取画面特征的“图像翻译官”，专门用来从图像中提取特征，用于识别物体、分析场景等。\u003c/p\u003e\u003cp data-pid=\"qsPV7STK\"\u003e类似人类看照片时自动识别 “物体轮廓”，该卷积网络提前在 ImageNet（大规模图像数据集）上学过识别猫、车、杯子等物体，能从摄像头画面中快速提取所要识别物体的关键特征。\u003c/p\u003e\u003cp data-pid=\"XZx2RtJs\"\u003e\u003cb\u003eb. FiLM 模块\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"gr_ia9fW\"\u003e指令首先通过通用句子编码器（USE）进行嵌入。然后，将该嵌入用作输入，输入到恒等初始化的 FiLM层，这些 FiLM 层被添加到预训练的 EfficientNet 中，以调节图像编码器。\u003c/p\u003e\u003cp data-pid=\"yn6hjXt6\"\u003eFiLM 模块好比是指令与图像的 “融合滤镜”。指令（比如 “把杯子拿到桌子上”）会先被转换成机器能懂的 “数字密码”（预训练嵌入向量）。FiLM 模块就像一个调节旋钮，用这个 “密码” 去调整图像特征：如果指令是 “拿杯子”，FiLM 会让卷积网络提取的特征更关注 “杯子的位置和把手”，忽略背景中的沙发。\u003c/p\u003e\u003cp data-pid=\"TeFHmv88\"\u003e\u003cb\u003ec. TokenLearner（令牌学习器）\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"dfrve5UQ\"\u003e加入 TokenLearner后，将从预训练的 FiLM-EfficientNet层输出的 81个视觉令牌二次采样到仅剩8个最终令牌，然后这些令牌被传递到Transformer 层。\u003c/p\u003e\u003cp data-pid=\"FgoNz0P6\"\u003eTokenLearner可类比成是特征的 “智能摘要工具”。卷积网络处理后的图像特征可能包含数万维数据（比如一张图拆成 1000 个小区域的特征），直接处理像 “读一本厚书”，效率很低。令牌学习器会自动挑选最重要的特征，把海量数据 “压缩” 成几十个关键 “令牌”（Token），类似从书中提取 “杯子、桌子、位置” 等关键词，扔掉无关细节（比如杯子上的花纹）。这样一来，数据量大幅减少，后续模型处理速度就像 “从读整本书变成看目录”，效率飙升。\u003c/p\u003e\u003cp data-pid=\"aMXh21lu\"\u003e\u003cb\u003ed. Transformer\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"L93rVqlL\"\u003e每幅图像产生的这8个令牌随后会与历史记录中的其他图像令牌拼接，形成总共 48 个令牌（并添加了位置编码），输入到 RT-1 的 Transformer 骨干网络中。该Transformer 是一个19M参数的，且仅包含解码器的序列模型，具有8个自注意力层，其输出是动作令牌。\u003c/p\u003e\u003cp data-pid=\"ZrFaoaCl\"\u003eTransformer可以看成是基于“关键词”的注意力决策者。Transformer 就像一个经验丰富的 “规划师”，它会分析压缩后的令牌（图像关键词）和指令密码，并最终输出机器人的动作执行指令。\u003c/p\u003e\u003cp data-pid=\"CRj5AJCz\"\u003e\u003cb\u003ee. 其它\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"JYXuIoKw\"\u003e\u003cb\u003e动作令牌化（Action Tokenization）—— 为了对动作进行令牌化，RT-1中的每个动作维度都被离散化为256个bins。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"OESoCgvi\"\u003e\u003cb\u003e动作维度包括：\u003c/b\u003e机械臂运动的七个变量（x, y, z, roll, pitch, yaw, 夹爪开合度）、底盘运动的三个变量（x, y, yaw）以及一个用于在三种模式（控制机械臂、控制底盘和终止任务片段）间切换的离散变量。\u003c/p\u003e\u003cp data-pid=\"57orpUrk\"\u003e对于每个变量，研究人员将目标值映射到这256个bins中的一个，这些bins在每个变量的取值范围内均匀分布。\u003c/p\u003e\u003cp data-pid=\"gTm3rc3d\"\u003e\u003cb\u003e损失函数 \u003c/b\u003e—— 研究人员使用了标准的分类交叉熵目标函数和因果掩码。\u003c/p\u003e\u003cp data-pid=\"LtFot8wI\"\u003e\u003cb\u003e推理速度\u003c/b\u003e—— 一个需要在真实机器人上实时运行的模型的独特要求之一是快速且稳定的推理速度。研究人员采用了两种技术来加速推理：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"5sfvvUVr\"\u003e通过使用TokenLearner，减少由预训练 EfficientNet 模型生成的令牌数量；\u003c/li\u003e\u003cli data-pid=\"G27EUACO\"\u003e仅计算这些令牌一次，并在后续存在重叠的推理窗口中复用它们。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"i3lsSsFm\"\u003e这两项技术将模型推理速度分别提升了2.4倍和1.7倍。\u003c/p\u003e\u003cp data-pid=\"WKa4JuBY\"\u003e\u003cb\u003e2) RT-1局限性\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"qIQURFAz\"\u003e\u003cb\u003e尽管\u003c/b\u003eRT-1 在多项关键指标上表现突出：以 97% 成功率执行超 700 条指令，在新任务、物体与环境的泛化能力上超越已发布基线模型；能有效融合模拟环境与异构机器人形态的数据，且在不削弱原任务性能的前提下增强新场景适应性；还可在SayCan框架中完成长达50步的长时程任务 —— 但该模型仍存在一定局限性。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"iz_FAh4t\"\u003eRT-1 的训练数据虽覆盖大规模操作任务，但主要针对灵巧度要求不高的操作场景；\u003c/li\u003e\u003cli data-pid=\"cIh1R1dS\"\u003eRT-1是一种模仿学习方法，继承了该类方法固有的挑战，例如，可能无法超越演示者的性能水平。\u003c/li\u003e\u003cli data-pid=\"DELVeTyW\"\u003eRT-1对新指令的泛化仅限于先前见过的概念组合，尚无法泛化到前所未见的全新动作。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e\u003cb\u003e2023年：谷歌具身智能基础模型关键布局\u003c/b\u003e\u003c/h2\u003e\u003ch3\u003e\u003cb\u003e1. 2023年3月：谷歌发布具身多模态视觉语言大模型PaLM-E\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"4M51q1or\"\u003e2023年3月，谷歌和柏林工业大学的团队正式推出PaLM-E。该模型之所以命名为 PaLM-E，是因为它采用 PaLM作为预训练语言模型，并使其具备了具身能力（Embodied）。\u003c/p\u003e\u003cp data-pid=\"Ntb_LPzv\"\u003ePaLM-E是一个单一的、大型的通用型多模态语言模型，能够将视觉-语言领域的知识迁移到具身推理领域，适用于具身推理任务、视觉-语言任务和语言任务，包括序列化机器人操作规划、视觉问答和图像描述生成等。\u003c/p\u003e\u003cp data-pid=\"YuQenQzs\"\u003e该模型可直接整合来自具身\u003cb\u003e\u003cu\u003e智能体传感器模态的连续输入\u003c/u\u003e\u003c/b\u003e，从而使语言模型本身能够做出更贴近现实的推理，以支持现实世界中的序列化决策。\u003c/p\u003e\u003cp data-pid=\"ujNBAZKG\"\u003e图像和状态估计等输入会被嵌入到与语言令牌（Language Tokens）相同的潜在嵌入空间中，并由基于 Transformer 的大型语言模型（LLM）的自注意力层以与处理文本相同的方式进行处理。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-360989ca12609952250153d5100a7692_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"385\" data-original-token=\"v2-360989ca12609952250153d5100a7692\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-360989ca12609952250153d5100a7692_r.jpg\"/\u003e\u003cfigcaption\u003ePaLM-E系统架构\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Iv1mGbYV\"\u003e\u003cb\u003e1）PaLM-E模型整体架构\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"mWKuFdiV\"\u003ePaLM-E模型的架构由三部分构成：\u003cb\u003e多模态编码器、投影器和仅含解码器的大型语言模型（LLM）\u003c/b\u003e。它能在给定前缀或提示的情况下，以自回归方式生成文本补全内容。\u003c/p\u003e\u003cp data-pid=\"F_Vy6f6l\"\u003e其主要架构思路是将连续的具身观察信息（如图像、状态估计或其他传感器模态）注入预训练语言模型的语言嵌入空间。\u003cb\u003e其具体实现方式：\u003c/b\u003e 将连续观察信息编码为一系列向量，这些向量的维度与语言标记的嵌入空间维度相同。因此，连续信息以类似于语言令牌的方式被注入语言模型中。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"euapFH0M\"\u003e\u003cb\u003ePaLM-E 的输入:\u003c/b\u003e 包括文本和（多个）连续观察数据。与这些观察数据对应的多模态令牌与文本交错排列，形成多模态句子。例如：What happened between \u0026lt;img_1\u0026gt; and \u0026lt;img_2\u0026gt;?  where\u0026lt;img_i\u0026gt; represents an embedding of an image. 其中，\u0026lt;img_i\u0026gt;代表图像的嵌入向量。\u003c/li\u003e\u003cli data-pid=\"KnGpla7t\"\u003e\u003cb\u003ePaLM-E 的输出: \u003c/b\u003e是模型通过自回归方式生成的文本，既可以是问题的答案，也可以是PaLM-E 以文本形式生成的、由机器人执行的一系列决策。\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"lWi5dlw_\"\u003e\u003cb\u003e为实现模型输出与具身智能体的连接，该研究将其区分为两种具体情况：\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"6tK8wB0C\"\u003ea. 如果任务仅通过输出文本即可完成（例如，在具身问答或场景描述任务中），那么模型的输出就直接被视为该任务的解决方案。\u003c/p\u003e\u003cp data-pid=\"hTnTct-1\"\u003eb. 如果是解决具身规划或控制任务，它会生成文本以调控低级指令。\u003c/p\u003e\u003cp data-pid=\"37QSHpZ2\"\u003e具体而言，假设有一些策略能够执行来自某个（小型）词汇表的低级技能，而PaLM-E 生成的有效规划必须由一系列此类技能构成。\u003c/p\u003e\u003cp data-pid=\"I_PRur7S\"\u003e但是，PaLM-E 必须根据训练数据和提示信息自行判断可用的技能，且没有其他机制用于约束或过滤其输出。尽管这些策略是受语言调控的，但它们无法解决长时程任务或处理复杂指令。\u003c/p\u003e\u003cp data-pid=\"wtFaxuKx\"\u003e因此，PaLM-E 被整合到控制环路中，其预测的决策由机器人通过低级策略执行，进而产生新的观察结果 —— 基于这些结果，PaLM-E 可在必要时重新规划。从这个意义上来说，PaLM-E可以被理解为一种高级策略，用于对低级策略进行排序和控制。\u003c/p\u003e\u003cp data-pid=\"ws73yIew\"\u003e\u003cb\u003e2) 不同传感器模态的输入与场景表征\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"iFK_EC1t\"\u003e针对不同传感器模态（如状态估计向量、二维图像等），需采用专用编码器进行处理。为此，谷歌提出差异化的编码器架构选择：通过映射函数将对应模态数据对齐至语言嵌入空间。具体研究涵盖以下模态处理方案：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"OLzklNDw\"\u003e\u003cb\u003e状态估计向量：\u003c/b\u003e采用多层感知器（MLP）。状态向量（例如来自机器人的向量或对象的状态估计向量）包含这些对象的位姿、大小、颜色等信息。多层感知器（MLP）会将状态向量映射到语言嵌入空间中。\u003c/li\u003e\u003cli data-pid=\"kJIRtgx-\"\u003e\u003cb\u003e二维图像特征：\u003c/b\u003e采用视觉 Transformer（ViT）。ViT是一种Transformer架构，能将图像映射为多个标记嵌入向量。研究人员考虑了多种变体，包括 40 亿参数模型的ViT-4B），以及220 亿参数模型 ViT-22B，这两种模型均在图像分类任务上进行过预训练。\u003c/li\u003e\u003cli data-pid=\"5RytCZUU\"\u003e\u003cb\u003e三维场景表征：\u003c/b\u003e采用对象场景表征Transformer（OSRT）。它不依赖于对象的外部知识，而是通过架构中的归纳偏置以无监督方式发现对象。基于 SRT，OSRT 通过新颖的视图合成任务，在领域内数据上学习以三维为中心的神经场景表征。\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"JzZcUtWg\"\u003e\u003cb\u003e3）PaLM-E模型训练方法\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"7FvbXa_8\"\u003ePaLM-E 以预训练的PaLM 模型（分别为80亿、620亿和5400亿参数的仅含解码器版本）为基础，通过\u003cb\u003e\u003cu\u003e输入编码器\u003c/u\u003e\u003c/b\u003e向其中注入\u003cb\u003e\u003cu\u003e连续观察数据\u003c/u\u003e\u003c/b\u003e。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"qKRDfCDW\"\u003ePaLM-E-12B：80 亿参数LLM与 40 亿参数ViT相结合；\u003c/li\u003e\u003cli data-pid=\"SsDvrqvp\"\u003ePaLM-E-84B：620亿参数LLM与220亿参数ViT相结合； \u003c/li\u003e\u003cli data-pid=\"CTd7HAUB\"\u003ePaLM-E-562B：5400亿参数LLM与 220 亿参数ViT相结合。\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"SwFbdG9m\"\u003e\u003cb\u003e备注：\u003c/b\u003ePaLM-E-562B ——尽管它仅在单图像样本上进行过训练，但却具备多种能力，包括零样本多模态思维链（CoT）推理、少样本提示、无需光学字符识别（OCR）的数学推理以及多图像推理。\u003c/p\u003e\u003cp data-pid=\"ZM8qf7HR\"\u003e\u003cb\u003ea. 训练方法1：\u003c/b\u003e更新所有组件的参数。然而，若能提供合适的提示，大型语言模型会展现出令人印象深刻的推理能力。\u003c/p\u003e\u003cp data-pid=\"hyOmUFpN\"\u003e\u003cb\u003eb. 训练方法2：\u003c/b\u003e冻结大型语言模型，仅训练输入编码器；在这种情况下，编码器必须生成嵌入向量，使冻结的大型语言模型能基于观察数据，并将具身实体的能力信息传递给大型语言模型。训练此类编码可理解为一种输入条件化的软提示，与常规软提示相关。\u003c/p\u003e\u003cp data-pid=\"r6Tb4Qs9\"\u003e\u003cb\u003ec. 训练方法3：\u003c/b\u003e跨任务协同训练: 在多种不同数据上对模型进行协同训练。其中，“完整混合数据集”主要包含大规模互联网级别的视觉 - 语言数据，涵盖多种任务。其采样频率设置为：完整混合数据集中仅有 8.9% 是具身数据，且每个具身场景下包含多个任务。\u003c/p\u003e\u003cp data-pid=\"sbj2cuZb\"\u003e实验结果显示，相较于仅在不同任务上单独训练，在“完整混合数据集” 上进行协同训练的模型，性能提升了一倍以上。\u003c/p\u003e\u003cp data-pid=\"M92YKulE\"\u003e\u003cb\u003e4）研究发现\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"nrmasbA0\"\u003ea. 尽管当前最先进的通用视觉- 语言模型在零样本情况下无法很好地解决具身推理问题，但通过训练，在多模态大型语言模型的训练中融入具身数据，可以训练出一个具备通用性、迁移学习能力且适用于多具身形式的决策智能体 ——  一个既能胜任通用视觉 - 语言任务，又能高效进行具身推理的模型。\u003c/p\u003e\u003cp data-pid=\"C3GLOj2f\"\u003eb. 相较于单任务训练范式，多任务联合训练显著提升模型综合性能。其核心价值在于跨任务知识迁移能力——该机制大幅提高机器人任务的数据利用效率（例如仅需10%的示范数据即可达到同等成功率），并赋予模型对新物体组合的强泛化能力（单样本泛化成功率提升37%）及开放世界物体的零样本操作能力。\u003c/p\u003e\u003cp data-pid=\"EMPrxulu\"\u003ec. 在多模态模型联合训练过程中，研究揭示两种保持PaLM-E语言能力的有效途径：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"mSKkHf_J\"\u003e\u003cb\u003e参数冻结策略：\u003c/b\u003e锁定大型语言模型（LLM）参数，仅训练输入编码器——此方案显著降低训练成本，成为构建具身语言模型的高效方法。\u003c/li\u003e\u003cli data-pid=\"e0rB-EAK\"\u003e\u003cb\u003e规模化端到端训练：\u003c/b\u003e当进行全模型端到端训练时，模型参数量与语言能力保留度呈正相关（例如562B的PaLM-E模型保留540B的PaLM模型96%的语言性能），可有效抑制具身化过程中的灾难性遗忘现象。\u003c/li\u003e\u003c/ul\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-f5c2e96c3c4ccff2d9c8cb2a5dc7a3cd_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"717\" data-original-token=\"v2-f5c2e96c3c4ccff2d9c8cb2a5dc7a3cd\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-f5c2e96c3c4ccff2d9c8cb2a5dc7a3cd_r.jpg\"/\u003e\u003cfigcaption\u003e基于语言任务的实验结果\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"qJlml8Qd\"\u003e\u003cb\u003e备注：\u003c/b\u003e通用语言任务实验结果表明——随着模型规模扩大，PaLM-E模型相较于其基础PaLM模型的灾难性遗忘程度显著降低。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e\u003cb\u003e2. 2023年6月：谷歌发布通用智能体RoboCat\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"LjFETVhf\"\u003e2023年6月，谷歌DeepMind推出多具身形态、多任务通用智能体RoboCat——一种基于视觉目标条件的决策Transformer，可处理动作标注的视觉经验数据，能够通过自身生成的数据进行训练迭代实现自我改进。\u003c/p\u003e\u003cp data-pid=\"xoTcH1O-\"\u003e\u003cb\u003e1）对Gato的继承和创新\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"d4feSe4p\"\u003eRoboCat直接沿用Gato的多模态Transformer架构作为基础，将视觉、语言、动作数据统一处理为离散token序列。这一设计被认为是DeepMind在通用智能体Gato的技术路线上的延续。\u003c/p\u003e\u003cp data-pid=\"YJouRHHt\"\u003e另外，在Gato基础上，RoboCat针对机器人任务强化了以下能力：\u003c/p\u003e\u003cp data-pid=\"bBu22c3O\"\u003e\u003cb\u003ea. 动作输出适配：RoboCat 针对机器人任务的动作头扩展并非简单的维度调整，而是通过动态动作空间映射实现的深度优化。\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"nqHFtBCk\"\u003e\u003cb\u003e多自由度兼容设计：\u003c/b\u003e动作头支持混合动作表示，可同时处理离散动作（如按键）和连续动作（如关节力矩）；引入动作头参数共享机制，即不同机械臂的动作头共享底层Transformer 参数，但通过任务特定的适配器（Adapter）实现自由度差异的动态适配。\u003c/li\u003e\u003cli data-pid=\"zFkEQPP2\"\u003e\u003cb\u003e硬件无关的控制接口\u003c/b\u003e：通过统一动作语义空间实现跨机械臂迁移。例如，抓取动作在不同机械臂中被抽象为\u0026#34;闭合夹具\u0026#34; 的语义指令，动作头根据当前机械臂的自由度自动生成具体的关节角度序列。另外，引入\u003cb\u003e动作空间正则化\u003c/b\u003e技术：在训练阶段，通过对抗训练使动作头输出分布与机械臂物理约束对齐，避免生成超出关节极限的动作。\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"JjOpi-oj\"\u003e\u003cb\u003eb. 目标条件策略：\u003c/b\u003eRoboCat 的目标图像输入通道并非简单的输入扩展，而是构建了端到端的视觉 - 动作闭环。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"89ANDGmv\"\u003e\u003cb\u003e目标图像的多模态融合\u003c/b\u003e：目标图像通过预训练的VQ-GAN 编码器转化为 token 序列，并与当前观测图像 token、动作 token、任务描述 token 共同输入 Transformer；引入目标-观测注意力机制：Transformer 在处理序列时，会动态计算目标图像 token 与当前观测 token 的相关性，优先关注需要调整的区域。\u003c/li\u003e\u003cli data-pid=\"gkAfJBuz\"\u003e\u003cb\u003e闭环控制的实时性优化：\u003c/b\u003e采用时序目标对齐技术，将目标图像分解为时间序列token，并与当前动作序列token 进行时序对齐训练；引入失败补偿机制 —— 当动作执行未达到目标时，模型会自动生成补偿动作。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Ky-BZJz-\"\u003e\u003cb\u003e2) 自我改进闭环学习机制\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"UCvqxeO4\"\u003e研究实验表明，RoboCat既能零样本泛化到新任务与新形态机器人，也可仅通过100-1000个目标任务样本的微调，快速适配到不同的新任务，包括新机器人具身、未见过的行为、物体和感知变体（光照/视角等感知条件变化），以及从仿真模拟到真实的迁移。\u003c/p\u003e\u003cp data-pid=\"DIQUf6xp\"\u003e此外，训练后的模型自身可生成数据用于后续训练迭代，从而构建\u003cb\u003e\u003cu\u003e自我改进闭环学习机制\u003c/u\u003e\u003c/b\u003e \u003cb\u003e—— \u003c/b\u003e研究人员使用多样化的训练数据集来训练该通用智能体的初始版本，该版本可通过100-1000 次演示数据微调至适配新任务，随后部署到真实机器人上，为这些任务生成更多数据。生成的新数据将被添加到训练数据集中，用于RoboCat的下一迭代版本训练，这种机制在一定程度上突破了传统机器人依赖真机数据的局限，使模型能持续进化变成可能。\u003c/p\u003e\u003cp data-pid=\"Ul6ZZbz4\"\u003e如下图所示，RoboCat通过自我改进闭环流程持续提升智能体能力——增强其跨任务迁移性、通过微调扩展适配任务范围，并在现有任务中实现性能突破。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-5b818f51de958aee07cbee68f52a1cba_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"364\" data-original-token=\"v2-5b818f51de958aee07cbee68f52a1cba\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-5b818f51de958aee07cbee68f52a1cba_r.jpg\"/\u003e\u003cfigcaption\u003eRoboCat自我改进闭环流程机制\u003c/figcaption\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"8vSKso7W\"\u003e\u003cb\u003e3）基础模型训练数据\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"iTVMyeb8\"\u003eRoboCat 的训练数据集聚焦于视觉目标条件下的机器人操作任务，且针对性覆盖了多形态硬件和复杂场景，包含400 万次机器人操作片段，涵盖物体分拣、工具使用、导航等多样化场景。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Kt8W_vIe\"\u003e\u003cb\u003e多具身形态适配：\u003c/b\u003e数据来自4 种不同类型的真实机器人（如 Sawyer、Panda 机械臂）及模拟环境，包含不同自由度、观察空间和动作规范的操作序列。\u003c/li\u003e\u003cli data-pid=\"Y8ggNiro\"\u003e\u003cb\u003e任务多样性覆盖：\u003c/b\u003e训练数据覆盖253 项基础任务及 141 项变体，涉及精密装配（如齿轮插入、积木堆叠等）、基础操作类（如抓取指定物体、分拣水果等）等场景。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"4fTPnWw7\"\u003e\u003cb\u003e4）RoboCat的局限性\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"xm1VaSOk\"\u003e在具身智能领域，机器人面临的最大挑战是如何像人类一样快速适应新任务与环境。RoboCat首次在通用机器人领域实现了“学习-实践-进化”的完整闭环，为破解这一难题提供了全新路径。\u003c/p\u003e\u003cp data-pid=\"rzr4Pjb8\"\u003e这一突破性技术通过在模拟与真实环境中融合跨机器人经验，结合生成式人工智能的自我数据增强能力，显著降低了新技能学习所需的人类演示数据量。然而，在动态环境适应性、跨本体泛化效率等方面仍存在明显局限。\u003c/p\u003e\u003cp data-pid=\"bCuvFcFh\"\u003e\u003cb\u003e1）动态环境应对不足：物理建模深度的不够\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"ag_ODqpg\"\u003eRoboCat在静态桌面操作（如抓取固定物体、堆叠积木）中表现出色，但面对动态交互场景时性能急剧下降。例如在抓取滚动球体任务中，其成功率不足30%，远低于工业场景要求的95%+的可靠性标准。这本质上是世界模型缺失的体现。与人类基于物理直觉预判行为后果不同，RoboCat仅建立“图像-动作”的统计关联，缺乏对“力-运动-形变”因果链的内在表征。当环境变量超出训练集分布时（如地面材质由木质变为金属），模型无法通过物理推理调整策略，导致跨场景泛化崩溃。\u003c/p\u003e\u003cp data-pid=\"kZhN3du6\"\u003e\u003cb\u003e2）硬件适配的柔性瓶颈：本体特化与通用性的两难\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"BW-ah8kY\"\u003e虽然RoboCat支持跨机械臂迁移，但其适配效率仍受限于本体动力学特性差异。当新硬件与训练集机械臂存在显著动力学差异时，微调成本剧增。这些问题暴露了跨本体适配的“表面泛化”特性：模型可适应外形相似、自由度相近的机械臂，但对动力学特性迥异的系统，仍需近乎重训级的深度调整。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e\u003cb\u003e3. 2023年7月：谷歌发布机器人VLA模型RT-2\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"VhYt4EDl\"\u003e2023年7月，谷歌DeepMind发布具身智能视觉-语言-动作模型（VLA）RT-2（Robotics Transformer 2）。其核心设计是使用互联网规模的网络数据和机器人动作轨迹数据对预训练好的视觉-语言模型（VLM）进行联合微调生成VLA模型。核心目标是将VLM模型的知识迁移到机器人控制中，实现端到端的语义推理与动作生成。\u003c/p\u003e\u003cp data-pid=\"29S2oaHB\"\u003e\u003cb\u003e1）模型架构\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"kPN09krN\"\u003eRT-2以预训练的视觉-语言模型为核心骨干，通过动作 Token 化将机器人控制任务统一到自然语言生成框架中，形成端到端单一模型架构。该模型分别以PaLI-X 和 PaLM-E 两种VLM 架构构建了对应的实例模型RT-2-PaLI-X 与 RT-2-PaLM-E 。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-14eedf222b627321ac9e50cad304a5c9_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"397\" data-original-token=\"v2-14eedf222b627321ac9e50cad304a5c9\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-14eedf222b627321ac9e50cad304a5c9_r.jpg\"/\u003e\u003cfigcaption\u003eRT-2模型架构\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Yh5J4Uq4\"\u003e2）联合微调（Co-Fine-Tune）\u003c/p\u003e\u003cp data-pid=\"_hOaQRZw\"\u003e将机器人动作数据（来自RT-1）与网络数据混合，共同输入模型进行联合微调。其中，网络数据内容包括视觉问答（VQA）、图像描述、非结构化图文交织样本等。机器人动作数据为13台机器人持续17个月采集的办公环境中的厨房场景数据（与RT-1训练所使用的数据相同）。\u003c/p\u003e\u003cp data-pid=\"mJ_3F232\"\u003e在具体实现方案中，谷歌DeepMind通过提高机器人动作数据集的采样权重，以平衡每批次训练数据中机器人动作数据与网络数据的比例。\u003c/p\u003e\u003cp data-pid=\"sdBnIoH9\"\u003e模型联合微调的关键步骤在于建立模型现有词汇表与离散动作空间之间的映射关系。为此，需要预先保留256 个标记（tokens）作为专用的动作标记。具体选择哪些标记作为动作标记，取决于所使用的视觉语言模型（VLM）的分词方案：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"pBTwTUHG\"\u003ePaLI-X：由于其分词方案为每个不超过 1000 的整数分配了唯一的标记，因此可直接将 action bins 映射到对应的整数标记上。\u003c/li\u003e\u003cli data-pid=\"GYlABeKy\"\u003ePaLM-E：该模型的分词方案不包含数字的直接表示，因此需要覆盖词汇表中 256个使用频率最低的标记，将它们重新定义为动作词汇表。\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"HKkGJJB7\"\u003eDeepMind通过实验证明 —— 提升机器人性能的关键训练技巧在于：将机器人数据与原始网络数据进行联合微调，而非仅对机器人数据实施简单微调。联合微调能生成泛化能力更强的策略，因为在此过程中，策略同时接触网络规模数据的抽象视觉概念和微调过程中的低层级机器人动作，而非仅局限于机器人动作。\u003c/p\u003e\u003cp data-pid=\"PLo_c6UC\"\u003e3）RT-2的局限性\u003c/p\u003e\u003cp data-pid=\"iRV8TQjt\"\u003e本文阐述了如何通过结合视觉语言模型（VLM）预训练与机器人数据来训练视觉语言动作（VLA）模型。\u003c/p\u003e\u003cp data-pid=\"m9T5UamL\"\u003e谷歌DeepMind提出了基于PaLM-E和PaLI-X的两种VLA实现方案，分别命名为RT-2-PaLM-E和RT-2-PaLI-X。这些模型通过机器人轨迹数据进行联合微调，以输出表示为文本标记的机器人动作。\u003c/p\u003e\u003cp data-pid=\"mZR77-6f\"\u003e研究表明，该方法不仅能生成高性能的机器人策略，更重要的是其泛化能力显著提升，并继承了大规模网络视觉- 语言预训练所赋予的涌现能力。\u003c/p\u003e\u003cp data-pid=\"wKcyETPA\"\u003e尽管RT-2展现出优异的泛化性能，该方法仍存在多重局限。\u003c/p\u003e\u003cp data-pid=\"67OWM-Dw\"\u003e1）局限一：可用的开源VLM模型少\u003c/p\u003e\u003cp data-pid=\"pJQMhoWJ\"\u003e目前仅有少量可用于创建RT-2 的通用视觉 - 语言模型（VLM），期待更多开源模型及开放专有模型的微调API——这是构建VLA模型的必要条件。\u003c/p\u003e\u003cp data-pid=\"rLAeguGt\"\u003e2）局限二：动作创新能力受限\u003c/p\u003e\u003cp data-pid=\"WcDNfjWt\"\u003eVLM通过网络规模预训练可提升语义与视觉概念的泛化能力，但机器人并未因包含这些额外经验而获得执行新动作的能力。\u003c/p\u003e\u003cp data-pid=\"8ggJ5o5o\"\u003e模型的物理技能仍局限于机器人数据中所见的技能分布，仅能创新性地组合已有技能。DeepMind认为这源于数据集的技能多样性不足所致。未来研究的关键方向是探索通过新数据收集范式（如人类操作视频）获取新技能。\u003c/p\u003e\u003cp data-pid=\"jHy0oxDc\"\u003e3）局限三：实时推理瓶颈\u003c/p\u003e\u003cp data-pid=\"LpTwK_dh\"\u003e尽管实现了大型VLA模型的实时运行，但其计算成本仍高昂。若应用于需高频控制的场景，实时推理将成为主要瓶颈。未来研究需探索量化和蒸馏技术，以提升模型速率或适配低成本硬件。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e\u003cb\u003e4. 2023年10月：谷歌发布机器人VLA模型RT-X \u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"oUDH8Y8J\"\u003e2023年10月，谷歌DeepMind发布了通用具身智能模型RT-X（含RT-1-X和RT-2-X），并开源其训练数据集Open X-Embodiment。\u003c/p\u003e\u003cp data-pid=\"O7IjxTdr\"\u003eRT-X 模型并非指单一模型，而是一个项目/系列。在原有 RT-1 和 RT-2 的框架基础上，使用大规模跨机器人数据集 Open X-Embodiment 进行训练（微调），从而得到了两个系列的模型：RT-1-X 系列和 RT-2-X系列。\u003c/p\u003e\u003cp data-pid=\"FThbaj2Q\"\u003e截至目前，Open X-Embodiment已整合 60个机器人数据集，覆盖 311 种场景与 22 类不同类型的机器人平台（含单臂/双臂/四足机器人等），提供超100万条真实机器人运动轨迹，涵盖 527 项技能及 160,266 项任务。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-2aad6e305569832f20f93484b03652d6_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"339\" data-original-token=\"v2-2aad6e305569832f20f93484b03652d6\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-2aad6e305569832f20f93484b03652d6_r.jpg\"/\u003e\u003cfigcaption\u003eOpen X-Embodiment 开源数据集\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"KDxYf-nd\"\u003e\u003cb\u003e1）RT-X模型训练使用数据集\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"A_4BmqrU\"\u003e据了解，在当时训练RT-1-X 和 RT-2-X 时，所使用的数据并非如今 Open X-Embodiment 数据集的全部内容。其数据仅涵盖 22 个机械臂中的 9 个，以及 60 组子数据集中的 12 组，总计 1,131,788 条数据。由于该数据集处于持续增长状态，在开展 RT-X 相关实验时，这 12 组数据便是当时数据集的全部内容。\u003c/p\u003e\u003cp data-pid=\"vqvVHlyt\"\u003e\u003cb\u003e这12组数据集包括：\u003c/b\u003eRT-1、QT-Opt、Bridge、Task Agnostic Robot Play、Jaco Play、Cable Routing、RoboTurk、NYU VINN、Austin VIOLA、Berkeley Autolab UR5、TOTO和Language Table。\u003c/p\u003e\u003cp data-pid=\"izj-pkH7\"\u003e其中，RT-1-X仅使用上述机器人数据（9类机械臂的12组数据集）进行训练；RT-2-X采用与原 RT-2类似的联合微调策略，以约 1:1 比例混合经任务筛选的VLM数据与机器人数据。\u003c/p\u003e\u003cp data-pid=\"sLMqJh7_\"\u003e\u003cb\u003e2）RT-X模型架构\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"sbuKeq_W\"\u003eRT-1-X 和 RT-2-X 均以图像和文本指令作为输入，并输出离散化的末端执行器动作。RT-1-X 是一个专为机器人设计的架构，包含一个 FiLM 条件化的 EfficientNet和一个 Transformer。RT-2-X 构建在一个视觉语言模型（VLM）主干之上，其方法是将动作表征为另一种语言，并将动作文本标记与视觉语言数据一起进行训练。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-f028459ccced18a86816460c7d9f28fe_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"287\" data-original-token=\"v2-f028459ccced18a86816460c7d9f28fe\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-f028459ccced18a86816460c7d9f28fe_r.jpg\"/\u003e\u003cfigcaption\u003eRT-X模型训练过程\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2\u003e\u003cb\u003e2024年：谷歌具身智能基础模型关键布局\u003c/b\u003e\u003c/h2\u003e\u003ch3\u003e\u003cb\u003e1. 2024年3月，谷歌推出具身智能模型RT-H\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"N-5WcrKB\"\u003e\u003cb\u003e2024年3月，谷歌DeepMind正式推出端到端的框架RT-H。它是一个带动作分层结构的机器人Transformer —— \u003cu\u003e将语言化动作作为高级任务描述与低级动作之间的中间预测层，从而通过语言化动作构建动作分层结构。\u003c/u\u003e\u003c/b\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-5a0096bf25a6e4811cd08876801eba6b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"597\" data-original-token=\"v2-5a0096bf25a6e4811cd08876801eba6b\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5a0096bf25a6e4811cd08876801eba6b_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"AesCxLAC\"\u003e在上图中，当给定“盖上开心果罐” 这类语言描述的任务以及场景图像后，RT-H 会利用视觉语言模型（VLM）预测 “向前移动手臂”、“向右旋转手臂” 等语言化动作；然后，再根据这些语言化动作，为机器人预测具体执行动作（Robot Action）。\u003c/p\u003e\u003cp data-pid=\"YjGAODsu\"\u003e这种动作分层结构能让模型学习到那些语言描述差异显著但存在共享结构的任务。与RT-2哪些直接从任务映射到动作的方式相比，这些语言化动作有助于在多样化的多任务数据集之间实现更好的数据共享。\u003c/p\u003e\u003cp data-pid=\"cMfjNivw\"\u003e此外，该分层结构还允许人类选择性地向机器人提供语言化动作修正，以避免任务失败，随后利用这些新的语言化动作预测更优的动作。当人类完成干预后，RT-H 会像之前一样继续预测语言化动作。\u003c/p\u003e\u003cp data-pid=\"01gmGtb8\"\u003e然而，当任务在语义上变得更加多样时（例如“拿起可乐罐” 和 “倒杯子里的东西”），任务间的数据共享就会变得更加困难，因此学习从高级任务到具体动作指令的映射需要大量的演示数据。\u003c/p\u003e\u003cp data-pid=\"KXzPAN-u\"\u003e为了弥合任务与动作之间的这一鸿沟，DeepMind的解法是赋予机器人『动作语义化』能力——使用原子级动作短语（如“前移机械臂”或“闭合夹爪”）描述底层运动。将语言化动作预测作为高层任务与底层执行间的中间步骤，倒逼策略模型学习表面异构任务间共享的底层运动结构。更重要的是，基于语言化动作条件生成的策略，可在执行过程中通过人类指定的语义指令实时修正。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"jFKtj7tc\"\u003e\u003cb\u003e1）RT-H：利用语言构建动作分层结构\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"rzGULuf2\"\u003e\u003cb\u003eRT-H的推理流程包含两个关键阶段：\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"cIclLsDd\"\u003e1）首先，RT-H 根据视觉观察结果和高级任务描述，预测当前的语言化动作（语言化动作指令请求），使模型能在细粒度层面推理任务执行方式；\u003c/p\u003e\u003cp data-pid=\"W-0HHKxz\"\u003e2）然后，RT-H联合视觉观察、任务描述及推断出的语言化动作预测当前的具体执行动作（机器人动作指令请求），其中语言化动作为精确动作预测提供了关键上下文补充。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Cz0nI6-4\"\u003eRT-H是以视觉语言模型（VLM）作为主干网络，并遵循 RT-2的训练流程来实现。与RT-2 类似，通过互联网规模数据的训练，模型可调用自然语言处理与图像理解领域的海量先验知识。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"QRut-fng\"\u003e为了将这些先验知识融入动作层级结构的各个层级，RT-H 使用单一的视觉语言模型（VLM）同时学习语言动作指令请求（Language Motion Query）和机器人动作指令请求(Action Query)。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-0a08b02306212c07cbd9fe0d4097d5b7_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"447\" data-original-token=\"v2-0a08b02306212c07cbd9fe0d4097d5b7\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-0a08b02306212c07cbd9fe0d4097d5b7_r.jpg\"/\u003e\u003cfigcaption\u003eRT-H的推理流程\u003c/figcaption\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"zOk1YqO2\"\u003e\u003cb\u003e图左侧：\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"15lnASwp\"\u003eRT-H利用语言构建分层策略学习架构：将动作预测拆分为语言化动作指令请求（πₕ）与机器人动作指令请求（πₗ）。其中：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"o2Iv9X8_\"\u003eπₕ：基于图像令牌（ Image Tokens）和任务描述令牌（Task Tokens）预测细粒度语言化动作（如“向前移动手臂”）；\u003c/li\u003e\u003cli data-pid=\"Kh6wkOhq\"\u003eπₗ：结合场景视觉上下文，将该语言化动作解码为具体的机器人动作指令。\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"E9E9VMVR\"\u003e\u003cb\u003e图右侧：\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"2Z2S24Ic\"\u003e用户可直接对机器人动作指令请求进行干预，为机器人行为提供语言化动作修正，例如此处将“向前移动手臂” 改为 “向左移动手臂”。为了从修正中学习，只需用新标注的语言化动作修正更新语言动作指令请求。随后，将更新后的模型重新部署到动作层级结构中。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"nqYg-Twr\"\u003eRT-H通过端到端的方式学习预测语言化动作指令和机器人动作指令，不仅能够在语言化动作空间中进行修正，还能从这些修正中高效学习。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"AL4AcQO-\"\u003e当所学策略难以顺利执行任务时，语言化动作能再次发挥作用：它们为在线人类修正提供了一个直观的交互界面，且这种修正与特定场景相关联。通过语言化动作训练的策略，能够自然地遵循人类的低级修正指令，并借助修正数据成功完成任务。此外，该策略甚至可以在语言修正数据上进行训练，从而进一步提升自身性能。\u003c/p\u003e\u003cp data-pid=\"w0WcvBkd\"\u003e\u003cb\u003ea. RT-H模型训练\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"CZzqUvEN\"\u003eRT-H采用基于互联网多模态数据联合预训练的单一视觉语言模型（VLM），学习高层任务策略πₕ与底层机器人控制策略πₗ。\u003c/p\u003e\u003cp data-pid=\"p4OIrHiS\"\u003eRT-H采用与RT-2相同的PaLI-X 55B架构实例化视觉语言模型（VLM）。该模型通过ViT编码器将图像处理为图像令牌（Image Tokens），再由编码器-解码器Transformer联合处理这些图像令牌与自然语言指令令牌，输出离散动作令牌（Action Tokens）。\u003c/p\u003e\u003cp data-pid=\"ralUfI2x\"\u003e这些动作令牌的生成方式沿袭RT-2的离散化机制：将每个动作维度离散化为256个区间（bins），并将区间编码为整数值。每个动作包含末端执行器的位置/轴角旋转变化量、夹爪开合动作指令以及任务终止标志。\u003c/p\u003e\u003cp data-pid=\"hDDE0r2x\"\u003e随后，RT-H 使用与 RT-2 相同的 PaLI-X 训练混合数据进行联合训练，并从预训练检查点开始。在该联合训练过程中，视觉 Transformer（ViT）编码器被冻结。RT-H 以相同的采样率，用语言化动作指令请求和机器人动作指令请求替代了 RT-2 中的机器人动作指令请求。使用单一模型简化了训练过程，并使语言化动作指令请求和机器人动作指令请求都能从 PaLI-X 训练混合数据中蕴含的广泛先验知识中获益。\u003c/p\u003e\u003cp data-pid=\"54Ypzd9M\"\u003e\u003cb\u003eb. 语言化动作的提取\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"wNfwRR5i\"\u003e为了低成本地提取每个片段中的每个时间步的可靠语言化动作，DeepMind开发了一种依赖机器人本体感知信息的自动标注方案。\u003c/p\u003e\u003cp data-pid=\"L3yWaYd4\"\u003e首先，将机器人末端执行器位姿变化的每个维度与空间维度相关联（例如，位置变化的z轴对应上下方向）。针对所有 9 个动作维度（3 个位置增量维度、3 个姿态增量维度、2 个基座移动维度、1 个夹爪维度）执行此操作后，就能确定机器人当前的主要空间运动列表，例如 “手臂向上并向右移动”“闭合夹爪”“手臂逆时针旋转” 或 “基座向左转动”。之后，可以过滤掉低于选定 “小动作” 阈值的维度，再按照动作幅度的顺序组合得到的动作。\u003c/p\u003e\u003cp data-pid=\"KjFw1m0x\"\u003e例如，如果机器人主要是向前移动手臂，同时开始闭合夹爪，研究人员会提取出“向前移动手臂并闭合夹爪” 这一语言化动作。通过这种方式，语言的组合性使得从一组简单的已知动作中能够提取出超过 2500 种语言化动作。\u003c/p\u003e\u003cp data-pid=\"ovTe1N1R\"\u003e此外，由于这些语言化动作直接源于动作本身，因此在RT-H 中运行动作指令请求时，它们对动作本身具有很强的预测能力。\u003c/p\u003e\u003cp data-pid=\"FuO7e7fs\"\u003e然而，语言化动作在抽象层级的选择上存在基本权衡的问题：语言化动作的颗粒度越细，语言化动作指令请求的预测难度越高，但对机器人动作指令请求的指导性越强；反之亦然。\u003c/p\u003e\u003cp data-pid=\"lhWzzd7x\"\u003e\u003cb\u003e2）RT-H：推理与修正\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"GJbXOVma\"\u003e在测试阶段，RT-H首先运行语言化动作指令请求（πₕ）以推导语言化动作序列，随后将该序列输入机器人动作指令请求（πₗ）生成具体的执行动作参数。\u003c/p\u003e\u003cp data-pid=\"AWVdzlDT\"\u003e然而，由于两个指令请求需在每一步时序中串行执行，该过程使推理耗时翻倍。虽对小规模模型影响甚微，但对于RT-H中使用的55B参数的大型模型而言，必将引发不可避免的请求处理延迟问题。\u003c/p\u003e\u003cp data-pid=\"4oaZ4bjL\"\u003e为应对这一挑战，谷歌DeepMind提出两种语言化动作推理模式：\u003c/p\u003e\u003cp data-pid=\"qWxHNr_H\"\u003e\u003cb\u003ea. 异步查询：\u003c/b\u003e仅训练RT-H中的语言化动作指令请求（πₕ）预测未来一步动作。测试时，利用上一时间步推导的语言化动作执行当前机器人动作指令请求，同时并行预测下一时间步的语言化动作。此方案通过批处理查询实现与RT-2近乎同等的请求延迟。\u003c/p\u003e\u003cp data-pid=\"XLGr789B\"\u003e\u003cb\u003eb.  固定频率：\u003c/b\u003e每H步执行一次语言化动作指令请求，分摊延迟压力。\u003c/p\u003e\u003cp data-pid=\"MbXsNVz0\"\u003e在实验中，DeepMind选择异步查询方案，因语言化动作常需在精确时间步变更，无法适配固定频率带来的约束。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e\u003cb\u003e2025年：谷歌具身智能基础模型关键布局\u003c/b\u003e\u003c/h2\u003e\u003ch3\u003e\u003cb\u003e1. 2025年3月：谷歌发布具身智能大模型Gemini Robotics\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"MCEtiuDJ\"\u003e2025年3月12日，谷歌Deep Mind发布了基于多模态通用大模型Gemini2.0构建的两类大模型：Gemini Robotics（VLA）和Gemini Robotics-ER（VLM）。\u003c/p\u003e\u003cp data-pid=\"XdgqD58-\"\u003e\u003cb\u003e1） Gemini Robotics-ER\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"wuyXfR5Z\"\u003eGemini Robotics - ER（VLM模型）,其中ER 代表 “embodied reasoning”（具身推理），将Gemini的多模态推理能力扩展至物理世界，具备增强的空间和时间理解能力，包括物体检测、指向、轨迹预测和抓取预测等2D空间概念理解能力，以及多视角3D场景理解和3D边界框检测等3D空间推理能力。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-3404e8a3e7f8601df6213d93ed64d0f9_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"740\" data-original-token=\"v2-3404e8a3e7f8601df6213d93ed64d0f9\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-3404e8a3e7f8601df6213d93ed64d0f9_r.jpg\"/\u003e\u003cfigcaption\u003e多视角3D场景理解： 通过关联不同视角的2D点来理解3D场景\u003c/figcaption\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"Bkb6Y_jK\"\u003e\u003cb\u003ea. 支持零样本和少样本机器人控制\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"_h8VNjIA\"\u003e在实验中，研究人员使用Gemini 2.0 Flash和Gemini Robotics-ER两类模型，分别采用两种不同的机器人控制方法进行实验。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"adWEPfCu\"\u003e\u003cb\u003e零样本（zero-shot）机器人控制\u003c/b\u003e——通过代码生成控制机器人。\u003c/li\u003e\u003cli data-pid=\"TWtLbGK0\"\u003e\u003cb\u003e少样本（few-shot）控制\u003c/b\u003e——通过上下文学习（in-context learning, ICL），基于少量示例适应新行为。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-43dc96ee542117393b0be881d50aeab4_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"247\" data-original-token=\"v2-43dc96ee542117393b0be881d50aeab4\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-43dc96ee542117393b0be881d50aeab4_r.jpg\"/\u003e\u003cfigcaption\u003e两类模型在模拟环境中执行一组操作任务的结果对比\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"zlY2OVU_\"\u003e\u003cb\u003e备注：\u003c/b\u003e这些任务涵盖了不同难度和物体类型，从简单的抓取任务（如抬起香蕉）到长时序、多步骤、多任务的操作（如将玩具放入盒子并关闭盒子）。\u003c/p\u003e\u003cp data-pid=\"outRgCpj\"\u003e试验结果表明，Gemini Robotics-ER 在两种控制方式下的任务完成率均表现良好。Gemini Robotics-ER能够利用上下文学习（in-context learning），仅凭少量示例就能提高更复杂的灵巧双臂任务（如折叠衣物）的执行能力，并能够直接输出末端执行器的轨迹以完成任务。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"IkycgYwt\"\u003e\u003cb\u003e在零样本机器人控制方面\u003c/b\u003e，Gemini Robotics-ER任务完成率相比Gemini 2.0 提高了近2倍。\u003c/li\u003e\u003cli data-pid=\"0X-sjk9A\"\u003e\u003cb\u003e在少样本机器人控制方面\u003c/b\u003e，Gemini 2.0 Flash 在模拟环境中平均成功率达到51%。然而，Gemini Robotics-ER 在模拟环境中的平均成功率达到 65%。\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"PdPY0k0O\"\u003e另外，实验还表明，模型的具身推理能力与下游机器人控制的性能之间存在强相关性。Gemini Robotics-ER 可以直接用于机器人控制，包括：作为感知模块（如物体检测），规划模块（如轨迹生成）以及通过生成和执行代码来协调机器人运动。\u003c/p\u003e\u003cp data-pid=\"OxHMm9_8\"\u003e不过，Gemini Robotics-ER作为VLM模型，也存在局限性，尤其是在更复杂的灵巧操作任务上。这主要是因为需要额外的中间步骤来将模型的具身推理能力与机器人执行动作关联起来。\u003c/p\u003e\u003cp data-pid=\"OSxuQilG\"\u003e\u003cb\u003e2）Gemini Robotics\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"mUZfGTPL\"\u003eGemini Robotics是一种端到端的VLA（视觉-语言-行动）模型，将强大的具身推理先验与现实世界机器人的灵巧低级控制相结合，能够在不同环境下解决灵巧任务，并支持不同的机器人形态。\u003c/p\u003e\u003cp data-pid=\"iFBeRfvu\"\u003eGemini Robotics是Gemini Robotics-ER的一个衍生版本，采用了双组件架构：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"vtYDXXus\"\u003e\u003cb\u003eGemini Robotics 主干网络：托管在云端，负责视觉-语言推理。\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"S9B3rp27\"\u003e\u003cb\u003eGemini Robotics 解码器：运行在机器人控制器上，负责动作执行。\u003c/b\u003e\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"KHIO98JP\"\u003eGemini Robotics 主干网络由Gemini Robotics-ER的一个蒸馏版本（distilled version）组成，其查询-响应延迟已优化至小于160ms（相比原始模型减少了数秒）。为了补偿主干网络的延迟，Gemini Robotics解码器在本地执行低级控制。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-8893b32c86d7b0ee7dc87a3be8370538_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"401\" data-original-token=\"v2-8893b32c86d7b0ee7dc87a3be8370538\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8893b32c86d7b0ee7dc87a3be8370538_r.jpg\"/\u003e\u003cfigcaption\u003eGemini Robotics模型架构概览\u003c/figcaption\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"nclVwqgs\"\u003e\u003cb\u003e3）Gemini Robotics 的优势\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"zgnTaztS\"\u003e\u003cb\u003eGemini Robotics模型在精细的柔性布料操作、铰接物体精准操控等多样化任务中展现突出能力。研究人员把该模型能力突出的原因归结于：\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"1UJhSXJJ\"\u003e强大的视觉语言模型，具备增强的具身推理能力；\u003c/li\u003e\u003cli data-pid=\"MSPeYmeC\"\u003e针对机器人任务，采用大规模机器人动作数据与多样化的非机器人数据的特定训练方案；\u003c/li\u003e\u003cli data-pid=\"rTlgDFiF\"\u003e专为低延迟机器人控制设计的独特架构。\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"iFRhn5v3\"\u003e\u003cb\u003eGemini Robotics模型的关键优势在于：\u003c/b\u003e成功继承Gemini Robotics-ER的具身推理特性，能高效遵循开放词汇指令，并展现强大的零样本泛化能力。通过微调实现专项适应，该模型在新任务/新实体形态中达成较高操作精度，并在挑战性场景中保持泛化能力。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e\u003cb\u003e2. 2025年6月：谷歌发布设备端具身智能模型Gemini Robotics On-Device\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"2_KmguUb\"\u003e2025年6月，谷歌DeepMind正式发布机器人模型Gemini Robotics On-Device。它是视觉-语言-动作（VLA）模型 Gemini Robotics 的轻量化版本。该模型重点解决在设备端部署的问题。\u003c/p\u003e\u003cp data-pid=\"77oiqiuv\"\u003e据悉，Gemini Robotics On-Device是首个支持本地微调的具身智能VLA模型。模型经过计算资源压缩，可在 Franka FR3 机械臂、Apollo 人形机器人等边缘设备上实现低延迟推理（\u0026lt;100ms），并支持全链路离线运行。\u003c/p\u003e\u003cp data-pid=\"Jc-xA-zV\"\u003e在任务泛化能力方面，在Visual Gen（视觉泛化）、Semantic Gen（语义泛化）、Action Gen（动作泛化） 三项核心测试中，该模型得分均接近旗舰版模型Gemini Robotics，且超越此前最佳设备端模型，尤其在处理未见过的物体（如异形拉链袋）和复杂多步骤指令（如 “拉开袋子→取出物品→折叠衣物”）时表现突出。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-14ca5cec6b62642cd52bf2db4bc2e265_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"667\" data-original-token=\"v2-14ca5cec6b62642cd52bf2db4bc2e265\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-14ca5cec6b62642cd52bf2db4bc2e265_r.jpg\"/\u003e\u003cfigcaption\u003e模型的任务泛化能力比较\u003c/figcaption\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"VP4rk4aF\"\u003e同时，谷歌还推出Gemini Robotics SDK，提供MuJoCo 物理模拟器集成、示范数据标注工具及模型微调接口，帮助开发者评估Gemini Robotics 在设备上的性能。开发者可通过 50-100 次真实操作演示完成模型适配。这一数据量显著低于传统强化学习方法（通常需数千次迭代），体现了该模型的高效迁移学习能力。\u003c/p\u003e\u003cp data-pid=\"G_49PCPC\"\u003eGemini Robotics On-Device的推出，是对机器人开发范式的革新。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"EzF3y4fV\"\u003eGemini Robotics On-Device 实现了端到端本地化运行，解决网络中断场景的可靠性问题，对工业巡检、应急救援等关键领域具有战略意义。\u003c/li\u003e\u003cli data-pid=\"PYQGQ3_o\"\u003e该模型通过高效的微调能力，显著降低跨平台适配成本，可扩展至Apollo人形机器人与Franka FR3机械臂等不同形态。\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e\u003cb\u003e结语\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"G8_UHnQ-\"\u003e具身智能的核心是让AI 从 “数字世界” 走向 “物理世界”，而物理推理能力是其关键瓶颈 —— 即模型能否让机器人像人类一样理解物体的物理属性（重量、硬度、弹性）、空间关系（距离、遮挡、方位）、因果逻辑（推、拉、碰撞的后果）等。\u003c/p\u003e\u003cp data-pid=\"TUJ2kaO2\"\u003e“物理推理能力” 的核心主体是具身智能大模型，机器人是这一能力的 “物理执行者”。谷歌在具身智能基础模型领域布局的本质是通过大模型突破物理推理的技术壁垒，再借助跨平台动作泛化框架与端云协同部署，实现机器人在现实应用场景中的可靠落地。\u003c/p\u003e\u003cp data-pid=\"_NT7QTcz\"\u003e当前，谷歌在正以「通用智能模型赋能者 + 跨场景生态协同者」的定位重塑行业技术范式。其通过 RT 系列与 Gemini Robotics 大模型构建核心智能引擎，结合 Open X-Embodiment 开源数据集与 Gemini Robotics SDK 开发工具，形成从模型研发到应用落地的全链条支撑体系，通过技术迭代与生态联动构建具身智能护城河。\u003c/p\u003e\u003cp data-pid=\"Q-s--Xki\"\u003e未来，随着具身智能基础模型物理推理能力的持续提升，谷歌有望成为具身智能时代“物理世界语言” 的标准制定者。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e\u003cb\u003e参考资料：\u003c/b\u003e\u003cbr/\u003e\u003c/h2\u003e\u003cp data-pid=\"WIiJNYMq\"\u003e1.论文：Do As I Can, Not As I Say: Grounding Language in Robotic Affordances\u003c/p\u003e\u003cp data-pid=\"dVbDLHYT\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2204.01691\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2204.0169\u003c/span\u003e\u003cspan class=\"invisible\"\u003e1\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"d9hlvNiY\"\u003e2.论文：A Generalist Agent\u003c/p\u003e\u003cp data-pid=\"XOeejOTC\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2205.06175\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2205.0617\u003c/span\u003e\u003cspan class=\"invisible\"\u003e5\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"fljfxOrb\"\u003e3.论文：RoboCat： a self-improving generalist agent for robotic manipulation\u003c/p\u003e\u003cp data-pid=\"TxxV-lO0\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2306.11706\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2306.1170\u003c/span\u003e\u003cspan class=\"invisible\"\u003e6\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"5-3ZMXvO\"\u003e4.论文：RT-1：Robotics Transformer for Real-World Control at Scale\u003c/p\u003e\u003cp data-pid=\"EFdE2q97\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//robotics-transformer.github.io/assets/rt1.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003erobotics-transformer.github.io\u003c/span\u003e\u003cspan class=\"invisible\"\u003e/assets/rt1.pdf\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"B-VkIiyZ\"\u003e5.论文：PaLM-E: An Embodied Multimodal Language Model\u003c/p\u003e\u003cp data-pid=\"W-dsEvS8\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2303.03378\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2303.0337\u003c/span\u003e\u003cspan class=\"invisible\"\u003e8\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"jBc6iC9g\"\u003e6.论文：RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control\u003c/p\u003e\u003cp data-pid=\"g6v_5DwH\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2307.15818\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2307.1581\u003c/span\u003e\u003cspan class=\"invisible\"\u003e8\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"QfUpI0-m\"\u003e7.论文：Open X-Embodiment: Robotic Learning Datasets and RT-X Models \u003c/p\u003e\u003cp data-pid=\"lu375eRJ\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.08864v8\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2310.0886\u003c/span\u003e\u003cspan class=\"invisible\"\u003e4v8\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"ALsNhu4M\"\u003e8.论文：RT-H: Action Hierarchies Using Language \u003c/p\u003e\u003cp data-pid=\"hp8W0pnr\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2403.01823\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2403.0182\u003c/span\u003e\u003cspan class=\"invisible\"\u003e3\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"O4nR82pj\"\u003e9.论文：Gemini Robotics: Bringing AI into the Physical World\u003c/p\u003e\u003cp data-pid=\"IfbOPVdz\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2503.20020\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003earxiv.org/pdf/2503.2002\u003c/span\u003e\u003cspan class=\"invisible\"\u003e0\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":98,"thumbnails":["https://pica.zhimg.com/v2-c5d2af4fa285f936873fdcdaf93ad5e2.jpg?source=7e7ef6e2\u0026needBackground=1","https://pica.zhimg.com/50/v2-43583433938f7111e89092e4ee13ed2c_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-9e726558f4483db3996aa48e788b2a06_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-af90ab51ececcfd9213d9d6bc15d676b_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-ca1cef915b6926958af0a51542a3ef21_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-a3065c0f108e2264958ed645b5866c0a_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-3404a87f7fc4d74ed3e07c5709ab42f5_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-2bd97b3c74998980ad8c00f843f363ad_720w.jpg?source=b6762063"],"favorite_count":25,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1930366515322019969}","attached_info":"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","action_card":false},{"id":"67_1753853777.196","type":"feed","offset":67,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853777,"updated_time":1753853777,"target":{"id":"1928255469778088661","type":"answer","url":"https://api.zhihu.com/answers/1928255469778088661","author":{"id":"80d94dc338d149689242bfa66df6fd25","url":"https://api.zhihu.com/people/80d94dc338d149689242bfa66df6fd25","user_type":"people","url_token":"yan-man-tian-2","name":"烟漫天","headline":"观光者","avatar_url":"https://picx.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":665,"is_following":false,"is_followed":false},"created_time":1752511929,"updated_time":1752511929,"voteup_count":988,"thanks_count":28,"comment_count":255,"is_copyable":true,"question":{"id":"473989265","type":"question","url":"https://api.zhihu.com/questions/473989265","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://picx.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"红楼梦癸酉本是真本吗？","created":1626938855,"answer_count":0,"follower_count":0,"comment_count":4,"bound_topic_ids":[248,285,20104,193144],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"真本无疑。 当我还没有看前面的内容，直接翻看到最后的情榜时，我被震惊了！红楼梦夹批里面曾提到过有情榜，但从来没有人见过。癸酉本是第一个。金陵十二钗，副册，又副册，每个角色一个情名，情主、情尊、情痴、情呆、情烈、情不情......出乎意料又在情理之中，无比贴切，让人拍案叫绝。好一个风月宝鉴。情榜一出，举世皆惊，谁与争锋！这不就是明代小说的套路么，水浒传的108好汉天罡地煞，封神演义的封神榜，西游记也有功成封…","excerpt_new":"真本无疑。 当我还没有看前面的内容，直接翻看到最后的情榜时，我被震惊了！红楼梦夹批里面曾提到过有情榜，但从来没有人见过。癸酉本是第一个。金陵十二钗，副册，又副册，每个角色一个情名，情主、情尊、情痴、情呆、情烈、情不情......出乎意料又在情理之中，无比贴切，让人拍案叫绝。好一个风月宝鉴。情榜一出，举世皆惊，谁与争锋！这不就是明代小说的套路么，水浒传的108好汉天罡地煞，封神演义的封神榜，西游记也有功成封…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"xTmXJ-C_\"\u003e真本无疑。\u003c/p\u003e\u003cp data-pid=\"N6-qedfx\"\u003e当我还没有看前面的内容，直接翻看到最后的情榜时，我被震惊了！红楼梦夹批里面曾提到过有情榜，但从来没有人见过。癸酉本是第一个。金陵十二钗，副册，又副册，每个角色一个情名，情主、情尊、情痴、情呆、情烈、情不情......出乎意料又在情理之中，无比贴切，让人拍案叫绝。好一个风月宝鉴。情榜一出，举世皆惊，谁与争锋！这不就是明代小说的套路么，水浒传的108好汉天罡地煞，封神演义的封神榜，西游记也有功成封佛，伟大的作品一脉相承。\u003c/p\u003e\u003cp data-pid=\"bc0Ui2Yu\"\u003e就这情榜，现代人，没有很高文学造诣的，别说想出来，就是列出十个都难，还要和人物的性格无比贴切，难之又难。即使当今中国聚集顶尖的文人团队研究10年，恐怕都整不出来。就凭这一点，癸酉本都不可能为假。如果这是假的，这个作者有如此高超精湛、出神入化的文笔，随便写个什么都出大名了。还需要作假么。\u003c/p\u003e\u003cp data-pid=\"jduyO1LT\"\u003e另外，说癸酉本文字差的人，是有多眼瞎，那么多的诗词，那么多的神来之笔，那么多暗线的巧妙回应，现代人能写出来哪怕一首一段么？谁能仿的这么天衣无缝？有癸酉本哪怕十分之一的功力，在当代恐怕都是大文豪了，再差也是中文系教授了。如果这样，那他根本没必要仿写，直接像那些红学家一样出名赚钱不香么？还需要隐姓埋名？也正是癸酉本的直白，才更凸显出他的真实。因为他没有像前80回那样，经历那么多的润色修改。但已经很好了。整书的机构分为春夏秋冬，一季节27章左右，春长夏灿秋萧冬亡，这丢失的28章正是严寒的冬天，是家族和王朝的冬天，处处显示出肃杀、死亡和绝望。这是文字该有的气质和力量。\u003c/p\u003e\u003cp data-pid=\"RzjjOAJu\"\u003e那为什么那么多人不敢承认癸酉本为真本呢？\u003c/p\u003e\u003cp data-pid=\"Cf5eUiB9\"\u003e无他，动了太多人的奶酪。打了满清遗老的脸。\u003c/p\u003e\u003cp data-pid=\"JKrxdCnJ\"\u003e好多所谓的红学家还要靠红楼梦来赚钱，好多所谓的网红、爱好者，以前也发表了不少解读视频、研究文章，现在倒好，癸酉本真本出来了，他们之前研究的东西都成了笑话，这不比杀了他们还难受啊。俗语说的好，断人财路，如杀人父母。触动利益比触动灵魂还难。\u003c/p\u003e\u003cp data-pid=\"eghwfHsk\"\u003e这就好比一群人在那解一道没出完的难题，公说公有理，婆说婆有理，争来争去都能到处讲学出书赚钱，这下可好，题目一下出完整了，正确答案也有了。我的乖乖，这些人都懵了，这不是说明之前的解读很傻么，他们不恼羞成怒才怪。这些人现在也不争了，转而一致对外，拼命污蔑打击，欲除之而后快。无他，刨了他们的利益之根而已。\u003c/p\u003e\u003cp data-pid=\"2d7hujdG\"\u003e再说这样一个伟大的作品，居然讲的不是曹性家奴的一家私事，而是悼明讽清，记录的是改朝换代的血淋淋的历史，这让那些清朝的遗老遗少们脸往哪搁呀。一讨论起来，仿佛就在抽他们的脸，他们能答应吗？大家记得十多年前满屏幕的辫子戏么，他们在文艺圈的势力不可小觑，动不动就怀念爱新觉罗祖上的荣光。你现在对他们说，古典四大小说之首的红楼梦就是批判他们祖宗的，用隐晦的笔法记录下来，让后人不要忘记满清历史上干的那些坏事，他们不跟你拼命才怪？！这样一想，自然啥都明白了。\u003c/p\u003e\u003cp data-pid=\"4h_-6Sds\"\u003e很简单的道理，如果红楼梦如果不是写的明亡血泪史，不是“家亡血史（贾王薛史）”，怎么可能让作者或修改者“一把辛酸泪”，“增删五次，批阅十载”，怎么可能“千红一窟，万艳同悲”，又怎么可能“天地白茫茫一片真干净”，更不用说“贾敬（嘉靖）”“昨夜朱楼梦，今宵水龙吟”等，几乎都是明示了。\u003c/p\u003e\u003cp data-pid=\"KFb7HH4X\"\u003e作者既怕你一眼看懂，导致书被禁，更怕书的结局被篡改，导致你看不懂，所以开头就各种判词，暗线，疯狂暗示，想尽办法让读者明白其良苦用心。这样一部奇书，后28回的高潮太过显眼，不被改编的话绝无可能流传下去，想想清朝恐怖的文字狱。而改编了又容易被不求甚解的人误读。作者心里苦啊。\u003c/p\u003e\u003cp data-pid=\"_HYJcsUh\"\u003e所以我猜想在历史上，后28回应该就是和前80回分开的，偷偷流传的。前80回可以在市面上正常流通，有人看不到结局，就张罗着补写，但不能写的太透，目的瞒过审查，公开发行挣钱。后28回在相关人的小圈子里隐秘流转，至少批书的人都是看过的。那么多人去批示而且都流传下来的原因，也是为了让读者在没看到后28回结局的情况下，尽可能地理解到书的真正的主旨。这是一个时代的文人的集体悲歌。\u003c/p\u003e\u003cp data-pid=\"o_VYzua7\"\u003e癸酉本的横空出世，是红楼之幸，汉家之幸，中华文明之幸。他完美回应了前80回埋下的各种伏笔，把红楼梦的悲剧，把天下兴亡的无奈，把繁华如梦的慨叹，把改朝换代、被异族统治的悲愤羞愧，演绎的惊苍天、泣鬼神，演绎的惊心动魄、抚卷泪流，让更多的人能毫无障碍地真正地读懂了这本书，让吴梅村等人的心血没有白流。我读了几遍，每次读完后，只有深深的震撼，只剩深深的叹服，这才是当之无愧的古典四大名著之首，这才是毫无争议的中国第一小说，这才应该是享誉世界的第一小说。可以这样说，有了癸酉本的后28回，红楼梦才真正完整了，才真正伟大起来！\u003c/p\u003e\u003cp data-pid=\"YeJH1bQi\"\u003e我有个梦想，也有很深的期望，希望能有一天，有导演或话剧团能把癸酉本的红楼梦搬上荧幕、搬上舞台：在贾府上下张灯结彩、兴高采烈的时候，在宝玉和黛玉即将成婚的前夜，抄家的官兵却突然而至，喜剧变悲剧，高潮戛然而止，形势急转而下，一个家族由盛而衰而亡，一个原本强大富饶的国家渐渐倾覆，这是何等的壮烈，何等的遗憾，如果能演出来，又是何等的激荡人心、催人泪下。这定然能超越古今中外的一众名场景了，哈姆雷特、泰坦尼克号也会在这巨作面前黯然失色。\u003c/p\u003e\u003cp data-pid=\"h-HrABcD\"\u003e我乐观地相信，等认真读过癸酉本的人越来越多，等年轻一代掌握更多的话语权，这一天迟早会来到。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":102983,"favorite_count":409,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1928255469778088661}","attached_info":"CuEFCJjd2/mMsvnglQEQBBoJNzM3MDQ0NzgzILnr1MMGKNwHMP8BQENKPwoqVFNfU09VUkNFX1pSRUNBTExfRkVFRFJFX05FV0JJRV9IT1VSTFlfUlVNEgEwGAAgADoKeyJyYXciOiIifVoINjc2MTA3MDhiIGIzN2MyNDFiMThhMDE0OTQyYTk0MmIzNWZmZDIyMDU5chMxOTI4MjU1NDY5Nzc4MDg4NjYxigEJNDczOTg5MjY1qgEJcmVjb21tZW5kwgEgODBkOTRkYzMzOGQxNDk2ODkyNDJiZmE2NmRmNmZkMjXyAQoIDBIGTm9ybWFs8gEoCAoSJDc5Nzc2NWI0LWM4YWYtNGI2OC05MGMxLWQzNzU2ODMyMWNiOPIBBggLEgIxMoICAIgC7qjTzoUzkgIgODBkOTRkYzMzOGQxNDk2ODkyNDJiZmE2NmRmNmZkMjWaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIUQ29udGVudEFnZVdlaWdodFJ1bGXaAipUU19TT1VSQ0VfWlJFQ0FMTF9GRUVEUkVfTkVXQklFX0hPVVJMWV9SVU3oAgP6AgtOT1JNQUxfRkxPV4oDIDg2M2JjNWUwYmVmZDQxNDI5NWVhMmYxMzU3OWVhM2ZlmgMNCgJ2MhAAGgVvdGhlcqgDx6QG2AMA6gMQbmV3YmllX2ZlZWRyZV92MvoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQGbWFudWFswgQDMTcwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAABAyuyjP4EFAAAAAAAAAACJBUzdtkaeY9M/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQyQBgCgBkOoBgOSAi4KCTczNzA0NDc4MxITMTkyODI1NTQ2OTc3ODA4ODY2MRgEIgpJTUFHRV9URVhU","action_card":false},{"id":"68_1753853777.457","type":"feed","offset":68,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853777,"updated_time":1753853777,"target":{"id":"1929665020456834901","type":"answer","url":"https://api.zhihu.com/answers/1929665020456834901","author":{"id":"0c60cf5968d90aadeedacaa8aaf002cb","url":"https://api.zhihu.com/people/0c60cf5968d90aadeedacaa8aaf002cb","user_type":"people","url_token":"kaer-11","name":"kaer","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-7f861d79816a0313db664751689b1acd_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":13368,"is_following":false,"is_followed":false},"created_time":1752847992,"updated_time":1752847992,"voteup_count":1802,"thanks_count":115,"comment_count":73,"is_copyable":true,"question":{"id":"20460015","type":"question","url":"https://api.zhihu.com/questions/20460015","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://picx.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"为什么从事技术的人普遍都比较难沟通？","created":1346747394,"answer_count":0,"follower_count":0,"comment_count":72,"bound_topic_ids":[99,707],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"说一下我的看法： 由于岗位特性，技术人员在沟通中，总是负收益的，没有正面激励。当我们跟技术人员沟通的时候， 一般否是让他多做事。“这个设计需求你满足一下” “那个方案你再修改一下” 每当有人跟技术人员沟通的时候，技术人员就要多做事。 设想一下，要是每次有人跟你说话就让你产生负面收益，你也会变得难讲话。 我们一般认为销售人员很好讲话，善于沟通，好讲话，那是因为每次有人跟销售人员沟通的时候，都是给销售员“…","excerpt_new":"说一下我的看法： 由于岗位特性，技术人员在沟通中，总是负收益的，没有正面激励。当我们跟技术人员沟通的时候， 一般否是让他多做事。“这个设计需求你满足一下” “那个方案你再修改一下” 每当有人跟技术人员沟通的时候，技术人员就要多做事。 设想一下，要是每次有人跟你说话就让你产生负面收益，你也会变得难讲话。 我们一般认为销售人员很好讲话，善于沟通，好讲话，那是因为每次有人跟销售人员沟通的时候，都是给销售员“…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"wxqC3-U9\"\u003e说一下我的看法：\u003c/p\u003e\u003cp data-pid=\"BmKOq3qG\"\u003e\u003cb\u003e由于岗位特性，技术人员在沟通中，总是负收益的，没有正面激励。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"pdyyrTb8\"\u003e当我们跟技术人员沟通的时候，\u003cb\u003e一般否是让他多做事。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"BZy5JI6d\"\u003e“这个设计需求你满足一下”\u003c/p\u003e\u003cp data-pid=\"ceovsCHs\"\u003e“那个方案你再修改一下”\u003c/p\u003e\u003cp data-pid=\"qtVt6qZt\"\u003e每当有人跟技术人员沟通的时候，技术人员就要多做事。\u003c/p\u003e\u003cp data-pid=\"aSMq_bsf\"\u003e设想一下，要是每次有人跟你说话就让你产生负面收益，你也会变得难讲话。\u003c/p\u003e\u003cp data-pid=\"DO8zJInu\"\u003e我们一般认为销售人员很好讲话，善于沟通，好讲话，那是因为每次有人跟销售人员沟通的时候，都是给销售员“送钱”。\u003c/p\u003e\u003cp data-pid=\"KZLVB0uy\"\u003e这一点我是很有体会的。\u003c/p\u003e\u003cp data-pid=\"MvrZzkhH\"\u003e我原本是一个性格内向不善言辞的人，并且不自觉地喜欢怼人，没耐心。\u003c/p\u003e\u003cp data-pid=\"N0rWFoPg\"\u003e与人沟通时，每当对方表述不清需求，我就非常厌烦。\u003c/p\u003e\u003cp data-pid=\"3xvJafGU\"\u003e他一开口说话，七里八里说不清个事。\u003c/p\u003e\u003cp data-pid=\"3I4xz_PR\"\u003e你给他讲解，他一脸呆萌地看着你，不断重复地提出你已经给他解答过的问题。\u003c/p\u003e\u003cp data-pid=\"Xjeiy2If\"\u003e我这暴脾气真是分分钟想要打人啊。\u003c/p\u003e\u003cp data-pid=\"NDpu0Y12\"\u003e自从我做了销售之后，我发现用好的方式，好的语气，好的言辞，\u003cb\u003e与人沟通可以产生直接的经济收益。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"PSxfc7Fu\"\u003e遇到那种讲不清自己需求的人，我会很耐心地根据我的经验去分析、揣测，把他的需求讲清楚，最后问他，你是这个需求吗？\u003c/p\u003e\u003cp data-pid=\"5160IzNr\"\u003e他会说，啊对对对，我就是这个意思。\u003c/p\u003e\u003cp data-pid=\"c6Z0bhPd\"\u003e我说，那你付款吧。\u003c/p\u003e\u003cp data-pid=\"VD6LUpZB\"\u003eDuang一下，我就进账一百块。\u003c/p\u003e\u003cp data-pid=\"pZFl-lj6\"\u003e遇到那种听不懂话的人，我会去设身处地地想理解对方的知识背景，尽量用他能听懂的语汇给他解释。\u003c/p\u003e\u003cp data-pid=\"k0G4iTbK\"\u003e他终于恍然大悟，啊对对对，你讲的真好。\u003c/p\u003e\u003cp data-pid=\"SyPgz_F8\"\u003e我说，你支付宝还是微信。\u003c/p\u003e\u003cp data-pid=\"qF4rY8WU\"\u003eDuang一下，我又进账一百块。\u003c/p\u003e\u003cp data-pid=\"7MGdbBtA\"\u003e啊，真是快乐啊，我喜欢那种有沟通难度的客户，因为没人跟我抢，\u003cb\u003e而他们愿意为我的沟通能力付费。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Fjzq6a3i\"\u003e我就这样吊儿郎当，整天在公司晃晃悠悠，轻轻松松月入过万。\u003c/p\u003e\u003cp data-pid=\"Ed-gBstW\"\u003e因为专业能力比较强，我还做过一段时间的支持岗。\u003c/p\u003e\u003cp data-pid=\"LNlOiflF\"\u003e真是甘妮娘啊！这个人找我做方案，那个人找我改报告，忙得不亦乐乎。\u003c/p\u003e\u003cp data-pid=\"ONi3L7ih\"\u003e这个鸟活不赚钱啊，拿的固定工资。\u003c/p\u003e\u003cp data-pid=\"FwXlhVox\"\u003e有人来找我，就意味着我要忙碌起来了。\u003c/p\u003e\u003cp data-pid=\"OupwkjPL\"\u003e只要有人找，我就多一份活。\u003c/p\u003e\u003cp data-pid=\"KUdeXxNW\"\u003e这帮鸟人还老是临下班的时候找我。\u003c/p\u003e\u003cp data-pid=\"luCpHgeK\"\u003e只要一沟通，劳资就要加班。\u003c/p\u003e\u003cp data-pid=\"M6_xCVeh\"\u003e我可去你大爷的吧。\u003c/p\u003e\u003cp data-pid=\"ApMO_lWZ\"\u003e碰到说不清自己需求的人，我就让他滚回去自己捋清楚了再来说。\u003c/p\u003e\u003cp data-pid=\"HcMmEUTu\"\u003e我可不会像以前一起循循善诱地跟他沟通，揣摩他的需求，引导他讲清楚需求。\u003c/p\u003e\u003cp data-pid=\"yzaBaXYi\"\u003e我要是这样做，那就要天天加班。\u003c/p\u003e\u003cp data-pid=\"pJtQmNFN\"\u003e即便如此，我依然是最专业最好讲话的支持岗。\u003c/p\u003e\u003cp data-pid=\"hAj04xZw\"\u003e毕竟我已经练出来。\u003c/p\u003e\u003cp data-pid=\"dBt3kSMQ\"\u003e拿着同样的工资，我的活永远干不完。这些人宁愿在我这里排队，也不愿意去找别的支持岗解决问题。\u003c/p\u003e\u003cp data-pid=\"NL4wAurI\"\u003e后来我晋升了，终于不用干那个鸟活了。\u003c/p\u003e\u003cp data-pid=\"tMGLPBH_\"\u003e我干过销售岗和支持岗，所以我理解两类岗位的心态和情景。\u003c/p\u003e\u003cp data-pid=\"l5N1fAew\"\u003e对于支持岗来说，沟通完全是负收益的。\u003c/p\u003e\u003cp data-pid=\"RQ4UFwq3\"\u003e做这样岗位的人，实在没有动力去提升自己的沟通能力。\u003c/p\u003e\u003cp data-pid=\"rIEKD8BJ\"\u003e即便像我这样沟通能力尚可的人，也不愿意把沟通能力用在工作中。\u003c/p\u003e\u003cp data-pid=\"YhEg7N4O\"\u003e只要一沟通，劳资就要加班。\u003c/p\u003e\u003cp data-pid=\"Lbze7w8q\"\u003e我要是听不懂话，不好讲话，那就没人找我，又是清闲的一天。\u003c/p\u003e\u003cp data-pid=\"AFetRtTF\"\u003e这很现实。\u003c/p\u003e\u003cp data-pid=\"EbVNgcqU\"\u003e换谁都是这样的。没人会在这样的工作内容中提升和运用自己的沟通能力。\u003c/p\u003e\u003cp data-pid=\"PbZy-e-K\"\u003e我认为这是一个重要原因。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":66064,"favorite_count":944,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1929665020456834901}","attached_info":"CsIFCJjd2/mMsvnglQEQBBoJNzM3NzIwMjc2IPis6cMGKIoOMElAREowChtUU19TT1VSQ0VfQkFTSUNfSU5GT19SRUNBTEwSATAYACAAOgp7InJhdyI6IiJ9WgYzNjQ0MDZiIGIzN2MyNDFiMThhMDE0OTQyYTk0MmIzNWZmZDIyMDU5chMxOTI5NjY1MDIwNDU2ODM0OTAxigEIMjA0NjAwMTWqAQlyZWNvbW1lbmTCASAwYzYwY2Y1OTY4ZDkwYWFkZWVkYWNhYThhYWYwMDJjYvIBCggMEgZOb3JtYWzyASgIChIkMDQ2OWQxMmItNWVmOS00ZDk3LTk1OTUtODcxMTA0OGY4MGIw8gEGCAsSAjEyggIAiALuqNPOhTOSAiAwYzYwY2Y1OTY4ZDkwYWFkZWVkYWNhYThhYWYwMDJjYpoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhZSZXZpc2l0VmFsdWVXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxl2gIbVFNfU09VUkNFX0JBU0lDX0lORk9fUkVDQUxM6AIC+gILTk9STUFMX0ZMT1eKAyA4NjNiYzVlMGJlZmQ0MTQyOTVlYTJmMTM1NzllYTNmZZoDDQoCdjIQABoFb3RoZXKoA5CEBNgDAOoDEWJhc2ljX2luZm9fcmVjYWxs+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATKgBACoBACwBAC6BAZtYW51YWzCBAMxNzDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAABd6rA/gQUAAAAAAAAAAIkFTN22Rp5j0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFDJAGAKAGRKgGAJICLgoJNzM3NzIwMjc2EhMxOTI5NjY1MDIwNDU2ODM0OTAxGAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"69_1753853777.281","type":"feed","offset":69,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853777,"updated_time":1753853777,"target":{"id":"1922139814511224697","type":"answer","url":"https://api.zhihu.com/answers/1922139814511224697","author":{"id":"55bf15a75efd2f8167c9be2d91fd6453","url":"https://api.zhihu.com/people/55bf15a75efd2f8167c9be2d91fd6453","user_type":"people","url_token":"he-yun-long-50","name":"宇佐美","headline":"喜欢的东西是大混乱，讨厌的东西当然是犯罪者，不喜欢也不讨厌的东西则是橡皮擦哦w","avatar_url":"https://pica.zhimg.com/50/v2-27a280b926e80a4f72095aa4b887ff67_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":9973,"is_following":false,"is_followed":false},"created_time":1751053843,"updated_time":1751055089,"voteup_count":3695,"thanks_count":248,"comment_count":215,"is_copyable":true,"question":{"id":"28726368","type":"question","url":"https://api.zhihu.com/questions/28726368","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://picx.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"男人的性魅力是什么?","created":1426217850,"answer_count":0,"follower_count":0,"comment_count":8,"bound_topic_ids":[404,740,4186,4729,114909],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"男性的一切优秀特质最终都可以归结为勇敢。 著名的政治家马基雅维利有一个核心观点——Fortune favors the bold.时运偏爱勇者。 这一观点虽说是体现了马基雅维利基于文艺复兴时期的意大利分崩离析的状况和疲软不堪的政治体制下，对于一个手腕强硬的政治领导者的渴望，其实用于谈论“性魅力”这个话题同样特别契合。 这是因为在马基雅维利看来，“时运”，或者说古罗马神话中的“命运女神”，是一种不可捉摸、难以控制的自然力量…","excerpt_new":"男性的一切优秀特质最终都可以归结为勇敢。 著名的政治家马基雅维利有一个核心观点——Fortune favors the bold.时运偏爱勇者。 这一观点虽说是体现了马基雅维利基于文艺复兴时期的意大利分崩离析的状况和疲软不堪的政治体制下，对于一个手腕强硬的政治领导者的渴望，其实用于谈论“性魅力”这个话题同样特别契合。 这是因为在马基雅维利看来，“时运”，或者说古罗马神话中的“命运女神”，是一种不可捉摸、难以控制的自然力量…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"Lju1sZAM\"\u003e男性的一切优秀特质最终都可以归结为勇敢。\u003c/p\u003e\u003cp data-pid=\"cDbK3VnC\"\u003e著名的政治家马基雅维利有一个核心观点——Fortune favors the bold.时运偏爱勇者。\u003c/p\u003e\u003cp data-pid=\"XmOhifvJ\"\u003e这一观点虽说是体现了马基雅维利基于文艺复兴时期的意大利分崩离析的状况和疲软不堪的政治体制下，对于一个手腕强硬的政治领导者的渴望，其实用于谈论“性魅力”这个话题同样特别契合。\u003c/p\u003e\u003cp data-pid=\"-clfJi0P\"\u003e这是因为在马基雅维利看来，“时运”，或者说古罗马神话中的“命运女神”，是一种不可捉摸、难以控制的自然力量。（某种程度像极了男性对于女性的看法（笑））同时，“勇敢者”的使命就是用自己的“力量”与“能力”去驾驭“时运”——无论是爱情、工作还是伟大的政治理想，只有主动出击、敢于冒险的人，才有机会脱颖而出。换句话说，就像女孩们择偶也并非是随机挑选一样，“时运”也并非是盲目的，敢于发起挑战的勇敢者总是能够更加获得“时运”的青睐，哪怕勇敢者可能会失败，但他们至少有过获胜的概率，也为日后的再次挑战和改变现状积累了宝贵的经验。\u003c/p\u003e\u003cp data-pid=\"vi7lGCow\"\u003e马基雅维利生平最看不上的就是把自己的命运诉诸宗教与信仰等决定论的人。在他看来，虽然时运不可捉摸，但人至少有一半的命运供自己掌握——类似于我们所谓的“谋事在人，成事在天”，至少谋事是掌握在我们手中的，只是马基雅维利的思想更加激进，他要表达的是尽管成事在天，但谋事在人。\u003c/p\u003e\u003cp data-pid=\"YiTaflDG\"\u003e大多数人其实终其一生面对的哲学问题只有一个，那就是如何相信明天会更好。这个问题对于生理上弱于男性且承担着生育职能的女性来说尤为重要——这意味着她的血脉能否得到一个“可预见的、稳定的、理想的预期”。而这，恰恰是她们渴望男性能够给予她们的，也就是她们常常挂在嘴边的安全感。\u003c/p\u003e\u003cp data-pid=\"xLb4iyjq\"\u003e因此，勇敢就是男性的性魅力之所在。当大多数人面对难以驾驭的困难、不可捉摸的命运、充满不确定性的未来感到退缩和畏惧的时候，一个勇敢者会以坚定的姿态带领大家继续前进，勇敢者有笃定的眼神、坚定的信念和果断的行动，他给予其他人的安全感就像在面对浩浩荡荡的人生洪流时抓住的一只坚实的臂膀，那种由内而外的坚韧令人深深地着迷，因为它蕴含着改变现状的力量。\u003c/p\u003e\u003cp data-pid=\"2GHPomhJ\"\u003e乳臭未干的男孩子们啊，你们看看草原上的狮子和羚羊，它们的雄性首领往往具备着与一个理想的男性伴侣具备相同的特质——强壮有力，果敢坚毅，充满攻击性和危险性，由内而外地散发着成熟和稳重，面对挑战从不退缩，勇敢和抗争是他们毫不动摇的第一选择。\u003c/p\u003e\u003cp data-pid=\"cmjr4neC\"\u003eBold是一个恰当的词，它有勇敢、英勇甚至鲁莽的意思。诚然，我们的决定未必每次都是对的，鲁莽未必能带来好的结果，但往往一个坚定的人会给自己的决定注入强大的力量，不仅散发着成熟的魅力，也会给自己增加一份使命感和责任感，而正是这种责任和坚定让我们会比其他人更加卓越而富有洞见。\u003c/p\u003e\u003cp data-pid=\"7EYdfK2Z\"\u003e试想，当你自己都毫无主见、瞻前顾后、畏首畏尾、甚至连面对表白失败的勇气都没有的时候，一个女孩子又如何会相信在遇到困难时你会冲锋在前与她共同面对，绝望时你会给她希望，在她踌躇不前的时候你会带领她找到前进的方向呢？你甚至连她都不敢挑战，又怎么会敢挑战日后的困难呢？因此，女孩子宁可相信一个自信满满的骗子，都不会相信一个懦弱的孩子，这是亘古不变的真理。\u003c/p\u003e\u003cp data-pid=\"PToTQ-3G\"\u003e综上，或许一个男人的成熟、慷慨、沉稳和品味会让他看起来气度非凡、魅力十足，但请务必牢记，上述的一切特质都只是皮毛和肌肉，真正撑起这一切的骨骼，则是他内心深处迸发出来的勇敢。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":131025,"favorite_count":4683,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1922139814511224697}","attached_info":"CtsFCJjd2/mMsvnglQEQBBoJNzM0MzY2NTMyIJPs+8IGKO8cMNcBQEVKMAobVFNfU09VUkNFX0JBU0lDX0lORk9fUkVDQUxMEgEwGAAgADoKeyJyYXciOiIifVoHMzY3MzQzMmIgYjM3YzI0MWIxOGEwMTQ5NDJhOTQyYjM1ZmZkMjIwNTlyEzE5MjIxMzk4MTQ1MTEyMjQ2OTeKAQgyODcyNjM2OKoBCXJlY29tbWVuZMIBIDU1YmYxNWE3NWVmZDJmODE2N2M5YmUyZDkxZmQ2NDUz8gEKCAwSBk5vcm1hbPIBKAgKEiQ3ZTY2OTIxNC1iODM3LTRlY2YtOGNkOS1hZDQyMmI2ZWI2ZGTyAQYICxICMTKCAgCIAu6o086FM5ICIDU1YmYxNWE3NWVmZDJmODE2N2M5YmUyZDkxZmQ2NDUzmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFlJldmlzaXRWYWx1ZVdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZdoCG1RTX1NPVVJDRV9CQVNJQ19JTkZPX1JFQ0FMTOgCA/oCC05PUk1BTF9GTE9XigMgODYzYmM1ZTBiZWZkNDE0Mjk1ZWEyZjEzNTc5ZWEzZmWaAw0KAnYyEAAaBW90aGVyqAPR/wfYAwDqAxFiYXNpY19pbmZvX3JlY2FsbPoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQGbWFudWFswgQDMTcwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAACARgisP4EFAAAAAAAAAACJBUzdtkaeY9M/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQyQBgCgBkWoBgCSAi4KCTczNDM2NjUzMhITMTkyMjEzOTgxNDUxMTIyNDY5NxgEIgpJTUFHRV9URVhU","action_card":false},{"id":"70_1753853777.412","type":"feed","offset":70,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853777,"updated_time":1753853777,"target":{"id":"1933641200637835083","type":"answer","url":"https://api.zhihu.com/answers/1933641200637835083","author":{"id":"fecf729a301fea76d505f4a2c77e2708","url":"https://api.zhihu.com/people/fecf729a301fea76d505f4a2c77e2708","user_type":"people","url_token":"54-82-62-35","name":"天空","headline":"","avatar_url":"https://pica.zhimg.com/50/v2-8c15ee81c813f0c437afeaf26fdc2592_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":12,"is_following":false,"is_followed":false},"created_time":1753795987,"updated_time":1753795987,"voteup_count":3,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"6017642848","type":"question","url":"https://api.zhihu.com/questions/6017642848","author":{"id":"6677e93dc70c4b3fdacc3b634b9b1fec","url":"https://api.zhihu.com/people/6677e93dc70c4b3fdacc3b634b9b1fec","user_type":"people","url_token":"69-45-56-90","name":"一土三金","headline":"流浪知乎的普通网民","avatar_url":"https://picx.zhimg.com/50/v2-400fa8503367c0ac1764beb6dcf2ae87_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":901,"is_following":false,"is_followed":false},"title":"拖延症懒惰的人如何提升写作的行动力？","created":1733370859,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[767,823,22173,37091],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://pic1.zhimg.com/50/v2-28b4271e7b211ee2777c7de349fdbab0_720w.jpg?source=b6762063","excerpt":"《打破“启动诅咒”的三个小开关》 心理学上说道，其实拖延不是懒，更像大脑的“防御机制”——面对“不确定”“太复杂”的任务时，它会自动按下暂停键。分享三个亲测有效的“破局点”，帮你绕过大脑的“抵抗雷达” 一、用“5秒启动法”骗过大脑 只有先行动起来，其他的交给时间，具体怎么做呢？你可以在想做某事却卡住时，倒数“5-4-3-2-1”后立刻行动。比如我每天打开知乎，我就会优先花五分钟完成基础的圆环任务，然后再去看…","excerpt_new":"《打破“启动诅咒”的三个小开关》 心理学上说道，其实拖延不是懒，更像大脑的“防御机制”——面对“不确定”“太复杂”的任务时，它会自动按下暂停键。分享三个亲测有效的“破局点”，帮你绕过大脑的“抵抗雷达” 一、用“5秒启动法”骗过大脑 只有先行动起来，其他的交给时间，具体怎么做呢？你可以在想做某事却卡住时，倒数“5-4-3-2-1”后立刻行动。比如我每天打开知乎，我就会优先花五分钟完成基础的圆环任务，然后再去看…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"cJbumi5W\"\u003e《打破“启动诅咒”的三个小开关》\u003c/p\u003e\u003cp data-pid=\"9BmTQcdR\"\u003e心理学上说道，其实拖延不是懒，更像大脑的“防御机制”——面对“不确定”“太复杂”的任务时，它会自动按下暂停键。分享三个亲测有效的“破局点”，帮你绕过大脑的“抵抗雷达”\u003c/p\u003e\u003cp data-pid=\"KwEjfP7x\"\u003e一、用“5秒启动法”骗过大脑\u003c/p\u003e\u003cp data-pid=\"RqGVRjJo\"\u003e只有先行动起来，其他的交给时间，具体怎么做呢？你可以在想做某事却卡住时，倒数“5-4-3-2-1”后立刻行动。比如我每天打开知乎，我就会优先花五分钟完成基础的圆环任务，然后再去看一下创作打卡花嗯，五分钟去完成创创作打卡任务，我一定是要逼着自己先把最基础的任务完成以后再去看其他的一些问题或热点，然后再根据自己的专业领域分点去回答呃，两到三个问题，这个时候回答问题呢，我就会增加我的那个数字时长，尽量千字加三张图。\u003c/p\u003e\u003cp data-pid=\"UHIOx7sQ\"\u003e         再比如我想写一段小的嗯短文，我会先给自己梳理三个点，就是开头中间的情节，最后的结尾，也就是说我得先让自己把这个事先开始，而不是一直在脑海里面去想想想，我觉得只有你做了你才容易有结果，如果你只是限于想，那一定是没有结果的。大脑对“微小指令”的抵抗力很弱，往往启动后就会自然进入状态。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-4905fc520a2cc6cc6153b10f59b3cb04_1440w.jpg\" data-rawwidth=\"1152\" data-rawheight=\"1536\" data-size=\"normal\" data-original-token=\"v2-b5f63497ca4e0304837d760731cad64e\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-07f95e31afac8606d5b114d46ea8723c_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1152\" data-original=\"https://pic1.zhimg.com/v2-4905fc520a2cc6cc6153b10f59b3cb04_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"9a8lXuMn\"\u003e二、给任务“瘦身”\u003c/p\u003e\u003cp data-pid=\"3bTA1Arm\"\u003e       我们常说，退一步海阔天空。尽量将大事化，小小事化无。我觉得在生活中也是同样的道理，我们要学着将复杂的事情简单化，简单的事情快速化。越是复杂的事情，你就要把它分步作答给这个复杂的事情瘦身。\u003c/p\u003e\u003cp data-pid=\"L1xJSLeW\"\u003e        举个例子：你想在知乎上赚取每个月赚取百元收益，那你就可以分板块来进行收益的叠加。1.你可以参与圆环运动，它每天都可以抽奖，可能能抽到盐粒。2.你要参与创作挑战，这是一个月一次收益也能获得盐粒。3.参与暑假计划，在计划中回答问题或提问也能分得盐粒。4.评论区开麦计划这个也是要按时打卡，也能获得盈利。5.红包任务，你能分得一定的资金。当你将你的任务分成这五个板块，当然还可以分成更多的板块，你每个板块获得呃10块或20块的收益，那你再乘以5×6，你这100块的收益是不是就容易了。但如果你指向在某一个板块耕耘，那我想说你只在这个板块想获得100块的概率是偏低的，除非你是大v，但是我说的只是针对一些初做自媒体的人\u003c/p\u003e\u003cp data-pid=\"0QeRfSz5\"\u003e       拖延的根源常是“任务太大”，而完成一个极小的步骤，能给大脑注入“我能做到”的正反馈。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-d364f297c4e1396e92787caf126a5a7b_1440w.jpg\" data-rawwidth=\"1152\" data-rawheight=\"1536\" data-size=\"normal\" data-original-token=\"v2-06ce5d99abfd6066eb9962b6e4ef58a7\" data-default-watermark-src=\"https://pica.zhimg.com/v2-a8f26aa800c43a32a2f9070505837684_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1152\" data-original=\"https://pic2.zhimg.com/v2-d364f297c4e1396e92787caf126a5a7b_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"a_W9OhHu\"\u003e三、给拖延留“专属时间”\u003c/p\u003e\u003cp data-pid=\"q6ANDHX5\"\u003e        每天留10分钟“允许自己拖延”，比如刷手机、发呆。这会减少大脑对“拖延的罪恶感”，反而让它在该专注时更配合——就像给哭闹的孩子一块糖，反而能让他安静下来听指令。\u003c/p\u003e\u003cp data-pid=\"C6YeuG6y\"\u003e        像我偶像肖战说的，我是一个人呀，我并不是一个机器，所以一定要在每天给自己留一点时间，让自己放松下来，让自己享受一下这个拖延带给自己的快乐，一定要定时定量，千万不要一朝回到解放前，完全的放飞自我\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-bef1a29f398b9fe0fdd94835990365ad_1440w.jpg\" data-rawwidth=\"1152\" data-rawheight=\"1536\" data-size=\"normal\" data-original-token=\"v2-00e673888a686bf4651f87b2f4e2e771\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-f29240c84732c6d286e09a35b78870b7_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1152\" data-original=\"https://pic2.zhimg.com/v2-bef1a29f398b9fe0fdd94835990365ad_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"B6aHM6w8\"\u003e试试从今天的一件小事开始：比如看到这里，立刻起身倒杯水。你会发现，打破诅咒的钥匙，往往藏在“先动起来”的那个瞬间里。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":true,"visited_count":27,"thumbnails":["https://pica.zhimg.com/50/v2-28b4271e7b211ee2777c7de349fdbab0_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-1f9d45f0e9becdc12cc93afb5c9d1300_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-559f77d487cdaa2fcaccecf124ef38f1_720w.jpg?source=b6762063"],"favorite_count":4,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1933641200637835083}","attached_info":"CpEHCJjd2/mMsvnglQEQBBoJNzM5NDk3ODkyIJObo8QGKAMwAEBGSiQKGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDISATAYACAAOgBKLwokVFNfU09VUkNFX1dBUk1VUF9UV09UT1dFUl9FWFBWMl9URVhUEgEwGAAgADoAWgkxMTIxODAyNzBiIGIzN2MyNDFiMThhMDE0OTQyYTk0MmIzNWZmZDIyMDU5chMxOTMzNjQxMjAwNjM3ODM1MDgzigEKNjAxNzY0Mjg0OKoBCXJlY29tbWVuZMIBIGZlY2Y3MjlhMzAxZmVhNzZkNTA1ZjRhMmM3N2UyNzA48gEKCAwSBk5vcm1hbPIBKAgKEiQ4ODIxM2U2Ni05ZDkxLTQ1ZDEtYjc3Yy1iNTk3YzQwNjliZDHyAQYICxICMTKCAgCIAu6o086FM5ICIGZlY2Y3MjlhMzAxZmVhNzZkNTA1ZjRhMmM3N2UyNzA4mgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCGENvbnRlbnRXYXJtVXBCcmVha0luUnVsZdoCGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDLoAgP6AgtOT1JNQUxfRkxPV4oDIDg2M2JjNWUwYmVmZDQxNDI5NWVhMmYxMzU3OWVhM2ZlmgMNCgJ2MhAAGgVvdGhlcqgDG9gDAOoDL2NvbnRlbnRXYXJtdXBUd29Ub3dlclR2cFRleHROb3JtYWxFeHBWMlJlY2FsbGVy+gOsARIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgEEIAJGIAMIiN2Mi1iNWY2MzQ5N2NhNGUwMzA0ODM3ZDc2MDczMWNhZDY0ZTotCAIQgAkYgAwiI3YyLTA2Y2U1ZDk5YWJmZDYwNjZlYjk5NjJiNmU0ZWY1OGE3Oi0IBBCACRiADCIjdjItMDBlNjczODg4YTY4NmJmNDY1MWY4N2IyZjRlMmU3NzGABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAOCpBXg/gQUAAAAAAAAAAIkFTN22Rp5j0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFDJAGAKAGRqgGAZICLgoJNzM5NDk3ODkyEhMxOTMzNjQxMjAwNjM3ODM1MDgzGAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"71_1753853777.726","type":"feed","offset":71,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853777,"updated_time":1753853777,"target":{"id":"1920842927908815772","type":"article","url":"https://api.zhihu.com/articles/1920842927908815772","author":{"id":"fa919734224a6c78b5b168b5fe586049","url":"https://api.zhihu.com/people/fa919734224a6c78b5b168b5fe586049","user_type":"organization","url_token":"a-li-ji-zhu","name":"阿里云开发者","headline":"阿里的技术创新均在此呈现","avatar_url":"https://picx.zhimg.com/50/v2-1d701f79d418cbcb2d7e991d9862aff6_l.jpg?source=b6762063","is_org":true,"gender":-1,"badge":[{"type":"identity_org","description":"已认证机构号"}],"followers_count":186926,"is_following":false,"is_followed":false},"title":"用Cursor开启JAVA+AI生涯","image_url":"https://pic1.zhimg.com/v2-1ddefb2f3dd6d2a861c814256977c33b.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1751346176,"updated":1751346176,"voteup_count":35,"voting":0,"comment_count":2,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"Cursor 是基于VS Code开发的一款编辑器，支持多种语言的开发编辑。与传统的开发工具相比，它有多种优势：与AI无缝集成，响应速度快，占用内存小。但很多同学在\u0026#34;起步\u0026#34;过程中遇到了一点点问题，导致起步不成功。本文描述一些实用的小技巧，帮助大家快速用Cursor开启AI生涯。 一、cursor下载 下载Cursor https://www.cursor.com/ 备注：使用cursor需要收费，且费用颇高 支付方式可以使用 Wildcard 虚拟信用卡 https://blog.csdn.net/qq_…","excerpt_new":"Cursor 是基于VS Code开发的一款编辑器，支持多种语言的开发编辑。与传统的开发工具相比，它有多种优势：与AI无缝集成，响应速度快，占用内存小。但很多同学在\u0026#34;起步\u0026#34;过程中遇到了一点点问题，导致起步不成功。本文描述一些实用的小技巧，帮助大家快速用Cursor开启AI生涯。 一、cursor下载 下载Cursor https://www.cursor.com/ 备注：使用cursor需要收费，且费用颇高 支付方式可以使用 Wildcard 虚拟信用卡 https://blog.csdn.net/qq_…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"u8DO28sQ\"\u003eCursor 是基于VS Code开发的一款编辑器，支持多种语言的开发编辑。与传统的开发工具相比，它有多种优势：与AI无缝集成，响应速度快，占用内存小。但很多同学在\u0026#34;起步\u0026#34;过程中遇到了一点点问题，导致起步不成功。本文描述一些实用的小技巧，帮助大家快速用Cursor开启AI生涯。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e一、cursor下载\u003c/h2\u003e\u003cp data-pid=\"SxwXNIFe\"\u003e\u003cb\u003e下载Cursor\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"7lrnBw4Q\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.cursor.com/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://www.\u003c/span\u003e\u003cspan class=\"visible\"\u003ecursor.com/\u003c/span\u003e\u003cspan class=\"invisible\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003e备注：使用cursor需要收费，且费用颇高\n\n支付方式可以使用 Wildcard 虚拟信用卡\nhttps://blog.csdn.net/qq_33146717/article/details/145172851\n\n或者免费版用15天，然后换一个邮箱。\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e二、安装插件\u003c/h2\u003e\u003cp data-pid=\"hfmOw6jX\"\u003e刚下载的cursor几乎没有任何功能，需要用安装插件的方式来把一个cursor组装为Java开发平台。切换到插件搜索下载即可。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-648d5b2c9d69b789413d9cfd60d45fd3_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"756\" data-rawheight=\"1280\" data-original-token=\"v2-648d5b2c9d69b789413d9cfd60d45fd3\" class=\"origin_image zh-lightbox-thumb\" width=\"756\" data-original=\"https://pic2.zhimg.com/v2-648d5b2c9d69b789413d9cfd60d45fd3_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"1qKPoYR9\"\u003e建议安装插件清单：为了实现和idea基本一致的开发体验，建议安装如下插件。\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003ebackground\nChinese(Simplified)(简体中文) Language Pack for Visual Studio Code\nChrome Extension Developer Tools\nCopy Reference\nDebugger for Java\nDiff Folders \nDraw.io Integration\nExtension Pack for Java\nGitLens — Git supercharged\nGradle for Java\nImage preview\nIntelliCode\nIntelliCode API Usage Examples\nindent-rainbow-blocks\nLanguage Support forJava(TM) by Red Hat\nMarkdown All in One\nMarkdown Image\nMarkdown Image Manage\nMarkdown Preview Enhanced\nMaterial Icon Theme\nMaven for Java\nProject Manager for Java\nProject Manager\nPrettier - Code formatter\nTest Runner for Java\nTodo Tree\nVS Code Counter\nvscode-pdf\nXML Tools\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"_c_Lbel5\"\u003e建议屏蔽的插件：\u003c/p\u003e\u003cp data-pid=\"rfeFynLd\"\u003ejava：oracle版本，有一堆java11以上版本的有创新但是生产环境用不到的功能。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e三、常用快捷键\u003c/h2\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003e快捷键  \n\n查看类源码：Ctrl+鼠标左键\n快捷键命令搜索：Ctrl+Shift+P+命令名称\n按文件名搜索文件：Ctrl+P\n根据类名查找类：Ctrl+T\n注释代码：Ctrl+/\n开启/关闭侧边栏：Ctrl+B\n全局查找并替换：Ctrl+Shift+F\n文件内查找：Ctrl+F\n修改变量或类名：F2\n查找变量或类的引用：Shift+F12\n查找接口或方法的实现：Ctrl+F12\n格式化代码：Shift+Alt+F\n重构抽取变量或方法：Ctrl+Shift+R\n回退到上一个操作：Alt+左箭头\n前进到下一个操作：Alt+右箭头\n提示可能的操作，如变量生成：Alt+.\n移动一行代码：Alt+上箭头或Alt+下箭头\n删除当前行代码：Ctrl+Shift+K\n\n打开AI窗口： Ctrl+Shift+L\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e四、建议配置\u003c/h2\u003e\u003cp data-pid=\"RbdTNJ1u\"\u003evscode的配置项目很多。为了快速启动项目，推荐下面的懒人配置。\u003c/p\u003e\u003cp data-pid=\"SC0Jx89w\"\u003e\u003cb\u003e4.1、settings.json\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"p8KSwCZr\"\u003esettings.json是关于整个项目的：启动内存，编辑器样式，自动编译等配置。\u003c/p\u003e\u003cp data-pid=\"yGmSvDTS\"\u003e一般位于项目的.vscode/settings.json\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003e{\n    //设置内存大小\n    \u0026#34;java.jdt.ls.vmargs\u0026#34;: \u0026#34;-XX:+UseParallelGC -XX:GCTimeRatio=4 -XX:AdaptiveSizePolicyWeight=90 -Dsun.zip.disableMemoryMapping=true -Xmx16G -Xms2G -Xlog:disable\u0026#34;,\n    //maven视图:分层\n    \u0026#34;maven.view\u0026#34;: \u0026#34;hierarchical\u0026#34;,\n    //构建失败继续:开启\n    \u0026#34;java.debug.settings.onBuildFailureProceed\u0026#34;: true,\n    //启动窗口、打开文件夹、保存文件时的自动编译开关\n    //影响启动速度，如有需要可启动后再手动打开\n    \u0026#34;java.autobuild.enabled\u0026#34;: false,\n    //debug启动时自动编译:关闭\n    //如果打开，则应用启动前需要编译整个项目，耗时1.5-5分钟\n    //建议手工编译，可提升启动速度\n    \u0026#34;java.debug.settings.forceBuildBeforeLaunch\u0026#34;: false,\n    //debug自动加载修改后的类\n    \u0026#34;java.debug.settings.hotCodeReplace\u0026#34;: \u0026#34;auto\u0026#34;,\n    //保存时自动编译:开启\n    //但似乎此参数无效，实操经验是：\n    //倘若java.autobuild.enabled为true，则保存后自动编译\n    //倘若java.autobuild.enabled为false，则保存后不自动编译\n    \u0026#34;java.compile.onSave\u0026#34;:true,\n    //问题装饰:关闭\n    \u0026#34;problems.decorations.enabled\u0026#34;: false,\n    //null分析:关闭\n    \u0026#34;java.compile.nullAnalysis.mode\u0026#34;: \u0026#34;disabled\u0026#34;,\n    //未使用导入:忽略\n    \u0026#34;editor.unusedImports.severity\u0026#34;: \u0026#34;ignore\u0026#34;,\n    //未使用变量:隐藏\n    \u0026#34;editor.showUnused\u0026#34;: false,\n    //自动保存:延迟\n    \u0026#34;files.autoSave\u0026#34;: \u0026#34;afterDelay\u0026#34;,\n    //自动保存延迟时间:1000毫秒\n    \u0026#34;files.autoSaveDelay\u0026#34;: 1000,\n    //JAVA项目层级展示\n    \u0026#34;java.dependency.packagePresentation\u0026#34;: \u0026#34;hierarchical\u0026#34;,\n    //Peek References窥视试图颜色配置\n    \u0026#34;workbench.colorCustomizations\u0026#34;: {\n        \u0026#34;peekView.border\u0026#34;: \u0026#34;#FF0000\u0026#34;, // 边框颜色\n        \u0026#34;peekViewEditor.background\u0026#34;: \u0026#34;#330099\u0026#34;, // 代码编辑区背景\n        \u0026#34;peekViewResult.background\u0026#34;: \u0026#34;#3300CC\u0026#34;, // 结果列表背景\n        \u0026#34;peekViewTitle.background\u0026#34;: \u0026#34;#FF0000\u0026#34;// 标题背景\n    }\n}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"AgfFqg-i\"\u003e\u003cb\u003e4.2、launch.json\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"RA0QC4KR\"\u003elaunch.json是关于项目可启动应用的配置。位于.vscode/launch.json\u003c/p\u003e\u003cp data-pid=\"al1JlE5a\"\u003e下面是一个我所在us团队启动应用的示例，关键参数在于projectName、mainClass、args。\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003e{\n    \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;,\n    \u0026#34;configurations\u0026#34;: [\n        {\n            \u0026#34;type\u0026#34;: \u0026#34;java\u0026#34;,\n            \u0026#34;name\u0026#34;: \u0026#34;Launch Java Program\u0026#34;,\n            \u0026#34;projectName\u0026#34;:\u0026#34;us-start\u0026#34;,\n            \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;,\n            \u0026#34;mainClass\u0026#34;: \u0026#34;com.amap.us.start.Application\u0026#34;,\n            //\u0026#34;preLaunchTask\u0026#34;: \u0026#34;mvn clean install\u0026#34;,\n            \u0026#34;args\u0026#34;: \u0026#34;--spring.profiles.active=testing,gray8\u0026#34;,\n            \u0026#34;vmArgs\u0026#34;: \u0026#34;-Xms516M -Xmx2048M -Djps.track.ap.dependencies=false  -Dspring-boot.run.fork=false\u0026#34;\n        }\n    ]\n}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"XUOtpv6A\"\u003e配置完成之后，即可在运行窗口启动项目进行调试等操作。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-bd28552b0974e5073d4e5e89669399f0_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1010\" data-rawheight=\"432\" data-original-token=\"v2-bd28552b0974e5073d4e5e89669399f0\" class=\"origin_image zh-lightbox-thumb\" width=\"1010\" data-original=\"https://pic1.zhimg.com/v2-bd28552b0974e5073d4e5e89669399f0_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"unMSU-0t\"\u003e\u003cb\u003e4.3、启动窗口、打开文件夹、保存文件时的自动编译-自动编译选项\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"fHV6MdE_\"\u003e如果你遇到了\u0026#34;打开cursor很耗时，需要3-5分钟甚至更久\u0026#34;的问题，这个配置很适合你。\u003c/p\u003e\u003cp data-pid=\"bahtskw0\"\u003esettings.json\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003e    //启动窗口、打开文件夹、保存文件时的自动编译开关\n    //影响启动速度，如有需要可启动后再手动打开\n    //打开后，启动窗口，打开文件夹时会编译一次项目，耗时1.5-5分钟\n    //因此不建议打开\n    \u0026#34;java.autobuild.enabled\u0026#34;: false,\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-9973604675f4dd125db10c044be79391_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"647\" data-original-token=\"v2-9973604675f4dd125db10c044be79391\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-9973604675f4dd125db10c044be79391_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"3-BhKREM\"\u003e\u003cb\u003e4.4、选择JDK\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"6IB1MUbV\"\u003ecommand+shift+p 搜索classpath\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-cff6e4f3c76e263742becd5c81471545_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"443\" data-original-token=\"v2-cff6e4f3c76e263742becd5c81471545\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-cff6e4f3c76e263742becd5c81471545_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-9cac0d550b05c2da893cc9bc023b5c69_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"588\" data-original-token=\"v2-9cac0d550b05c2da893cc9bc023b5c69\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-9cac0d550b05c2da893cc9bc023b5c69_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"QbNl1ASA\"\u003e建议选择1.8.0_300以上版本。如果启动报错 diamond serverlist 未加载成功，是jdk版本太低，如果启动报错duplicateKey[bizId,scenario]，需要手动重新编译gbf。\u003c/p\u003e\u003cp data-pid=\"EZ5ThNGK\"\u003e\u003cb\u003e4.5、debug配置-应用启动前的强制自动编译\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"rHeDZ4Ez\"\u003e如果你想『debug』的时候，代码改动后立即热部署生效，可以尝试如下配置。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-8f4c4f9b1bb7f2e6a385da66fc8cdbee_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"599\" data-original-token=\"v2-8f4c4f9b1bb7f2e6a385da66fc8cdbee\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-8f4c4f9b1bb7f2e6a385da66fc8cdbee_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"oGiiA8Y2\"\u003e等价于配置\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003e//启动窗口、打开文件夹、保存文件时的自动编译开关\n    //影响启动速度，如有需要可启动后再手动打开\n    \u0026#34;java.autobuild.enabled\u0026#34;: true,\n    //debug应用启动自动编译:打开\n    //如果打开，则应用启动前需要编译整个项目，耗时1.5-5分钟\n    //建议手工编译，可提升启动速度\n    \u0026#34;java.debug.settings.forceBuildBeforeLaunch\u0026#34;: true,\n    //denig自动加载修改后的类\n    \u0026#34;java.debug.settings.hotCodeReplace\u0026#34;: \u0026#34;auto\u0026#34;,\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"y2fCnD6_\"\u003e\u003cb\u003e4.6、清空缓存--很有用\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"FO033gJb\"\u003e在项目运行期间，偶然会遇到项目崩溃，例如：\u003c/p\u003e\u003cp data-pid=\"A4P4-7dK\"\u003e在项目中新建了一个maven的module，但是pom.xml的格式不正确，导致maven组件运行崩溃。\u003c/p\u003e\u003cp data-pid=\"EkiSVdCt\"\u003e此时：重启cursor也不管用，表现是：无法打开文件，无法运行插件，无法浏览git历史，无法和ai对话。\u003c/p\u003e\u003cp data-pid=\"OTx2IAlA\"\u003e解决办法：找到缓存文件夹，删掉缓存文件，再重启cursor。\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003e缓存文件夹目录地址\n~/Library/Application Support/Cursor/User/workspaceStorage\n例如我的地址\n/Users/kanmars/Library/Application Support/Cursor/User/workspaceStorage\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"K89v48E_\"\u003e删除文件位置：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-a6be3d68209dec1866e57ac4db75a9bc_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"626\" data-original-token=\"v2-a6be3d68209dec1866e57ac4db75a9bc\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a6be3d68209dec1866e57ac4db75a9bc_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"0nlOW5Nd\"\u003e问题表现：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-62e5305aa9a9aa74b9831f41de470333_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"674\" data-original-token=\"v2-62e5305aa9a9aa74b9831f41de470333\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-62e5305aa9a9aa74b9831f41de470333_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"Kw9JYg1W\"\u003e\u003cb\u003e4.7、转化为大写-小写 设置快捷键\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"FhsaaxDp\"\u003ecommand+shift+p  搜索转化为大写/小写。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-0fd7fe50a9eeef950cdb58e9f08f39e7_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"372\" data-original-token=\"v2-0fd7fe50a9eeef950cdb58e9f08f39e7\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-0fd7fe50a9eeef950cdb58e9f08f39e7_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-984ffeca8b15961e0716b519f1aa3494_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"450\" data-original-token=\"v2-984ffeca8b15961e0716b519f1aa3494\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-984ffeca8b15961e0716b519f1aa3494_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"uCyUNp-J\"\u003e我的习惯是绑定 cmd+shift+u为大写，cmd+shift+m为小写。\u003c/p\u003e\u003cp data-pid=\"4E93r7YI\"\u003e\u003cb\u003e4.8、编译单个文件\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"vCVeRgQr\"\u003e\u003cb\u003e方法一：打开自动编译开关java.autobuild.enabled=true\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Jw2Dp7fa\"\u003e但启动窗口，打开文件夹时都会编译整个项目，耗费1.5-5分钟。\u003c/p\u003e\u003cp data-pid=\"_ebfTPAC\"\u003e\u003cb\u003e方法二：maven增量编译\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"-8YKQODt\"\u003e经了解资料，maven并不支持增量编译，但支持按模块编译。\u003c/p\u003e\u003cp data-pid=\"QmTY4noI\"\u003emvn compile -DskipTests -pl \u0026lt;模块名称\u0026gt;，可以避免编译整个项目。\u003c/p\u003e\u003cp data-pid=\"xXHoup1h\"\u003e\u003cb\u003e方法三：task。未实验成功\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"0N9pvPJM\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//blog.51cto.com/u_16213457/12958417\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003eblog.51cto.com/u_162134\u003c/span\u003e\u003cspan class=\"invisible\"\u003e57/12958417\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"loPP7ke3\"\u003e建议：方案一  日常情况下关闭自动编译开关，如果确实需要本地debug的，再打开此开关。\u003c/p\u003e\u003cp data-pid=\"W7rrXpl_\"\u003e\u003cb\u003e4.9、maven如何始终跳过测试\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-600527c7b85e448f5cd557892bf22a0a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"260\" data-original-token=\"v2-600527c7b85e448f5cd557892bf22a0a\" class=\"origin_image zh-lightbox-thumb\" width=\"706\" data-original=\"https://pic1.zhimg.com/v2-600527c7b85e448f5cd557892bf22a0a_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"Kp3YXkfI\"\u003eidea使用maven编译时可以关闭testCase避免每次打包自动运行测试用例，vscode也有同等能力，在设置中查找maven配置（@ext:vscjava.vscode-maven），添加选项：-DskipTests。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-6250e407ea0882749d17353dc51ad245_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"658\" data-rawheight=\"434\" data-original-token=\"v2-6250e407ea0882749d17353dc51ad245\" class=\"origin_image zh-lightbox-thumb\" width=\"658\" data-original=\"https://pic2.zhimg.com/v2-6250e407ea0882749d17353dc51ad245_r.jpg\"/\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-ad45fa7004217e2ef423fe704e2c832f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"49\" data-original-token=\"v2-ad45fa7004217e2ef423fe704e2c832f\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-ad45fa7004217e2ef423fe704e2c832f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"JEF5R-ri\"\u003e\u003cb\u003e4.10、主题-建议用深色visualStudio\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-704daa06987776f481ca9fd517a736ea_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"483\" data-original-token=\"v2-704daa06987776f481ca9fd517a736ea\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-704daa06987776f481ca9fd517a736ea_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"T8HI5AzZ\"\u003e原因：点击某个方法的调用方的区别（窥视视图）\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-b26cb2fffd28fa30ec3449eaeec56276_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"456\" data-original-token=\"v2-b26cb2fffd28fa30ec3449eaeec56276\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-b26cb2fffd28fa30ec3449eaeec56276_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"d3NI9a1L\"\u003e默认 cursor dark\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-fc47e55077183d624a01ab1ecd872cac_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"403\" data-original-token=\"v2-fc47e55077183d624a01ab1ecd872cac\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-fc47e55077183d624a01ab1ecd872cac_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"dZOWXw3J\"\u003e使用深色visualStudio，略微的有个浅色的提示框。\u003c/p\u003e\u003cp data-pid=\"nylInSI0\"\u003e\u003cb\u003e4.11、修改窥视视图颜色\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"CYoeTjx5\"\u003ecmd+点击方法，查看调用链颜色太浅了，看不清楚。可以手工修改颜色。\u003c/p\u003e\u003cp data-pid=\"_CV8-hNm\"\u003esettings.json中需要加入如下配置：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-text\"\u003e    \u0026#34;workbench.colorCustomizations\u0026#34;: {\n        \u0026#34;peekView.border\u0026#34;: \u0026#34;#FF0000\u0026#34;, // 边框颜色\n        \u0026#34;peekViewEditor.background\u0026#34;: \u0026#34;#330099\u0026#34;, // 代码编辑区背景\n        \u0026#34;peekViewResult.background\u0026#34;: \u0026#34;#3300CC\u0026#34;, // 结果列表背景\n        \u0026#34;peekViewTitle.background\u0026#34;: \u0026#34;#FF0000\u0026#34;// 标题背景\n    }\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e五、常见操作\u003c/h2\u003e\u003cp data-pid=\"DYX4dF7t\"\u003e\u003cb\u003e5.1、git\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"NeWpaQM_\"\u003e\u003cb\u003e提交记录\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-9cddc97306f8f67882c2d6b44a663f32_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"672\" data-original-token=\"v2-9cddc97306f8f67882c2d6b44a663f32\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-9cddc97306f8f67882c2d6b44a663f32_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"BbEeVWOD\"\u003e缺陷：idea可合并多次提交，查看变更。\u003c/p\u003e\u003cp data-pid=\"UYn_p7G2\"\u003e而git graph只能查看单个提交的变更。\u003c/p\u003e\u003cp data-pid=\"eaiKu5G7\"\u003e\u003cb\u003e代码比对\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-13c49580d66fcdaaf22e6d98479ecf75_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"822\" data-original-token=\"v2-13c49580d66fcdaaf22e6d98479ecf75\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-13c49580d66fcdaaf22e6d98479ecf75_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"_Ag93Hfl\"\u003e\u003cb\u003e单个文件比对\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-3f42a6d20e006405e207f068f26c00ef_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"1150\" data-original-token=\"v2-3f42a6d20e006405e207f068f26c00ef\" class=\"origin_image zh-lightbox-thumb\" width=\"1064\" data-original=\"https://picx.zhimg.com/v2-3f42a6d20e006405e207f068f26c00ef_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"_C2sRrkT\"\u003e\u003cb\u003e5.2、返回上一次command+点击的位置\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"KOVv5lN6\"\u003e默认：control+-\u003c/p\u003e\u003cp data-pid=\"NH-5nMGq\"\u003e可以command+shift+p搜索『返回』，修改为习惯的快捷键。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-8dcc51e25ed15cc93f9f6136a10570bc_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"662\" data-original-token=\"v2-8dcc51e25ed15cc93f9f6136a10570bc\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8dcc51e25ed15cc93f9f6136a10570bc_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"SsuwFqm_\"\u003e例如command+option+左箭头。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e六、cursor-rules\u003c/h2\u003e\u003cp data-pid=\"HUN4bLcH\"\u003e可以定制cursor的AI运行时的规则，位于.cursor文件中。\u003c/p\u003e\u003cp data-pid=\"Ju-71KrH\"\u003erules可以作为一种预设的规则，为AI提出：运行指导、限制规范。帮助我们更好地完成目标。\u003c/p\u003e\u003cp data-pid=\"71jzTJhZ\"\u003e示例：\u003c/p\u003e\u003cp data-pid=\"5GyRrWdJ\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//juejin.cn/post/7471044704647053353\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003ejuejin.cn/post/74710447\u003c/span\u003e\u003cspan class=\"invisible\"\u003e04647053353\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"MLA7HtU-\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//cursorrulescn.oosakana.com/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003ecursorrulescn.oosakana.com\u003c/span\u003e\u003cspan class=\"invisible\"\u003e/\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"GwQ_16NW\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/heiyeshuwu/article/details/145951789\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003eblog.csdn.net/heiyeshuw\u003c/span\u003e\u003cspan class=\"invisible\"\u003eu/article/details/145951789\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e七、MCP\u003c/h2\u003e\u003cp data-pid=\"YFre1yOy\"\u003e通过mcp可以接入很多有意思的工具，为日常工作提效。例如接入mcp-server-playwright工具，替你完成：\u0026#34;打开浏览器，打开日志查询平台，根据gsid查询到一条日志，再把日志复制到debug平台的页面中，运行debug平台，获取日志，排查问题，给出结论\u0026#34;等一套完整的\u0026#34;日志排查\u0026#34;工作。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-56d7a0666df0167b70159ed33ea039b3_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"411\" data-original-token=\"v2-56d7a0666df0167b70159ed33ea039b3\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://picx.zhimg.com/v2-56d7a0666df0167b70159ed33ea039b3_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"w1x00yxs\"\u003e推荐mcp搜索网站：\u003ca href=\"https://link.zhihu.com/?target=http%3A//mcp.so\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttp://\u003c/span\u003e\u003cspan class=\"visible\"\u003emcp.so\u003c/span\u003e\u003cspan class=\"invisible\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"ZyDnv1KL\"\u003e好用的mcp推荐：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"xZ9Od6za\"\u003e浏览器：\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/executeautomation/mcp-playwright\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003egithub.com/executeautom\u003c/span\u003e\u003cspan class=\"invisible\"\u003eation/mcp-playwright\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"bkyzulg8\"\u003e文件：@modelcontextprotocol/server-filesystem\u003c/li\u003e\u003cli data-pid=\"Y80COTBL\"\u003e知识图谱记忆：@modelcontextprotocol/server-memory\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"4VzvJnqd\"\u003e\u003cb\u003ebeyond资料\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"scVAgvv2\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/modelcontextprotocol/servers\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003egithub.com/modelcontext\u003c/span\u003e\u003cspan class=\"invisible\"\u003eprotocol/servers\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"HJCZBqXk\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//smithery.ai/server\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003esmithery.ai/server\u003c/span\u003e\u003cspan class=\"invisible\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"n9mhEMkZ\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//cursor.directory/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003ecursor.directory/\u003c/span\u003e\u003cspan class=\"invisible\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"fpqYhIGN\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/punkpeye/awesome-mcp-servers\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003egithub.com/punkpeye/awe\u003c/span\u003e\u003cspan class=\"invisible\"\u003esome-mcp-servers\u003c/span\u003e\u003cspan class=\"ellipsis\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"woCD24TW\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//mcp.so/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e\u003cspan class=\"invisible\"\u003ehttps://\u003c/span\u003e\u003cspan class=\"visible\"\u003emcp.so/\u003c/span\u003e\u003cspan class=\"invisible\"\u003e\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e八、最后加两句\u003c/h2\u003e\u003cp data-pid=\"L9sR4ZKy\"\u003ecursor是AI时代的研发工具，作为『研发工具』，它给了我们一个机会，去结合AI做一些事情。比如：业务分析，整合创新，研发提效等。\u003c/p\u003e\u003cp data-pid=\"il2zDhGI\"\u003e本文只是『如何启动cursor并能简单使用』的一个\u0026#34;如何起步\u0026#34;的简单说明。如何『用工具创造价值』还有待后续补充。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"VRbfO19A\"\u003e\u003cb\u003e多模态数据信息提取\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"kz4g6PCh\"\u003e随着信息技术的快速发展，数据的获取与处理变得尤为重要。本方案提供多模态文件信息抽取能力，通过先进的人工智能技术，能够识别和解析各种格式的文件，包括文本、图像、音频和视频，从而提取出有价值的信息，大幅提升数据处理效率。    \u003c/p\u003e\u003cp data-pid=\"XaPVgJbQ\"\u003e点击\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.aliyun.com/solution/tech-solution/information-extraction%3Futm_content%3Dg_1000404621\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e多模态数据信息提取\u003c/a\u003e查看详情。\u003c/p\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":2365,"thumbnails":["https://pic1.zhimg.com/v2-1ddefb2f3dd6d2a861c814256977c33b.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-971b6c21bd75284326b36f597ae25d72_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-f8d6a783d2b79644657cf1db605fd42c_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-5567c33234e88c9f1eff65d893b1112e_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-c11f8d23a0242b32a1b861ab876798f5_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-c2c3e42a68a304068c941fd7b99c9bb9_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-9e12b591c6121bde8beb1803793eafda_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-c94c1850f41469b456e2dae83ee884dc_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-07b65ea94ae3bb628dc76ce3964eb409_720w.jpg?source=b6762063"],"favorite_count":124,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1920842927908815772}","attached_info":"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","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=71\u0026end_offset=71\u0026page_number=13\u0026session_token=b37c241b18a014942a942b35ffd22059","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=71\u0026end_offset=71\u0026page_number=13\u0026session_token=b37c241b18a014942a942b35ffd22059","totals":0},"fresh_text":"推荐已更新"}
