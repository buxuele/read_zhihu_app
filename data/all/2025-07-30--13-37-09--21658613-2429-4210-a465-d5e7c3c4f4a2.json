{"data":[{"id":"126_1753853847.119","type":"feed","offset":126,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853847,"updated_time":1753853847,"target":{"id":"1902861995214091953","type":"answer","url":"https://api.zhihu.com/answers/1902861995214091953","author":{"id":"f8d46c64917ac7f16deade62c28bc4fa","url":"https://api.zhihu.com/people/f8d46c64917ac7f16deade62c28bc4fa","user_type":"people","url_token":"vincent-32-59-94","name":"Lawrence","headline":"除了英俊，其他也没什么好说的。","avatar_url":"https://pic1.zhimg.com/50/v2-e741ecad44e45e469726e5b0cde61987_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":469,"is_following":false,"is_followed":false},"created_time":1746457653,"updated_time":1746457653,"voteup_count":220,"thanks_count":3,"comment_count":43,"is_copyable":true,"question":{"id":"656279316","type":"question","url":"https://api.zhihu.com/questions/656279316","author":{"id":"016d864c0318103cc28481ac6373e10d","url":"https://api.zhihu.com/people/016d864c0318103cc28481ac6373e10d","user_type":"people","url_token":"ffhgytt","name":"链鼎资源笔记","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-2b3a5f0471575b4919a87bc4c630e030_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":405,"is_following":false,"is_followed":false},"title":"有哪些事，技师不说，你还真不知道？","created":1715909495,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[13123,35095,208038],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"我有一次在长沙开会，中午无处可去，就去对面的浴足房洗个脚躺一躺。 进来的技师对我不是洗澡而仅是洗脚感到非常遗憾，称洗个澡也就只是多一百来元，我十分抱歉的报告称今天没力，只想躺一躺而已。 她曾经在成都的浴足店呆过一两年，不知道怎么就扯到两个城市在按摩领域的差异来。她讲：在成都，她们跟客人推荐一个项目，有时客人会选另外一种稍简单点的，不问清楚价格的话，她们就让客人换一个房间开始做客人选中的项目，但是价…","excerpt_new":"我有一次在长沙开会，中午无处可去，就去对面的浴足房洗个脚躺一躺。 进来的技师对我不是洗澡而仅是洗脚感到非常遗憾，称洗个澡也就只是多一百来元，我十分抱歉的报告称今天没力，只想躺一躺而已。 她曾经在成都的浴足店呆过一两年，不知道怎么就扯到两个城市在按摩领域的差异来。她讲：在成都，她们跟客人推荐一个项目，有时客人会选另外一种稍简单点的，不问清楚价格的话，她们就让客人换一个房间开始做客人选中的项目，但是价…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"697p-GM3\"\u003e我有一次在长沙开会，中午无处可去，就去对面的浴足房洗个脚躺一躺。\u003c/p\u003e\u003cp data-pid=\"rh_psR0c\"\u003e进来的技师对我不是洗澡而仅是洗脚感到非常遗憾，称洗个澡也就只是多一百来元，我十分抱歉的报告称今天没力，只想躺一躺而已。\u003c/p\u003e\u003cp data-pid=\"vEVCjk8p\"\u003e她曾经在成都的浴足店呆过一两年，不知道怎么就扯到两个城市在按摩领域的差异来。她讲：在成都，她们跟客人推荐一个项目，有时客人会选另外一种稍简单点的，不问清楚价格的话，她们就让客人换一个房间开始做客人选中的项目，但是价格还是跟她推荐的一样。\u003c/p\u003e\u003cp data-pid=\"qdozmgVL\"\u003e她的结论是：成都人太瓜了。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":189259,"favorite_count":43,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1902861995214091953}","attached_info":"CqAGCOTe9sWEv8/zngEQBBoJNzI1OTk3ODA4ILWo48AGKNwBMCtAfkooCh1UU19TT1VSQ0VfSU5URVJFU1RfV09SRF9NRVJHRRIBMBgAIAA6AFoJMTA4MTE3NzIwYiBiMzdjMjQxYjE4YTAxNDk0MmE5NDJiMzVmZmQyMjA1OXITMTkwMjg2MTk5NTIxNDA5MTk1M4oBCTY1NjI3OTMxNqoBCXJlY29tbWVuZMIBIGY4ZDQ2YzY0OTE3YWM3ZjE2ZGVhZGU2MmMyOGJjNGZh8gEKCAwSBk5vcm1hbPIBKAgKEiQwMGY0YTJlNy1jNGE0LTQyZGUtODFhNS1lOTBjMWMxZDk2NTPyAQYICxICMjKCAgCIAtfL186FM5ICIGY4ZDQ2YzY0OTE3YWM3ZjE2ZGVhZGU2MmMyOGJjNGZhmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFkFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhtJbnRlcmFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZcoCF1Rlc3RlZEFuZFdvcmtXZWlnaHRSdWxl2gIdVFNfU09VUkNFX0lOVEVSRVNUX1dPUkRfTUVSR0XoAgL6AgtOT1JNQUxfRkxPV4oDIDE5Njg5YzQ0NzA5MTQxMjlhOGU3YjI2Yzg4Yjg3YThimgMNCgJ2MhAAGgVvdGhlcqgDy8YL2AMA6gMiSW50ZXJlc3RXb3JkTWVyZ2VWMU5ld1Bvb2xSZWNhbGxlcvoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQGbWFudWFswgQDMTYwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAABA+3HFP4EFAAAAAAAAAACJBTTexUhpc9M/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRaQBgCgBn6oBgGSAi4KCTcyNTk5NzgwOBITMTkwMjg2MTk5NTIxNDA5MTk1MxgEIgpJTUFHRV9URVhU","action_card":false},{"id":"127_1753853847.179","type":"feed","offset":127,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853847,"updated_time":1753853847,"target":{"id":"95027628746","type":"answer","url":"https://api.zhihu.com/answers/95027628746","author":{"id":"f57132b3b6a650e5fd0838b1eb888ad3","url":"https://api.zhihu.com/people/f57132b3b6a650e5fd0838b1eb888ad3","user_type":"people","url_token":"kang-ri-ming-jiang","name":"浴血东瓜守","headline":"抗金，抗元，抗清，抗日","avatar_url":"https://pic1.zhimg.com/50/v2-bbe16ddfff4e39ac2b1f708d7db950e1_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":1454,"is_following":false,"is_followed":false},"created_time":1738916096,"updated_time":1738920344,"voteup_count":3512,"thanks_count":322,"comment_count":576,"is_copyable":true,"question":{"id":"304137885","type":"question","url":"https://api.zhihu.com/questions/304137885","author":{"id":"039d0ba5ffe3840843948e739a0305df","url":"https://api.zhihu.com/people/039d0ba5ffe3840843948e739a0305df","user_type":"people","url_token":"li-wan-yi-21-48","name":"李皖伊","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"为什么武昌起义后清政府迅速土崩瓦解了？","created":1543543942,"answer_count":0,"follower_count":0,"comment_count":20,"bound_topic_ids":[7573,8075,26547],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://picx.zhimg.com/50/v2-606e6d95c28c26f8f3036ce5eb17a618_720w.jpg?source=b6762063","excerpt":"现在的学校历史教科书，对辛亥革命的讲解过于简单，甚至有许多重要的东西讲都没讲，让许多人误以为，1912年满清皇帝在退位前并没有到山穷水尽的地步。 真正的历史是，武昌起义后，各省武装起义风起云涌，相继光复，后来北伐军都快打到溥仪家门口了。 首先我们来说说武昌起义后各地的情况。 辛亥反清战争初期，1911年十月十日在武昌打响了第一枪，南方各省先后宣布起义独立。武汉三镇，福州，杭州等大城市都爆发了战斗，革命军以…","excerpt_new":"现在的学校历史教科书，对辛亥革命的讲解过于简单，甚至有许多重要的东西讲都没讲，让许多人误以为，1912年满清皇帝在退位前并没有到山穷水尽的地步。 真正的历史是，武昌起义后，各省武装起义风起云涌，相继光复，后来北伐军都快打到溥仪家门口了。 首先我们来说说武昌起义后各地的情况。 辛亥反清战争初期，1911年十月十日在武昌打响了第一枪，南方各省先后宣布起义独立。武汉三镇，福州，杭州等大城市都爆发了战斗，革命军以…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"Fmk-2Ap7\"\u003e现在的学校历史教科书，对辛亥革命的讲解过于简单，甚至有许多重要的东西讲都没讲，让许多人误以为，1912年满清皇帝在退位前并没有到山穷水尽的地步。\u003c/p\u003e\u003cp data-pid=\"p8Xy_NE-\"\u003e真正的历史是，武昌起义后，各省武装起义风起云涌，相继光复，后来北伐军都快打到溥仪家门口了。\u003c/p\u003e\u003cp data-pid=\"u1UKTfMY\"\u003e首先我们来说说武昌起义后各地的情况。\u003c/p\u003e\u003cp data-pid=\"zBTxbMXK\"\u003e辛亥反清战争初期，1911年十月十日在武昌打响了第一枪，南方各省先后宣布起义独立。武汉三镇，福州，杭州等大城市都爆发了战斗，革命军以武力光复了南方全境，其中在南京，江浙联军于11月20日对城内外清军展开了进攻，12月3日占领了南京，为日后中华民国临时政府定都南京创造了条件。在北方，山西和陕西紧跟反清革命的脚步，也打出了独立旗帜，其中以西安的战斗最为激烈，张凤翙，井勿幕等将领领导的革命军（后更名为秦陇复汉军），攻破了西安满城，全歼了城内所有八旗军和旗人，之后光复全省。\u003c/p\u003e\u003cp data-pid=\"JL7Odx7G\"\u003e但满清异族政府并不甘心束手就擒，虽然剩余的八旗军早已沦为酒囊饭袋，但还有北洋军可用，于是清廷请求袁世凯出山，领兵南下攻击南方革命军。\u003c/p\u003e\u003cp data-pid=\"lUqWJZ93\"\u003e下图：辛亥革命时，成功独立的省份\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-90f3d500cce6743f67d49a9eb09f77fb_1440w.jpg\" data-rawwidth=\"350\" data-rawheight=\"350\" data-size=\"normal\" data-original-token=\"v2-7c261eabe418ea1813d9dfea82e75783\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-eedcf39c70677db6c5993f768a43f44d_b.jpg\" class=\"content_image\" width=\"350\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"sTv1E1rE\"\u003e下图：辛亥各省义军旗帜\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-e7d8aa80b8a80a683c5675b55c015f84_1440w.jpg\" data-rawwidth=\"960\" data-rawheight=\"1280\" data-size=\"normal\" data-qrcode-action=\"none\" data-original-token=\"v2-6bcab69cc138e4ef7c9b5b5f6cecd45f\" data-default-watermark-src=\"https://pic1.zhimg.com/v2-0cbb4dac50410d6a9b40a28c9afcefbe_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-e7d8aa80b8a80a683c5675b55c015f84_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"soP8a4JB\"\u003e之后的历史，从袁世凯领兵南下到清帝退位这段时间的情况，想必就有许多人不了解了。一些人张口就来，说“南方革命军打不过北洋军，所以只能谈判解决问题，在清帝退位前，北洋军依旧有着优势。”这句话是大错特错的，只能说讲这些话的人，根本没有真正研究过辛亥革命史……\u003c/p\u003e\u003cp data-pid=\"aRKkCC6N\"\u003e其实，在袁世凯的北洋军南下后，第一时间就发动了对武汉三镇的进攻，湖北革命军拼死抵抗，在汉阳，汉口两地失守后，依旧坚守在武昌，屹立不倒，这一战役史称阳夏保卫战。与此同时，在北方，清军也拼命向山西陕西的革命军发动进攻，甘肃的清军将领升允，率领军队向西安发起了攻势，与秦陇复汉军展开了一场激烈的恶战，河南清军也对潼关发起了攻势。好在秦陇复汉军战斗力强悍，顶住了清军的反扑，让清军在满清皇帝退位当天，也没能染指西安。在山西，袁世凯改派北洋新军第三镇统制曹锟及所属的第五协协统卢永祥部进攻山西，12月中旬，攻陷了娘子关，并且进而攻陷了太原，但山西革命军并没有投降，依旧在三晋与敌坚持作战。\u003c/p\u003e\u003cp data-pid=\"BZyWzh2i\"\u003e而秦晋和湖北三地的战事，吸引了清军诸多兵力，从而给革命军在其他战线上的胜利提供了条件。随着南方全境的光复，各省的起义军精锐先后来到湖北地区，为了实现战事真正的破局，反清革命政府制订了黄孝战役计划与北伐计划，意在彻底消灭满清最后的优势兵力，从而逼迫清帝退位，光复全中国。\u003c/p\u003e\u003cp data-pid=\"VmYcyjwn\"\u003e辛亥革命战争进入中后期的时候，南方革命政府开始与满清北洋军展开谈判，但谈判依旧不顺利，于是，对满清给予痛击再次成为了革命军的首要任务。\u003c/p\u003e\u003cp data-pid=\"9Ny9_sYI\"\u003e1911年12月31日，黄孝战役打响，此战役为辛亥革命战争期间最大的会战，且此战役分为三个阶段，湖北省文旅厅《谈辛亥革命中的黄孝战役》一文中这样写到：\u003c/p\u003e\u003cp data-pid=\"-Vx1s30H\"\u003e战役第一阶段：\u003c/p\u003e\u003cp data-pid=\"T4AVhSY6\"\u003e“……下午2点，湖北革命军（也称革命军第一军、军长：吴兆麟、杜锡钧）沿武昌江堤一线发起进攻，清军的炮弹雨点般地落在民军阵地上，双方激战一整天，鄂军佯攻目的达到，左翼军（又叫革命军第三军、由湖南革命军独立混成第二协和广西陆军混成协（旅）组成、军长沈秉堃、湖南革命军独立混成第二协第一标为先锋军、标统姚宏陶）、右翼军（也叫革命军第二军、由江西革命军第一、二协，安徽陆军混成协（旅），江苏、上海革命军第一镇及浙江、广东部分部队组成，军长李烈钧）未受多大阻力全部渡过了长江。渡江当天下雪，凛冽江风裹着冰渣砸在革命军人的身上，官兵们决心打倒清军的斗志是任何恶劣条件阻挡不了的，个个奋勇向前。过江后左翼军攻克蔡甸。清军蔡甸外围的两个营畏惧被歼灭，遂向湖南革命军独立混成第二协联系投降，革命军予以接受，至日末，共有200余人前来投降。清军守军仓惶逃向汉阳，在即将被合围的情况下，借助停战的机会，清军开始了急速撤军。”\u003c/p\u003e\u003cp data-pid=\"4zdsF7tB\"\u003e战役第二阶段：\u003c/p\u003e\u003cp data-pid=\"pUPmZXsu\"\u003e“……1912年1月5日，汉阳清军除留下1000余名官员、巡警“保护治安”，其余全部退往汉口。在汉口的清军开始向黄陂、孝感退却，渐渐摆脱了合围的威胁，于是清廷在和谈中日趋强硬，驳回民军的全部要求。南京临时政府于1月6日下达了用兵方略，湖北军政府下令，准备战斗，左翼军按原计划突击。1月11日，鄂军从阳逻出发，经三山铺向黄陂方向进攻。1月15日，清军退到孝感。三叉埠、杨店附近的清军有一个师，退到广水附近的武胜关到东篁店的清军有4个团。退到河南信阳附近的清军有1个旅，另有一些清军在颖州附近。1月16 日，革命军右翼军占领祁家湾向三叉埠突击，在三叉埠消灭了清军100余人。清军为此增援了一个镇（师）的兵力疯狂反扑，清军炮队由铁路之西沿轨道炮击革命军右翼军，炮火非常密集，右翼军退回祁家湾以东地区。清军与他们只有半里之隔，由于两军相距太近，冲突不断发生，清军的主力被右翼军牵制。革命军第一军向三山铺前进占领甘家店。革命军第三军从蔡甸出发，渡过汉水，向新沟、汉川前进，占领新沟以东阵地。1月18日，清廷拒绝共和，一方面假装谈判，另一方面积极备战。1912年1月28日，孝感县的敌人，约有混成一协（旅）以上，三叉埠的敌人，约有混成一协以下，杨店附近的敌人，约有混成一个标（团），云梦、应城也有小股部队的敌人。直到1月28 日，谈判仍没有任何结果。”\u003c/p\u003e\u003cp data-pid=\"fTXfGmFR\"\u003e战役第三阶段：\u003c/p\u003e\u003cp data-pid=\"E2Sbj0im\"\u003e“……1月31日，革命军第二军、第三军抗令向前推进。革命军第一军占领孝感附近的杨店，第二军占领孝感附近的三叉埠，第三军占领孝感县城，捣毁了清军在湖北的总司令部，乘胜追击占领孝感火车站，缴获了清军大批的军用物资。清军遭受这几个军的攻击，死伤颇重，被迫丢下大量的粮食、武器、弹药，向北逃窜。2月2日，革命军第一军三个支队收复了汉阳、汉口，湖北境内的清军退往广水、武胜关。由于清军中有大量士兵逃跑，段祺瑞无可奈何率部反正，带领42名团以上的军官表示赞成共和。2月9 日，革命军第一军在广水附近接受了一个整营的清军投降，革命军继续追击，将清军赶出湖北，清军沿铁路向北逃窜。”\u003c/p\u003e\u003cp data-pid=\"lQZ-bPdS\"\u003e至此，湖北革命军光复了湖广全境，并把战线推进到了豫鄂边境，而就如前面写到的，清军内部也发生了大规模的投诚起义事件，可见当时湖北战场上的清军已经士气低落到了极点，如果不是当时南京临时政府要与北洋军谈判，恐怕革命军早就打到河南去了。\u003c/p\u003e\u003cp data-pid=\"IGvrTcbw\"\u003e在东线，姚雨平和蓝天蔚两位将军先后发动了北伐攻势。\u003c/p\u003e\u003cp data-pid=\"psweDwGf\"\u003e当时，革命大佬胡汉民命姚雨平为广东北伐军总司令，马锦春为副司令，组织北伐军。当时姚雨平的北伐军只有8000余人，但都斗志昂扬，且武器装备也很先进，全军有德国造新式火炮54门，炮弹一万余发。该部之前参与了攻克南京的大战，在南京光复后，又立即投入北上中原的战斗。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-b4adf2ee1f3fc87f9c844f9724acb8b5_1440w.jpg\" data-rawwidth=\"1600\" data-rawheight=\"714\" data-size=\"normal\" data-original-token=\"v2-b8bb9934c1b7e83ba8dd7329e389b203\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-efcfbb920b97c497f30d5ceccb377b53_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://picx.zhimg.com/v2-b4adf2ee1f3fc87f9c844f9724acb8b5_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"bfZIFckH\"\u003e民国元年（1912年）元旦，孙中山先生在南京就任临时大总统，但南北议和未成。姚雨平奉大总统命率粤军于1月上旬沿津浦铁路北进，讨伐清廷，渡长江时，颁渡江誓师文，军威雄壮。他派李济深先到浦口，寻找驻地；又派林震为前敌总指挥，设总部于蚌埠东站。粤军北进至江苏之固镇，2月4日上午8时与清军张勋、倪嗣冲所部发生遭遇战，至下午2时大破张勋等；北伐军士气旺盛，愈战愈猛，追至宿州城外，又与清军展开大战。清军的骑炮占优势，但粤军英勇杀敌，以一当百，从拂晓战至下午3时，清军大败，退出宿州。此役杀敌千余人，生擒数十人，投降者一百余人，缴获大量军械、马匹。粤军乘胜追至徐州以南的夹沟。9日，退保徐州的张勋接受北洋军首领袁世凯的指令，派代表乞和。10日，双方代表于符离集车站会谈。在谈判中，姚雨平要求张勋退出徐州100里之外。但条件不合，和议决裂。12日，姚雨平再度挥军北指，一战而克徐州，北追30里，张勋败走济南，革命军军威大振。（该段战役过程转自广东省文史资料）\u003c/p\u003e\u003cp data-pid=\"CP8r6Yf2\"\u003e另一路，蓝天蔚将军也率领部队发起了北伐，1912年1月3日，孙中山总统正式任命蓝天蔚为关外大都督，北伐第二军总司令。北伐开始后，部队以山东为首要目标，连续光复了烟台，登州，黄县等地，攻势之凌厉让满清政府大惊。之后，蓝天蔚将军的北伐部队更是越过大海，直捣辽东半岛。将军在烟台庄严宣布:“北伐军不日将开赴东三省攻略各地，东北将克日光复”。\u003c/p\u003e\u003cp data-pid=\"HPtCbQiA\"\u003e1912年2月1日，蓝天蔚将军指挥北伐军主力分别在辽东半岛的貔子窝、花园口、大孤山、安东等海岸同时进行突击登陆。并深入辽宁省腹地，清军力战不敌，节节败退，这下，鞑靼人连他们的“龙兴之地”都快要守不住了。蓝天蔚将军的北伐辽东战役，也有着巨大的历史意义，这一战役，是自明末辽东沦陷以来，汉人的反清军队第一次在辽东关外的土地上出现，当年萨尔浒之战牺牲的明军抗清烈士们，也终于得到了告慰。\u003c/p\u003e\u003cp data-pid=\"dNLfsZTS\"\u003e其他方面，在新疆，1912年1月7日，汉人革命军发动了伊犁起义，宣布伊犁脱离满清政府统治。自此，自唐末中原汉人王朝丢失新疆以来，一千多年后，汉人政府终于实质性的收复了新疆。另一方面，新疆的光复，也让满清政府死了迁都西域继续对抗革命军的心思。\u003c/p\u003e\u003cp data-pid=\"s75RnirR\"\u003e在直隶，1912年1月3日，同盟会会员王金铭、施从云、白雅雨响应武昌起义号召，率滦州新军宣布起义，通电全国，宣布独立，成立北方革命军政府，并计划攻占京津，彻底消灭满清。虽然该起义后来在叛徒的出卖下失败了，但无疑给了满清政府沉重的一击，也标志着满清占领下的北京城不再安全。当然，后来的事实证明，对满清贵族而言，北京确实也埋伏着革命军特工的杀机，1912年1月26日，满清主战派大臣良弼，被革命志士彭家珍用炸弹刺杀于北京城内，从此，满清朝堂内基本上没有想要顽抗到底的满人大臣了。\u003c/p\u003e\u003cp data-pid=\"oyumwLTf\"\u003e到了1912年初，南方革命政府在多条战线上都取得了胜利，明眼人都能看出来，满清政府已经大势已去了，满清贵族唯有投降汉人，才能保全旗人的安全，顽抗到底的话只有死路一条。最终，走投无路的满清皇室听从了袁世凯的建议而退位，向中华民国政府投降，并把北方剩余的小部分领土和蒙藏之地割让给了中华民国，满清皇室退位后也以外国君主之礼相待。至此，满清灭亡，辛亥革命反清战争胜利了。\u003c/p\u003e\u003cp data-pid=\"XbJcY4f7\"\u003e所以，为什么武昌起义后满清政府迅速就瓦解了？主要原因就是因为革命军的攻势，还有就是当时的满清确实是山穷水尽了。辛亥革命后期的清廷，除了京畿周边的亲军，就剩下了袁世凯的几万北洋军（关键是袁家的北洋军还有二心），且满清政府连军费也拿不出来了，反观革命军，在数量上，士气上，战果上，都远超清军。到了最后，满清政府也只有投降一条路了。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-d1f3cba6d1c51ad04086900477eb7e7c_1440w.jpg\" data-rawwidth=\"800\" data-rawheight=\"1523\" data-size=\"normal\" data-original-token=\"v2-68f9357a75baf412f61e3834cac08dc3\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-3f125045e3881f2eea79c8c311a70586_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic1.zhimg.com/v2-d1f3cba6d1c51ad04086900477eb7e7c_r.jpg\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":212033,"thumbnails":["https://picx.zhimg.com/50/v2-606e6d95c28c26f8f3036ce5eb17a618_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-4bf6904ce23e5f7e1c0736a694641ad1_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-3d321a1560ef4617cfd823bb4f64959f_720w.jpg?source=b6762063"],"favorite_count":1960,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 95027628746}","attached_info":"CpYICOTe9sWEv8/zngEQBBoJNzEyMTIxMDIyIICCl70GKLgbMMAEQH9KVAooVFNfU09VUkNFX1RBR19CQVNFRF9URVhUX1RIRU1FX0RFVEVDVElPThIidGhlbWU6ZGV0ZWN0OmNvbnRlbnQ6dGhlbWU6aWQ6MjQwMhgAIAA6AEo/CipUU19TT1VSQ0VfWlJFQ0FMTF9GRUVEUkVfTkVXQklFX0hPVVJMWV9SVU0SATAYACAAOgp7InJhdyI6IiJ9WggyOTg2MjY1NmIgYjM3YzI0MWIxOGEwMTQ5NDJhOTQyYjM1ZmZkMjIwNTlyCzk1MDI3NjI4NzQ2igEJMzA0MTM3ODg1qgEJcmVjb21tZW5kwgEgZjU3MTMyYjNiNmE2NTBlNWZkMDgzOGIxZWI4ODhhZDPyAQoIDBIGTm9ybWFs8gEoCAoSJDRhZjBkN2QxLTNhNjItNDU4OC1hYjFiLTUyZGYwZmEzMDEwYvIBBggLEgIyMoICAIgC18vXzoUzkgIgZjU3MTMyYjNiNmE2NTBlNWZkMDgzOGIxZWI4ODhhZDOaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWUmV2aXNpdFZhbHVlV2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxl2gIoVFNfU09VUkNFX1RBR19CQVNFRF9URVhUX1RIRU1FX0RFVEVDVElPTugCA/oCC05PUk1BTF9GTE9XigMgMTk2ODljNDQ3MDkxNDEyOWE4ZTdiMjZjODhiODdhOGKaAw0KAnYyEAAaBW90aGVyqAPB+AzYAwDqAyNUaGVtZURldGVjdFRoZW1lQWlEZXRlY3RUYWdSZWNhbGxlcvoD2wESDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAxDeAhjeAiIjdjItN2MyNjFlYWJlNDE4ZWExODEzZDlkZmVhODJlNzU3ODM6LQgCEMAHGIAKIiN2Mi02YmNhYjY5Y2MxMzhlNGVmN2M5YjViNWY2Y2VjZDQ1ZjotCAMQwAwYygUiI3YyLWI4YmI5OTM0YzFiN2U4M2JhOGRkNzMyOWUzODliMjAzOi0IAhCgBhjzCyIjdjItNjhmOTM1N2E3NWJhZjQxMmY2MWUzODM0Y2FjMDhkYzOABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQGbWFudWFswgQDMTYwyAQB0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAACgftm1P4EFAAAAAAAAAACJBTTexUhpc9M/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQHwBRaQBgCgBn+oBgOSAiYKCTcxMjEyMTAyMhILOTUwMjc2Mjg3NDYYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"128_1753853847.767","type":"feed","offset":128,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853847,"updated_time":1753853847,"target":{"id":"1921925939434099727","type":"answer","url":"https://api.zhihu.com/answers/1921925939434099727","author":{"id":"e745ae4335a718ad186b6d65065a67e7","url":"https://api.zhihu.com/people/e745ae4335a718ad186b6d65065a67e7","user_type":"people","url_token":"fan-fan-44-39","name":"愚者的圣杯","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-a2f528561b4b2426e727c052dc557f54_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":797,"is_following":false,"is_followed":false},"created_time":1751002851,"updated_time":1751003125,"voteup_count":15,"thanks_count":0,"comment_count":5,"is_copyable":true,"question":{"id":"469450888","type":"question","url":"https://api.zhihu.com/questions/469450888","author":{"id":"cfce35514e9bd6c80440b53cf5d76f1c","url":"https://api.zhihu.com/people/cfce35514e9bd6c80440b53cf5d76f1c","user_type":"people","url_token":"li-43-53","name":"树的葡萄","headline":"高中老师，公众号：晓技术","avatar_url":"https://pica.zhimg.com/50/ac19340e02767cc5e70f2703864fb1b8_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":33339,"is_following":false,"is_followed":false},"title":"你最满意的10款 PC 软件是什么？","created":1625187850,"answer_count":0,"follower_count":0,"comment_count":38,"bound_topic_ids":[3232,3458,3798,154649,72317],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"原则：能免费就不选收费，能绿色就不选安装 日常工具： 系统：Windows，工作上这个没得选，Linux还是进虚拟机吧。 数据：BitLocker，微软原生的数据加密，带笔记本出差时安心很多。 文件：Everything，加个空格exe跟Listary没啥区别。 平铺：GlazeWM，在Windows下i3wm的最好平替，这个纯属各人喜好了。 网页：Chrome，其实不算完美，但Edge继承了微软的坏毛病，会偷偷乱改设置。生产力工具： 邮件：Outlook，依然微软式蠢笨，但…","excerpt_new":"原则：能免费就不选收费，能绿色就不选安装 日常工具： 系统：Windows，工作上这个没得选，Linux还是进虚拟机吧。 数据：BitLocker，微软原生的数据加密，带笔记本出差时安心很多。 文件：Everything，加个空格exe跟Listary没啥区别。 平铺：GlazeWM，在Windows下i3wm的最好平替，这个纯属各人喜好了。 网页：Chrome，其实不算完美，但Edge继承了微软的坏毛病，会偷偷乱改设置。生产力工具： 邮件：Outlook，依然微软式蠢笨，但…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"T3G9byQK\"\u003e原则：能免费就不选收费，能绿色就不选安装\u003c/p\u003e\u003cp data-pid=\"zqKElzVV\"\u003e日常工具：\u003c/p\u003e\u003cp data-pid=\"77xacz2L\"\u003e系统：Windows，工作上这个没得选，Linux还是进虚拟机吧。\u003cbr/\u003e数据：BitLocker，微软原生的数据加密，带笔记本出差时安心很多。\u003cbr/\u003e文件：Everything，加个空格exe跟Listary没啥区别。\u003cbr/\u003e平铺：GlazeWM，在Windows下i3wm的最好平替，这个纯属各人喜好了。\u003cbr/\u003e网页：Chrome，其实不算完美，但Edge继承了微软的坏毛病，会偷偷乱改设置。\u003c/p\u003e\u003cp data-pid=\"r91RKbG8\"\u003e生产力工具：\u003c/p\u003e\u003cp data-pid=\"xyRl2bJf\"\u003e邮件：Outlook，依然微软式蠢笨，但好在兼容性强，国内外乱七八糟的邮箱都能吃。\u003cbr/\u003e资料：\u003ca href=\"https://link.zhihu.com/?target=http%3A//obsidian.md/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eObsidian.md\u003c/a\u003e，登录系统后的第一件事就是开它，用Tasks和Dataviewjs来管理任务。\u003cbr/\u003e文献：Zotero，越来越强大了，而且有绿色版，加上插件功能非常全。\u003cbr/\u003e代码：VS Code，从Markdown、Matlab到Latex全都能吃，简直瑞士·奥卡姆·军刀。\u003cbr/\u003e游戏：Steam，科研生产力的驱动核心。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":3306,"favorite_count":44,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921925939434099727}","attached_info":"CsYFCOTe9sWEv8/zngEQBBoJNzM0MjcwNDUxIOPd+MIGKA8wBUCAAUogChVUU19TT1VSQ0VfVEhFTUVfTUVSR0USATAYACAAOgBKKAodVFNfU09VUkNFX05FQVJMSU5FX0NPTlRFTlRfVjISATAYACAAOgBaCDY2NjAyMTU2YiBiMzdjMjQxYjE4YTAxNDk0MmE5NDJiMzVmZmQyMjA1OXITMTkyMTkyNTkzOTQzNDA5OTcyN4oBCTQ2OTQ1MDg4OKoBCXJlY29tbWVuZMIBIGU3NDVhZTQzMzVhNzE4YWQxODZiNmQ2NTA2NWE2N2U38gEKCAwSBk5vcm1hbPIBKAgKEiRjZDRiNGU1NC1hZmM4LTQyYjAtYWI4MC04YzQ4YzYwYzUxNGLyAQYICxICMjKCAgCIAtfL186FM5ICIGU3NDVhZTQzMzVhNzE4YWQxODZiNmQ2NTA2NWE2N2U3mgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZdoCFVRTX1NPVVJDRV9USEVNRV9NRVJHRegCA/oCC05PUk1BTF9GTE9XigMgMTk2ODljNDQ3MDkxNDEyOWE4ZTdiMjZjODhiODdhOGKaAw0KAnYyEAAaBW90aGVyqAPqGdgDAOoDG1RoZW1lTWVyZ2VOZXdWMVBvb2xSZWNhbGxlcvoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAGCdSKU/gQUAAAAAAAAAAIkFNN7FSGlz0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFFpAGAKAGgAGoBgCSAi4KCTczNDI3MDQ1MRITMTkyMTkyNTkzOTQzNDA5OTcyNxgEIgpJTUFHRV9URVhU","action_card":false},{"id":"129_1753853847.280","type":"feed","offset":129,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1753853847,"updated_time":1753853847,"target":{"id":"1923094066398226353","type":"answer","url":"https://api.zhihu.com/answers/1923094066398226353","author":{"id":"546f87a3b3bb0d283ffdbd30860f2625","url":"https://api.zhihu.com/people/546f87a3b3bb0d283ffdbd30860f2625","user_type":"people","url_token":"superpeople-wudi","name":"当时明月在","headline":"那个叫明月的姑娘去哪了","avatar_url":"https://picx.zhimg.com/50/v2-2658c24b633521bf33ac2f143c20bb29_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"best_answerer","description":"职场等 2 个话题下的优秀答主","topic_names":["职场","程序员"],"topic_ids":[2566,707]},{"type":"zhihu_yearly_answerer","description":"新知答主"}],"followers_count":86337,"is_following":false,"is_followed":false},"created_time":1751281354,"updated_time":1751282559,"voteup_count":113,"thanks_count":3,"comment_count":14,"is_copyable":false,"question":{"id":"5238193883","type":"question","url":"https://api.zhihu.com/questions/5238193883","author":{"id":"5790fa3c54e38be49fcee4a68adccbec","url":"https://api.zhihu.com/people/5790fa3c54e38be49fcee4a68adccbec","user_type":"people","url_token":"cute-aaa","name":"cute aaa","headline":"","avatar_url":"https://pic1.zhimg.com/50/v2-25f5ad6bdd74e9f989c0095a7eac7160_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":9,"is_following":false,"is_followed":false},"title":"你认为以后程序员的工作会被 AI 取代吗？为什么呢？","created":1732656816,"answer_count":0,"follower_count":0,"comment_count":2,"bound_topic_ids":[707,12585,12591],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"之前有个客户找我做个简单的程序。 涉及一些偏冷门的技术，和一些算法。 我和团队都没做过这个方向。 客户报价不高，但毕竟是老客户，考虑到其他大单，没法拒绝。 我问了做这个方向的朋友，朋友说友情价3万，只收开发成本。 我虽然不熟悉技术细节，但是查了查资料，也觉得他的报价真的相当优惠了。 然后用AI查资料的过程中，我灵机一动，自己写个demo试试？ 说干就干，于是……我就一不小心把程序完成了，并且交付了。 一共用了…","excerpt_new":"之前有个客户找我做个简单的程序。 涉及一些偏冷门的技术，和一些算法。 我和团队都没做过这个方向。 客户报价不高，但毕竟是老客户，考虑到其他大单，没法拒绝。 我问了做这个方向的朋友，朋友说友情价3万，只收开发成本。 我虽然不熟悉技术细节，但是查了查资料，也觉得他的报价真的相当优惠了。 然后用AI查资料的过程中，我灵机一动，自己写个demo试试？ 说干就干，于是……我就一不小心把程序完成了，并且交付了。 一共用了…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"jFZs8LIZ\"\u003e之前有个客户找我做个简单的程序。\u003c/p\u003e\u003cp data-pid=\"4aBz3Zit\"\u003e涉及一些偏冷门的技术，和一些算法。\u003c/p\u003e\u003cp data-pid=\"BqF3GKHa\"\u003e我和团队都没做过这个方向。\u003c/p\u003e\u003cp data-pid=\"FSG2EnlN\"\u003e客户报价不高，但毕竟是老客户，考虑到其他大单，没法拒绝。\u003c/p\u003e\u003cp data-pid=\"638zFEIo\"\u003e我问了做这个方向的朋友，朋友说友情价3万，只收开发成本。\u003c/p\u003e\u003cp data-pid=\"gGzSjmVd\"\u003e我虽然不熟悉技术细节，但是查了查资料，也觉得他的报价真的相当优惠了。\u003c/p\u003e\u003cp data-pid=\"1ivAFrKg\"\u003e然后用AI查资料的过程中，我灵机一动，自己写个demo试试？\u003c/p\u003e\u003cp data-pid=\"T5KNaudJ\"\u003e说干就干，于是……我就一不小心把程序完成了，并且交付了。\u003c/p\u003e\u003cp data-pid=\"Krmta5HM\"\u003e一共用了五天。\u003c/p\u003e\u003cp data-pid=\"bMhQH3we\"\u003e用完全陌生的语言，去做完全陌生的领域，并且完成了交付。\u003c/p\u003e\u003cp data-pid=\"Sq2jC_J1\"\u003e……\u003c/p\u003e\u003cp data-pid=\"zLt91lgB\"\u003e我用1.5天研究了一下理论知识，以及开发环境的安装、调试、写helloworld测试。\u003c/p\u003e\u003cp data-pid=\"rrGzt2IY\"\u003e用半天时间系统的做了整体程序设计，画了接口图，定义类的调用关系、出参和入参等。\u003c/p\u003e\u003cp data-pid=\"Rtxm7fvY\"\u003e剩下的都是AI写的代码，我几乎一行代码也没写。\u003c/p\u003e\u003cp data-pid=\"PzUW1i_H\"\u003e调试了三天也不是全职，每天就抽三、四个小时的样子。\u003c/p\u003e\u003cp data-pid=\"xxLTzUP6\"\u003e从工作量来说，按照八小时计算，工作量还不到三天。\u003c/p\u003e\u003cp data-pid=\"PzyYktUL\"\u003e……\u003c/p\u003e\u003cp data-pid=\"2KQMZlnk\"\u003e需要解释一下，并非我花5天时间，就比我这个朋友强。\u003c/p\u003e\u003cp data-pid=\"0tJSEpjO\"\u003e而是客户的需求，AI完成的程序恰好可以满足，另一方面毕竟我这么多年的经验，计算机体系熟悉，陌生语言只是熟练度的问题，而非什么挑战。\u003c/p\u003e\u003cp data-pid=\"eLNsOAk3\"\u003e还有个关键点就是，这个程序需要具备一定的算法基础，这个可能很多程序员都不具备。\u003c/p\u003e\u003cp data-pid=\"R5nlL3NR\"\u003e客户要求也不高，最后也没怎么做测试，如果是商用程序就必须得专业的人来了。\u003c/p\u003e\u003cp data-pid=\"fC9n8wPT\"\u003e我想先自己试试……嗯，结果挺么么哒……\u003c/p\u003e\u003cp data-pid=\"XLIgIKU1\"\u003e……\u003c/p\u003e\u003cp data-pid=\"Gml8_gF0\"\u003e理论上，一个完全没有编程经验的人，也可以做这个事情。\u003c/p\u003e\u003cp data-pid=\"XssjEi33\"\u003e但实践上很困难。\u003c/p\u003e\u003cp data-pid=\"IDyw5fpt\"\u003e别说没经验的，就算是一些初中级程序员，可能也干不了这个活。\u003c/p\u003e\u003cp data-pid=\"uA84Ug6_\"\u003e他们起码有系统级的设计经验才行。\u003c/p\u003e\u003cp data-pid=\"WVLMRbXp\"\u003e近两年，我身边创业圈，不少从来没学过编程的人，都有自己尝试学习，主要是为了给自己省钱。\u003c/p\u003e\u003cp data-pid=\"RwukSVbj\"\u003e很多外行已经可以用demo愉快的玩耍了。\u003c/p\u003e\u003cp data-pid=\"dsSDW6Nu\"\u003e当然demo只是个demo，和真正的商业应用完全不是一码事情。\u003c/p\u003e\u003cp data-pid=\"UU2N81pk\"\u003e说远了。\u003c/p\u003e\u003cp data-pid=\"qDeBF58C\"\u003e只是说这个故事给了我一点启发，未来程序员可能的趋势：\u003c/p\u003e\u003cp data-pid=\"JNb-rxQg\"\u003e\u003cb\u003e写代码将做为基础学科，类似过去的学五笔，以后小学生都能随便做个网页、APP，对程序员的需求量自然会减少\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"xFA6k1sF\"\u003e大量初级的被淘汰不是很正常了么。\u003c/p\u003e\u003cp data-pid=\"bW12FXXO\"\u003eAI在发展两年，更强大了，有些人出于省钱的需求，干脆自己学。\u003c/p\u003e\u003cp data-pid=\"zm57l0oY\"\u003e另一方面，公司的资深程序员，用AI让自己效率更快。\u003c/p\u003e\u003cp data-pid=\"Y_I_LtH9\"\u003e整体需求降低，一部分程序员被挤出这个赛道。\u003c/p\u003e\u003cp data-pid=\"7y8dWrO2\"\u003e在AI时代，程序员需要重构自己的知识体系了。\u003c/p\u003e\u003cp data-pid=\"76YEaOte\"\u003e过去程序员的门槛，普遍就是卷熟练度和背题，一旦卷入一个不错的公司，依赖公司发展，就能蹭到经验以及成长的机会，于是又加高了自身的从业门槛。\u003c/p\u003e\u003cp data-pid=\"uAESZAmy\"\u003e但是未来这样的机会大概会大量减少，因为建设大型系统的需求没那么多了，很多东西也上云了、组件化了。\u003c/p\u003e\u003cp data-pid=\"YZIhXc-l\"\u003e未来，可能程序员的知识体系，\u003cb\u003e需要从实践方向转向为理论方向\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"7GiEFH3q\"\u003e即过去实践消耗了大量的时间，未来会节省下来这个时间。\u003c/p\u003e\u003cp data-pid=\"PULk3YBU\"\u003e但是深度还是得够，否则无法从整体上看待技术，也看不透技术背后蕴含的东西。\u003c/p\u003e\u003cp data-pid=\"yncCOgbm\"\u003e需要深一门，然后增加广度。\u003c/p\u003e\u003cp data-pid=\"JT3NBnA3\"\u003e广度够了，就能跨行业完成解决方案。\u003c/p\u003e\u003cp data-pid=\"NKi76OKM\"\u003e比如说我开头说这个故事，就是个跨行业的事情。\u003c/p\u003e\u003cp data-pid=\"soW6yRNm\"\u003e理论熟悉了，并且需要掌握系统性的思维，包括但不限于：对不同行业的理解、计算机整个体系的构成、项目管理经验、系统工程学等等。\u003c/p\u003e\u003cp data-pid=\"oXP00-nm\"\u003e这样，即使是陌生的领域，也一样可以快速完成程序的构建。\u003c/p\u003e\u003cp data-pid=\"akTIk36F\"\u003e还有另一种，资深程序，需要在某个领域不仅仅要深度，还要多年的经验、资历等等。\u003c/p\u003e\u003cp data-pid=\"gARxaXmO\"\u003e简单说就是肉身卷赢AI。\u003c/p\u003e\u003cp data-pid=\"odVJfCPV\"\u003eAI干不了的事儿，这种人能干。\u003c/p\u003e\u003cp data-pid=\"QAwwVxbr\"\u003e然后这些人可以利用AI，不断放大自己的效率。\u003c/p\u003e\u003cp data-pid=\"0DJqN3Xw\"\u003e最终从业者两极分化。\u003c/p\u003e\u003cp data-pid=\"FKAA7DUI\"\u003e一种是普通人，大家都能做点简单的程序。\u003c/p\u003e\u003cp data-pid=\"1e1T9kWp\"\u003e另一种是专业的。\u003c/p\u003e\u003cp data-pid=\"icam-kjw\"\u003e类比就是游戏里的建筑师。\u003c/p\u003e\u003cp data-pid=\"h-6lXcC2\"\u003e大家都能利用游戏给的模型盖房子，但是高手和低手，是不一样的。\u003c/p\u003e\u003cp data-pid=\"gMpY9lbO\"\u003e总之，个人认为，\u003cb\u003e程序员的整个知识和技术体系，在AI时代，需要重构\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"WHNr1Icd\"\u003e我们都需要思考，自身做为从业者，门槛在哪里。\u003c/p\u003e\u003cp data-pid=\"9FXBCXiK\"\u003e\u003cb\u003e其实最大的门槛在业务上\u003c/b\u003e，但我们暂且只探讨技术。\u003c/p\u003e\u003cp data-pid=\"nPtC0YYC\"\u003e技术上的门槛，可能不仅仅需要深度，更需要广度。\u003c/p\u003e\u003cp data-pid=\"yo_evlKR\"\u003e比如大家都用Java写CRUD，都用spring，你会的别人也会，技术上的差距没多大。\u003c/p\u003e\u003cp data-pid=\"8J6i8z8p\"\u003e这个时候你如果会一点前端、Windows桌面端等等，还会点别的……可能都不深，但你能用AI完成快速的交付，技术上的门槛不就有了么？\u003c/p\u003e\u003cp data-pid=\"Ezy1M8z3\"\u003e另一种是在效率上下功夫，琢磨如何提升个人效率，这方面可以结合AI agent等工具，来进一步提升效率。\u003c/p\u003e\u003cp data-pid=\"jCKhbuP3\"\u003e从我的个人角度观测，AI写代码的效率还没有被最大化的挖掘。\u003c/p\u003e\u003cp data-pid=\"b9OcnqwW\"\u003e\u003cb\u003e也就是说，未来写代码的效率会比现在还快。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"ai_JOS2x\"\u003e过去完成个新语言、框架的入门训练，至少一周半个月，现在，可能两三天就够了。\u003c/p\u003e\u003cp data-pid=\"QogoEo79\"\u003e当然，这一切的建立条件，都需要对编程有系统性的认识，即深度。\u003c/p\u003e\u003cp data-pid=\"nhijtMnc\"\u003e数据结构、算法、计算机原理、编译原理……这些基本功还是少不了的。\u003c/p\u003e\u003cp data-pid=\"um0Ksr2K\"\u003e深度就是内功，广度就是吃饭的技术，内功好技术进步才快。\u003c/p\u003e\u003cp data-pid=\"CLUnFCXf\"\u003e还有另一条路，就是不在深度上下功夫，只研究交付。\u003c/p\u003e\u003cp data-pid=\"bK_t5f-p\"\u003e这条路能走多远我不确定，只能依赖于AI的进化速度。\u003c/p\u003e\u003cp data-pid=\"WaKJQdCQ\"\u003e估计最终还是得回头锻炼内功才行。\u003c/p\u003e\u003cp data-pid=\"iebyONMs\"\u003e目前来看，可能程序员的知识体系需要重构了。\u003c/p\u003e\u003cp data-pid=\"nC6zA_b5\"\u003e不仅仅是程序员，所有行业都是。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"uQTuwPTF\"\u003ePS：假如想利用技术赚点钱，市场先行。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":6752,"favorite_count":94,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1923094066398226353}","attached_info":"CosGCOTe9sWEv8/zngEQBBoJNzM0NzU2MjcxIMrdicMGKHEwDkCBAUo4Ci1UU19TT1VSQ0VfSU5URVJFU1RfV09SRF9DT05URU5UX1ZBTFVFX0xFVkVMX0ISATAYACAAOgBKKAodVFNfU09VUkNFX05FQVJMSU5FX0NPTlRFTlRfVjISATAYACAAOgBaCTExMjAyNDEzM2IgYjM3YzI0MWIxOGEwMTQ5NDJhOTQyYjM1ZmZkMjIwNTlyEzE5MjMwOTQwNjYzOTgyMjYzNTOKAQo1MjM4MTkzODgzqgEJcmVjb21tZW5kwgEgNTQ2Zjg3YTNiM2JiMGQyODNmZmRiZDMwODYwZjI2MjXyAQoIDBIGTm9ybWFs8gEoCAoSJDAwYzkzYWMxLWMxMzUtNDc4ZC1hMWU3LTRmZTRiMTBjMDk3NfIBBggLEgIyMoICAIgC18vXzoUzkgIgNTQ2Zjg3YTNiM2JiMGQyODNmZmRiZDMwODYwZjI2MjWaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxl2gItVFNfU09VUkNFX0lOVEVSRVNUX1dPUkRfQ09OVEVOVF9WQUxVRV9MRVZFTF9C6AIF+gILTk9STUFMX0ZMT1eKAyAxOTY4OWM0NDcwOTE0MTI5YThlN2IyNmM4OGI4N2E4YpoDDQoCdjIQABoFb3RoZXKoA+A02AMA6gMqSW50ZXJlc3RXb3JkQ29udGVudFZhbHVlTGV2ZWxCUG9vbFJlY2FsbGVy+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATWgBACoBACwBAC6BAZtYW51YWzCBAMxNzDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAMCTlqY/gQUAAAAAAAAAAIkFNN7FSGlz0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFFpAGAKAGgQGoBgCSAi4KCTczNDc1NjI3MRITMTkyMzA5NDA2NjM5ODIyNjM1MxgEIgpJTUFHRV9URVhU","action_card":false},{"id":"130_1753853847.871","type":"feed","offset":130,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853847,"updated_time":1753853847,"target":{"id":"1933537874306921961","type":"article","url":"https://api.zhihu.com/articles/1933537874306921961","author":{"id":"a036e7c73e2c6b497d915822bb31cc72","url":"https://api.zhihu.com/people/a036e7c73e2c6b497d915822bb31cc72","user_type":"people","url_token":"cpeqvl","name":"国产三蹦子","headline":"没有人是一座孤岛","avatar_url":"https://pic1.zhimg.com/50/v2-e1faf03c66e5b862b2fcfe906f587344_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":9,"is_following":false,"is_followed":false},"title":"都说一千个人中，就有一千个哈姆雷特，你怎么看？","comment_permission":"all","created":1753772005,"updated":1753772005,"voteup_count":0,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"读书笔记 莎士比亚—《哈姆雷特》 “一千个读者心中，就有一千个哈姆雷特。”就是这样我翻开了莎士比亚的《哈姆雷特》，一位少年踏上复仇之路，寻找真相，在生存还是毁灭中踱步，悲剧从开始一直贯穿到结尾，这或许是最好的命运归宿，复仇又显得一切多余，最后小说中的故事人物全都有了归属，我不觉得是悲剧，而这更像是命运的终结。 书中讲述的是丹麦王国的一个王子叫做哈姆雷特，知道了自己的叔父杀死了父亲谋权篡位，而后霸占…","excerpt_new":"读书笔记 莎士比亚—《哈姆雷特》 “一千个读者心中，就有一千个哈姆雷特。”就是这样我翻开了莎士比亚的《哈姆雷特》，一位少年踏上复仇之路，寻找真相，在生存还是毁灭中踱步，悲剧从开始一直贯穿到结尾，这或许是最好的命运归宿，复仇又显得一切多余，最后小说中的故事人物全都有了归属，我不觉得是悲剧，而这更像是命运的终结。 书中讲述的是丹麦王国的一个王子叫做哈姆雷特，知道了自己的叔父杀死了父亲谋权篡位，而后霸占…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"PNTxIT6O\"\u003e读书笔记\u003c/p\u003e\u003cp data-pid=\"E7atRkzA\"\u003e莎士比亚—《哈姆雷特》\u003c/p\u003e\u003cp data-pid=\"cWQmuvWF\"\u003e“一千个读者心中，就有一千个哈姆雷特。”就是这样我翻开了莎士比亚的《哈姆雷特》，一位少年踏上复仇之路，寻找真相，在生存还是毁灭中踱步，悲剧从开始一直贯穿到结尾，这或许是最好的命运归宿，复仇又显得一切多余，最后小说中的故事人物全都有了归属，我不觉得是悲剧，而这更像是命运的终结。\u003c/p\u003e\u003cp data-pid=\"_63DR9kw\"\u003e书中讲述的是丹麦王国的一个王子叫做哈姆雷特，知道了自己的叔父杀死了父亲谋权篡位，而后霸占了自己的母亲。\u003c/p\u003e\u003cp data-pid=\"d23AKl9x\"\u003e一个悲催的少年被乌云笼罩，一路上跌宕起伏找寻报仇机会，装疯卖傻在退缩中鼓起勇气，又在追逐中燃烧起熊熊怒火，最终完结复仇之路的故事。\u003c/p\u003e\u003cp data-pid=\"l9OI0dqE\"\u003e原本的哈姆雷特是一个养尊处优的太子，一人之下万人之上，从小到大就没有缺少过什么，他热爱旅行，衣食无忧，追他的女孩子一大把，他都不屑一顾。\u003c/p\u003e\u003cp data-pid=\"pnnkKz5f\"\u003e在父母的庇护下茁壮成长，为了以后能更好的胜任王位，父母安排出国留学，小伙来到了德国威登堡，每天哼着小曲哈啤酒恋地摊，日子过得有滋有味。\u003c/p\u003e\u003cp data-pid=\"qQnBspiV\"\u003e人生永远充满着不确定性，和谐美满的日子里来了一道晴天霹雳，他接到父亲暴毙的死讯，他火急火燎的赶回丹麦。\u003c/p\u003e\u003cp data-pid=\"_AGJQLBY\"\u003e从德国到丹麦我查了一下大约七百多公里，哈姆雷特连吃带喝，日夜兼程走了一个多月时间，回到城中，物是人非，这个生他长他的地方突然变得陌生，进入城里似乎总感觉有诧异的目光注视着他，等他环顾四周，一切又那么安静。\u003c/p\u003e\u003cp data-pid=\"xrOxph4f\"\u003e可怜的孩子也没能看父亲最后一眼。\u003c/p\u003e\u003cp data-pid=\"0wVYjDNg\"\u003e而此时丹麦也有了新的国王，那正是他的叔父克劳狄斯，更惊讶无语的是王后还是他的妈妈。\u003c/p\u003e\u003cp data-pid=\"5rBETXWN\"\u003e这一切来的太突然，他有点接受不了，头疼欲裂，那到底见了他叔父是叫叔还是叫父呢，一个尖锐的问题在他脑海浮现......\u003c/p\u003e\u003cp data-pid=\"vTMVEfy7\"\u003e此时懵逼的哈姆雷特，耳边响起筷子兄弟的《父亲》“多想和从前一样，牵你温暖手掌，可是你不在我身旁，托清风捎去安康......一生要强的爸爸，我能为你做些什么”。\u003c/p\u003e\u003cp data-pid=\"llsz3-UF\"\u003e于是又泪流满面！可是当下的哈姆不能过于煽情，一首BGM《无法抉择》又在耳边响起，跟随着音乐的哈姆雷特攥起了拳头......\u003c/p\u003e\u003cp data-pid=\"xDQ93RcL\"\u003e宫殿里正中央的金灿灿的王位上坐着一位中年男子，那就是哈姆雷特的叔父克劳狄斯，在旁边坐着一位女子，丰满带着韵味脸上看不出一丝悲伤。\u003c/p\u003e\u003cp data-pid=\"T2tOGf-W\"\u003e没错，这个女人就是他的母亲。\u003c/p\u003e\u003cp data-pid=\"K7g71a4r\"\u003e气愤的哈姆雷特先是行礼作揖，拜见叔父与母亲，他知道自己不能发脾气，忍辱负重是他当下最好的选择，但他又接受不了自己的懦弱。克劳狄斯雄壮浑厚的声音说道：\u003c/p\u003e\u003cp data-pid=\"qnyB19V6\"\u003e可怜的小哈你来晚了，你父亲走了。\u003c/p\u003e\u003cp data-pid=\"z_-Roan2\"\u003e强挤了几滴眼泪又说道：国不能一日无主，我们把你父亲安葬后，怎么都等不来你，他们就推举我来主持工作。\u003c/p\u003e\u003cp data-pid=\"MSb71szs\"\u003e你的母后也太过伤心，我只能牺牲自己来抚慰她的心灵，终于在我日以继夜的努力下，抚平了你母后的悲伤，我牺牲自己，换来了平静的日子。\u003c/p\u003e\u003cp data-pid=\"MWVAHrh0\"\u003e你不用感谢我，以后王位还是你的，我先帮你代理一下。\u003c/p\u003e\u003cp data-pid=\"niDjI6oz\"\u003e你不要着急，面包牛奶都是你的。\u003c/p\u003e\u003cp data-pid=\"-ipvMPG2\"\u003e哈姆雷特咬着牙，心里想问候他祖宗，奈何他们是一个祖宗。牙齿从咬紧到松动只用了三秒，谢谢叔父为我操心，哦不对，我应该叫您爹爹。\u003c/p\u003e\u003cp data-pid=\"JXmWgMfL\"\u003e一阵官方寒暄，哈姆雷特实在受不了跑进了后宫，此时他的母亲正在卸妆，梳妆台上摆着一对耳环，他认得这对耳环，那是他父亲之前在结婚纪念日送给他母亲的礼物。\u003c/p\u003e\u003cp data-pid=\"e3W47XmM\"\u003e他心里谩骂道：还算有良心，但是他根本接受不了这一切，恨不得又要爆出口。\u003c/p\u003e\u003cp data-pid=\"4NVV8Ss6\"\u003e哈姆雷特：你的日子过得不错吗，王后当的也不错，还是那么爱梳妆打扮，就是不知道你晚上会做恶梦吗？我父亲刚死一个月你就改嫁，你说你要脸吗？悲伤在你这一文不值，你告诉我，我父亲怎么死的？\u003c/p\u003e\u003cp data-pid=\"SwVfty_E\"\u003e孩子，不要这么说，你以为我不悲伤吗，我都是为了你们家啊，为了阻止动荡，为了让你以后继承王位，你难道看不出我的内心吗。哈姆雷特的母亲哭诉着说道。\u003c/p\u003e\u003cp data-pid=\"95erCtsN\"\u003e小哈，哈哈大笑：我呸，我不需要你的良苦用心，去球吧！\u003c/p\u003e\u003cp data-pid=\"8sXhTRFT\"\u003e晚上的丹麦星星很多，哈姆雷特要了两串羊肉串加一盘花毛一体，在路边小摊喝起了酒。此时的他感觉，自己是这个世界最不幸的人，而且他不想要这个王位，他只想回到之前的无忧无虑，他觉得自己抬不起头，他觉得满城风言风语，所有人欲言又止。\u003c/p\u003e\u003cp data-pid=\"lhvMkEgo\"\u003e他更不想给朋友诉说，几瓶啤酒喝光了，羊肉串一串没吃，天空下起了蒙蒙细雨，他想离开这里，离开这个现实。\u003c/p\u003e\u003cp data-pid=\"QcMpD1H3\"\u003e他的一举一动都被一双眼睛注视着，他的叔父根本放心不下他，一直在派人监视他，而哈姆雷特整日喝的五迷三道，想用酒麻醉自己。他知道自己虽然还是王位继承人，但这一切都像是泡沫，如露亦如电，一切都是虚的。\u003c/p\u003e\u003cp data-pid=\"Xmem6zu9\"\u003e一次醉酒后哈姆的好友霍拉旭告诉他，兄弟我同情你，我不得不给你说真心话，你父亲暴毙而亡不是被毒蛇咬死的，是被人下毒致死的。你要问我怎么知道的，我前两天夜里值班见到一位穿着盔甲在城里来回游荡的鬼魂，他告诉我，正义永远不会迟到！还说了老国王的死因。\u003c/p\u003e\u003cp data-pid=\"aM5IwvnN\"\u003e此时的哈姆雷特如被大雨浇醒一样，满脸布满愤恨与狰狞：\u003c/p\u003e\u003cp data-pid=\"MACjjuKe\"\u003e爹啊，孩儿不孝，我来晚了，我会给你报仇的！\u003c/p\u003e\u003cp data-pid=\"X6q8ZHIU\"\u003e或许小哈的发愿感召了鬼魂，当夜老国王的鬼魂就来找哈姆雷特：\u003c/p\u003e\u003cp data-pid=\"MNqJtOWX\"\u003e好久不见小哈，城里人都传我是在花园让毒蛇咬了一口，暴毙而亡，其实是你那人面兽心的叔父在我熟睡的时候，给我耳朵里灌进了毒药，没错比鹤顶红还要毒的药，我只坚持了一天就一命呜呼了。\u003c/p\u003e\u003cp data-pid=\"FhymEkWI\"\u003e你的母亲平日里爱我，在我死后不到一个月便和克劳狄斯这个混蛋上了床，寂寞在忠贞面前一文不值。\u003c/p\u003e\u003cp data-pid=\"dhJK55h5\"\u003e再加上那巧言令色的狗东西，迷惑了尼玛！我冤枉啊，我迟迟不愿去鬼门关，就是一直在等你，你要为我报仇！杀了克劳狄斯，将他千刀万剐，至于你的母亲，看在他还爱我的份上，就打入冷宫吧！\u003c/p\u003e\u003cp data-pid=\"Lyx4-nqG\"\u003e一阵风吹拂而过，鬼魂不见了，哈姆雷特半醒半睡，他呆坐在床上，毫无头绪。大脑暗示他，世界怎么会有鬼魂呢，听鬼魂的话，未免太离谱，毕竟自己是新时代青年。\u003c/p\u003e\u003cp data-pid=\"wVHIArRM\"\u003e可是又想一下，这一切肯定和克劳狄斯脱不了干系，我又该怎么办呢，整个城里城外全是他的人，我太难了，他一头又扎进了被子里。醒来的哈姆雷特抽插着宝剑，辗转反侧，一会想冲上大殿杀了这个狗贼，一会又想去买砒霜，一会又想人生苦苦，让他们随风去吧。\u003c/p\u003e\u003cp data-pid=\"W5pH2rfl\"\u003e对于一个人心理建设防备没有完善时，最好的办法就是逃避，打不过只能逃，哈姆雷特选择了装疯，有点像咱们当时战国时期的孙膑，只不过我觉得他装疯卖傻的境界还不够高。\u003c/p\u003e\u003cp data-pid=\"-lndleyN\"\u003e克劳狄斯知道哈姆雷特疯了，简直高兴极了，但又心生怀疑，大侄子是不是装的呢，不行我必须要找人试探一下。于是他先是安排哈姆雷特的发小罗森与吉尔前去试探，这俩伙计二话没说就抛弃了友情。\u003c/p\u003e\u003cp data-pid=\"XkVme1ex\"\u003e从小光着屁股和泥的发小也没能抵挡住权力欲望的诱惑。\u003c/p\u003e\u003cp data-pid=\"j7JuoCSQ\"\u003e俩人到了哈姆的房间，上来就想先套个近乎，结果哈姆雷特穿着大裤衩蓬头垢面的说了起来，一口气说了十分钟还没说完，喋喋不休的叙述同时也是对命运的阐述。\u003c/p\u003e\u003cp data-pid=\"LPSl0s-r\"\u003e这哥俩一看哈姆给变个人似的，的确疯疯癫癫。此时的哈姆雷特除了喋喋不休的话语更多的还是愤恨，这两个小伙伴是拜把子结拜兄弟，居然受新国王指使来试探我，真是世态炎凉！\u003c/p\u003e\u003cp data-pid=\"img1DxwU\"\u003e新国王还是不放心于是又安排了一个美貌的女子去试探，这个女子是御前大臣波洛涅斯的女儿，曾经与哈姆谈过恋爱，关系也是不一般。起先老国王在位时，波洛涅斯就催着自己的女儿奥菲利亚，去引诱哈姆雷特，现在新国王上位了，又立刻站出来要奥菲利亚与哈姆雷特断绝关系，并且主动请缨，要女儿去哈姆雷特那里试探。\u003c/p\u003e\u003cp data-pid=\"rZ1eihBY\"\u003e被迫无奈，奥菲利亚来到了哈姆的宫中，那是她俩心生情愫的地方，或坐或立或亲或吻，一下浮现在她的脑海。\u003c/p\u003e\u003cp data-pid=\"hXmHaqJB\"\u003e哈姆雷特一直爱着奥菲利亚，留学期间没事就飞鸽传书，天天你情我浓。哈姆来到城里还未曾见过奥菲利亚。第一眼看见奥菲利亚，哈姆雷特差点叫出亲爱的，然而她为什么这个时候出现？\u003c/p\u003e\u003cp data-pid=\"DZaVKmx0\"\u003e她也是来试探我的，也是新王的走狗吗？\u003c/p\u003e\u003cp data-pid=\"1CtMKfJx\"\u003e其实奥菲利亚来，只是想弄清哈姆是否还爱她，而我们的王子殿下却继续装疯，装作不认识，也没给予过什么，并且探讨了美丽带给贞洁的灾难，最后还告诉奥菲利亚，从来没爱我她，奥菲利亚哭泣着跑了出来。\u003c/p\u003e\u003cp data-pid=\"Z9mlzRf5\"\u003e哈姆雷特内心清楚这个女孩是爱他的，但他知道奥菲利亚的父亲是那个墙头草，他要一并铲除，现在不能暴漏理智。\u003c/p\u003e\u003cp data-pid=\"a4FAMjvW\"\u003e奥菲利亚的哥哥与老爹把这段一五一十的讲给了新国王克劳狄斯听，克劳狄斯心里还是半信半疑，半夜睡觉都在寻思这件事情。\u003c/p\u003e\u003cp data-pid=\"OMsiZNdi\"\u003e另一边哈姆雷特也展开了复仇计划，他知道新王爱看戏，于是找人编排了一场话剧，内容正是弟弟谋权篡位杀死哥哥霸占嫂子的故事情节。\u003c/p\u003e\u003cp data-pid=\"s2WioTrS\"\u003e此时看演出的克劳狄斯，脸色大变吓得一激灵，暗中观察得哈姆更加确信了克劳狄斯就是那个杀害父亲的凶手。\u003c/p\u003e\u003cp data-pid=\"4qrjSqgf\"\u003e俗话说：不做亏心事不怕鬼敲门，之后新国王克劳狄斯一直惶恐不安，半夜跑到了海边，跪在那里忏悔，哈姆雷特暗中跟随着，看着一切，他掏出了匕首，原本可以从背后来上一刀，可他又把匕首收了起来，他犹豫不决，他内心得另一个声音出现了，不能这么轻易杀了他，不是时候，我们要将他千刀万剐。\u003c/p\u003e\u003cp data-pid=\"o2XwmIJ9\"\u003e这内心得想法其实带着一丝退却与恐惧，就这样哈姆雷特错失了杀掉叔父克劳狄斯得最好机会。那可悲的勇气偷偷溜走了。\u003c/p\u003e\u003cp data-pid=\"Q308pa9X\"\u003e复仇不是那么容易的，是生存还是毁灭，这真是个问题。错过了机会，哈姆雷特又开始懊恼，情绪更加低落，晚上难以入睡，他跑进了他母亲的房间，并大声呵斥，把老国王鬼魂的事情告诉了他母亲。\u003c/p\u003e\u003cp data-pid=\"mgBwfGha\"\u003e这位母亲还不相信，应该是进入了恋爱脑还没清醒。\u003c/p\u003e\u003cp data-pid=\"LrncxFO8\"\u003e躲在房间窗外偷听的御前大臣波洛涅斯，暗中高兴，终于知道了秘密，要去给新国王报信，刚要走，一把白色冰凉的金属插进了胸腔，没错哈姆雷特发现了他。一剑解决了他的小命。\u003c/p\u003e\u003cp data-pid=\"z38pfRCo\"\u003e哈姆雷特舔了舔手上的血，继续对母亲说，人是要有良心的，做恶事是要下地狱的，黄泉之下你有脸见我父亲吗？\u003c/p\u003e\u003cp data-pid=\"2PvHTrW-\"\u003e最后母亲痛哭流泪，不要再说恶哈姆。\u003c/p\u003e\u003cp data-pid=\"3vldYTFp\"\u003e此时鬼魂再次出现了，告诉哈姆，你搞啥哪，我叫你复仇，没叫你惩罚你母亲，她是爱我的，只是她想给你一个完整的家，仅此而已。\u003c/p\u003e\u003cp data-pid=\"mci26SMP\"\u003e我在不出现你的复仇之火就熄灭了。你还天天不知道正事，人五人六的，我知道你在隔靴挠痒，逃避问题。这边哈姆雷特的母亲，内心开始忏悔，恍惚回到了与老国王爱恋的日子，思念的眼泪伴随着懊悔。\u003c/p\u003e\u003cp data-pid=\"y93OnZbQ\"\u003e她没有去揭发哈姆，她知道这一切早晚都要结束。至于波洛涅斯的死，哈姆的母亲给新国王说，完全是误伤，交通意外，又伪造了意外现场。\u003c/p\u003e\u003cp data-pid=\"D_3oeCgZ\"\u003e新国王姑且相信了王后，但心里放不下，这也不是办法，最后写信要把哈姆雷特送到英国，到时找机会做掉更容易。\u003c/p\u003e\u003cp data-pid=\"DVkY0Euy\"\u003e哈姆雷特被迫离开了丹麦。\u003c/p\u003e\u003cp data-pid=\"kQwFxb07\"\u003e柔弱的奥菲利亚，接受不了双重打击，一边被情郎抛弃，一边父亲死亡，她 彻底真疯了，跌跌撞撞跳进了大海，香薰陨落。\u003c/p\u003e\u003cp data-pid=\"_3NnguL6\"\u003e哈姆雷特去往英国的路上，越想越不对劲，于是抽上厕所的空就逃跑了，回到丹麦，才发现，自己心爱的奥菲利亚已经死了。\u003c/p\u003e\u003cp data-pid=\"JX6zVbWd\"\u003e哈姆雷特痛苦万分，一把鼻涕一把泪，差点酷昏厥过去，为了复仇他抛弃了爱情，弄丢了心爱的人。\u003c/p\u003e\u003cp data-pid=\"38ORCWff\"\u003e还没在痛苦中走出来，奥菲利亚的哥哥雷欧提斯就找上门来，要报杀父抛妹之仇，这小雷脾气也是火爆，当即提出比武决斗，签生死状那种！新国王克劳狄斯早就知道了这一切，正好将计就计，于是从中间挑唆。\u003c/p\u003e\u003cp data-pid=\"HIoxSDJZ\"\u003e雷欧提斯想致哈姆雷特于死地，找印度三哥买了毒药，涂抹在剑上，只要破皮就一命呜呼。新王克劳狄斯说：\u003c/p\u003e\u003cp data-pid=\"rJDxnO-D\"\u003e你要是杀不死他怎么办，我安排个B计划，到时我观看比武，在桌子提前备上功能性饮料，里面加了鹤顶红，他喝完就会一命呜呼。\u003c/p\u003e\u003cp data-pid=\"tPvbdKLC\"\u003e计划一切照旧，决斗开始了，新王得意洋洋，就等着给哈姆雷特收尸了，看的挺入神，而哈姆的母亲心脏快蹦出来，她比擂台里的人还紧张，口干舌燥嘴也馋，于是拿起了饮料一口干了，下一秒就倒在了地上。\u003c/p\u003e\u003cp data-pid=\"MUzmny7g\"\u003e这边擂台上两人不分伯仲，刀剑无眼，两人你一刀我一剑，最后小雷的毒剑不仅刺伤了哈姆，还误伤了自己。\u003c/p\u003e\u003cp data-pid=\"Js4JX71o\"\u003e两人瞬间没了力气，口吐鲜血，此时哈姆看向母亲，发现母亲也倒在地上。\u003c/p\u003e\u003cp data-pid=\"1Ju-Ac5F\"\u003e哈姆雷特发出血腥的味道高呼道：\u003c/p\u003e\u003cp data-pid=\"NJhM_lvi\"\u003e你把她怎么了，你这个混蛋克劳狄斯。\u003c/p\u003e\u003cp data-pid=\"HKGeC8Ui\"\u003e克劳狄斯说：不要大惊小怪，你的母亲只是看见血晕倒了。\u003c/p\u003e\u003cp data-pid=\"VW4yR2Zh\"\u003e旁边的小雷奄奄一息，人之将死其言也善，他说道：\u003c/p\u003e\u003cp data-pid=\"VbOHy0Ii\"\u003e是克劳狄斯在功能饮料中下的毒，他要杀害你的。\u003c/p\u003e\u003cp data-pid=\"ZC-pipx2\"\u003e哈姆雷特凭借着最后一口力气，跳起来冲到了看台，大呼道，卧槽你个香蕉巴拉，一剑封喉，克劳狄斯死在了看台上，就这样故事结束了。复仇的怒火终结了，把权力把欲望，把所有的人和事一同带走了。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-efcfdfe8b01a05dd597fb2298903b79f_1440w.jpg\" data-rawwidth=\"800\" data-rawheight=\"1067\" data-size=\"normal\" data-original-token=\"v2-dbb9fca3933b3c4359a25da16f6f4a92\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic4.zhimg.com/v2-efcfdfe8b01a05dd597fb2298903b79f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"AXt2LtTK\"\u003e关于作者\u003c/p\u003e\u003cp data-pid=\"puhuiRVq\"\u003e威廉·莎士比亚，1564年生于英国沃里克郡斯特拉福镇，是英国文艺复兴时期剧作家、诗人、戏剧家，被誉为“人类文学奥林匹斯山上的宙斯”。歌德曾评价：莎士比亚的舞台是一个美丽的百象镜，在镜箱里世界的历史挂在一根看不见的时间的线索上从我们眼前掠过。维克多·雨果曾说：莎士比亚这种天才的降临，使得艺术、科学、哲学或者整个社会焕然一新。他的光辉照耀着全人类，从时代的这一个尽头到那一个尽头。莎士比亚代表作有，《哈姆雷特》、《奥赛罗》、《李尔王》、《麦克白》、《仲夏之夜》、《威尼斯商人》等。\u003c/p\u003e\u003cp data-pid=\"J91-qKK2\"\u003e《哈姆雷特》和《奥赛罗》、《李尔王》、《麦克白》并称为莎士比亚四大悲剧。\u003c/p\u003e\u003cp data-pid=\"gcjl2ccM\"\u003e莎士比亚作为文学巨匠，其作品不仅在当时引起轰动，更跨越时空界限，深远影响了后世。尼采曾强调感受而非理解其作品的重要性，而莎士比亚通过创新题材、革新艺术形式、塑造典型人物和丰富语言，为文学界树立了典范。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-05bb69808cbdbc09b7d29450665e8062_1440w.jpg\" data-rawwidth=\"1750\" data-rawheight=\"2048\" data-size=\"normal\" data-original-token=\"v2-1fdfcc2863ca01f14ceda64b4a4464ff\" class=\"origin_image zh-lightbox-thumb\" width=\"1750\" data-original=\"https://pic1.zhimg.com/v2-05bb69808cbdbc09b7d29450665e8062_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"pS5Q7A1N\"\u003e最后的金句\u003c/p\u003e\u003cp data-pid=\"BAknA93p\"\u003e当悲伤来临的时候，不是单个来的，而是成群结队的。\u003c/p\u003e\u003cp data-pid=\"_aVblh-s\"\u003e我们常装出信仰的表情和虔诚的举动，却用糖衣来包裹恶魔的本性。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"5cb0w7vo\"\u003e注定在今天，就不会是明天；不是明天，就是今天；逃过了今天，明天还是逃不了，随时准备着就是了。\u003c/p\u003e\u003cp data-pid=\"OrXMV4im\"\u003e你可以疑心星星是火把；你可以疑心太阳会移转；你可以疑心真理是谎话；可是我的爱永没有改变。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"ks-yLGwm\"\u003e生存还是毁灭，这是一个值得考虑的问题。默然忍受命运的暴虐的毒箭，或是挺身反抗人世的无涯的苦难，通过斗争把它们扫清，这两种行为，哪一种更高贵？死了，睡着了，什么都完了。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-23fc223ab16ebbf6732b6b24df3d83ae_1440w.jpg\" data-rawwidth=\"1080\" data-rawheight=\"1080\" data-size=\"normal\" data-original-token=\"v2-a3108dfd2d9904a7d01e8d28dbce19a7\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-23fc223ab16ebbf6732b6b24df3d83ae_r.jpg\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":24,"thumbnails":["https://pica.zhimg.com/50/v2-13d7625f999453d17ec8d1024df21124_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-795fa35953ce73b5b91cdb95350ffa10_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-594c8a28d07f77cfde6533316c621996_720w.jpg?source=b6762063"],"favorite_count":0,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1933537874306921961}","attached_info":"CroGCOTe9sWEv8/zngEQBxoJMjYwOTY5Mjc0IOXfocQGKAAwAECCAUokChlUU19TT1VSQ0VfV0FSTV9VUF9OT1JNQUwyEgEwGAAgADoAYiBiMzdjMjQxYjE4YTAxNDk0MmE5NDJiMzVmZmQyMjA1OXITMTkzMzUzNzg3NDMwNjkyMTk2MaoBCXJlY29tbWVuZMIBIGEwMzZlN2M3M2UyYzZiNDk3ZDkxNTgyMmJiMzFjYzcy8gEKCAwSBk5vcm1hbPIBKAgKEiQ0NjM4M2I1MC1jZjMwLTRmZWUtODg5ZS02Mzg4N2I5NDZmMGbyAQYICxICMjKCAgCIAtfL186FM5ICIGEwMzZlN2M3M2UyYzZiNDk3ZDkxNTgyMmJiMzFjYzcymgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCGENvbnRlbnRXYXJtVXBCcmVha0luUnVsZdoCGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDLoAgP6AgtOT1JNQUxfRkxPV4oDIDE5Njg5YzQ0NzA5MTQxMjlhOGU3YjI2Yzg4Yjg3YThimgMNCgJ2MhAAGgVvdGhlcqgDGNgDAOoDH3RleHRfMTJob3VyX3VuaWZpbnNoZWRfcmVjYWxsZXL6A6wBEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAIQoAYYqwgiI3YyLWRiYjlmY2EzOTMzYjNjNDM1OWEyNWRhMTZmNmY0YTkyOi0IAhDWDRiAECIjdjItMWZkZmNjMjg2M2NhMDFmMTRjZWRhNjRiNGE0NDY0ZmY6LQgEELgIGLgIIiN2Mi1hMzEwOGRmZDJkOTkwNGE3ZDAxZThkMjhkYmNlMTlhN4AEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAYEMlij+BBQAAAAAAAAAAiQU03sVIaXPTP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUWkAYAoAaCAagGAZICLgoJMjYwOTY5Mjc0EhMxOTMzNTM3ODc0MzA2OTIxOTYxGAciCklNQUdFX1RFWFQ=","action_card":false},{"id":"131_1753853847.404","type":"feed","offset":131,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1753853847,"updated_time":1753853847,"target":{"id":"719621121","type":"article","url":"https://api.zhihu.com/articles/719621121","author":{"id":"70985431dfff855991bccba876784906","url":"https://api.zhihu.com/people/70985431dfff855991bccba876784906","user_type":"people","url_token":"muxiaoxiong","name":"牧小熊","headline":"心中有理想，脚下才有力量","avatar_url":"https://pic1.zhimg.com/50/v2-7f11606c25e712301b99ffc6a9bd084b_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":4632,"is_following":false,"is_followed":false},"title":"风控算法的一些面试题小结","comment_permission":"all","created":1726407130,"updated":1726409083,"voteup_count":242,"voting":0,"comment_count":5,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"最近没什么事，总结了机器学习风控算法的一些面试点，这里的风控主要是说的金融风控1.逻辑回归的优缺点，局限性在哪？优点： 实现简单，速度快，占用内存小，可在短时间内迭代多个版本的模型。模型的可解释性非常好，可以直接看到各个特征对模型结果的影响，可解释性在金融领域非常重要，所以在目前业界大部分使用的仍是逻辑回归模型。模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，鲁棒性更强。特征工程做得好，模…","excerpt_new":"最近没什么事，总结了机器学习风控算法的一些面试点，这里的风控主要是说的金融风控1.逻辑回归的优缺点，局限性在哪？优点： 实现简单，速度快，占用内存小，可在短时间内迭代多个版本的模型。模型的可解释性非常好，可以直接看到各个特征对模型结果的影响，可解释性在金融领域非常重要，所以在目前业界大部分使用的仍是逻辑回归模型。模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，鲁棒性更强。特征工程做得好，模…","preview_type":"default","preview_text":"","content":"\u003cblockquote data-pid=\"rchjLb4w\"\u003e最近没什么事，总结了机器学习风控算法的一些面试点，这里的风控主要是说的金融风控\u003c/blockquote\u003e\u003ch2\u003e1.\u003cb\u003e逻辑回归的优缺点，局限性在哪？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"oRzyXf-_\"\u003e优点：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"URw_hGwK\"\u003e实现简单，速度快，占用内存小，可在短时间内迭代多个版本的模型。\u003c/li\u003e\u003cli data-pid=\"c9B8fJ92\"\u003e模型的可解释性非常好，可以直接看到各个特征对模型结果的影响，可解释性在金融领域非常重要，所以在目前业界大部分使用的仍是逻辑回归模型。\u003c/li\u003e\u003cli data-pid=\"-OYjL5Ml\"\u003e模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，鲁棒性更强。\u003c/li\u003e\u003cli data-pid=\"Wk1-GJMz\"\u003e特征工程做得好，模型的效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。\u003c/li\u003e\u003cli data-pid=\"oKEcS0vQ\"\u003e模型的结果可以很方便的转化为策略规则，且线上部署简单。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"1-ru6sI_\"\u003e2）缺点和局限性:\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"WhY4tXeo\"\u003e容易欠拟合，相比集成模型，准确度不是很高。\u003c/li\u003e\u003cli data-pid=\"Xns6Ym0K\"\u003e对数据的要求比较高，逻辑回归对缺失值，异常值，共线性都比较敏感，且不能直接处理非线性的特征。所以在数据清洗和特征工程上会花去很大部分的时间。\u003c/li\u003e\u003cli data-pid=\"CkcE3dle\"\u003e在金融领域对场景的适应能力有局限性，例如数据不平衡问题，高维特征，大量多类特征，逻辑回归在这方面不如决策树适应能力强。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"PZPMZTyX\"\u003e总结而言 就是 模型简单、解释性强、方便部署、对负责数据处理能力有限。\u003c/p\u003e\u003ch2\u003e2.逻辑回归输出值是0到1之间的值，这个值是真实的概率吗？\u003c/h2\u003e\u003cp data-pid=\"l-17g__J\"\u003e   逻辑回归输出的值在0到1之间，并非所有在0到1之间的值都可以表示概率。这个0到1之间的值是通过sigmoid函数将线性回归的结果映射到0到1之间，我们也可以采用其他函数将数值映射到非0到1之间，因此，用sigmoid函数得到的结果并不是绝对的真实概率值，但是可以认为他是一个接近真实概率的值。\u003c/p\u003e\u003ch2\u003e3.逻辑回归做分类时样本应满足什么条件\u003c/h2\u003e\u003cp data-pid=\"fhkWE2hV\"\u003e逻辑回归做分类时样本应满足的条件包括因变量为分类变量、观测间独立性、样本量要求、线性关系、无多重共线性、无明显的离群点等。\u003c/p\u003e\u003ch2\u003e 4.\u003cb\u003e逻辑回归解决过拟合的方法有哪些？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"HtHKaUZc\"\u003e减少特征数量，在实际使用中会用很多方法进行特征筛选，例如基于IV值的大小，变量的稳定性，变量之间的相关性等。\u003c/li\u003e\u003cli data-pid=\"DL2oDa5V\"\u003e正则化，常用的有L1正则化和L2正则化。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e5.\u003cb\u003e什么是特征的离散化和特征交叉？逻辑回归为什么要对特征进行离散化？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"dVPq8sRv\"\u003e特征离散化是将数值型特征（一般是连续型的）转变为离散特征，例如评分卡中的woe转化，就是将特征进行分箱，再将每个分箱映射到woe值上，就转换为了离散特征。特征交叉也叫作特征组合，是将单独的特征进行组合，使用相乘/相除/笛卡尔积等形成合成特征，有助于表示非线性关系。比如使用One-Hot向量的方式进行特征交叉。这种方式一般适用于离散的情况，我们可以把它看做基于业务理解的逻辑和操作，例如经度和纬度的交叉，年龄和性别的交叉等。\u003c/li\u003e\u003cli data-pid=\"Jz5e4FNd\"\u003e实际工作中很少直接将连续型变量带入逻辑回归模型中，而是将特征进行离散化后再加入模型，例如评分卡的分箱和woe转化。这样做的优势有以下几个：\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"UZhURV9k\"\u003e1）特征离散化之后，起到了简化模型的作用，使模型变得更稳定，降低了模型过拟合的风险。\u003c/li\u003e\u003cli data-pid=\"ebLCvzXb\"\u003e2）离散化之后的特征对异常数据有很强的鲁棒性，实际工作中的哪些很难解释的异常数据一般不会做删除处理，如果特征不做离散化，这个异常数据带入模型，会给模型带来很大的干扰。\u003c/li\u003e\u003cli data-pid=\"t539Ih_x\"\u003e3）离散特征的增加和减少都很容易，且稀疏向量的内积乘法运算速度快，易于模型的快速迭代。\u003c/li\u003e\u003cli data-pid=\"WrspU2cm\"\u003e4）逻辑回归属于广义线性模型，表达能力有限，特征离散化之后，每个离散变量都有单独的权重，相当于给模型引入了非线性，能够提高模型的表达能力。\u003c/li\u003e\u003cli data-pid=\"rtKNwqRE\"\u003e5）离散化后的特征可进行特征交叉，进一步引入非线性，提高模型的表达能力。\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp data-pid=\"_4U1Mxm4\"\u003e总而言之，特征离散化就是将数值型特征转为离散型特征，便于逻辑回归进行建模，特征交叉就是提高特征的非线性关系。\u003c/p\u003e\u003cp data-pid=\"vFnRGWdO\"\u003e\u003cb\u003e6.做评分卡中为什么要进行WOE化？\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"wPy1wKdD\"\u003e更好的解释性，变量离散化之后可将每个箱体映射到woe值，而不是通常做one-hot转换。\u003c/li\u003e\u003cli data-pid=\"ko2ZaZYn\"\u003ewoe化之后可以计算每个变量的IV值，可用来筛选变量。\u003c/li\u003e\u003cli data-pid=\"b9LFbFbI\"\u003e对离散型变量，woe可以观察各个level间的跳转对odds的提升是否呈线性。\u003c/li\u003e\u003cli data-pid=\"x9pBDfSs\"\u003e对连续型变量，woe和IV值为分箱的合理性提供了一定的依据，也可分析变量在业务上的可解释性。\u003c/li\u003e\u003cli data-pid=\"Ngsdlnqn\"\u003e用woe编码可以处理缺失值问题。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e7.\u003cb\u003e逻辑回归的特征系数的绝对值可以认为是特征的重要性吗？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"vnrmJpFz\"\u003e首先特征系数的绝对值越大，对分类效果的影响越显著，但不能表示系数更大的特征重要性更高。因为改变变量的尺度就会改变系数的绝对值，而且如果特征是线性相关的，则系数可以从一个特征转移到另一个特征，特征间相关性越高，用系数解释变量的重要性就越不可靠。\u003c/p\u003e\u003ch2\u003e8.逻辑回归为什么要用极大似然函数作为损失函数？\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-92b4c97fbc3cf76a9b14d839ca4a247f_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"831\" data-rawheight=\"222\" data-original-token=\"v2-92b4c97fbc3cf76a9b14d839ca4a247f\" class=\"origin_image zh-lightbox-thumb\" width=\"831\" data-original=\"https://picx.zhimg.com/v2-92b4c97fbc3cf76a9b14d839ca4a247f_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e9.\u003cb\u003e决策树模型的优缺点及适用性？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"GCqKagpu\"\u003e优点\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"dZEu8Idn\"\u003e直观易理解：决策树的结构类似于人类的决策过程，因此它的机制解释起来简单，即使是非专业人士也能理解其工作原理。\u003c/li\u003e\u003cli data-pid=\"xnfXyxbg\"\u003e可解释性强：决策树可以生成清晰的规则，这些规则可以直接用于业务决策或者进一步的分析。\u003c/li\u003e\u003cli data-pid=\"d-620MFB\"\u003e数据处理能力：决策树能够处理标称型和数值型数据，同时也能处理有缺失属性的样本以及不相关的特征。\u003c/li\u003e\u003cli data-pid=\"_TN20n2q\"\u003e运行速度快：在测试数据集时，决策树的运行速度比较快，这使得它在实际应用中非常高效。\u003c/li\u003e\u003cli data-pid=\"SGkjQfya\"\u003e适应性强：决策树算法适用于小数据集，并且时间复杂度较小，适合快速处理大量数据。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"GOLf_lWq\"\u003e缺点\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"X093CIBy\"\u003e容易过拟合：决策树模型容易对训练数据过度学习，导致泛化性能差，即在新数据上的表现可能不佳。\u003c/li\u003e\u003cli data-pid=\"EcAt2pyL\"\u003e稳定性问题：决策树对于数据的微小变化可能非常敏感，这可能导致模型的不稳定。\u003c/li\u003e\u003cli data-pid=\"sddJCBHg\"\u003e复杂性控制：随着数据量的增加，决策树可能会变得过于复杂，难以管理和解释。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e10.\u003cb\u003e简述一下决策树的原理以及树的构建过程。\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"PkAuBd9D\"\u003e决策树时基于树的结构进行决策的，学习过程包括特征选择，决策树的生成和剪枝过程。决策树的学习过程通常是递归地选择最优特征，并用最优特征对数据集进行分割。决策树通过一系列的分裂和判断条件来预测目标变量的值，其构建过程是一个递归地将数据集划分为越来越小的子集的过程，直到满足停止条件。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e11.简述一下ID3，C4.5，CART三类决策树的原理和异同点。\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"PYQThXPc\"\u003eID3选择最佳分割点是基于信息增益的，信息增益越大，表明使用这个属性来划分所获得的“纯度提升”越大。C4.5对ID3进行了改进，因为ID3使用的信息增益对数据划分时，可能出现每个结点只包含一个样本，这些子节点的纯度已经达到最大，但是，这样的决策树并不具有泛化能力，无法对新样本进行预测。且ID3不能处理连续型变量和缺失值。而C4.5使用信息增益率来选择属性，克服了信息增益选择属性时偏向选择值多的属性的不足。且可以处理连续型变量和缺失值。\u003c/li\u003e\u003cli data-pid=\"TUYCyPPj\"\u003eC4.5是基于ID3的改进版，只能用于分类。而CART树既可以做分类，也可以做回归。CART的本质是对特征空间进行二元划分，所以CART生成的是一颗二叉树，且可以对类别型变量和数值型变量进行分裂。对分类型变量进行划分时，分为等于该属性和不等于该属性，在对连续型变量进行划分时，分为大于和小于，在做分类是使用的是GINI系数作为划分标准，在做回归时使用的是均方误差。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e12.\u003cb\u003e决策树对缺失值是如何处理的？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"p2O-rwUD\"\u003e决策树处理缺失要考虑以下三个问题：\u003c/p\u003e\u003cp data-pid=\"0R1LviJ7\"\u003e当开始选择哪个属性来划分数据集时，样本在某几个属性上有缺失怎么处理：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"ZhXNMipt\"\u003e忽略这些缺失的样本。\u003c/li\u003e\u003cli data-pid=\"3qqa-sPg\"\u003e填充缺失值，例如给属性A填充一个均值或者用其他方法将缺失值补全。\u003c/li\u003e\u003cli data-pid=\"uaZMtmA7\"\u003e计算信息增益率时根据缺失率的大小对信息增益率进行打折，例如计算属性A的信息增益率，若属性A的缺失率为0.9，则将信息增益率乘以0.9作为最终的信息增益率。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"CarVzqbo\"\u003e一个属性已经被选择，那么在决定分割点时，有些样本在这个属性上有缺失怎么处理？\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"5W3L7KES\"\u003e忽略这些缺失的样本。\u003c/li\u003e\u003cli data-pid=\"7mr5GX1m\"\u003e填充缺失值，例如填充一个均值或者用其他方法将缺失值补全。\u003c/li\u003e\u003cli data-pid=\"uRKXUrKq\"\u003e把缺失的样本，按照无缺失的样本被划分的子集样本个数的相对比率，分配到各个子集上去，至于那些缺失样本分到子集1，哪些样本分配到子集2，这个没有一定准则，可以随机而动。\u003c/li\u003e\u003cli data-pid=\"OIghBU3j\"\u003e把缺失的样本分配给所有的子集，也就是每个子集都有缺失的样本。\u003c/li\u003e\u003cli data-pid=\"_-weAzcZ\"\u003e单独将缺失的样本归为一个分支。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"HTFfzs8u\"\u003e决策树模型构建好后，测试集上的某些属性是缺失的，这些属性该怎么处理？\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"u3M9VL2m\"\u003e如果有单独的缺失值分支，依据此分支。\u003c/li\u003e\u003cli data-pid=\"HCwQFLPV\"\u003e把待分类的样本的属性A分配一个最常出现的值，然后进行分支预测。\u003c/li\u003e\u003cli data-pid=\"HzVNR8F-\"\u003e待分类的样本在到达属性A结点时就终止分类，然后根据此时A结点所覆盖的叶子节点类别状况为其分配一个发生概率最高的类。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"pFAfh02_\"\u003e总结而言就是 删除、填充、分配权重、专门分支、概率最大类别\u003c/p\u003e\u003ch2\u003e\u003cb\u003e13.为什么决策树不需要对数据做归一化等预处理？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"m6acgR1m\"\u003e决策树是一种概率模型，所以不需要做归一化，因为它不关心变量的值，而是关心变量的分布和变量之间的条件概率，所以归一化这种数值缩放，不影响分裂结点位置。\u003c/p\u003e\u003ch2\u003e14.\u003cb\u003e如何解决决策树的过拟合问题？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"JO9b-Ooz\"\u003e剪枝技术\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"ENM44f8B\"\u003e预剪枝：预剪枝是在决策树完全生成之前就停止树的构建过程。这种方法可以通过设定树的最大深度、节点包含的最小样本数等规则来实现。预剪枝的优点是简单易行，但缺点是很难精确控制何时停止增长，可能会导致欠拟合。\u003c/li\u003e\u003cli data-pid=\"6P7opVbl\"\u003e后剪枝：后剪枝是先让决策树完整生成，然后根据一定的标准（如验证集的误差）来剪掉某些子树。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"o_WHsgKI\"\u003e限制树的深度\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"-fzRnPLV\"\u003e设定最大深度：通过设置决策树的最大深度，可以限制树的复杂度，从而减少过拟合的风险。这种方法简单直观，但需要合理选择深度参数。\u003c/li\u003e\u003cli data-pid=\"IPgX-9Ml\"\u003e设定最小叶节点样本数：当某个节点的样本数小于预设的阈值时，停止该节点的进一步分裂。这可以防止树在只有少量样本的情况下继续生长，从而避免过拟合。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"5YGogte6\"\u003e特征选择\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"0CjNkEVV\"\u003e去除无关特征：只保留对目标变量有显著影响的特征，可以减少模型的复杂度，从而降低过拟合的风险。\u003c/li\u003e\u003cli data-pid=\"2YuBKvJk\"\u003e特征工程：通过对原始数据进行转换和组合，创造出更有信息量的特征。良好的特征工程可以提高模型的性能，同时减少过拟合的可能性。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e15.\u003cb\u003e什么是集成学习？集成学习有哪些框架？简单介绍各个框架的常用算法。\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"laWQlI3m\"\u003e集成学习是一种优化手段和策略，通常是结合多个简单的弱分类器来集成模型组，去做更可靠的决策。一般的弱分类器可以是决策树，SVM，kNN等构成，其中的模型可以单独来训练，并且这些弱分类器以某种方式结合在一起去做出一个总体预测。集成学习就是找出哪些弱分类器可以结合在一起，以及如何结合的方法。目前集成学习主要有bagging，boosting，stacking三种：\u003c/li\u003e\u003cli data-pid=\"pXtbQvhk\"\u003ebagging：对训练集进行随机子抽样，对每个子训练集构建基模型，对所有的基模型的预测结果进行综合产生最后的预测结果。如果是分类算法，则用多数投票法确定最终类别，如果是回归算法，则将各个回归结果做算术平均作为最终的预测值。常用的bagging算法：随机森林\u003c/li\u003e\u003cli data-pid=\"xJDZXgqs\"\u003eboosting：训练过程为阶梯状，基模型按照次序进行训练（实际上可以做到并行处理），先给定一个初始训练数据，训练出第一个基模型，根据基模型的表现对样本进行调整，在之前基模型预测错误的样本上投入更多的关注，然后用调整后的样本训练下一个基模型，重复上述过程N次，将N个基模型进行加权结合，输出最后的结果。常用的算法有GBDT，XGBOOST等。\u003c/li\u003e\u003cli data-pid=\"ylfJwhnk\"\u003estacking：是一种组合分类器的方法，以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输出作为训练集进行再训练(一般用LR进行回归组合），从而得到完整的stacking模型。要得到stacking模型，关键在于如何构造第二层的特征，构造第二层特征的原则是尽可能的避免信息泄露，因此对原始训练集常常采用类似于K折交叉验证的划分方法。各个基模型要采用相同的Kfold，这样得到的第二层特征的每一折（对应于之前的K折划分）都将不会泄露进该折数据的目标值信息 ，从而尽可能的降低过拟合的风险。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e16.\u003cb\u003e简单描述一下模型的偏差和方差？bagging和boosting主要关注哪个？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"cbFt2GDa\"\u003e偏差描述的是预测值与真实值的差距，偏差越大，越偏离真实数据。\u003c/li\u003e\u003cli data-pid=\"2bEwETQD\"\u003e方差描述的是预测值的变化范围，离散程度，方差越大，数据分布越分散。\u003c/li\u003e\u003cli data-pid=\"RWeUtG3z\"\u003ebagging主要关注的是降低方差，boosting主要关注降低偏差。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e17.\u003cb\u003e简述一下随机森林的原理，随机森林的构造过程。\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"iaoI995L\"\u003e随机森林是bagging算法的代表，使用了CART树作为弱分类器，将多个不同的决策树进行组合，利用这种组合来降低单棵决策树的可能带来的片面性和判断不准确性。对于普通的决策树，是在所有样本特征中找一个最优特征来做决策树的左右子树划分，而随机森林会先通过自助采样的方法（bootstrap）得到N个训练集，然后在单个训练集上会随机选择一部分特征，来选择一个最优特征来做决策树的左右子树划分，最后得到N棵决策树，对于分类问题，按多数投票的准则确定最终结果，对于回归问题，由多棵决策树的预测值的平均数作为最终结果。随机森林的随机性体现在两方面，一个是选取样本的随机性，一个是选取特征的随机性，这样进一步增强了模型的泛化能力。\u003c/p\u003e\u003ch2\u003e18.\u003cb\u003e随机森林的优缺点？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"OgCTwRrR\"\u003e优点：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"7A1u8NEA\"\u003e训练可以高度并行化，训练速度快，效率高。\u003c/li\u003e\u003cli data-pid=\"IA5UO_90\"\u003e两个随机性的引入，使得随机森林不容易过拟合，具有很好的抗噪声能力。\u003c/li\u003e\u003cli data-pid=\"2WSiBs1Q\"\u003e由于每次不再考虑全部的特征属性，二是特征的一个子集，所以相对于bagging计算开销更小，效率更高。\u003c/li\u003e\u003cli data-pid=\"-77y5yYE\"\u003e对于数据的适应能力强，可以处理连续型和离散型的变量，数据无需规范化。\u003c/li\u003e\u003cli data-pid=\"1e-WGLwa\"\u003e可以输出变量的重要程度，被认为是一种不错的降维方法。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"m_T208Tl\"\u003e缺点：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Vu6OTwGv\"\u003e在某些噪声较大的分类问题和或回归问题上容易过拟合。\u003c/li\u003e\u003cli data-pid=\"m4ismJLH\"\u003e模型的可解释性比较差，无法控制模型内部的运行。\u003c/li\u003e\u003cli data-pid=\"wZ6PHal6\"\u003e对于小数据或者低维数据，效果可能会不太好。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e19.随机森林为什么不容易过拟合？随机森林的随机怎么体现？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"3vD9nDNm\"\u003e随机森林之所以不容易过拟合，是因为它通过随机性引入了模型的多样性，同时利用集成学习的优势提高了模型的泛化能力。尽管每棵决策树都可能过拟合，但当它们组合在一起时，过拟合的部分会被相互抵消，从而使得整个模型具有良好的泛化性能。\u003c/p\u003e\u003cp data-pid=\"oIVGZFzf\"\u003e随机性\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"w518ToJk\"\u003e样本的随机选择：在构建每棵树时，随机森林采用自助采样法从原始数据集中抽取样本，大约有1/3的样本被抽样多次，2/3的样本一次都没有被抽中。这种随机性确保了每棵树的训练集都是不同的，从而增加了模型的多样性。\u003c/li\u003e\u003cli data-pid=\"Z3eHrDCm\"\u003e特征的随机选择：在每个节点分裂时，随机森林不是考虑所有的特征，而是从所有特征中随机选择一部分特征，再从中选出最佳的分割特征。这样的做法进一步增加了模型的随机性和多样性。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e20.随机森林怎么判断特征重要性的？\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"Y2iB92I0\"\u003e随机森林对于特征重要性的评估思想：判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是基尼指数或袋外数据错误率。\u003c/li\u003e\u003cli data-pid=\"xO-FTPBr\"\u003e基于\u003cb\u003e基尼系数\u003c/b\u003e：如果特征X出现在决策树J中的结点M，则计算节点M分枝前后的Gini指数变化量，假设随机森林由N棵树，则计算N次的Gini系数，最后将所有的Gini系数做一个归一化处理就得到了该特征的重要性。\u003c/li\u003e\u003cli data-pid=\"cO4uJdm_\"\u003e基于\u003cb\u003e袋外数据错误率\u003c/b\u003e：袋外数据指的是每次随机抽取未被抽取达到的数据，假设袋外的样本数为O，将这O个数据作为测试集，代入已生成好的随机森林分类器，得到预测的分类结果，其中预测错误的样本数为X，则袋外数据误差为X/O，这个袋外数据误差记为errOOB1，下一步对袋外数据的特征A加入噪声干扰，再次计算袋外误差errOOB2，假设随机森林由N个分类器，则特征A的重要性为：sum(errOOB2-errOOB1)/N,其依据就是，如果一个特征很重要，那么其变动后会非常影响测试误差，如果测试误差没有怎么改变，则说明特征A不重要。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e21.\u003cb\u003eXGBOOST和GBDT的区别在哪里？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"4qUlfQm8\"\u003eXGBoost和GBDT都是集成学习中用于回归和分类的梯度提升算法。以下是两者的对比介绍：\u003c/p\u003e\u003cp data-pid=\"bxnBYws7\"\u003e优化方法\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"CpOm9w4J\"\u003eXGBoost：XGBoost使用二阶泰勒展开，利用了损失函数的一阶和二阶导数信息，这允许算法在优化时更加精确地调整步长和方向。\u003c/li\u003e\u003cli data-pid=\"Wfl9SNVo\"\u003eGBDT：GBDT仅使用一阶导数信息，即梯度信息进行优化，通常采用简单的梯度下降法。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"fvycpSC1\"\u003e正则化\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"r2iOOCN6\"\u003eXGBoost：XGBoost在代价函数中加入了正则项来控制模型的复杂度，如L1和L2正则化，这有助于减少过拟合。\u003c/li\u003e\u003cli data-pid=\"aPqYIipJ\"\u003eGBDT：GBDT通常不包括显式的正则项。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"4JRdO7sN\"\u003e缺失值处理\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"E709mM3R\"\u003eXGBoost：XGBoost能够自动处理数据中的缺失值，通过分裂点的选择来优化含有缺失值的特征。\u003c/li\u003e\u003cli data-pid=\"dqbasSND\"\u003eGBDT：GBDT需要对缺失值进行预处理，如填充或删除。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"3Uro56PI\"\u003e并行计算\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"3cUZ0Y8B\"\u003eXGBoost：XGBoost支持特征级别的并行计算，显著提高了算法的计算效率。\u003c/li\u003e\u003cli data-pid=\"mWhSwXV-\"\u003eGBDT：GBDT通常是串行执行，虽然可以通过一些技术实现并行化，但不如XGBoost高效。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e22.\u003cb\u003e为什么XGBOOST要用泰勒展开，优势在哪里？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"-EXF0ku3\"\u003exgboost使用了一阶和二阶偏导，二阶导数有利于梯度下降的更快更准，使用泰勒展开取得函数做自变量的二阶导数形式，可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂优化计算，本质上也就把损失函数的选取和模型算法的优化分开来了，这种去耦合增加了xgboost的适用性，使得它按需选取损失函数，既可以用于分类，也可以用于回归。\u003c/p\u003e\u003ch2\u003e23.\u003cb\u003eXGBOOST是如何处理缺失值的？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"zABhXugc\"\u003exgboost为缺失值设定了默认的分裂方向，xgboost在树的构建过程中选择能够最小化训练误差的方向作为默认的分裂方向，即在训练时将缺失值划入左子树计算训练误差，再划入右子树计算训练误差，然后将缺失值划入误差小的方向。\u003c/p\u003e\u003ch2\u003e24.\u003cb\u003eXGBOOST的并行化是如何实现的？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"M1pBB19b\"\u003exgboost的并行不是在tree粒度上的并行，xgboost也是一次迭代完才能进行下一次迭代（第t次迭代的损失函数包含了第t-1次迭代的预测值），它的并行处理是在特征粒度上的，在决策树的学习中首先要对特征的值进行排序，然后找出最佳的分割点，xgboost在训练之前，就预先对数据做了排序， 然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。\u003c/li\u003e\u003cli data-pid=\"VbzhOJrK\"\u003e可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e25.\u003cb\u003eXGBOOST采样时有放回的还是无放回的\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"SJx0irnR\"\u003exgboost属于boosting方法的一种，所以采样时样本是不放回的，因而每轮计算样本不重复，另外，xgboost支持子采样，每轮计算可以不使用全部的样本，以减少过拟合。另外一点是xgboost还支持列采样，每轮计算按百分比随机抽取一部分特征进行训练，既可以提高速度又能减少过拟合。\u003c/p\u003e\u003ch2\u003e26.\u003cb\u003eXGBOOST的调参步骤是怎样的？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"oEMtSlf6\"\u003e保持learning rate和其他booster相关的参数不变，调节和estimators的参数。learing_rate可设为0.1, max_depth设为4-6之间，min_child_weight设为1，subsample和colsample_bytree设为0.8 ，其他的参数都设为默认值即可。\u003c/li\u003e\u003cli data-pid=\"4G73Vprz\"\u003e调节max_depth 和 min_child_weight参数，首先，我们先大范围地粗调参数，然后再小范围地微调。\u003c/li\u003e\u003cli data-pid=\"RmvwnsFl\"\u003egamma参数调优\u003c/li\u003e\u003cli data-pid=\"RLWvPbMj\"\u003esubsample和colsample_bytree 调优\u003c/li\u003e\u003cli data-pid=\"ctFwjxP9\"\u003e正则化参数调优，选择L1正则化或者L2正则化\u003c/li\u003e\u003cli data-pid=\"6DlV-6Gr\"\u003e缩小learning rate，得到最佳的learning rate值\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e27.\u003cb\u003eLightGBM相比XGBOOST在原理和性能上的差异？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"23fbm98s\"\u003e1.速度和内存上的优化：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"pmIvONDT\"\u003exgboost用的是预排序（pre-sorted）的方法， 空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。\u003c/li\u003e\u003cli data-pid=\"C0EJgW1w\"\u003eLightGBM用的是直方图（Histogram）的决策树算法，直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"fN4V52WU\"\u003e2.准确率上的优化：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"KG26b_x0\"\u003exgboost 通过level（depth）-wise策略生长树， Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。\u003c/li\u003e\u003cli data-pid=\"UlpExapz\"\u003eLightGBM通过leaf-wise（best-first）策略来生长树， Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"V1OPL6U2\"\u003e3.对类别型特征的处理\u003cb\u003e：\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Lesr4o3R\"\u003exgboost不支持直接导入类别型变量，需要预先对类别型变量作亚编码等处理。如果类别型特征较多，会导致哑变量处理后衍生后的特征过多，学习树会生长的非常不平衡，并且需要非常深的深度才能来达到较好的准确率。\u003c/li\u003e\u003cli data-pid=\"EbcPqSpJ\"\u003eLightGBM可以支持直接导入类别型变量（导入前需要将字符型转为整数型，并且需要声明类别型特征的字段名），它没有对类别型特征进行独热编码，因此速度比独热编码快得多。LightGBM使用了一个特殊的算法来确定属性特征的分割值。基本思想是对类别按照与目标标签的相关性进行重排序，具体一点是对于保存了类别特征的直方图根据其累计值(sum_gradient/sum_hessian)重排序,在排序好的直方图上选取最佳切分位置。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e28.\u003cb\u003e特征工程的一般步骤是什么？什么是特征工程的迭代？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"fdxNK3CQ\"\u003e特征工程常规步骤：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"gCCMTK38\"\u003e数据获取，数据的可用性评估（覆盖率，准确率，获取难度）\u003c/li\u003e\u003cli data-pid=\"9jcRUBH_\"\u003e探索性数据分析，对数据和特征有一个大致的了解，同时进行数据的质量检验，包括缺失值，异常值，重复值，一致性，正确性等。\u003c/li\u003e\u003cli data-pid=\"eKuMYznb\"\u003e特征处理，包括数据预处理和特征转换两部分，数据预处理主要做清洗工作（缺失值，异常值，错误值，数据格式），特征转换即对连续特征，离散特征，时间序列特征进行转换，便于入模。\u003c/li\u003e\u003cli data-pid=\"JsFmVgQ9\"\u003e特征构建，特征构建的目的是找寻与目标变量相关且区分度较好的特征。常用的方法有特征交叉，四则运算，基于业务理解进行头脑风暴构建特征等。\u003c/li\u003e\u003cli data-pid=\"gzY1TKoK\"\u003e特征筛选，大量的特征中选择少量的有用特征，也叫作特征降维，常用的方法有过滤法，包装法，嵌入法。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"lwkunwrj\"\u003e特征工程的迭代:\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"KvsIMR3U\"\u003e选择特征：具体问题具体分析，通过查看大量的数据和基于对业务的理解，从数据中查找可以提出出数据的关键。\u003c/li\u003e\u003cli data-pid=\"x_QTVG5f\"\u003e设计特征：可以自动进行特征提取工作，也可以手工进行特征的构建。\u003c/li\u003e\u003cli data-pid=\"VyQ6TsC0\"\u003e选择特征：使用不同的特征构造方法，从多个角度来评判这个特征是否适合放入模型中。\u003c/li\u003e\u003cli data-pid=\"dmy1BPBz\"\u003e计算模型：计算模型在该特征上所提升的准确率。\u003c/li\u003e\u003cli data-pid=\"H_qIC2Qi\"\u003e上线测试：通过在线测试的效果来评估特征是否有效。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e29.\u003cb\u003e常用的特征工程方法有哪些？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"U677n2FI\"\u003e特征处理：数据的预处理包括异常值和缺失值，要根据实际的情况来处理。特征转换主要有标准化，归一化，区间缩放，二值化等，根据特征类型的不同选择合适的转换方法。\u003c/li\u003e\u003cli data-pid=\"mS9PsERs\"\u003e特征构建：特征之间的四则运算（有业务含义）,基于业务理解构造特征，分解类别特征，特征交叉组合等。\u003c/li\u003e\u003cli data-pid=\"wGRTRDH_\"\u003e特征筛选：过滤法，封装法，嵌入法。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e30.\u003cb\u003e在实际的风控建模中怎么做好特征工程？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"vQk_FiYA\"\u003e因为做风控模型大部分的数据源来自第三方，所以第三方数据的可用性评估非常重要，一方面需要了解这些特征底层的衍生逻辑，判断是否与目标变量相关。另一方面考察数据的覆盖率和真实性，覆盖率较低和真实性存疑的特征都不能使用在模型中。\u003c/li\u003e\u003cli data-pid=\"_LBj9tI9\"\u003e基于金融的数据特点，在特征筛选这个步骤上考量的因素主要有：一个是时间序列上的稳定性，衡量的指标可以是PSI，方差或者IV。一个是特征在样本上覆盖率，也就是特征的缺失率不能太高。另外就是特征的可解释性，特征与目标变量的关系要在业务上要解释的通。\u003c/li\u003e\u003cli data-pid=\"qiF6VYME\"\u003e如果第三方返回有用户的原始底层数据，例如社保的缴纳记录，运营商的通话/短信记录，则需要在特征衍生上进行特殊处理，基于自身对数据的敏感性和业务的理解，构建具有金融，风险属性的特征，也可以与业务部门进行沟通找寻与业务相关的特征。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e31.在风控项目中原始数据通常有哪些问题？该如何解决？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"mpD11iUi\"\u003e一些特征的底层逻辑不清晰，字面上的意思可能与实际的衍生逻辑相悖，这个需要与第三方数据供应商进行沟通，了解清楚特征的衍生逻辑。\u003c/li\u003e\u003cli data-pid=\"ZKjI8j3p\"\u003e数据的真实性可能存在问题。比如一个特征是历史总计，但第三方只是爬取了用户近2年的数据，这样的特征就不符合用户的真实情况。所以对数据的真实性校验显得非常重要。\u003c/li\u003e\u003cli data-pid=\"G3tVjm3k\"\u003e有缺失的特征占的比例较高。在进行缺失值处理前先分析缺失的原因，而不是盲目的进行填充，删除等工作。另外也要分析缺失是否有风险属性，例如芝麻分缺失的用户相对来说风险会较高，那么缺失可以当做一个类别来处理。\u003c/li\u003e\u003cli data-pid=\"kxcSSAX3\"\u003e大量多类特征如何使用。例如位置信息，设备信息这些特征类别数较多，如果做亚编码处理会造成维度灾难，目前常用的方法一个是降基处理，减少类别数，另一个是用xgboost来对类别数做重要性排序，筛选重要性较高的类别再做亚编码处理。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e32.\u003cb\u003e在做评分卡或其他模型中，怎么衡量特征(数据)的有用性？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"_Wu0WYgl\"\u003e特征具有金融风险属性，且与目标变量的关系在业务上有良好的可解释性。\u003c/li\u003e\u003cli data-pid=\"8Sg_6RhM\"\u003e特征与目标变量是高度相关的，衡量的指标主要是IV。\u003c/li\u003e\u003cli data-pid=\"IHxo7crb\"\u003e特征的准确率，这个需要了解特征的衍生逻辑，并与实际一般的情况相比较是否有异常。\u003c/li\u003e\u003cli data-pid=\"wZkPwA-t\"\u003e特征的覆盖率，一般来说覆盖率要达到70%以上。\u003c/li\u003e\u003cli data-pid=\"ytV7ulDx\"\u003e特征的稳定性，特征的覆盖率，分布，区分效果在时间序列上的表现比较稳定。\u003c/li\u003e\u003cli data-pid=\"r9oMn_7S\"\u003e特征的及时性，最好是能代表用户最近的信用风险情况。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e33.\u003cb\u003e缺失值的处理方式有哪些？风控建模中该如何合理的处理缺失？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"0M_GxFQ4\"\u003e首先要了解缺失产生的原因，因数据获取导致的缺失建议用填充的方式(缺失率比较低的情况下），因用户本身没有这个属性导致的缺失建议把缺失当做一个类别。另外可以分析缺失是否有风险属性，有的话最好当做一个类别来处理。\u003c/li\u003e\u003cli data-pid=\"avVYaUPT\"\u003e风控模型对于缺失率的要求比较高，尤其是评分卡。个人认为，缺失率在30%以上的特征建议不要用，缺失率在10%以下的变量可用中位数或随机森林来填充，10%-30%的缺失率建议当做一个类别。对于xgboost和lightgbm这类可以自动处理缺失值的模型可以不做处理。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e34.如何发现数据中的异常值？对异常值是怎么处理的？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"ofY_apOf\"\u003e一种是基于统计的异常点检测算法例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。另一种主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，检测的标准有欧式距离，绝对距离。\u003c/li\u003e\u003cli data-pid=\"jrZ8uw7S\"\u003e对于异常值先检查下是不是数据错误导致的，数据错误的异常作删除即可。如果无法判别异常的原因，要根据实际情况而定，像评分卡会做WOE转换，所以异常值的影响不大，可以不做处理。若异常值的数量较多，建议将异常值归为一类，数量较少作删除也可以。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e35.对于时间序列特征，连续特征，离散特征这三类是怎么做特征转换的？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"LpotsySS\"\u003e时间序列特征：将时间变量的维度进行分离（年/月/日/时/分/秒），或者与位置变量进行结合衍生成新的特征。\u003c/li\u003e\u003cli data-pid=\"A8VD4oWj\"\u003e连续型特征：标准化，归一化，区间缩放，离散化。在评分卡中主要用的是离散化，离散化常用的方法有卡房分箱，决策树分箱，等频和等深分箱。\u003c/li\u003e\u003cli data-pid=\"L08ulUVg\"\u003e离散型特征：如果类别数不是很多，适合做亚编码处理，对于无序离散变量用独热编码，有序离散变量用顺序编码。如果类别数较多，可用平均数编码的方法。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e36.如何处理样本不平衡的问题？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"y2X6MBjZ\"\u003e在风控建模中出现样本不平衡主要是坏样本的数量太少，碰到这个问题不要急着试各种抽样方法，先看一下坏用户的定义是否过于严格，过于严格会导致坏样本数量偏少，中间样本偏多。坏用户的定义一般基于滚动率分析的结果，不过实际业务场景复杂多样，还是得根据情况而定。\u003c/li\u003e\u003cli data-pid=\"q76iQMyW\"\u003e确定好坏用户定义是比较合理的之后，先尝试能不能扩大数据集，比如一开始取得是三个月的用户数据，试着将时间线延长来增加数据。因为机器学习是使用现在的数据在整个数据分布上进行估计，因此更多的数据往往能够得到更多的分布信息，以及更好的分布估计。\u003c/li\u003e\u003cli data-pid=\"q2TcDkN7\"\u003e对数据集进行抽样，一种是进行欠采样，通过减少大类的数据样本来降低数据的不平衡，另一种是进行过采样，通过增加小类数据的样本来降低不平衡，实际工作中常用SMOTE方法来实现过采样。\u003c/li\u003e\u003cli data-pid=\"YzbSMnsU\"\u003e尝试使用xgboost和lightgbm等对不平衡数据处理效果较好的模型。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e37.特征衍生的方法有哪些？说说你平时工作中是怎么做特征衍生的？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"fNdvR2UX\"\u003e常规的特征衍生方法：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"tycy6ClA\"\u003e基于对业务的深入理解，进行头脑风暴，构造特征。\u003c/li\u003e\u003cli data-pid=\"lMGn3qZy\"\u003e特征交叉，例如对类别特征进行交叉相乘。\u003c/li\u003e\u003cli data-pid=\"STmUElAr\"\u003e分解类别特征，例如对于有缺失的特征可以分解成是否有这个类别的二值化特征，或者将缺失作为一个类别，再进行亚编码等处理。\u003c/li\u003e\u003cli data-pid=\"C2Hszfqj\"\u003e重构数值量（单位转换，整数小数拆分，构造阶段性特征）\u003c/li\u003e\u003cli data-pid=\"wPVeLv1_\"\u003e特征的四则运算，例如取平均/最大/最小，或者特征之间的相乘相除。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"wzgyOY5U\"\u003e平时工作特征衍生的做法：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"S8NK_ShW\"\u003e因为风控模型通常需要好的解释能力，所以在特征衍生时也会考虑到衍生出来的特征是否与目标变量相关。例如拿到运营商的通话记录数据，可以衍生一个\u0026#34;在敏感时间段（深夜）的通话次数占比\u0026#34;，如果占比较高，用户的风险也较大。\u003c/li\u003e\u003cli data-pid=\"o4JSDiu5\"\u003e平常会将大量的时间和精力花在底层数据的衍生上，这个不仅需要对业务的理解，也需要一定的想象力进行头脑风暴，即使衍生出来的特征90%都效果不佳，但只要剩下的10%是好的特征，那对于模型效果的提升是很显著的。\u003c/li\u003e\u003cli data-pid=\"pZnxl2U5\"\u003e对于评分卡来说，特征需要好的解释能力，所以一些复杂的衍生方法，像特征交叉，log转换基本不会用到。但如果是xgboost等复杂模型，进行特征交叉等方法或许有比较好的效果。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e38.特征筛选的作用和目的？筛选的特征需要满足什么要求？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"v1BBs-Uz\"\u003e作用和目的：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Km-xwScq\"\u003e简化模型，增加模型的可解释性， 降低模型过拟合的风险。\u003c/li\u003e\u003cli data-pid=\"RZ_rimdR\"\u003e缩短模型的训练时间。\u003c/li\u003e\u003cli data-pid=\"ivLVKJlK\"\u003e避免维度灾难。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"g2b1Ri5I\"\u003e筛选特征满足的要求：\u003c/p\u003e\u003cp data-pid=\"bHSCyNg6\"\u003e具有良好的区分能力。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Ex6wRegc\"\u003e可解释性好，与目标变量的关系在业务上能解释的通。\u003c/li\u003e\u003cli data-pid=\"jbTot8wu\"\u003e在时间序列上有比较好的稳定性。\u003c/li\u003e\u003cli data-pid=\"OHdBzWDb\"\u003e特征的用户覆盖率符合要求。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e39.特征筛选的方法有哪些？每种方法的优缺点？实际工作中用到了哪些方法？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"RKwKIAvN\"\u003eFilter（过滤法）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"hGmbKi5s\"\u003e相关系数，方差（适用于连续型变量），卡方检验（适用于类别型变量），信息熵，IV。实际工作中主要基于IV和相关性系数（皮尔逊系数）。\u003c/li\u003e\u003cli data-pid=\"izwKItUt\"\u003e优点：算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。\u003c/li\u003e\u003cli data-pid=\"YoHfrfWQ\"\u003e缺点：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"QDdsgAf7\"\u003eWrapper（封装法）：封装式特征选择是利用学习算法的性能评价特征子集的优劣。因此，对于一个待评价的特征子集，Wrapper方法需要训练一个分类器，根据分类器的性能对该特征子集进行评价。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"LEDetNkH\"\u003e方法有完全搜索（递归消除法），启发式搜索（前向/后向选择法，逐步选择法），随机搜索（训练不同的特征子集）。实际工作中主要用到启发式搜索，例如评分卡的逐步逻辑回归。\u003c/li\u003e\u003cli data-pid=\"vjKVQq6I\"\u003e优点：相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。\u003c/li\u003e\u003cli data-pid=\"IMs00754\"\u003e缺点：Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Ikoneg0U\"\u003eEmbedded（嵌入法）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"i3l_w7la\"\u003e一种是基于惩罚项，例如岭回归，lasso回归，L1/L2正则化。另一种是基于树模型输出的特征重要性，在实际工作中较为常用，可选择的模型有随机森林，xgboost，lightgbm。\u003c/li\u003e\u003cli data-pid=\"NvIDrI2D\"\u003e优点：效果最好速度最快，模式单调，快速并且效果明显。\u003c/li\u003e\u003cli data-pid=\"b9_aoDXm\"\u003e缺点：如何参数设置， 需要对模型的算法原理有较好的理解。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e40.简单介绍一下风控模型常用的评估指标。\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"EUc-KlUU\"\u003e混淆矩阵指标：精准率，查全率，假正率。当模型最后转化为规则时，一般用这三个指标来衡量规则的有效性。要么注重精准率，要么注重查全率，两者不可兼而得之。\u003c/li\u003e\u003cli data-pid=\"HTT4R3j1\"\u003eROC曲线和AUC值，ROC曲线是一种对于查全率和假正率的权衡，具体方法是在不同阈值下以查全率作为纵轴，假正率作为横轴绘制出一条曲线。曲线越靠近左上角，意味着越多的正例优先于负例，模型的整体表现也就越好。AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。在对角线（随机线）左边的点上TPR总大于FPR，意为正例被判为正例的概率大于负例被判为正例的概率。从另一个角度看，由于画ROC曲线时都是先将所有样本按分类器的预测概率排序，所以AUC反映的是分类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\u003c/li\u003e\u003cli data-pid=\"IcyP1cDV\"\u003eKS：用于区分预测正负样本分隔程度的评价指标，KS越大，表示模型能将好坏样本区分开的程度越大。KS的绘制方法是先将每个样本的预测结果化为概率或者分数，将最低分到最高分（分数越低，坏的概率越大）进行排序做样本划分，横轴就是样本的累计占比，纵轴则是好坏用户的累计占比分布曲线，KS值为两个分布的最大差值（绝对值）。KS值仅能代表模型的区隔能力，KS不是越高越好，KS如果过高，说明好坏样本分的过于开了，这样整体分数（概率）就是比较极端化的分布状态，这样的结果基本不能用。\u003c/li\u003e\u003cli data-pid=\"ELIHz8AG\"\u003e基尼系数：其横轴是根据分数（概率）由高到低累计的好用户占总的好用户的比例，纵轴是分数（概率）从高到低坏用户占总的坏用户的比例。由于分数高者为低风险用户，所以累计坏用户比例的增长速度会低于累计好用户比例，因此，基尼曲线会呈现向下弯曲的形式，向下突出的半月形的面积除以下方三角形的面积即是基尼系数。基尼系数越大，表示模型对于好坏用户的区分能力越好。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e41.什么是模型的欠拟合和过拟合？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"YU6LnEAy\"\u003e欠拟合指的是模型没有很好的捕捉到数据特征，不能很好的拟合数据。\u003c/li\u003e\u003cli data-pid=\"r5ntO8fe\"\u003e过拟合指的是模型把数据学习的太彻底，以至于把噪声数据学习进去了，这样模型在预测未知数据时，就不能正确的分类，模型的泛化能力太差。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e42.如何判断模型是否存在过拟合或欠拟合？对应的解决方法有哪些？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"lL_QCVQu\"\u003e判断模型是否存在过拟合/欠拟合主要用学习曲线，学习曲线指的是通过画出不同训练集大小时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高（过拟合）或偏差过高（欠拟合）。当训练集和测试集的误差收敛但却很高时，即为欠拟合，当训练集和测试集的误差之间有大的差距时，为过拟合。\u003c/li\u003e\u003cli data-pid=\"WvQ7ZARq\"\u003e解决欠拟合的方法：增加效果好的特征，添加多项式特征，减小正则化参数等。\u003c/li\u003e\u003cli data-pid=\"DReTgQ9O\"\u003e解决过拟合的方法：使用更多的数据，选择更加合适的模型，加入正则项等。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e43.什么是正则化？什么是L1正则化和L2正则化？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"kbTZmaJz\"\u003e正则化是在模型的loss function的基础上，加上了一些正则化项或者称为模型复杂度惩罚项，它会向学习算法略微做些修正，从而让模型能更好地泛化。这样反过来能提高模型在不可见数据上的性能。\u003c/li\u003e\u003cli data-pid=\"_bnWSWfn\"\u003eL1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解，所以L1正则化会趋向于产生少量的特征。\u003c/li\u003e\u003cli data-pid=\"aLoVx6KS\"\u003eL2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），所以L2正则化会使特征的解趋近于0，但不会为0。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e44.正则化为什么可以防止过拟合？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"DEHO42V8\"\u003e最简单的解释是正则化对模型参数添加了先验，在数据少的时候，先验知识可以防止过拟合。举个例子：抛一枚硬币5次，得到的全是正面，则得出结论：正面朝上的概率为1，这类似于模型的过拟合，如果加上硬币朝上的概率是0.5的先验，结果就不会这么离谱，这就是正则。\u003c/p\u003e\u003ch2\u003e\u003cb\u003e45.什么是交叉验证？交叉验证的目的是什么？有哪些优点？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"0V8CUcTu\"\u003e交叉验证概念：\u003c/p\u003e\u003cp data-pid=\"thyl8Snz\"\u003e交叉验证，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓\u0026#34;交叉\u0026#34;。　\u003c/p\u003e\u003cp data-pid=\"oDUiX9Dw\"\u003e交叉验证的目的：\u003c/p\u003e\u003cp data-pid=\"8yo1ZQZm\"\u003e评估给定算法在特定数据集上训练后的泛化性能，比单次划分训练集和测试集的方法更加稳定，全面。\u003c/p\u003e\u003cp data-pid=\"mRV54_HJ\"\u003e交叉验证的优点：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"2Hri8hLW\"\u003e如果只是对数据随机划分为训练集和测试集，假如很幸运地将难以分类的样本划分进训练集中，则在测试集会得出一个很高的分数，但如果不够幸运地将难以分类的样本划分进测试集中，则会得到一个很低的分数。所以得出的结果随机性太大，不够具有代表性。而交叉验证中每个样本都会出现在训练集和测试集中各一次，因此，模型需要对所有样本的泛化能力都很好，才能使其最后交叉验证得分，及其平均值都很高，这样的结果更加稳定，全面，具有说服力。\u003c/li\u003e\u003cli data-pid=\"JQWcOLZG\"\u003e对数据集多次划分后，还可以通过每个样本的得分比较，来反映模型对于训练集选择的敏感性信息。\u003c/li\u003e\u003cli data-pid=\"4UDpb8oJ\"\u003e对数据的使用更加高效，可以得到更为精确的模型。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e46.交叉验证常用的方法有哪些？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"nZwc01Mi\"\u003e标准K折交叉验证：K是自定义的数字，通常取5或10，如果设为5折，则会训练5个模型，得到5个精度值。\u003c/li\u003e\u003cli data-pid=\"mn_PaZJw\"\u003e分层K折交叉验证：如果一个数据集经过标准K折划分后，在测试集上只有一种类别，则无法给出分类器整体性能的信息，这种情况用标准K折是不合理的。而在分层K折交叉验证中，每个折中的类别比例与整个数据集类别比例相同，这样能对泛化性能做出更可靠的估计。\u003c/li\u003e\u003cli data-pid=\"I8AlY10y\"\u003e留一法交叉验证：每次划分时，把单个数据点作为测试集，如果数据量小，能得到更好的估计结果，数据量很大时则不适用。\u003c/li\u003e\u003cli data-pid=\"zAhgr-go\"\u003e打乱划分交叉验证：每次划分数据时为训练集取样train_size个点，为测试集取样test_size个点，将这一划分划分方法重复n_splits次。这种方法还允许每次迭代中使用部分数据，可通过设置train_size和test_size之和不为0来实现，用这种方法对数据进行二次采样可能对大型数据上的试验很用用。另外也有分层划分的形式（ StratifiedShuffleSplit），为分类任务提供更可靠的结果。\u003c/li\u003e\u003cli data-pid=\"TiK5BWyQ\"\u003e分组交叉验证：适用于数据中的分组高度相关时，以group数组作为参数，group数组表示数据中的分组，在创建训练集和测试集的时候不应该将其分开，也不应该与类别标签弄混。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e47.\u003cb\u003e互联网金融场景下的的风控模型种类？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"QowKRWVW\"\u003e获客阶段：用户响应模型，风险预筛选模型。\u003c/li\u003e\u003cli data-pid=\"7hfGOz8M\"\u003e授信阶段：申请评分模型，反欺诈模型，风险定价模型，收益评分模型。\u003c/li\u003e\u003cli data-pid=\"3znkSYae\"\u003e贷后阶段：行为评分模型，交易欺诈模型，客户流失模型。\u003c/li\u003e\u003cli data-pid=\"Ifcejz2_\"\u003e催收阶段：早期催收模型，晚期催收模型。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e48.\u003cb\u003e简单描述一下风控建模的流程？、\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"QDQAQEjD\"\u003e前期准备工作：不同的模型针对不同的业务场景，在建模项目开始前需要对业务的逻辑和需求有清晰的理解，明确好模型的作用，项目周期时间和安排进度，以及模型效果的要求。\u003c/li\u003e\u003cli data-pid=\"ReG9KEnC\"\u003e模型设计：包括模型的选择（评分卡还是集成模型），单个模型还是做模型的细分，是否需要做拒绝推论，观察期，表现期的定义，好坏用户的定义，数据的获取途径等都要确定好。\u003c/li\u003e\u003cli data-pid=\"xpxzwxqJ\"\u003e数据拉取及清洗：根据观察期和表现期的定义从数据池中取数，并进行前期的数据清洗和稳定性验证工作，数据清洗包括用户唯一性检查，缺失值检查，异常值检查等。稳定性验证主要考察变量在时间序列上的稳定性，衡量的指标有PSI，平均值/方差，IV等。\u003c/li\u003e\u003cli data-pid=\"ZlI9Dcif\"\u003e特征工程：主要做特征的预处理和筛选，如果是评分卡，需要对特征进行离散化，归一化等处理，再对特征进行降维，降维的方法有IV筛选，相关性筛选，显著性筛选等。另外会基于对业务的深入理解做特征构造工作，包括特征交叉，特征转换，对特征进行四则运算等。\u003c/li\u003e\u003cli data-pid=\"TZ2qfDSd\"\u003e模型建立和评估：选择合适的模型，像评分卡用逻辑回归，只需要做出二分类预测可以选择xgboost等集成模型，模型建好后需要做模型评估，计算AUC,KS，并对模型做交叉验证来评估泛化能力及模型的稳定性。\u003c/li\u003e\u003cli data-pid=\"J6jMmSHn\"\u003e模型上线部署：在风控后台上配置模型规则，对于一些复杂的模型还得需要将模型文件进行转换，并封装成一个类，用Java等其他形式来调用。\u003c/li\u003e\u003cli data-pid=\"WpGmgE9v\"\u003e模型监控：前期主要监控模型整体及变量的稳定性，衡量标准主要是PSI，并每日观察模型规则的拒绝率与线下的差异。后期积累一定线上用户后可评估线上模型的AUC,KS，与线下进行比较，衡量模型的线上的实际效果。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e49.\u003cb\u003e评分卡，集成模型在线上是如何部署的？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"cgM4LKqD\"\u003e评分卡的部署较为简单，因为评分卡将变量映射到了一个个区间及得分，所以在普通的风控决策引擎上就可配置。\u003c/li\u003e\u003cli data-pid=\"jN-8CQVL\"\u003e像一些比较复杂的模型，例如xgboost和lightgbm，一般是将模型文件转换为pmml格式，并封装pmml，在风控后台上上传pmml文件和变量参数文件，并配置好模型的阈值。python模型和R模型都可以用这种方式来部署。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e50.\u003cb\u003e对于金融场景，稳定胜于一切，那在建模过程中如何保证模型的稳定性？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"OT3D9Ads\"\u003e在数据预处理阶段可以验证变量在时间序列上的稳定性，通过这个方法筛掉稳定性不好的变量，也能达到降维的目的。筛选的手段主要有：计算月IV的差异，观察变量覆盖率的变化，两个时间点的PSI差异等。\u003c/li\u003e\u003cli data-pid=\"Hy6xZVEQ\"\u003e异常值的检查，剔除噪声，尤其对于逻辑回归这种对于噪声比较敏感的模型。\u003c/li\u003e\u003cli data-pid=\"J6rgnZOa\"\u003e在变量筛选阶段剔除与业务理解相悖的变量，如果是评分卡，可以剔除区分度过强的变量，这种变量一般不适合放入模型中，否则会造成整个模型被这个变量所左右，造成模型的稳定性下降，过拟合的风险也会增加。\u003c/li\u003e\u003cli data-pid=\"8FF90Eyi\"\u003e做交叉验证，一种是时间序列上的交叉验证，考察模型在时间上的稳定性，另一种是K折随机交叉验证，考察模型的随机稳定性。\u003c/li\u003e\u003cli data-pid=\"56Jy-3Fa\"\u003e选择稳定性较好的模型，例如随机森林或xgboost这类泛化能力较好的模型。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e51.\u003cb\u003e模型转化为规则后决策点（cutoff点）怎么设定？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"XWd29fea\"\u003e规则只是判断用户好坏，而不会像模型会输出违约概率，所以设定决策点时需要考虑到规则的评估指标（精准率，查全率，误伤率，拒绝率），一般模型开发前会设定一个预期的拒绝率，在这个拒绝率下再考量精确率，查全率和误伤率的取舍，找到最佳的平衡点。\u003c/li\u003e\u003cli data-pid=\"Z3d0jMz3\"\u003e好的模型能接受更多的好用户，拒绝掉更多的坏用户，也就是提高好坏件比例，所以可事先设定一个预期目标的好坏件比例来选择最佳的决策点。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e52.模型上线后是怎么监控的？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"iM6uLdqZ\"\u003e前期监控（模型上线后一个月内）：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"Z8D7Ki18\"\u003e模型最后设定cutoff点后可以得出模型的拒绝率（线下拒绝率）, 上线后需要比较模型每日的拒绝率与线下拒绝率。如果两者差异较大，说明线上的用户与建模的用户分布有很大差异，原因可能是没做拒绝推断，或者用户属性随着时间发生了偏移。\u003c/li\u003e\u003cli data-pid=\"39UeLVQt\"\u003e监控模型整体的稳定性，通常用PSI来衡量两个时间点的差异程度。模型的稳定性是一个需要长期观察的指标，可绘制月/周PSI变化趋势图来分析稳定性的变化，从中可以发现用户是否随着时间推移属性发生了变化，以便及时对模型做出合理的调整。\u003c/li\u003e\u003cli data-pid=\"TIpC_omb\"\u003e变量稳定度分析，目的是如果模型的稳定性不好，可利用变量稳定度分析来了解是哪些变量造成的。对于不稳定的变量要分析其原因，并对模型做出调整，弃用不稳定的变量或者找其他变量来替换。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"VPQrhqkD\"\u003e后期监控（用户表现出了好坏程度）：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"rgZ8NJT9\"\u003e此时已积累了一些线上的好坏用户，可做模型的线上效果的评估，评估的指标有AUC, KS, 基尼系数，如果模型的线下效果好，但线上效果却不理想，这个模型是要做优化的。\u003c/li\u003e\u003cli data-pid=\"R0CGg-wF\"\u003e好坏用户的评分分布。绘制线上好坏用户的评分分布图，如果符合期望（高分段好用户占比多，低分段坏用户占比多），则说明模型的线上的区隔能力较好。\u003c/li\u003e\u003cli data-pid=\"GF4k0rga\"\u003e变量鉴别力分析。用线上的好坏用户来计算变量的IV值，评价变量的预测能力，预测能力不好的变量可以考虑弃用。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e53.\u003cb\u003e当模型上线后发现稳定性不佳，或者线上的区分效果不好，你是怎么对模型作调整的？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"wqHvxpuj\"\u003e模型稳定性不佳先检查当初建模时有没有考量过特征的稳定性，在模型前期监控一般会做变量的稳定性分析，如果发现稳定性不佳的变量，考虑弃用或用其他变量替代。另外可以分析下线上用户和建模用户的分布差异，考虑在建模时增加拒绝推断的步骤，让建模样本的分布更加接近于实际整体的申请用户。\u003c/li\u003e\u003cli data-pid=\"iW4vCALi\"\u003e线上的效果不好可以从变量角度分析，做一下变量鉴别度分析，剔除掉效果不好的变量，挖掘新的变量入模。如果一个模型已上线较长的时间，用户的属性也慢慢发生偏移，建议重新取数做一个新的模型替代旧模型。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cb\u003e54.对于高维稀疏特征，或者是弱特征，你是怎么处理的？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"WKnHg1_L\"\u003e对于高维稀疏特征，逻辑回归的效果要比GBDT好。这是由于逻辑回归的正则项是对特征权重的惩罚，以至于特征的权重不至于过大，而树模型的惩罚项主要是深度和叶子节点数目，而对于高维稀疏特征,10000个样本可能9990个值是0，那只需要一个节点就可以划分9990和剩下的10个样本，可见惩罚项之小，所以GBDT对于高维稀疏特征很容易过拟合。平时工作中如果用的是逻辑回归评分卡，则可以对稀疏特征进行离散化，离散成值为0或不为0，再用woe进行编码。而如果使用xgboost等集成模型，最好还是不要用高维的稀疏特征。\u003c/li\u003e\u003cli data-pid=\"Sl_z5ujV\"\u003e弱特征指的是与目标变量关系不大的特征，或者是区分能力较弱的特征。在大数据风控中弱特征的种类很多，包括社交，通话，位置等信息，而且建模时弱特征会多达数百个。如果是用评分卡建模，弱特征一般会被舍弃掉，因为评分卡的入模特征数不宜过多，一般在15个以下，所以要找寻比较强的特征。而对于xgboost等模型，本身对数据的要求不是很高，并且精度好，一些弱特征进行交叉组合或许能给模型带来不错的效果。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e55.\u003cb\u003e如何根据风险因素对用户分层，构建客群差异化的模型？\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"BHxlRysE\"\u003e做客群差异化模型之前最好做一下用户画像，在风控领域中做用户画像的目的是：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"rqUYSxUQ\"\u003e系统性的梳理用户群体，找到异同点对用户进行划分群体，分类的维度很多，可以是静态属性，购买偏好，也可以是褥羊毛党等风险属性。\u003c/li\u003e\u003cli data-pid=\"6KQZ_Fz4\"\u003e便于更深刻的理解业务，理解用户需求，风控离不开业务，只有深刻理解业务后，才能发现更多潜在的风险。\u003c/li\u003e\u003cli data-pid=\"uihp5FS4\"\u003e便于后续的数据挖掘，了解坏用户的行为特征，并且根据用户特征做关联规则分析。\u003c/li\u003e\u003cli data-pid=\"mKRY_1EB\"\u003e对不同类型的用户，做针对性的风控规则和风控模型。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"dU6eOvYp\"\u003e平常工作中的做法：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"fOKQIg7Y\"\u003e对用户做静态属性的划分，比如按性别，年龄，收入，职业等。例如刚毕业工作的年轻人和收入比较稳定的中年人，他们的借款需求，风险程度就不一样，可以先对用户群体做这样的划分，再对每个群体单独建立模型。\u003c/li\u003e\u003cli data-pid=\"Rrcs5BPK\"\u003e根据用户风险属性做差异化模型，例如对手机分期业务做一个套现风险模型，挖掘套现风险属性，目标变量变成是否为套现用户。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e56.\u003cb\u003e额度，利率的风险定价模型你是如何设计的？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"VmsPHgWD\"\u003e首先做风险定价模型需要熟悉产品的属性和特点，像小额现金贷和大额分期贷两种产品的额度定价逻辑就不同。另外也要了解产品的盈利模式和预期的利润，这点需要与业务部门做好沟通，通常关于额度，利率也是业务或者产品制定的。\u003c/li\u003e\u003cli data-pid=\"xxY5uBg3\"\u003e风险定价模型一般采用评分卡模型，最后设定cutoff点后对通过的用户进行风险等级划分，对于风险高的用户给的额度较低，或者利率较高。一般来说中低额度的用户占大部分，高额度用户占小部分，最后可以得出一个平均额度或利率，这个值事先可以根据预期的利润/资损来计算。\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e57. \u003cb\u003e风控流程中不同环节的评分卡是怎么设计的？\u003c/b\u003e\u003c/h2\u003e\u003cul\u003e\u003cli data-pid=\"5dvjux1W\"\u003e申请评分A卡用在贷前审核阶段，主要的作用是决定用户是否准入和对用户进行风险定价（确定额度和利率），用到的数据是用户以往的信用历史，多头借贷，消费记录等信息，并且做A卡一般需要做拒绝推断。A卡一般预测用户的首笔借款是否逾期，或者预测一段时间内是否会逾期，设计的方式也多种多样，有风险差异化评分卡，群体差异化评分卡，或者做交叉评分卡等。\u003c/li\u003e\u003cli data-pid=\"GBCpXu0c\"\u003e行为B卡主要用在借贷周期较长的产品上，例如手机分期。作用一是防控贷中风险，二是对用户的额度做一个调整。用到的数据主要是用户在本平台的登录，浏览，消费行为数据，还有借还款，逾期等借贷表现数据。\u003c/li\u003e\u003cli data-pid=\"VDfUwBNk\"\u003e催收C卡主要是对逾期用户做一个画像分析，通过深度挖掘用户特征，对逾期用户进行分群，做智能催收策略等。\u003c/li\u003e\u003c/ul\u003e","is_labeled":false,"visited_count":10216,"favorite_count":901,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 719621121}","attached_info":"CrgGCOTe9sWEv8/zngEQBxoJMjQ4MDQ2MTIwINrDm7cGKPIBMAVAgwFKSQofVFNfU09VUkNFX1pSRUNBTExfSVRFTUNGX1VQVk9URRIgZG9jX3R5cGU6IEFydGljbGUKaWQ6IDE2MjU2NjAxOQoYACAAOgBKKAodVFNfU09VUkNFX05FQVJMSU5FX0NPTlRFTlRfVjISATAYACAAOgBiIGIzN2MyNDFiMThhMDE0OTQyYTk0MmIzNWZmZDIyMDU5cgk3MTk2MjExMjGqAQlyZWNvbW1lbmTCASA3MDk4NTQzMWRmZmY4NTU5OTFiY2NiYTg3Njc4NDkwNvIBCggMEgZOb3JtYWzyASgIChIkN2NjMzVkNWQtMzE3NS00NTIzLWExZTMtNDMxYzkwNmYzMDgz8gEGCAsSAjIyggIAiALXy9fOhTOSAiA3MDk4NTQzMWRmZmY4NTU5OTFiY2NiYTg3Njc4NDkwNpoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZcoCF1Rlc3RlZEFuZFdvcmtXZWlnaHRSdWxl2gIfVFNfU09VUkNFX1pSRUNBTExfSVRFTUNGX1VQVk9URegCA/oCC05PUk1BTF9GTE9XigMgMTk2ODljNDQ3MDkxNDEyOWE4ZTdiMjZjODhiODdhOGKaAw0KAnYyEAAaBW90aGVyqAPoT9gDAOoDGXRleHRBbGxTaXRlQWN0aW9uSXRlbUNGVjL6A04SDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFOi0IAhC/BhjeASIjdjItOTJiNGM5N2ZiYzNjZjc2YTliMTRkODM5Y2E0YTI0N2aABACIBACSBAZOb3JtYWyaBAEzoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAOB8t5U/gQUAAAAAAAAAAIkFNN7FSGlz0z+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFFpAGAKAGgwGoBgCSAiQKCTI0ODA0NjEyMBIJNzE5NjIxMTIxGAciCklNQUdFX1RFWFQ=","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=131\u0026end_offset=131\u0026page_number=23\u0026session_token=b37c241b18a014942a942b35ffd22059","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=131\u0026end_offset=131\u0026page_number=23\u0026session_token=b37c241b18a014942a942b35ffd22059","totals":0},"fresh_text":"推荐已更新"}
