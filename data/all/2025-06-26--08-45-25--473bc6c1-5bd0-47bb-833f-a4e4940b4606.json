{"data":[{"id":"84_1750898770.487","type":"feed","offset":84,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750898770,"updated_time":1750898770,"target":{"id":"1919807521729779188","type":"article","url":"https://api.zhihu.com/articles/1919807521729779188","author":{"id":"ca22f5a668a7554a4ad3887b8e362718","url":"https://api.zhihu.com/people/ca22f5a668a7554a4ad3887b8e362718","user_type":"people","url_token":"yi-tian-di","name":"AI布道师Warren","headline":"一名 LLM Agent 研发工程师~全平台同名","avatar_url":"https://pica.zhimg.com/50/v2-11ce4924277003157ac2b32fc083407b_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":1516,"is_following":false,"is_followed":false},"title":"Lovart 创始人陈冕：从账户剩4千到融资数亿","comment_permission":"all","created":1750499736,"updated":1750499736,"voteup_count":7,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"完整采访： 103. Lovart创始人陈冕复盘应用创业这两年：这一刻就是好爽啊！！哈哈哈哈哈 我的思考： 在选择垂直赛道时，不能只看哪个功能点可以被AI优化，而要思考：哪个价值链条最长、最痛苦、最依赖人工协作，并且有可能被AI Agent端到端地打通。成功的创业者能够在信息混沌、市场迷茫的早期，凭借超前的认知，下注一个看似遥远的未来，并用日复一日的战斗，把这个未来变成现实。 “好消息是，所有的竞争对手都被下架了。坏消息…","excerpt_new":"完整采访： 103. Lovart创始人陈冕复盘应用创业这两年：这一刻就是好爽啊！！哈哈哈哈哈 我的思考： 在选择垂直赛道时，不能只看哪个功能点可以被AI优化，而要思考：哪个价值链条最长、最痛苦、最依赖人工协作，并且有可能被AI Agent端到端地打通。成功的创业者能够在信息混沌、市场迷茫的早期，凭借超前的认知，下注一个看似遥远的未来，并用日复一日的战斗，把这个未来变成现实。 “好消息是，所有的竞争对手都被下架了。坏消息…","preview_type":"default","preview_text":"","column":{"id":"smash","type":"column","url":"https://api.zhihu.com/columns/smash","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pic1.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"阿杰哥的妙妙屋","imageUrl":"https://picx.zhimg.com/v2-2df49fca91f0e39f4f480c7cd524641b_720w.jpg?source=d16d100b","comment_permission":"public","intro":"LLM、后端、工程设计 \u0026 前沿技术研究。对了你想进我的 AI 妙妙屋么？","updated":1746198041,"is_following":false},"content":"\u003cp\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-034e105e7745e7ff796d915a741ad05a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2084\" data-rawheight=\"886\" data-original-token=\"v2-fde4a8ea94c7c2a78acf76128c2cf053\" class=\"origin_image zh-lightbox-thumb\" width=\"2084\" data-original=\"https://pic3.zhimg.com/v2-034e105e7745e7ff796d915a741ad05a_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"7lGmdVmQ\"\u003e完整采访：\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.xiaoyuzhoufm.com/episode/68455e0a6dbe9284e75c6fbf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e103. Lovart创始人陈冕复盘应用创业这两年：这一刻就是好爽啊！！哈哈哈哈哈\u003c/a\u003e\u003c/p\u003e\u003cp data-pid=\"fAnNEPAb\"\u003e我的思考：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"JOSspsRs\"\u003e在选择垂直赛道时，不能只看哪个功能点可以被AI优化，而要思考：\u003cb\u003e哪个价值链条最长、最痛苦、最依赖人工协作，并且有可能被AI Agent端到端地打通。\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"LGf-fG6f\"\u003e成功的创业者能够在信息混沌、市场迷茫的早期，凭借超前的认知，下注一个看似遥远的未来，并用日复一日的战斗，把这个未来变成现实。\u003c/li\u003e\u003c/ol\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003chr/\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"mpC-wzGZ\"\u003e“好消息是，所有的竞争对手都被下架了。坏消息是，我们没钱了。”\u003c/p\u003e\u003cp data-pid=\"Ekl6kgW1\"\u003e这是Lovart创始人陈冕在描述他上一次创业经历的结尾时，一句充满黑色幽默的总结。这句话，也浓缩了他创业之路的惊心动魄：在绝望的边缘徘徊，又在时代的浪潮中重生。\u003c/p\u003e\u003cp data-pid=\"5ZE3x_rP\"\u003e陈冕的经历，是AI时代一个充满戏剧张力的创业者缩影。他并非一路坦途的天才少年，更像一个在互联网战场上身经百战、伤痕累累，最终找到自己使命的“战斗型CEO”。他的故事，充满了对商业本质的深刻反思、对时机与战略的精准拿捏，以及一种近乎偏执的战斗激情。\u003c/p\u003e\u003cp data-pid=\"d2LDT77A\"\u003e这不仅仅是一个创业故事，更是一份来自AI前线的，滚烫的经验分享。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e漂泊的十年：在中国互联网大厂的“集邮”之旅\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"YodVGFZN\"\u003e在创立Lovart之前，陈冕的履历几乎集齐了中国互联网的“龙珠”：腾讯、360、滴滴、摩拜、每日优鲜、字节跳动。这看似漂泊不定的十年，却让他以极高的效率完成了对商业模式的深刻洞察。\u003c/p\u003e\u003cp data-pid=\"2sLMV9Xm\"\u003e他亲身经历了移动互联网最惨烈的两场战争：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"T6i23tOD\"\u003e\u003cb\u003e出行之战：\u003c/b\u003e 在滴滴，他见证了与快滴、Uber的补贴大战，理解了网络效应和资本如何重塑市场。\u003c/li\u003e\u003cli data-pid=\"4qEUjjYU\"\u003e\u003cb\u003e共享单车之战：\u003c/b\u003e 在摩拜，作为产品总负责人，他更深刻地体会到，一个看似性感的商业模式，如果单位经济模型（Unit Economics）算不过来，最终只会沦为资本的附庸。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"8ah5TFfh\"\u003e\u003cb\u003e他的核心教训是：不要被表面的繁荣和高光所迷惑，要回归商业常识。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"HJBCy_UG\"\u003e他反思道：“摩拜和每日优鲜的经历让我明白，有些商业模式，它就是不成立的。你以为用户量很大，日活很高，但它的供给是无限的，成本也是无限的，无法形成一个健康的闭环。这种业务，最终既不赚钱，也留不住流量。”\u003c/p\u003e\u003cp data-pid=\"foYMWCIt\"\u003e这段“集邮”般的经历，让他对“什么能做，什么不能做”形成了近乎直觉的判断力，也为他在AI时代的爆发埋下了最重要的伏笔。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e至暗时刻：3000万美金都没人要的绝境\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"KTKRj_7z\"\u003e在All in AI之前，陈冕的第一次创业以一种极其惨烈的方式收场。他的产品，在数据优秀、用户喜爱的情况下，因为政策原因被突然下架。\u003c/p\u003e\u003cp data-pid=\"v5j3iQ7K\"\u003e这是他最崩溃的时期：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"bv1ZV2g4\"\u003e\u003cb\u003e弹尽粮绝：\u003c/b\u003e 账户上只剩几千块钱。\u003c/li\u003e\u003cli data-pid=\"rJDXl5SG\"\u003e\u003cb\u003e投资人冷眼：\u003c/b\u003e 他拿着优秀的市场数据和用户反馈，开出3000万美金的“白菜价”，却没有任何投资人愿意接盘。理由简单而粗暴：“你产品没上架，我们不投。” 陈冕对此愤懑不已：“你聊之前就知道我没上架，还非得问我那么多问题！”\u003c/li\u003e\u003cli data-pid=\"j4DioudI\"\u003e\u003cb\u003e团队流失：\u003c/b\u003e 核心团队走了一半。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"hgdXyXr_\"\u003e他形容那段时间：“你要想着用户会不会回来，团队可能都要走，投资人还天天问你一堆问题……那种绝望，是全方位的。”\u003c/p\u003e\u003cp data-pid=\"g0-LBFMp\"\u003e这段经历让他明白，创业不仅是产品和市场的博弈，更是对创始人韧性、心态和现金流管理的极限考验。\u003c/p\u003e\u003ch3\u003e\u003cb\u003eAI浪潮，一场个人救赎\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"SaOVQqte\"\u003e“AI浪潮对我而言，是一种救赎。”\u003c/p\u003e\u003cp data-pid=\"YjEsXDj_\"\u003e当ChatGPT横空出世，陈冕感到自己被“点亮”了。这不仅仅是看到了一个新的风口，更重要的是，\u003cb\u003eAI让过往十年看似碎片化的经历，瞬间有了意义。\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"ehl5FIh8\"\u003e他在不同大厂、不同赛道积累的经验，让他对用户需求、商业模式、团队管理有了更立体的认知。\u003c/li\u003e\u003cli data-pid=\"aA8l12ns\"\u003e他经历的失败，让他对风险和人性有了更清醒的认识。\u003c/li\u003e\u003cli data-pid=\"pH6PDziJ\"\u003e他内心的“产品之魂”和“战斗欲望”，终于找到了一个足够广阔、足够深刻的战场。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Fdtotwtt\"\u003e他意识到，AI不是移动互联网的延伸，而是一场堪比“信息化”的“智能化”革命。它不是简单的场景拓展，而是生产力工具的根本性变革。他判断，这是一个比移动互联网更大的机会。\u003c/p\u003e\u003cp data-pid=\"V_yIP_5S\"\u003e那一刻，过往所有的迷茫、不甘和愤怒，都转化成了创业的燃料。他不再犹豫，果断辞职，创立Lovart。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e战斗打法：给AI创业者的核心洞察\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"bvUZS5YC\"\u003eLovart的火爆并非偶然，背后是陈冕基于过往经验总结出的一套清晰打法。\u003c/p\u003e\u003cp data-pid=\"1x0CCa3e\"\u003e\u003cb\u003e1. 垂直、垂直、再垂直：避开巨头，做深价值\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"8iGGA9iS\"\u003e在所有人都涌向通用大模型和Agent时，陈冕的第一天就确定了方向：\u003cb\u003e做垂直领域的创作工具。\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"wKi9Vo8N\"\u003e\u003cb\u003e避开主航道：\u003c/b\u003e 他认为，通用领域（如搜索、Office）必然是巨头的主战场，创业公司硬碰硬毫无胜算。\u003c/li\u003e\u003cli data-pid=\"j80fmHA5\"\u003e\u003cb\u003e深耕价值链：\u003c/b\u003e 垂直领域有独特的行业知识（Domain Knowledge）、特定的工作流（Workflow）和专业的用户交互界面。这些是通用模型难以渗透的壁垒。\u003c/li\u003e\u003cli data-pid=\"VqlpxpaE\"\u003e\u003cb\u003e不迷信模型：\u003c/b\u003e “AI创业公司的能力，应该在模型之外。” 他强调，真正的壁垒在于你如何结合行业认知，为用户提供差异化的数据、交互和价值。Lovart的“画布+对话框”界面，就是为了模拟设计师与客户的真实沟通场景，而不是简单的聊天。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Lth47cJi\"\u003e\u003cb\u003e2. 抓住时机，品效合一：引爆流量的关键\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"xqE3Xi1F\"\u003e陈冕认为，在AI时代，新产品的发布时机至关重要。\u003c/p\u003e\u003cp data-pid=\"VtyxuaNi\"\u003e“在合适的时机，一个创新的产品可以实现‘品效合一’。”\u003c/p\u003e\u003cp data-pid=\"qPD4LiDt\"\u003eLovart的发布就是一次教科书式的案例。在海外设计圈和AI圈对AIGC工具充满期待但又缺乏惊艳产品时，Lovart横空出世。产品本身足够酷，迅速在Twitter上形成了病毒式传播，连马斯克都点了赞。\u003c/p\u003e\u003cp data-pid=\"6lPk7lDc\"\u003e这一波巨大的自然流量，为他们省下了天价的获客成本，也迅速确立了在创作AIGC领域的领先心智。\u003c/p\u003e\u003cp data-pid=\"ZTCBCnLB\"\u003e\u003cb\u003e3. 战斗精神与“现实扭曲力场”\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"7ju0iLIJ\"\u003e“我可能比一鸣更‘P’(感性)一点，更注重感受和感性。”\u003c/p\u003e\u003cp data-pid=\"RktC_vG2\"\u003e陈冕不认为自己是一个运筹帷幄的“老谋深算”型CEO，他更像一个充满激情、能感染团队的“战斗领袖”。他每天工作到凌晨三点，不断在团队群里分享思考、激发热情。\u003c/p\u003e\u003cp data-pid=\"0WsPHgJU\"\u003e他认为，在激烈竞争中，CEO的这种“现实扭曲力场”至关重要。它能让团队在面对困难和不确定性时，依然保持高昂的斗志和凝聚力。\u003c/p\u003e\u003cp data-pid=\"YYTpD7-4\"\u003e“你必须相信，你正在做一件能够改变世界的事情。这种信念，会让你在最困难的时候，也能找到坚持下去的理由。”\u003c/p\u003e\u003ch3\u003e\u003cb\u003e结语：大幕才刚刚拉开\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"QhQglqU4\"\u003e从被资本抛弃的失意者，到引领潮流的明星创业者，陈冕的故事，是关于“看见、相信、然后做到”的最好诠释。他用亲身经历告诉我们：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"KE6IVCVq\"\u003e\u003cb\u003e经验不会白费：\u003c/b\u003e 所有的失败和探索，都会在未来的某个时刻，成为你最宝贵的财富。\u003c/li\u003e\u003cli data-pid=\"K9WTx5ow\"\u003e\u003cb\u003e常识大于风口：\u003c/b\u003e 尊重商业规律，才能在浪潮中站稳脚跟。\u003c/li\u003e\u003cli data-pid=\"GkL4a4FK\"\u003e\u003cb\u003e战斗永不停止：\u003c/b\u003e 创业是一场极限挑战，唯有保持饥渴和激情，才能穿越周期。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"QlC3dJll\"\u003e正如陈冕自己所说，他最快乐的时刻，是“当你预见到的未来，真的在你手中发生了”。今天，AI的浪潮才刚刚拉开大幕，对于所有心怀梦想的创业者来说，这或许正是创造属于自己未来的最好时代。\u003c/p\u003e","is_labeled":false,"visited_count":522,"thumbnails":["https://pic1.zhimg.com/50/v2-7d041b9a974775721ff9c80425f3a846_720w.jpg?source=b6762063"],"favorite_count":16,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1919807521729779188}","attached_info":"CvoFCOfTv8rry6mZjQEQBxoJMjU5Mzc5MjY2IJiD2sIGKAcwAEBUSkEKLFRTX1NPVVJDRV9UV09UT1dFUl9TSE9SVElOVEVSRVNUX1JFQ0FMTF9URVhUEgEwGAAgADoKeyJyYXciOiIifVoGMTQ4MjI3YiBiMWVhMWZlOGMyOTVkOTdkZGM1MTY3YzM2ZGQxNjc2N3ITMTkxOTgwNzUyMTcyOTc3OTE4OIoBBXNtYXNoqgEJcmVjb21tZW5kwgEgY2EyMmY1YTY2OGE3NTU0YTRhZDM4ODdiOGUzNjI3MTjyAQoIDBIGTm9ybWFs8gEoCAoSJDViNDFjNmI5LTg5NmQtNGVkOS1hMWI5LWVmNTQ5MmZiYTk1YfIBBggLEgIxNYICAIgC9oXMzfoykgIgY2EyMmY1YTY2OGE3NTU0YTRhZDM4ODdiOGUzNjI3MTiaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxl2gIsVFNfU09VUkNFX1RXT1RPV0VSX1NIT1JUSU5URVJFU1RfUkVDQUxMX1RFWFToAgP6AgtOT1JNQUxfRkxPV4oDIDA5NmU4YjQyODk1NjRlYzE5OTk0YjdjODYyNzMxYzIzmgMNCgJ2MhAAGgVvdGhlcqgDigTYAwDqAxpmZWVkX2F0dG1fdHdvdG93ZXJfdjJfdGV4dPoDThIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgCEKQQGPYGIiN2Mi1mZGU0YThlYTk0YzdjMmE3OGFjZjc2MTI4YzJjZjA1M4AEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAoFaXtT+BBQAAAAAAAAAAiQV+opJji5bSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUPkAYAoAZaqAYBkgIuCgkyNTkzNzkyNjYSEzE5MTk4MDc1MjE3Mjk3NzkxODgYByIKSU1BR0VfVEVYVA==","action_card":false},{"id":"85_1750898770.973","type":"feed","offset":85,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898770,"updated_time":1750898770,"target":{"id":"1905118809741858143","type":"answer","url":"https://api.zhihu.com/answers/1905118809741858143","author":{"id":"c17577d34038a30d7e2c8ee5cd3e6a60","url":"https://api.zhihu.com/people/c17577d34038a30d7e2c8ee5cd3e6a60","user_type":"people","url_token":"18-66-52-46-37","name":"荒野大神","headline":"","avatar_url":"https://pic1.zhimg.com/50/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":286,"is_following":false,"is_followed":false},"created_time":1746995719,"updated_time":1747129250,"voteup_count":341,"thanks_count":39,"comment_count":18,"is_copyable":false,"question":{"id":"404483426","type":"question","url":"https://api.zhihu.com/questions/404483426","author":{"id":"9ecf373b76dcd8e565bacee8cc1d6f46","url":"https://api.zhihu.com/people/9ecf373b76dcd8e565bacee8cc1d6f46","user_type":"people","url_token":"liu-yan-95-25-81","name":"华妹子","headline":"躺平看热闹，闲坐吃西瓜","avatar_url":"https://picx.zhimg.com/50/463137b783390c0ab054c7d537fdc3da_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":12,"is_following":false,"is_followed":false},"title":"为什么互联网大厂的工资那么高？","created":1593603549,"answer_count":0,"follower_count":0,"comment_count":4,"bound_topic_ids":[99,18459,39097],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"thumbnail":"https://pic1.zhimg.com/50/v2-bbfaeed10ae5932748ab957c76b027e5_720w.jpg?source=b6762063","excerpt":"先说结论:因为互联网大厂拥有权力，因为他们提供了当下社会稀缺的资源。 首先，为什么大厂工资高？那是因为他们有钱，腾讯收入6600亿元，阿里9000多亿，同时大厂的利润率还高，腾讯50%，网易29%，字节20% ，那些制造业，普遍只有5%上下。 那么，大厂为什么辣么有钱？本质上是他们拥有权力。《权力与特权》这本书里提到，某个经济学研究，研究了200年以来由穷变富的人，95%以上是因为当了骑士，教士，国王的官僚，获得了权力。一…","excerpt_new":"先说结论:因为互联网大厂拥有权力，因为他们提供了当下社会稀缺的资源。 首先，为什么大厂工资高？那是因为他们有钱，腾讯收入6600亿元，阿里9000多亿，同时大厂的利润率还高，腾讯50%，网易29%，字节20% ，那些制造业，普遍只有5%上下。 那么，大厂为什么辣么有钱？本质上是他们拥有权力。《权力与特权》这本书里提到，某个经济学研究，研究了200年以来由穷变富的人，95%以上是因为当了骑士，教士，国王的官僚，获得了权力。一…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"7lYhXPXI\"\u003e先说结论:因为互联网大厂拥有权力，因为他们提供了当下社会稀缺的资源。\u003c/p\u003e\u003cp data-pid=\"LrzhYG8_\"\u003e首先，为什么大厂工资高？那是因为他们有钱，腾讯收入6600亿元，阿里9000多亿，同时大厂的利润率还高，腾讯50%，网易29%，字节20% ，那些制造业，普遍只有5%上下。\u003c/p\u003e\u003cp data-pid=\"PwozgOuD\"\u003e那么，大厂为什么辣么有钱？本质上是他们拥有权力。《权力与特权》这本书里提到，某个经济学研究，研究了200年以来由穷变富的人，95%以上是因为当了骑士，教士，国王的官僚，获得了权力。一句话，无特权者不发财，有钱只是表象，背后的特权才是根源。\u003c/p\u003e\u003cp data-pid=\"bmwFornF\"\u003e没有特权，没有垄断，你想发大财，那是不可能的。那么，互联网大厂是怎样获得特权的呢？\u003c/p\u003e\u003cp data-pid=\"e4ytr7Q_\"\u003e我原来在研究院上班的时候，有一张特别流行的图，大意是说，2009年的时候，全球市值最高的十家公司，都是一些工业企业，能源公司，到了2018年，全球市值最大的十家公司，大部分都是（美国）互联网大厂。\u003c/p\u003e\u003cp data-pid=\"qsefhQ90\"\u003e显然，这不是大厂的技术造成的，因为2009年已经有互联网了，而且很成熟。那么，这段时间发生了什么？没错，移动互联网。\u003c/p\u003e\u003cp data-pid=\"TRMdLHUO\"\u003e一夜之间，大家上网的方式从电脑变成了手机，别小看这个转变。\u003c/p\u003e\u003cp data-pid=\"bImK5gFN\"\u003e工信部有个报告，结论是正常人每天使用的APP，不超过10个。也就是说，十亿人每天就盯着这十个App，他们每天6小时都花在这上面。成为其中之一，你就有了十亿人的注意力，腾讯有十亿用户，就有十亿人的注意力。\u003c/p\u003e\u003cp data-pid=\"eWRN6RoD\"\u003e电脑时代不是这样的，你可以开好多个网页，淘宝和qq还可以同时存在于一个界面，什么叫挂QQ，80，90后应该很熟悉。\u003c/p\u003e\u003cp data-pid=\"k3zvnZFO\"\u003e从电脑到手机，还有一个转变就是用户多了一个数量级。用电脑，你得会点IT知识吧，至少会拨号上网，会打字吧？这就难住了很多人。所以2000年的时候，CNNIC统计，全国互联网用户只有几百万，最多一千万，60%的用户拥有硕士以上学历，95%以上本科学历。\u003c/p\u003e\u003cp data-pid=\"0CziT7Ta\"\u003e手机出现的原因之一，是它可以降低用户门槛，简单说，电脑不见得人人会用，但是手机APP，猩猩都会用，因为你只要会戳就行了，其他都不需要。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-c4df3e352ab3c0ed51d52d560734b4d2_1440w.jpg\" data-rawwidth=\"1080\" data-rawheight=\"2400\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-e5c08755a57949bfd5be0edd7e312e0c\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-fdd50ca3ea7d0c8f1fc481401795ef8e_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-c4df3e352ab3c0ed51d52d560734b4d2_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"tisWeigH\"\u003e猩猩都会玩手机，不是文学修辞，是真的，实际发生的。不是猩猩聪明，而是大厂需要用户越来越傻【】，设备和操作系统需要越来越傻【】，这样才能有足够多的用户，毕竟，全国本科人口多少？\u003c/p\u003e\u003cp data-pid=\"5suyEnyl\"\u003e也就是后来，我才知道，世界上存在有手机而没有电脑的人，这样的人有很多，这令我大为震惊。这样，腾讯的用户终于到了10亿，脸书到了30亿。\u003c/p\u003e\u003cp data-pid=\"xjU0FEWu\"\u003e啥意思呢？10亿人每天用你的产品，注意是每天，每天花几个小时，你就有了10亿流量，和流量的分配权，这就是注意力控制权。\u003c/p\u003e\u003cp data-pid=\"dQJe0YSU\"\u003e你可以把流量给王小姐，也可以给李大波。一家店，你可以让这家店日进斗金，也可以一个按钮让他一毛不赚（阿里格兰仕事件）；一个老太太在微信放了30万，你可以一键封了她，让她破产都不知道从何说起（福建钱包事件）。而这样的人有10亿，这样的店有几千万，呵呵，你想想，这是多大的权力？能赚多少钱？\u003c/p\u003e\u003cp data-pid=\"eXjtLX0t\"\u003e互联网大厂赚钱，不是因为别的，就是因为偶然获得了权力，而他们并不知道权力是从哪里来的，以为技术天然可以攫取权力，技术会带来财富，所以现在他们又迷信起AI来了。\u003c/p\u003e\u003cp data-pid=\"RLP-k-Ys\"\u003e不垄断，你永远不赚钱，就是这样。垄断有五种形式，这里说一下前三种:\u003c/p\u003e\u003cp data-pid=\"D0w2Y-j1\"\u003e第一种，就是行z垄断，就是某个生意，只能我做或者我指定的人做，其他人做了咋办？抓起来就地打死。比如烟草，石油，不拉不拉。。进了这种单位，你简直不要太爽。\u003c/p\u003e\u003cp data-pid=\"RTkAGOLH\"\u003e第二种，就是竞争垄断，比如苹果手机屏幕的前十个APP位置，谁占了就是谁的，不是每个人都能成功的，全国有几百万个app，绝大多数都进不了第一屏。\u003c/p\u003e\u003cp data-pid=\"mUA29phh\"\u003e第三种，自然垄断，建了一套，就没必要再建第二套。比如铁路网，电网，当然，腾讯也有点类似，你敢删掉淘宝，因为你可以用拼多多，但你敢删掉微信吗？你不敢，因为你的社会关系都在上面，而且没有替代，比如你妈用微信，你爸用微信，你总不能用米聊。\u003c/p\u003e\u003cp data-pid=\"xHTHnqBf\"\u003e进了这三种单位，尤其前两种，你的工资福利待遇不会低，因为只有有钱的单位才发得起高工资。\u003c/p\u003e\u003cp data-pid=\"L-_CutqQ\"\u003e互联网大厂是个特例，因为【】太爷当时不知道这玩意会长这么快，还没来得及垄断，就几万亿了，这一切也就十年时间。\u003c/p\u003e\u003cp data-pid=\"Uvj9Ni-f\"\u003e那互联网大厂就没有一点贡献吗？有，它提供了这个社会的稀缺产品-秩序。\u003c/p\u003e\u003cp data-pid=\"mBYe5aUb\"\u003e你一个卖红薯的老太太，在村里卖红薯，连自己都养不活，去天桥底下卖红薯，可以供儿子上大学，但是，你还得搞定城管和当地小混混对不对？\u003c/p\u003e\u003cp data-pid=\"RsYbHSPc\"\u003e去网上卖就不用了（本人不懂外卖或电商，只是打个比方，别杠细节），平台会保证你的网店不会被人砸了，也会对纠纷进行干预和裁判。\u003c/p\u003e\u003cp data-pid=\"vM_kw2LU\"\u003e你作为消费者，去买东西觉得质量不好，以前和商家理论一般都要闹上法院，法院一般不立案，至少也得去消协对不对？拼多多可以给你仅退款。\u003c/p\u003e\u003cp data-pid=\"dHlXLlmY\"\u003e你别管这些事他们做的对不对，判得合不合理，那是另一回事，至少有人处理这事，以前是没人处理的。\u003c/p\u003e\u003cp data-pid=\"VTIqEL4F\"\u003e从这个意义上讲，互联网大厂赚的这个钱，实际上是保护费，和黑社会一个性质。\u003c/p\u003e\u003cp data-pid=\"BGJcad8h\"\u003e正常社会，除了杀人放火之外，一些灰色纠纷，比如遛狗不牵绳什么的，是不会闹到法院的，很简单，因为法官不够用。\u003c/p\u003e\u003cp data-pid=\"5YUTMOSZ\"\u003e这些事一般都是黑社会解决的，当然你也可以说得好听点，叫社团。黑社会给你解决这事，当然要收点保护费。90年代广东有个小摊主，让城管打了，东西也被抢了，结果第二天城管被一伙纹身哥暴揍一顿，帮摊主抢回了东西，因为摊主交了保护费。\u003c/p\u003e\u003cp data-pid=\"nHlNciGo\"\u003e你说美团剥削，是，他是剥削，曾经有个卖炊饼的大妈告诉我（正式调研），他在北京卖炊饼，一年能卖100万，但是店租40万，房租8万，原料成本20万，平台服务费12万（算上配送），杂费几万，最后也就赚10万，你说谁更剥削？40万那大哥，他罩不罩你？\u003c/p\u003e\u003cp data-pid=\"K6EREgpg\"\u003e明白了吗？互联网大厂为啥赚钱？因为他们无意识的提供了这个社会的稀缺产品-秩序。曾经有大厂高管提议平台交出裁判权，说我们不要这种权力，太麻烦了，我们只想好好服务商家，好好赚钱。\u003c/p\u003e\u003cp data-pid=\"YrI2wN2X\"\u003e这些傻【】，他们都不知道自己挣的是啥钱。\u003c/p\u003e\u003cp data-pid=\"yap7lNSr\"\u003e互联网的成本是什么？人，所以马云说，你去参观工厂，还能看到产品，你来互联网公司参观，只能看到人。\u003c/p\u003e\u003cp data-pid=\"CpK9ZoFE\"\u003e所以，互联网公司都倾向于让一个人干三个人的活，但给他两个人的工资。\u003c/p\u003e\u003cp data-pid=\"wystTsst\"\u003e垄断企业讲人效，这两Buff一叠加，就是大厂员工拿高新了。\u003c/p\u003e\u003cp data-pid=\"1cakwVBj\"\u003e但是，你确定你能拿多久？有个大厂HR说过，我工作这么久，只办过一个退休的手续，然后他就被裁了（指HR被裁了）。有赞的员工，超过40岁的，只有个位数。\u003c/p\u003e\u003cp data-pid=\"l3NLP-JB\"\u003e以前，大厂很喜欢讲“赋能”这个词，这是有原因的。往高级一点说，就是他们觉得国家不会允许他们存在多久，所以要证明自己的价值，比如提供了多少就业岗位，赋能了多少中小企业。这样，领导下笔的时候，会想一想。\u003c/p\u003e\u003cp data-pid=\"xoRguban\"\u003e往低级一点说，就是他们觉得劳资这么挣钱，肯地是因为劳资牛逼。所以我肯定创造了不可替代的社会价值。\u003c/p\u003e\u003cp data-pid=\"pcXck4_j\"\u003e这是错的，因为至少从2010年以来，全要素生产率（TFP)就是下降的，2014前后，降到了负值。\u003c/p\u003e\u003cp data-pid=\"fP128ugb\"\u003e也就是说，互联网大厂没有根本上带来社会效率的提升，这么多年来效率反而下降了。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-2136f5cb116fb619d0f014e543669993_1440w.jpg\" data-rawwidth=\"1838\" data-rawheight=\"1304\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-de60cec68debb86f135ed88dc10f38aa\" data-default-watermark-src=\"https://pic3.zhimg.com/v2-338ab8e4771d4b5e28571d5d531dbe84_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"1838\" data-original=\"https://pic4.zhimg.com/v2-2136f5cb116fb619d0f014e543669993_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-a4fe1e5b14f748f39b02b170b2d4c309_1440w.jpg\" data-rawwidth=\"716\" data-rawheight=\"476\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-283276be6a90b1952d207f5b3cda3a41\" data-default-watermark-src=\"https://pic4.zhimg.com/v2-6164d1dafa0dc59e228ed859c5392451_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"716\" data-original=\"https://pic2.zhimg.com/v2-a4fe1e5b14f748f39b02b170b2d4c309_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"6-wlNIXG\"\u003e上图可以清晰的看见，代表社会效率的指标，全要素生产率（TFP)对经济的贡献度，2010年以后迅速下降。而劳动力（工资）对经济的贡献度从很少，降到了几乎没有。\u003c/p\u003e\u003cp data-pid=\"MSrWhchD\"\u003e啥意思呢？就是牛马从节衣缩食勉强度日过渡到能不花钱就不花钱，说的好听点就叫躺平，说的难听点叫维持呼吸。\u003c/p\u003e\u003cp data-pid=\"U-TLa8Uu\"\u003e2014年前后，TFP转为负数，2015年前后，大厂传出了996这个词，似乎一开始是华为。为啥？因为当效率下降，你还要赚钱，就必须延长劳动时间，实际上就是变相降薪，劳动时间延长了，工资不变，不就是降薪了。而先从大厂开始，是因为大厂本身是个劳动密集型产业，他的大部分成本，都是人。\u003c/p\u003e\u003cp data-pid=\"X_QuT3ZZ\"\u003e2020年前后，中国互联网用户达到10亿，超过了成年人的人口总数，然后，大厂就增长不动了。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-bc3b0030848de6db8927527498232b00_1440w.jpg\" data-rawwidth=\"672\" data-rawheight=\"405\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-c6e8a7d1ff32120c40140ee3095d0602\" data-default-watermark-src=\"https://picx.zhimg.com/v2-1f8a8fe8f1a3b75a964893e6e84e4fe3_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"672\" data-original=\"https://pica.zhimg.com/v2-bc3b0030848de6db8927527498232b00_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"weR79Cby\"\u003e因为很简单，没人了，你大厂交易额怎么增长的？多了一个注册用户，这个用户在平台买了一根10块钱的香肠，你的交易额就增长了10块。现在连猩猩都上网了，没有用户了，就没有增长了，除非老用户多花钱，可是前面说过，牛马连饲料都不够了，又怎么可能多花钱呢？所以拼夕夕迅速崛起了。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-f8e5d9e43650258602045a0079987ee3_1440w.jpg\" data-rawwidth=\"511\" data-rawheight=\"288\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-74b183955f596016141fb69964a710ad\" data-default-watermark-src=\"https://picx.zhimg.com/v2-7ded18845d76257f4d32ea2378c51713_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"511\" data-original=\"https://pic2.zhimg.com/v2-f8e5d9e43650258602045a0079987ee3_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"V91sCT8A\"\u003e2020年前后，年度购房支出达到了城镇人口年度收支结余的95%以上，啥意思？6个花生已经榨干，没有油了，啥意思？就是你再榨，也榨不出东西了。（数据来自路财主的报告）\u003c/p\u003e\u003cp data-pid=\"TYSTf_0Z\"\u003e我曾经说过，互联网大厂的增长，取决于没有上网的人，所以你会看到，2018年前后，支付宝老年客服，春晚教爸妈上淘宝陆续出现了，为啥？没用户了，拉老人；腾讯各种游戏合规，字节各种青少年合规出现了，为啥？没用户了，拉小孩，小孩要多小？7岁，对，你没听错，用户的年龄已经被拉到了7岁，老美惊呼现在的小孩10岁就要参加成人礼，因为他有了第一步手机。老美真的拿衣服，我们这里小孩注册账号的年龄，平均是7岁，秦朝规定服徭役的年纪，古语称“使小男”，意思就是说，一个男孩过了7岁，就可以使用了，再拉，就得去产房了。\u003c/p\u003e\u003cp data-pid=\"LIcHW3PO\"\u003e收入涨不动了，于是就要降本增效，怎么降本？996，优化，35，怎么增效？营收不变，本下去了，不就增效了么？以前2个人干6个人的活，给4个人的工资，优化掉一个，一个人干6个人的活，给你1.5个人的工资，你干吗？很多人抢着干。\u003c/p\u003e\u003cp data-pid=\"ZSR6km83\"\u003e所以你会看见，百度收入降低了2%，利润却增长了100%，阿里收入增长了5% ，利润提高了300%。以前高增长的时候，资本市场不要求你赚钱，现在没增长了，市场就要求你立刻赚钱，怎么赚钱？降本增效，比如网易砍掉好几个工作室，阿里两年裁掉6万人。\u003c/p\u003e\u003cp data-pid=\"rU2EJJSJ\"\u003e所以为什么近年来大厂总是流行“表演型工作”？很简单，大厂评绩效的时候，都喜欢评判，我有什么价值，你有什么价值？你凭什么得这个绩效？\u003c/p\u003e\u003cp data-pid=\"kljmIfWU\"\u003e为什么会这样？因为都没价值。用户都见顶了，你能创造毛的价值？所以，老板觉得谁有价值，谁就有价值。\u003c/p\u003e\u003cp data-pid=\"fPWfKsKd\"\u003e简单的说，大厂从自然选择状态，进入了人为选择状态。韩信在秦末，是有价值的，因为那时是自然选择，你不行很快被敌人干掉。但进入汉初，韩信就没价值，因为已经进入人为选择阶段，历史使命已经完成，没事了，这时皇上认为谁有价值，谁就有价值。\u003c/p\u003e\u003cp data-pid=\"lerqaeBq\"\u003e所以你会看到，老板的注意力成了稀缺资源，大家都卖力表演，明明半天能干完的活，非要开一天会，晚上6点才开始干活，干到12点，看上去好像很忙，工作群里疯狂灌水，其实都是争夺老板的注意力。老板也觉得挺好，钱没白花。\u003c/p\u003e\u003cp data-pid=\"1Ns22-sr\"\u003e你真的以为，平台员工拿高薪，他们不知道不是长久之计？他们当然知道。\u003c/p\u003e\u003cp data-pid=\"7WTjc-pD\"\u003e长期996，只有两种结局，一种是猝噶，一种是35退休，你如果知道是这样，你会怎么办？当然是短期内，能吃多少吃多少。\u003c/p\u003e\u003cp data-pid=\"GKnJYEKi\"\u003e你一个员工，流量能给王小姐，也能给李大波，你33了，秉公办理，后年退休，你会怎么办？\u003c/p\u003e\u003cp data-pid=\"JnbPub8h\"\u003e嗯，这就是为啥杭州某互联网公司员工一年能贪一个亿。腾讯，美团，网易，每年抓进去的人上千，这就是马化腾说的，触目惊心。据说抖音电商有一段时间电商运营只敢用女的，拿衣服，后来发现女的吃起来，比男的猛多了。。。。\u003c/p\u003e\u003cp data-pid=\"SESBpN6R\"\u003e总结:大厂工资高，因为垄断利润+产业性质+降本增效。而且大厂里的人都知道，这活就是个临时工，和农民工没有本质区别，带金链子的链狗，还是随时放生的那种。\u003c/p\u003e\u003cp data-pid=\"yb0INfDJ\"\u003e梯子外普通人的出路只有两条，包括大厂内和大厂外的，要么前半生在垄断企业拿到足够籽油的钱，要么自己创业自己干。想996过完一生，不存在的，就算可以，又有何意义呢？将来你孙子问你，爷爷，普大帝打乌克兰的时候，你在做什么呢？你难道只能告诉他，爷爷在大厂996呢。\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"d9X57f3-\"\u003e热度竟然不少，那我更一下：\u003c/p\u003e\u003cp data-pid=\"zrejFphS\"\u003e很多盆友说大厂垄断的事，我科普一下，因为我也参与过好几个反垄断案子。\u003c/p\u003e\u003cp data-pid=\"qCAZ1E_z\"\u003e垄断的关键是什么？没有进入，没有替代。比如你们街道只有一家拉面馆，你想吃拉面，没有别的可以选择，就是无替代，这个街道10年内，也没有出现新的拉面馆，你就只能去他家吃，他一碗光头面卖1000块，你也得买。\u003c/p\u003e\u003cp data-pid=\"NX8Vbh1s\"\u003e首先，垄断不等于坏，或者说，不是所有垄断都是坏的\u003c/p\u003e\u003cp data-pid=\"waDXC3ag\"\u003e前面说了五种垄断：\u003c/p\u003e\u003cp data-pid=\"vS_wu_CV\"\u003e第一种，行Z垄断，就是某个生意，只能我自己做或者我指定的人做，比如盐铁专卖。贩卖私盐，满门抄斩，啥意思？就是只有我能卖盐，我没允许你卖盐，你卖了，灭你满门。\u003c/p\u003e\u003cp data-pid=\"uhJGZKVC\"\u003e第二种，竞争垄断，就是评论区说的，那么多APP，几百万个，我胜出了，怎么了？你有本事你也胜出啊。\u003c/p\u003e\u003cp data-pid=\"62cESBDu\"\u003e第三种，自然垄断，比如铁路网，北京到上海，一条高铁线路够了，你要再建一条，成本巨高，也没必要。\u003c/p\u003e\u003cp data-pid=\"rHtP2Eo2\"\u003e第四种，专利创新垄断，比如我发明了晶体管，别人问我要技术，我不给他，或者要收钱，以至于10年内只有我一家卖晶体管，别人进不来，你也只能在我家买。\u003c/p\u003e\u003cp data-pid=\"ZpXlOIID\"\u003e第五种，资源禀赋垄断，比如邓丽君垄断了演歌，普尔垄断了普洱茶，湘潭问为啥我不能生产普洱茶？酒吧歌星问为什么观众只爱听邓丽君的歌，人家就是有那个嗓子，咋办？你没法进入。\u003c/p\u003e\u003cp data-pid=\"qSj2ACGd\"\u003e只有第一种垄断是混乱邪恶的。\u003c/p\u003e\u003cp data-pid=\"KilHSh1O\"\u003e《反垄断法》定罪有几个步骤：1确定相关市场 2确定支配地位 3 确定垄断行为\u003c/p\u003e\u003cp data-pid=\"gnws_nx0\"\u003e三者缺一都不能定罪，通俗意义上的垄断是你有垄断地位，你垄断，但是你没干坏事，就不能说你有罪。\u003c/p\u003e\u003cp data-pid=\"DcERYJdM\"\u003e所以脸书说我无罪，TikTok也是他的竞争者，如果相关市场这个饼从社交市场画到流媒体，那他的市场份额就不够大，司法部就说这太荒谬了，他那叫短视频，能一样么？\u003c/p\u003e\u003cp data-pid=\"XI6p7_r2\"\u003e所以谷歌说，我无罪，因为亚马逊也能搜索啊，我哪里垄断了搜索市场?法官说，人家那是电商，我想正常人还是能分清电商和搜索引擎不是一个东西的。\u003c/p\u003e\u003cp data-pid=\"rDL68xvc\"\u003e所以腾讯说，我无罪，什么叫即时通讯？你发一条信息，别人能收到并给你回，那微博也阔以啊，微博也能发私信啊，这些都阔以啊，这个市场足够大啊，我很小啊，所以我没有支配地位啊。\u003c/p\u003e\u003cp data-pid=\"8xYnIKcP\"\u003e国内两个知名反垄断案，一个阿里，一个美团，说实话，他们就像《算死草》里的阿宽，你问他丑不丑，他很丑，就像星爷说的，觉得他们极度猥琐，面目可憎的，请举手。但是他丑不丑和他杀没杀人是两回事，你不能因为他长得猥琐，就说他是杀人犯。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-f521b078b3201acc989760d2a90c981d_1440w.jpg\" data-rawwidth=\"578\" data-rawheight=\"782\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-34b9954d670be4a7068d7f50165aaf34\" data-default-watermark-src=\"https://pic2.zhimg.com/v2-ebe108c4dda8c10e472151f07f4b9da3_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"578\" data-original=\"https://pic2.zhimg.com/v2-f521b078b3201acc989760d2a90c981d_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"ZKHukNc2\"\u003e   比如阿里这种企业，他市场份额从2013年开始就是下降的，他垄断个毛线，他垄断能被拼夕夕打成这样？为什么？很简单因为中国的快递网络没有控制在阿里手里，你从没听说过圆通不能给夕夕送货对不对？所以，三通一达很快就能把夕夕和抖音养起来，阿里京东一点办法都没有。所以芒格才会说他是该死的搞零售的，啥意思？就是你不能垄断，我投资你真是有病。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-36027d97f820a117c2734c84535a29b2_1440w.jpg\" data-rawwidth=\"551\" data-rawheight=\"407\" data-size=\"normal\" data-caption=\"\" data-original-token=\"v2-09e515035622fbace67a7c2b8b260986\" data-default-watermark-src=\"https://picx.zhimg.com/v2-f84b5b2b03ad418e380f8cd3bfda195d_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"551\" data-original=\"https://pica.zhimg.com/v2-36027d97f820a117c2734c84535a29b2_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"9gBm7xEe\"\u003e       今天的美团同理，京东能够进入，恰恰说明他不垄断，否则还有京东啥事。\u003c/p\u003e\u003cp data-pid=\"w-pzoNsk\"\u003e   美团和阿里为什么被反垄断，因为他们都是B2C企业。就是晏子说的那个理论，看你得罪的是谁，晏子说，第一个任期，我对百姓好，对士绅坏，结果国王骂我是个贪官，第二个任期，我盘剥百姓，讨好士绅，结果国王夸我真是清官。对，关键就是看你得罪的是谁，你得罪了老百姓，没事，因为老百姓没有话语权；可你要是得罪了士绅，国王身边的人会整天说你不是好东西，因为他们有话语权。\u003c/p\u003e\u003cp data-pid=\"EQzLNREd\"\u003eB2C企业得罪的是谁？商家，商家和他们背后的人，有一定话语权，用不了一年，领导的桌子就会被告你的状子埋了，你要是领导，你会啥也不干吗？\u003c/p\u003e\u003cp data-pid=\"dBC83OvT\"\u003e如果你得罪了用户呢？没事，尤其是少数用户，用户跳楼都没事。\u003c/p\u003e\u003cp data-pid=\"GgImnwCg\"\u003e当然，这只是直接原因，不是根本原因，根本原因是啥？《wall street journal》，《New York Times》上面大把深度文章，自己看去吧。\u003c/p\u003e\u003cp data-pid=\"YVEzSmix\"\u003e还有人提到，钱也是一种权力，某种程度上，这没错。钱只是一种奖惩的权力，除了这种，还有五种权力，但一切权力的根源都是暴力，改开前，钱算个屁，改开后，钱可以算个屁了，无他。\u003c/p\u003e\u003cp data-pid=\"jw498bVr\"\u003e比如只有一台电脑，你想用，老王也想用，咋办？\u003c/p\u003e\u003cp data-pid=\"nPCGq0Xq\"\u003e你出100，老王出1000，老王用，这是市场经济\u003c/p\u003e\u003cp data-pid=\"4fwls5Aj\"\u003e你是科员，老王是处长，老王用，这是计划经济\u003c/p\u003e\u003cp data-pid=\"YseWWCv4\"\u003e你10点来排队，老王6点就来了，老王用，这是习惯法\u003c/p\u003e\u003cp data-pid=\"1qlhXPka\"\u003e几种分配机制交织，你就会知道，很多东西，你花钱买不到。\u003c/p\u003e\u003cp data-pid=\"SndJNWxm\"\u003e一切权力的根源，都是暴力，所以大厂跟官府杠，通常都被吊打。\u003c/p\u003e\u003cp data-pid=\"nN_WtH2h\"\u003e在不讲法制的国家，直接封掉就是，比如巴西和尼日利亚直接封掉X，印尼就不让苹果卖16，完全不跟你废话。\u003c/p\u003e\u003cp data-pid=\"NVZhn8aA\"\u003e在讲法制点的国家，通常就是出台一部法律，把大厂往死里罚，比如谷歌在欧洲已经被罚了差不多100亿欧元，这还不包括其他大厂，我都怀疑欧洲公务员基本不用干活了，每年光薅这几家大厂就够了。\u003c/p\u003e\u003cp data-pid=\"Tjeu18aS\"\u003e歐盟《数字市场法》中数字守门人概念的提出者林司机最早注意到，这些大厂是不能简单用反垄断规制的，因为反垄断是一种经济力量，而大厂拥有一种类行z权力，所以干脆不定义相关市场，直接划线点名。\u003c/p\u003e\u003cp data-pid=\"4-PK5mMM\"\u003e所以川普骂欧洲这是不公平征税，要我说，这哪里是征税了，这是抢劫好不好，你家动不动被抢几百亿，这已经是战争行为了。然而这些美国大厂，也是毫无节操可言，谷歌在川普上来之前，疯狂搞多元化，就是规定每个部门，按照比例晋升招聘少数族裔，运营部黑人比例不能低于15%（玩笑）。川皇一上任，立刻裁撤这些规定。脸书在2020年封掉川普的号，现在又舔着脸去求人家，说能不能和欧洲人谈谈，别搞我们了，一年罚出去十几亿，我们受不了。\u003c/p\u003e\u003cp data-pid=\"jqQOt6KB\"\u003e普通人可以进大厂，但你要想清楚，这个地方不是你的归宿。\u003c/p\u003e\u003cp data-pid=\"MW2l6tE-\"\u003e而且你要去的话，去那些第二曲线部门，不要去老业务部门。\u003c/p\u003e\u003cp data-pid=\"D5TwqhB0\"\u003e如果你在梯子内，15年前去大厂，你可能已经籽油，现在，你的职业周期很难超过3年，如果不是压抑到注定会神经病的话，先别动。\u003c/p\u003e\u003cp data-pid=\"sRx5yriG\"\u003e今年谷歌2个反垄断案子都输了，现在司法部在讨论怎么拆分谷歌，谷歌大概率不会被拆分的，当一个行业走进反垄断的时候，就是他盛极而衰的时候。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":25926,"thumbnails":["https://pic1.zhimg.com/50/v2-bbfaeed10ae5932748ab957c76b027e5_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-7115464679d22fb1f281ad6ce3f2950f_720w.jpg?source=b6762063"],"favorite_count":383,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1905118809741858143}","attached_info":"CooJCOfTv8rry6mZjQEQBBoJNzI2OTIxNjA1IIeUhMEGKNUCMBJAVUpBCixUU19TT1VSQ0VfVFdPVE9XRVJfU0hPUlRJTlRFUkVTVF9SRUNBTExfVEVYVBIBMBgAIAA6CnsicmF3IjoiIn1aCDUyMTYzMTI4YiBiMWVhMWZlOGMyOTVkOTdkZGM1MTY3YzM2ZGQxNjc2N3ITMTkwNTExODgwOTc0MTg1ODE0M4oBCTQwNDQ4MzQyNqoBCXJlY29tbWVuZMIBIGMxNzU3N2QzNDAzOGEzMGQ3ZTJjOGVlNWNkM2U2YTYw8gEKCAwSBk5vcm1hbPIBKAgKEiRiYjg1OGVmOS1jZjU1LTQ3OTAtOTc1NS1iN2FiOWZlOGIzZGTyAQYICxICMTWCAgCIAvaFzM36MpICIGMxNzU3N2QzNDAzOGEzMGQ3ZTJjOGVlNWNkM2U2YTYwmgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCFkFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhtJbnRlcmFjdGlvblNob3JJbnRlcmVzdFJ1bGXKAhhQZXJpb2RJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZdoCLFRTX1NPVVJDRV9UV09UT1dFUl9TSE9SVElOVEVSRVNUX1JFQ0FMTF9URVhU6AID+gILTk9STUFMX0ZMT1eKAyAwOTZlOGI0Mjg5NTY0ZWMxOTk5NGI3Yzg2MjczMWMyM5oDDQoCdjIQABoFb3RoZXKoA8bKAdgDAOoDGmZlZWRfYXR0bV90d290b3dlcl92Ml90ZXh0+gPoAhIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgCELgIGOASIiN2Mi1lNWMwODc1NWE1Nzk0OWJmZDViZTBlZGQ3ZTMxMmUwYzotCAQQrg4YmAoiI3YyLWRlNjBjZWM2OGRlYmI4NmYxMzVlZDg4ZGMxMGYzOGFhOi0IAhDMBRjcAyIjdjItMjgzMjc2YmU2YTkwYjE5NTJkMjA3ZjViM2NkYTNhNDE6LQgEEKAFGJUDIiN2Mi1jNmU4YTdkMWZmMzIxMjBjNDAxNDBlZTMwOTVkMDYwMjotCAIQ/wMYoAIiI3YyLTc0YjE4Mzk1NWY1OTYwMTYxNDFmYjY5OTY0YTcxMGFkOi0IAxDCBBiOBiIjdjItMzRiOTk1NGQ2NzBiZTRhNzA2OGQ3ZjUwMTY1YWFmMzQ6LQgEEKcEGJcDIiN2Mi0wOWU1MTUwMzU2MjJmYmFjZTY3YTdjMmI4YjI2MDk4NoAEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAZtYW51YWzCBAMxNzDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAGAhwcA/gQUAAAAAAAAAAIkFfqKSY4uW0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFD5AGAKAGW6gGA5ICLgoJNzI2OTIxNjA1EhMxOTA1MTE4ODA5NzQxODU4MTQzGAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"86_1750898770.781","type":"feed","offset":86,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750898770,"updated_time":1750898770,"target":{"id":"1921289925552210138","type":"article","url":"https://api.zhihu.com/articles/1921289925552210138","author":{"id":"acb36951daada520a426b822c9d1c020","url":"https://api.zhihu.com/people/acb36951daada520a426b822c9d1c020","user_type":"people","url_token":"lemonround","name":"猛猿","headline":"公众号：大猿搬砖简记","avatar_url":"https://pica.zhimg.com/50/v2-6304b8f8dd717ed99eeddd211d5714d1_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":29397,"is_following":false,"is_followed":false},"title":"万字长文图解Qwen2.5-VL实现细节","image_url":"https://picx.zhimg.com/v2-fa725e5f69c2994059b74b6cf094888d.jpg?source=7e7ef6e2\u0026needBackground=1","comment_permission":"all","created":1750853066,"updated":1750853066,"voteup_count":89,"voting":0,"comment_count":6,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"大家好，这篇文章是今年年初就写的了， 为了防止盗版猖獗，一直没有公开，只给我的同事和周边做框架开发的朋友们内部分享过。时隔半年，qwen2.5-VL应该已经渗入到mllm开发的各个场景中了，所以我选择在这个时候放出来。初读qwenVL的朋友们，可能都觉得不太好读，其实最绕的部分，应该是各种复杂的数据预处理（常洗数据的你我他应该都知道我在说什么 ），算法原理和模型架构上其实不难。所以我依然选择从源码中抽出图例，帮助大家…","excerpt_new":"大家好，这篇文章是今年年初就写的了， 为了防止盗版猖獗，一直没有公开，只给我的同事和周边做框架开发的朋友们内部分享过。时隔半年，qwen2.5-VL应该已经渗入到mllm开发的各个场景中了，所以我选择在这个时候放出来。初读qwenVL的朋友们，可能都觉得不太好读，其实最绕的部分，应该是各种复杂的数据预处理（常洗数据的你我他应该都知道我在说什么 ），算法原理和模型架构上其实不难。所以我依然选择从源码中抽出图例，帮助大家…","preview_type":"default","preview_text":"","content":"\u003cp data-pid=\"1fRyENg7\"\u003e大家好，这篇文章是今年年初就写的了，\u003cb\u003e为了防止盗版猖獗，一直没有公开\u003c/b\u003e，只给我的同事和周边做框架开发的朋友们内部分享过。时隔半年，qwen2.5-VL应该已经渗入到mllm开发的各个场景中了，所以我选择在这个时候放出来。\u003c/p\u003e\u003cp data-pid=\"0dh5_-_6\"\u003e初读qwenVL的朋友们，可能都觉得不太好读，其实最绕的部分，应该是各种复杂的数据预处理（常洗数据的你我他应该都知道我在说什么 ），算法原理和模型架构上其实不难。所以我依然选择从源码中抽出图例，帮助大家理解代码。\u003cb\u003e本文涵盖了window attention，动态帧率采样(dynamic fps sampling)，多模态rope等大家常关注的内容。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"zVw8gAML\"\u003e\u003cb\u003e最后再插句题外话，如果发现有人非法盗版，请和我说，谢谢！\u003c/b\u003e\u003c/p\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/654910335\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\"\u003e【必看】历史技术文章导航\u003c/a\u003e\u003chr/\u003e\u003ch2\u003e总览\u003c/h2\u003e\u003ch3\u003e1. 相比于qwen2-VL的改进\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"IvxkcKCC\"\u003e\u003cb\u003eWindow attention\u003c/b\u003e：\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"iJCiTxAQ\"\u003e在\u003cb\u003evit encoder\u003c/b\u003e部分，引入window attention，即一张图只在window范围内做双向attention，每个窗口的最大大小为112*112（即8*8个14*14的patch/token），这样可以进一步节省计算，详情参见5.3\u003c/li\u003e\u003cli data-pid=\"yswPdMwo\"\u003e如果一个窗口的尺寸小于112*112，也不会对他做padding，这样可以尽量保证图片在原生分辨率下做操作，详情参见代码注释\u003c/li\u003e\u003cli data-pid=\"H2_nWMML\"\u003e在vit enocder部分，只有4层用的是基于整张图的full attention，其余层用的都是window attention\u003c/li\u003e\u003c/ul\u003e\u003cli data-pid=\"USd92rVA\"\u003e\u003cb\u003e动态帧率采样(dynamic fps sampling)\u003c/b\u003e：即可以人为设定采样帧率，按这个帧率对原始视频进行采样，详情参见3.2\u003c/li\u003e\u003cli data-pid=\"0B1YIcaE\"\u003e\u003cb\u003e改进3D mrope在T维度上的position_id计算方式\u003c/b\u003e：从原来的默认值1，改成使用实际时间间隔进行加权计算，详情参见6.1\u003c/li\u003e\u003cli data-pid=\"Xghqojej\"\u003e大幅扩展了qwen2vl的预训练数据集，从1.2万亿tokens增加到4万亿tokens\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2. qwen2.5-VL训练的三个阶段\u003c/h3\u003e\u003cp data-pid=\"-2mo-NE9\"\u003e\u003cb\u003estage1: 从头训练一个vit encoder\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"RQO0h5P5\"\u003e使用的数据如下\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"7-S8Xt7x\"\u003e\u003cb\u003eImage captions\u003c/b\u003e：图像和对应的文本描述\u003c/li\u003e\u003cli data-pid=\"dm63RSMD\"\u003e\u003cb\u003eVisual knowledge\u003c/b\u003e: 涵盖名人、地标、动植物等识别数据，帮助模型积累视觉常识。\u003c/li\u003e\u003cli data-pid=\"xiPJcpl0\"\u003e\u003cb\u003eOCR数据\u003c/b\u003e：从图像中提取的文本信息\u003c/li\u003e\u003c/ul\u003e\u003cli data-pid=\"eRhWTPZR\"\u003e\u003cb\u003e使用clip预训练\u003c/b\u003e：使用（image, text）数据，在clip框架下进行预训练\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"Aq6xaH_O\"\u003e\u003cbr/\u003e\u003cb\u003estage2: vit 和 qwenvl decoder的联合预训练\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003estage3：长上下文预训练\u003c/b\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"upw8BwBy\"\u003e\u003cb\u003e视频和代理任务数据\u003c/b\u003e：为了处理更复杂的视觉-语言任务，强调对视频和代理任务的长时依赖理解\u003c/li\u003e\u003cli data-pid=\"v4oVw-fW\"\u003e\u003cb\u003e增加序列长度\u003c/b\u003e：将序列长度从8,192增加到32,768，使模型能够处理更长的上下文。\u003c/li\u003e\u003c/ul\u003e\u003chr/\u003e\u003ch2\u003e一、推理请求数据格式\u003c/h2\u003e\u003cp data-pid=\"ZAUmV-y1\"\u003e这里以“单条数据-图像”推理为例，更多例子请参见\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/QwenLM/Qwen2.5-VL\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e（例如视频推理，batch推理等）\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e处理image\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etransformers\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eQwen2_5_VLForConditionalGeneration\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eAutoProcessor\u003c/span\u003e\n\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eqwen_vl_utils\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eprocess_vision_info\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ======================================================================================================\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 读取模型权重，推荐情况下用下面那个被注释掉的引入flashattn的做法\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# ======================================================================================================\u003c/span\u003e\n\u003cspan class=\"n\"\u003emodel\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eQwen2_5_VLForConditionalGeneration\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_pretrained\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"s2\"\u003e\u0026#34;Qwen/Qwen2.5-VL-3B-Instruct\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etorch_dtype\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edevice_map\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     \u0026#34;Qwen/Qwen2.5-VL-7B-Instruct\u0026#34;,\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     torch_dtype=torch.bfloat16,\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     attn_implementation=\u0026#34;flash_attention_2\u0026#34;,\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     device_map=\u0026#34;auto\u0026#34;,\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# )\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ======================================================================================================\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 读取用于处理数据中图像和文本的processor，细节如下：\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#（1） 默认情况下，在qwen2.5 LM Decoder的输入中，一张图片最少占据4个token，最多占据16384个token\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# (2) 你也可以自己权衡模型效果和计算成本，自行设定一张图片最少/最多占据的token数量，\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     然后把这个自定义值传入process初始化的参数重，例如：\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     min_pixels = 256*28*28，你希望一张图片最少占据256个token，由于每个token对应一块28*28的区域，所以这张图片至少拥有256*28*28个pixel\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     max_pixels = 1280*28*28，道理同上\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     processor = AutoProcessor.from_pretrained(\u0026#34;Qwen/Qwen2.5-VL-7B-Instruct\u0026#34;, min_pixels=min_pixels, max_pixels=max_pixels)\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     这里，取28是因为，最初的patch_size我们打算设为14，由此得到原始patch，这也是vit部分的输入。\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     但在vit输出层，为了进一步节省token，我们决定将 2*2 个patch合并起来作为一个token\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     这个token才是最后作为qwen2.5 LM Decoder的vision部分输入，所以是14*2 = 28\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     （更多细节参见后文对vit部分的解读）\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# ======================================================================================================\u003c/span\u003e\n\u003cspan class=\"n\"\u003eprocessor\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eAutoProcessor\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_pretrained\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Qwen/Qwen2.5-VL-3B-Instruct\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ======================================================================================================\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 传入prompt\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# （1）一个text可以对应多个image（把每个image表示成1个dict就好）\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# （2）你还可以定制化地去设定针对这张图片的参数，例如你只想对这张图片改变min_pixels和max_pixels，你就可以\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#     在这张图片对应的字典中去添加这两个参数，详情参见文档 https://github.com/QwenLM/Qwen2.5-VL\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# ======================================================================================================\u003c/span\u003e\n\u003cspan class=\"n\"\u003emessages\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"s2\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"s2\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;image\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"s2\"\u003e\u0026#34;image\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e},\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Describe this image.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e],\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# Preparation for inference\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 对 messages 做一些处理，主要是加上一些诸如特殊字符\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# text返回结果：\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#（1）增加了默认的sys_msg\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#（2）每一个角色（system或者user）说的话，其开头和结尾分别添加\u0026lt;|im_start|\u0026gt;和\u0026lt;|im_end|\u0026gt;标记\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#（3）image的表达方式为\u0026lt;|vision_start|\u0026gt;\u0026lt;|image_pad|\u0026gt;\u0026lt;|vision_end|，其中\u0026lt;|image_pad|\u0026gt;是预留给图像的位置\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026lt;|im_start|\u0026gt;system\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# You are a helpful assistant.\u0026lt;|im_end|\u0026gt;\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026lt;|im_start|\u0026gt;user\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026lt;|vision_start|\u0026gt;\u0026lt;|image_pad|\u0026gt;\u0026lt;|vision_end|\u0026gt;Describe this image.\u0026lt;|im_end|\u0026gt;\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026lt;|im_start|\u0026gt;assistant\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"n\"\u003etext\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eprocessor\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eapply_chat_template\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003emessages\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etokenize\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eadd_generation_prompt\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\n\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 预处理图像和视频数据。这里以图像数据举例，视频数据处理见后文\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# image_inputs: List[PIL.Image.Image], 列表长度为这个batch中对应的所有图片数量，这点非常重要\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 假设列表长度为1，那么image_inputs形如：[\u0026lt;PIL.Image.Image image mode=RGB size=2044x1372\u0026gt;]\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# process_vision_info的代码请看：https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L352\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# process_vision_info对每个image都做了如下处理：\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#（1）检查每张图片的 max(h,w)/min(h,w)是否在阈值范围内，如果超过阈值则认为该图片高宽比太离谱，会直接抛出异常（当前阈值200）\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#（2）通过四舍五入的方式，重新设置图片的 h 和 w 值，确保它们可以被28整除\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#（3）如果这张图片太大，超过了上述 max_pixels 的范围，那么就在尽量维持其宽高比例不变的情况下，缩小其宽高\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#（4）这张图片太小时用同样的方式放大其宽高\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#（5）经过前面的4步，我们得到了这张图片最终理想的h和w值，我们采用resize的方式把图片按这个值缩放，就得到image_inputs中的每一个图片\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 你可以发现，这里没有经过任何的“多裁少pad操作”，你只是在缩放图片。\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# process_vision_info对每个video都做了如下处理：TODO\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"n\"\u003eimage_inputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evideo_inputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eprocess_vision_info\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emessages\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 假设这里我们做的是batch inference，有2个text，text0对应2张图， text1对应1张图。\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 那么最终inputs的形式如：       \u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# {\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# input_ids尺寸是：(text_num, token_num), 这里我们已经算好了每张图片会占据多少个token，并用相应个数的\u0026lt;|image_pad|\u0026gt;在text文本里做了替换\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026#39;input_ids\u0026#39;: tensor([[151644,   8948,    198,  ..., 151644,  77091,    198],\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                      [151644,   8948,    198,  ..., 151643, 151643, 151643]]), \u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# attention_mask尺寸是：(text_num, token_num)\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026#39;attention_mask\u0026#39;: tensor([[1, 1, 1,  ..., 1, 1, 1],\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                           [1, 1, 1,  ..., 0, 0, 0]]), 0表示第2条数据做了padding\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#  pixel_values尺寸为：(image_num * grid_t * grid_h * grid_w, \u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                      channel * temporal_patch_size(2) * patch_size(14) * patch_size(14))，image_num是这个batch中的image数量\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#  \u0026#39;pixel_values\u0026#39;: tensor([[ 0.8501,  0.8501,  0.8647,  ...,  1.3922,  1.3922,  1.3922],\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                          [ 0.9376,  0.9376,  0.9376,  ...,  1.4491,  1.4491,  1.4491],\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                          [ 0.9084,  0.9376,  0.9376,  ...,  1.4065,  1.4207,  1.4207],\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                          ...,\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                          [-0.1280, -0.1280, -0.1426,  ..., -0.2431, -0.2715, -0.3000],\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                          [-0.3324, -0.3324, -0.3032,  ..., -0.3000, -0.2715, -0.2857],\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                          [-0.3762, -0.4054, -0.4054,  ..., -0.4279, -0.4422, -0.4564]]),\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#         \u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# image_grid_thw尺寸是：(image_num, 3)，其中3分别表示这张图片的grid_t, grid_h, grid_w\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026#39;image_grid_thw\u0026#39;: tensor([[  1,  98, 146],\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                           [  1,  98, 146],\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e#                           [  1,  98, 146]])\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# }\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 到这一步为止，我们还没有对图像做具体的转token处理，这个应该是在model.forward中做的，我们只是对图像做了一些初步的resize，rescale等处理\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"n\"\u003einputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eprocessor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eimages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eimage_inputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evideos\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003evideo_inputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003epadding\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ereturn_tensors\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;pt\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003einputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003einputs\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edevice\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# Inference: Generation of the output\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"n\"\u003egenerated_ids\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egenerate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003einputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003emax_new_tokens\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003egenerated_ids_trimmed\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eout_ids\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ein_ids\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e:]\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ein_ids\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eout_ids\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003ezip\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einputs\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einput_ids\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003egenerated_ids\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003cspan class=\"n\"\u003eoutput_text\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eprocessor\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebatch_decode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003egenerated_ids_trimmed\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eskip_special_tokens\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eclean_up_tokenization_spaces\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"k\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eoutput_text\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e二、整体架构\u003c/h2\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-cffd1cbd01d21096357496010da2d4df_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"808\" data-original-token=\"v2-35d6b6b29b0f93d46410e4e173a5392f\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://picx.zhimg.com/v2-cffd1cbd01d21096357496010da2d4df_r.jpg\"/\u003e\u003cfigcaption\u003e来自qwen论文\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"WyOgEgdo\"\u003e以下把图像和视频数据统称为“视觉数据”\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"ruvd0ijb\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py%23L352\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eprocess_vision_info\u003c/a\u003e:  视觉数据在进入vit前，会先做一些预处理，包括对图像数据尺寸的动态调整、视频动态抽帧等操作。\u003c/li\u003e\u003cli data-pid=\"-Ph6mq45\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py%23L404\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003evit\u003c/a\u003e：对视觉数据进行处理。在输入部分，使用3D conv把视觉数据做成14*14 patch分块，并使用window attention节省计算。在输出部分，通过一个mlp层将2*2个patch块合并成一个merged_patch，这个merged_patch代表的token才是qwen2.5 LM decoder中的一个输入。\u003c/li\u003e\u003cli data-pid=\"w11qhU6C\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py%23L1102\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eQwen2.5 LM Decoder\u003c/a\u003e：对文本和视觉数据使用3D M-rope（3D多模态rope），然后送入主体模型中。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"VEAfxOxq\"\u003e\u003cbr/\u003e\u003cbr/\u003e接下来分别看这几块的细节。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e三、process_vision_info\u003c/h2\u003e\u003ch3\u003e3.1 图像数据的预处理\u003c/h3\u003e\u003cp data-pid=\"x-fGimON\"\u003e\u003cb\u003e代码在\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py%23L97\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e\u003c/b\u003e\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 预处理图像和视频数据。\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# image_inputs: List[PIL.Image.Image], 列表长度为这个batch中对应的所有图片数量，这点非常重要\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# 假设列表长度为1，那么image_inputs形如：[\u0026lt;PIL.Image.Image image mode=RGB size=2044x1372\u0026gt;]\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# ====================================================================================================\u003c/span\u003e\n\u003cspan class=\"n\"\u003eimage_inputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evideo_inputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eprocess_vision_info\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emessages\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"4ajzulvZ\"\u003e\u003cb\u003eprocess_vision_info对每个image都做了如下处理。\u003c/b\u003e （1）检查每张图片的 \u003ccode\u003emax(h,w) / min(h,w)\u003c/code\u003e是否在阈值范围内，如果超过阈值。则认为该图片高宽比太离谱，会直接抛出异常（当前阈值200）\u003cbr/\u003e（2）通过四舍五入的方式，重新设置图片的 h 和 w 值，确保它们可以被28整除\u003cbr/\u003e（3）如果这张图片太大，超过了上述 max_pixels 的范围，那么就在\u003cb\u003e尽量维持其宽高比例不变的情况下，重新计算其符合max_pixels范围的h和w。图片太小也是同理。\u003c/b\u003e （4）经过前面的步骤，我们得到了这张图片最终理想的h和w值（resized_height，resized weight），我们采用\u003cbr/\u003eresize的方式把图片按这个值缩放，就得到image_inputs中的每一个图片 \u003cbr/\u003e\u003cb\u003e你可以发现：\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"tUgCV5F5\"\u003e这里没有经过任何的“多裁少pad到固定分辨率的操作”，你只是在合理的范围内缩放图片，尽量保存了原始图片的信息。你允许各图片的分辨率各不相同。\u003c/li\u003e\u003cli data-pid=\"YrAb89Tm\"\u003e每一张图片所含的patch数量是不同的（也就是要送入vit的输入tokens数量是不同的）。\u003c/li\u003e\u003cli data-pid=\"UYNo4NFa\"\u003e\u003cb\u003e以上两者共同表达了qwenvl中动态分辨率的含义\u003c/b\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e3.2 视频数据的预处理\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"oANw3kTt\"\u003e入口代码在\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py%23L277\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e\u003c/li\u003e\u003cli data-pid=\"4xNTOl1G\"\u003e核心处理代码在\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py%23L226\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e。（假设用的处理后端是decord，当然还有torchvision之类的可选，处理逻辑都差不多）\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"8pPLkPa7\"\u003e对这个视频核心处理逻辑的代码做一个解读，这个代码负责处理每一个视频：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003e_read_video_decord\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003edict\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;read video using decord.VideoReader\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e    Args:\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        ele (dict): a dict contains the configuration of video.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        support keys:\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            - video: the path of video. support \u0026#34;file://\u0026#34;, \u0026#34;http://\u0026#34;, \u0026#34;https://\u0026#34; and local path.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            - video_start: the start time of video.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            - video_end: the end time of video.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e    Returns:\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        torch.Tensor: the video tensor with shape (T, C, H, W).\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n    \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003edecord\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evideo_path\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;video\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n    \u003cspan class=\"n\"\u003est\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etime\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evr\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edecord\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eVideoReader\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evideo_path\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# TODO: support start_pts and end_pts\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;video_start\u0026#39;\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eele\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;video_end\u0026#39;\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eraise\u003c/span\u003e \u003cspan class=\"ne\"\u003eNotImplementedError\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;not support start_pts and end_pts in decord for now.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 以下都是视频的原始属性\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# total_frames：原始视频的总帧数\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# video_fps：原始视频的fps（每秒的帧数）\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evideo_fps\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003elen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evr\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"n\"\u003evr\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_avg_fps\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einfo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;decord:  {video_path=}, {total_frames=}, {video_fps=}, time={time.time() - st:.3f}s\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# nframes：经过我们的计算，最终要对这个视频采样的总帧数。计算逻辑解读见下。\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"n\"\u003enframes\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esmart_nframes\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evideo_fps\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003evideo_fps\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# idx: 以均匀采样的方式，采样出被选中的视频帧id\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# （均匀采样的目的是尽量保证不丢失原始视频所有时间轴上的信息）\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 例如，假设total_frames = 20 (原始视频20帧), video_fps = 5(每秒5帧)，那么原始视频一共4秒，其\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 那么原始图片帧为[[0,1,2,3,4], [5,6,7,8,9], [10,11,12,13,14], [15,16,17,18,19]]\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 假设现在nframes = 10，即我们最终要采样10帧。\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 那么抽样后idx = [0, 2, 4, 6, 8, 11, 13, 15, 17, 19]\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eidx\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elinspace\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003enframes\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eround\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elong\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etolist\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evideo\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evr\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_batch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eidx\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003easnumpy\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 在相应位置抽帧，并将其转换为numpy数组\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evideo\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etensor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evideo\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epermute\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# Convert to TCHW format\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# sample_fps：表示抽样后的视频的帧率。\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 例如total_frames = 20 (原始视频20帧), video_fps = 5(每秒5帧)，那么原始视频一共4秒\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 现在nframes=10，说明抽样后的每秒帧数为 nframes/(total_frames/video_fps) = 10/4 = 2.5fps\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esample_fps\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enframes\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"nb\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mf\"\u003e1e-6\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003evideo_fps\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# video: 抽样后的视频数据，已经转成tensor，尺寸为(T, C, H, W)\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# smaple_fps: 抽样后的视频帧率（fps）\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003evideo\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003esample_fps\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"P90l1Jq5\"\u003e下面对代码中\u003ccode\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py%23L143\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003esmart_nframes\u003c/a\u003e\u003c/code\u003e这个对原始视频帧的采样过程进行解读。\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003esmart_nframes\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003edict\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evideo_fps\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;calculate the number of frames for video used for model inputs.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e    Args:\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        ele (dict): a dict contains the configuration of video.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e            support either `fps` or `nframes`:\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e                - nframes: the number of frames to extract for model inputs.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e                - fps: the fps to extract frames for model inputs.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e                    - min_frames: the minimum number of frames of the video, only used when fps is provided.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e                    - max_frames: the maximum number of frames of the video, only used when fps is provided.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        total_frames (int): the original total number of frames of the video.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        video_fps (int | float): the original fps of the video.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e    Raises:\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        ValueError: nframes should in interval [FRAME_FACTOR, total_frames].\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e    Returns:\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e        int: the number of frames for video used for model inputs.\n\u003c/span\u003e\u003cspan class=\"s2\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# nframes 和 fps 都来自用户自己在msg里的配置。\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# - nframes：决定最终对这个视频采样的总帧数\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# - fps：    假设保持原始视频的时长不变，这个值表示用户想按每s多少帧的方式来采样视频（默认值为2）\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e#            理论上，原始视频总时长 * 用户配置的fps = 最终采样出的视频总帧数\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e#            但实际上，受到qwenvl的限制（不让视频数据占据太少/太多token），所以最终采样出的视频\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e#            fps可能不是完全吻合用户配置的这个fps\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 基于上述2者的定义，你要么配fps，要么配nframes，不要两者都配\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;fps\u0026#34;\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eele\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;nframes\u0026#34;\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Only accept either `fps` or `nframes`\u0026#34;\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 如果配置了nframes，就让他成为FRAME_FACTOR的倍数（默认FRAME_FACTOR=2）\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# （因为最终在进入vit前，我们希望把2帧视频合起来处理）\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;nframes\u0026#34;\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"n\"\u003enframes\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eround_by_factor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;nframes\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003eFRAME_FACTOR\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# 如果配置了fps（没有配置的话就用默认值FPS = 2）\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# =====================================================================================\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"n\"\u003efps\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;fps\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eFPS\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# 对视频数据来说要求的最小总帧数（默认4），并保证它是FRAME_FACTOR的倍数\u003c/span\u003e\n        \u003cspan class=\"n\"\u003emin_frames\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eceil_by_factor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;min_frames\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eFPS_MIN_FRAMES\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"n\"\u003eFRAME_FACTOR\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# 对视频数据要求的最大总帧（默认764）\u003c/span\u003e\n        \u003cspan class=\"n\"\u003emax_frames\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003efloor_by_factor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eele\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;max_frames\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nb\"\u003emin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eFPS_MAX_FRAMES\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e)),\u003c/span\u003e \u003cspan class=\"n\"\u003eFRAME_FACTOR\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# 理论上来说，最终我们需要的总帧数 = 原始视频长度（单位：秒）* 人为定义的每秒视频采集帧\u003c/span\u003e\n        \u003cspan class=\"n\"\u003enframes\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"n\"\u003evideo_fps\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003efps\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003enframes\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n            \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewarning\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;smart_nframes: nframes[{nframes}] \u0026gt; total_frames[{total_frames}]\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# 同样对nframes做一些限制处理\u003c/span\u003e\n        \u003cspan class=\"n\"\u003enframes\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003emin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003emin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enframes\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003emin_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"n\"\u003emax_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003enframes\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003efloor_by_factor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enframes\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eFRAME_FACTOR\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"ow\"\u003enot\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eFRAME_FACTOR\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;=\u003c/span\u003e \u003cspan class=\"n\"\u003enframes\u003c/span\u003e \u003cspan class=\"ow\"\u003eand\u003c/span\u003e \u003cspan class=\"n\"\u003enframes\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;=\u003c/span\u003e \u003cspan class=\"n\"\u003etotal_frames\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eraise\u003c/span\u003e \u003cspan class=\"ne\"\u003eValueError\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;nframes should in interval [{FRAME_FACTOR}, {total_frames}], but got {nframes}.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003enframes\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"LV_aUSu0\"\u003e\u003cb\u003e最后，再来看来自这份代码里的、关于video部分的一个重要\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py%23L375\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e返回args\u003c/a\u003e\u003c/b\u003e:\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ereturn_video_kwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eimage_inputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evideo_inputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;fps\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003evideo_sample_fps_list\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"7s0_A6-X\"\u003e\u003ccode\u003e{\u0026#39;fps\u0026#39;: video_sample_fps_list}\u003c/code\u003e就是我们上面计算出的、每个视频采样被采样后的fps（即代码里的sample_fps）。之所以是一个list，是因为这里装着输入数据中全部视频的sample_fps信息。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e这个部分之所以重要，是因为后面在计算3D mrope中，它会被用在计算Temporal维度上的位置编码信息\u003c/b\u003e。在这部分计算中，有一个重要参数\u003ccode\u003e\u003cb\u003esecond_per_grid_ts\u003c/b\u003e\u003c/code\u003e，它的计算方法如下：\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"n\"\u003esecond_per_grid_ts\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"n\"\u003esample_fps\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003etemporal_patch_size\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"err\"\u003e默认为\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"TQCf4Fq-\"\u003e结合下图解释一下这里在做什么：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-257917c4c3ba173a6f7bf48439108ae2_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"1166\" data-original-token=\"v2-6d232cfadce1b9590b900627c289f83f\" class=\"origin_image zh-lightbox-thumb\" width=\"1634\" data-original=\"https://pica.zhimg.com/v2-257917c4c3ba173a6f7bf48439108ae2_r.jpg\"/\u003e\u003cfigcaption\u003e根据qwen代码原创绘制\u003c/figcaption\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e四、Processor\u003c/h2\u003e\u003cp data-pid=\"9ivPQYTi\"\u003e在前面的处理中，对于一个batch中的数据：\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"5aypZ-Ue\"\u003e我们对text数据做了add_chat_template的处理，增加了一些表示start，end、image、video的placeholder（参见一）\u003c/li\u003e\u003cli data-pid=\"FOyUrH9h\"\u003e我们对vision数据做了一些预处理，例如动态resize图片的尺寸、对视频数据进行抽帧及同样resize视频帧的尺寸等。\u003c/li\u003e\u003cli data-pid=\"9RMZ70b8\"\u003e但是到这里，我们没有对text做tokenize，也没有对vision做更深入的处理。所以这一些都在Processor中进一步完成，调用Processor的方式如下（第一部分写过）\u003c/li\u003e\u003c/ul\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"n\"\u003einputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eprocessor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eimages\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eimage_inputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"c1\"\u003e# List[PIL.Image.Image]，这个batch中所有的图片数据展平成一维List\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evideos\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003evideo_inputs\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"c1\"\u003e# List[Tensor], 每个Tensor尺寸为(T, C, H, W)，表示一个视频抽帧后的结果\u003c/span\u003e\n    \u003cspan class=\"n\"\u003epadding\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ereturn_tensors\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;pt\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003einputs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003einputs\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edevice\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\u003cli data-pid=\"SXfZRmDm\"\u003eProcessor入口函数在\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py%23L48\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e，其中\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py%23L129-L142\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这个部分\u003c/a\u003e计算了3.2中所说的\u003ccode\u003esecond_per_grid_ts\u003c/code\u003e，这个值将在mrope中被使用。\u003c/li\u003e\u003cli data-pid=\"Fvhg95D-\"\u003eProcssor中，负责处理vision数据的\u003ccode\u003eself.image_processor\u003c/code\u003e核心实现在\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py%23L87\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e。\u003c/li\u003e\u003cli data-pid=\"iOqLb423\"\u003eProcessor中，负责处理text数据的\u003ccode\u003eself.tokenizer\u003c/code\u003e比较常规，这里不展示细节。\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e4.1 self.image_processor: Qwen2VLImageProcessor\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"ZEulQ1_A\"\u003e代码在\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py%23L87\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"J80ACMMR\"\u003e\u003cbr/\u003e\u003cu\u003e\u003cb\u003e不展示全部的代码了，这里总结下它在做的事情，先以单张图片数据为例\u003c/b\u003e\u003c/u\u003e：\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e（1）do_resize / do_rescale / do_normalize\u003c/b\u003e：根据配置决定是否要做这3个操作，分别表示调整图片大小 / 将像素值缩放到0-1之间（乘上1/255）/ 在每个channel上指定mean和std做normalize。\u003cb\u003e这边的do_resize其实应该在上面process_vision_info中就做过了，也就是这里的操作和之前是有点重复的。\u003c/b\u003e\u003cbr/\u003e\u003cb\u003e（2）把每张图片复制temporal_patch_size次（默认为2）\u003c/b\u003e。这是为了在image数据上也增加 T 这个维度，保证image和video的处理逻辑一致（因为video也是把相邻的2帧组成一组）\u003cbr/\u003e\u003cb\u003e（3）最终，对于1张图\u003c/b\u003e：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"fMudgwVQ\"\u003e它将被处理成\u003ccode\u003e[grid_t * grid_h * grid_w, channel * temporal_patch_size(2) * patch_size(14) * patch_size(14)\u003c/code\u003e]这样的尺寸。其中grid_t表示这张图在 T 方向上的格子数（就是有多少张2帧组成的图，对于图像来说grid_t的值为1）；grid_h = resize_h / patch_size，表示如果按照patch_size(14)来看，h方向上可以被切分成多少个格子；grid_w也是同理\u003c/li\u003e\u003cli data-pid=\"0R0caekf\"\u003e这张图的对应的\u003ccode\u003e(grid_t, grid_h, grid_w)\u003c/code\u003e会被保存下来。\u003c/li\u003e\u003cli data-pid=\"_OsHKgi5\"\u003e\u003cb\u003e\u003cu\u003e对于这个batch中全部的图，我们会将其concat起来\u003c/u\u003e。也即最终输出结果的尺寸为\u003c/b\u003e\u003ccode\u003e[sum(grid_t * grid_h * grid_w), channel * temporal_patch_size(2) * patch_size(14) * patch_size(14)\u003c/code\u003e]，\u003cb\u003e最终图像的处理结果可以参见上面代码块中的\u003ccode\u003epixel_values\u003c/code\u003e和\u003ccode\u003eimage_grid_thw\u003c/code\u003e数据\u003c/b\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"1gRSOScm\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e（4）最后再强调很重要的一点，根据上面的描述，我们知道对于1张图，它最终会被处理成\u003c/b\u003e\u003ccode\u003e[grid_t * grid_h * grid_w, channel * temporal_patch_size(2) * patch_size(14) * patch_size(14)\u003c/code\u003e]这个尺寸，其中第0维表示某个patch，\u003cb\u003e这里patch不是按照一张图从左到右，从上到下的顺序排列的，而是按照把2 * 2 区域内的4个patch变成连续的4个patch排列的\u003c/b\u003e。具体例子如下图：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-ac50b6222eab18e66df2b0984f2e1a0b_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"3114\" data-rawheight=\"496\" data-original-token=\"v2-ac50b6222eab18e66df2b0984f2e1a0b\" class=\"origin_image zh-lightbox-thumb\" width=\"3114\" data-original=\"https://pic4.zhimg.com/v2-ac50b6222eab18e66df2b0984f2e1a0b_r.jpg\"/\u003e\u003cfigcaption\u003e根据qwen代码原创绘制\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"xY6vC2w1\"\u003e\u003cu\u003e\u003cb\u003e对于单个视频数据，也是类似处理，返回的结果尺寸依然是\u003ccode\u003e[grid_t * grid_h * grid_w, channel * temporal_patch_size(2) * patch_size(14) * patch_size(14)\u003c/code\u003e，只是这里的grid_t往往不再是1，而是视频总帧数/temporal_patch_size（视频两两帧堆叠方式参见3.2的图）\u003c/b\u003e\u003c/u\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e4.2 self.tokenzier\u003c/h3\u003e\u003cp data-pid=\"zRHzfNY1\"\u003etokenizer的具体内容不展开，这里只强调，对于原始text部分，在进入tokenizer前我们还要做一些处理，处理核心代码在\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py%23L151-L177\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"c1\"\u003e# 原始text例如：\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026lt;|im_start|\u0026gt;system\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# You are a helpful assistant.\u0026lt;|im_end|\u0026gt;\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026lt;|im_start|\u0026gt;user\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026lt;|vision_start|\u0026gt;\u0026lt;|image_pad|\u0026gt;\u0026lt;|vision_end|\u0026gt;Describe this image.\u0026lt;|im_end|\u0026gt;\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e# \u0026lt;|im_start|\u0026gt;assistant\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp data-pid=\"JTNKPunz\"\u003e\u003cb\u003e在之前的预处理中，我们是把整个batch中的text和image都展平成一个list，那么image和text如何对应起来呢？\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"i4QZDZts\"\u003e对应的方法是，在text中有预留一个\u003ccode\u003e\u0026lt;|image_pad|\u0026gt;\u003c/code\u003e位置，一个位置表示一张图，你只要把list中的image顺次填入就可以了。\u003c/li\u003e\u003cli data-pid=\"z_0fgIOc\"\u003e在知道这种对应方法的基础上，现在我们想让\u003ccode\u003e\u0026lt;|image_pad|\u0026gt;\u003c/code\u003e做进一步展开，即这个位置的图表示几个token，我们就拷贝几次\u003ccode\u003e\u0026lt;|image_pad|\u0026gt;\u003c/code\u003e。\u003c/li\u003e\u003cli data-pid=\"iUxwnx9l\"\u003e\u003cb\u003e每一张图对应的token数量为：(grid_t * grid_h * grid_w)/ (merge_size**2)，其中merge_size表示你想让多少个patch合并成一个token，默认值是2，即在原始图像上做出14*14大小的patch后，我将2*2个patch合并成1个token表示。根据这个计算方式我们扩展\u003ccode\u003e\u0026lt;|image_pad|\u0026gt;\u003c/code\u003e。\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"wwdm00-e\"\u003e按这个方式处理完text后就可以送入self.tokenizer取做tokenize了，会按照max_len做截断，也会在一个批次中做padding\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e五、VIT Encoder\u003c/h2\u003e\u003cp data-pid=\"DpecNkpc\"\u003e处理好了text和vision，现在就可以送入模型开始正式做fwd了。整个qwenVL模型架构在\u003ccode\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py%23L1511\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eQwen2_5_VLForConditionalGeneration\u003c/a\u003e\u003c/code\u003e中，其中包含了VIT encoder和qwenVL主体decoder。这里我们只关注VIT encoder（\u003ccode\u003e\u003ci\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py%23L404\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eQwen2_5_VisionTransformerPretrainedModel\u003c/a\u003e\u003c/i\u003e\u003c/code\u003e），下面介绍vit encoder的forward方法。\u003c/p\u003e\u003ch3\u003e5.1 使用3D Conv将原始图像patch转变为vit的输入token\u003c/h3\u003e\u003cul\u003e\u003cli data-pid=\"7jMvljXd\"\u003e这个过程也称为embedding\u003c/li\u003e\u003cli data-pid=\"I08PkgtW\"\u003e这部分的代码在\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py%23L88\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"qz-Ni2bZ\"\u003e在进入vit encoder前，我们vision部分的输入如下，我们会有\u003ccode\u003eflatten_patches + 对应的(grid_t, grid_h, grid_w)\u003c/code\u003e数据作为输入。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-ac50b6222eab18e66df2b0984f2e1a0b_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3114\" data-rawheight=\"496\" data-original-token=\"v2-ac50b6222eab18e66df2b0984f2e1a0b\" class=\"origin_image zh-lightbox-thumb\" width=\"3114\" data-original=\"https://pic4.zhimg.com/v2-ac50b6222eab18e66df2b0984f2e1a0b_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"Oldxw4ft\"\u003e\u003cb\u003e那么3D conv要做的事情就是（相当于拿out_channels（= vit_hidden_size）个3D kernel，每个kernel都在所有patch方块上滚一遍， 得到hidden_size的一个数值，所有out_channels个kernel滚完就得到了完整的hidden_size）：\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-64b49e351a9db01e9f209d4d63172710_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3114\" data-rawheight=\"1436\" data-original-token=\"v2-d9ae815ff9aadac6af7d5ebeb7b18b95\" class=\"origin_image zh-lightbox-thumb\" width=\"3114\" data-original=\"https://pic3.zhimg.com/v2-64b49e351a9db01e9f209d4d63172710_r.jpg\"/\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e5.2 2D rope\u003c/h3\u003e\u003cp data-pid=\"JYer886b\"\u003e\u003cb\u003e接下来，要做vit encoder的rope，这里用的是2D rope\u003c/b\u003e，之所以用2D而不用3D，我猜主要原因是vit的主要作用是负责提供单图/单帧上的特征（特别是后面在vit部分引入window attention后，就更不需要跨Temporal维度去计算注意力了），而Temporal维度上的关联性，留到qwenvl的主体部分去做，那个时候再引入3D mrope。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e我们先来看比较熟悉的1D rope：\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-da4233aabcd42f7a1e0d5456a349f71c_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"938\" data-original-token=\"v2-373f301de9945dfc543164377ea0b7c1\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic3.zhimg.com/v2-da4233aabcd42f7a1e0d5456a349f71c_r.jpg\"/\u003e\u003cfigcaption\u003e截图的我自己的rope文章\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"A35swKZ3\"\u003e\u003cb\u003e注意，Rm矩阵的解不是唯一的，例如1D rope的Rm还可以写成下面这种形式，因为它同样满足下面蓝体公式（这也是llama系列的改写方式），在qwenvl后续的各种rope中，沿袭了此种方法（代码更好写了）：\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-8d0e374c164c282e3ef5d3d56dc7bbd8_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"884\" data-original-token=\"v2-8689d08f6abee0fbfb943c89ae165486\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic3.zhimg.com/v2-8d0e374c164c282e3ef5d3d56dc7bbd8_r.jpg\"/\u003e\u003cfigcaption\u003e请给我一点时间找下这张图片链接，应该是来自huggingface的某个discussion\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Lccg7GUR\"\u003e\u003cb\u003e那么对应到vit 2D rope的某个token中，它的2D位置编码如下（这里vit_hidden_size指的是head_dim）：\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-1514637451e1bff4e7af5ff1aa128232_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"1067\" data-original-token=\"v2-b3d8f7d959b0475a8bb1f8308ed2c1c2\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic3.zhimg.com/v2-1514637451e1bff4e7af5ff1aa128232_r.jpg\"/\u003e\u003cfigcaption\u003e是我的丑字，希望没有辣到大家眼睛\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"LCNxAxoC\"\u003e\u003cb\u003e虽然看起来麻烦，但可以粗糙理解成半个head_dim在负责做h位置的rope，半个head_dim在负责做w位置的rope，这两个部分共享一套\u003c/b\u003e \u003cimg src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/\u003e \u003cb\u003e值\u003c/b\u003e\u003cbr/\u003e详细实现可以去看代码，分别在\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py%23L114\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e和\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py%23L436\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这里\u003c/a\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e5.3 window attention\u003c/h3\u003e\u003cp data-pid=\"gn-hC3Po\"\u003e\u003cb\u003eWindow attention的定义如下图：\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-285daabd1a4a0e9563997736a416abad_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"856\" data-original-token=\"v2-53d7b6c6932d830fa8aa36ce158f2536\" class=\"origin_image zh-lightbox-thumb\" width=\"1614\" data-original=\"https://picx.zhimg.com/v2-285daabd1a4a0e9563997736a416abad_r.jpg\"/\u003e\u003cfigcaption\u003e根据qwen代码原创绘制\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"bilhCvy0\"\u003e但是按照5.1节中的展开方法，现在我们输入vit模型的tokens排序如下，这种方式下同一个窗口内的patch（也就是vit tokens）无法连续地排列在一起！\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-7ca3bbda05d26d5beef75f9e6a114c05_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2414\" data-rawheight=\"856\" data-original-token=\"v2-c6f25dedd2e4d852e326fa2b1333858c\" class=\"origin_image zh-lightbox-thumb\" width=\"2414\" data-original=\"https://pic2.zhimg.com/v2-7ca3bbda05d26d5beef75f9e6a114c05_r.jpg\"/\u003e\u003cfigcaption\u003e根据qwen代码原创绘制\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"uITWBFTz\"\u003e\u003cb\u003e为了做window attention，我们希望：\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-a635f83d96f40ad6df357485cf26f9ba_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2995\" data-rawheight=\"1637\" data-original-token=\"v2-a12dc0126191628f14d5424a542af7bd\" class=\"origin_image zh-lightbox-thumb\" width=\"2995\" data-original=\"https://pic1.zhimg.com/v2-a635f83d96f40ad6df357485cf26f9ba_r.jpg\"/\u003e\u003cfigcaption\u003e根据qwen代码原创绘制\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"r-nLxiqP\"\u003e这种调换vit tokens顺序的方法，就是\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py%23L519-L535\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这块代码\u003c/a\u003e在做的事情。我们按照这个顺序作为vit的输入，然后正常经过vit blocks，得到最后的输出，\u003cu\u003e\u003cb\u003e但是要注意，在输出结果中，我们必须要把顺序再次调换回来。\u003c/b\u003e\u003c/u\u003e\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e5.4 patch merge\u003c/h3\u003e\u003cp data-pid=\"vcBcnOxN\"\u003e在整个vit的处理中，我们都是使用14*14的patch作为一个token处理的，但是最终进入到qwenvl decoder（也就是主体模型）中时，我们需要将原来2*2个patch合并成一个token，这可以通过先把2*2个token的结果concat起来，然后经过一个mlp层做hidden_size维度的映射得到，如下图：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-1f708b3a85f0cd60557f2e7477270cd5_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2414\" data-rawheight=\"1236\" data-original-token=\"v2-16bba73c470d90edf2b671ce36679afb\" class=\"origin_image zh-lightbox-thumb\" width=\"2414\" data-original=\"https://pic2.zhimg.com/v2-1f708b3a85f0cd60557f2e7477270cd5_r.jpg\"/\u003e\u003cfigcaption\u003e根据qwen代码原创绘制\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"cXPRfq7-\"\u003e这就是\u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py%23L146\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e这块代码\u003c/a\u003e在做的事情。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch2\u003e六、mrope\u003c/h2\u003e\u003ch3\u003e6.1 确定每个token的position_id\u003c/h3\u003e\u003cp data-pid=\"_PmwG33y\"\u003e即确定在mrope下，表示每个每个token位置编码的（t, h, w）分别是多少。\u003cb\u003e这里先来看qwen2vl的做法：\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-e6d08c686a68fb25fb3be108a9c6c4e1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"426\" data-original-token=\"v2-74b7774f744a101fd4641e500814ed30\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic2.zhimg.com/v2-e6d08c686a68fb25fb3be108a9c6c4e1_r.jpg\"/\u003e\u003cfigcaption\u003e来自qwen2vl论文\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"lKX4zS3d\"\u003e在3D rope中，不管是text还是vision，它们的position_id都表达成\u003ccode\u003e(t, h, w)\u003c/code\u003e，\u003cb\u003e其中text的这3个值相等。\u003c/b\u003e\u003cbr/\u003e现在重点解读图中的视频部分，它正好之前给出的这张图对上：\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-6541a3263f76f32a2987e9d399ec803d_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1674\" data-rawheight=\"1179\" data-original-token=\"v2-ff71b7bb712862d2b1c447c74dced9ce\" class=\"origin_image zh-lightbox-thumb\" width=\"1674\" data-original=\"https://pic4.zhimg.com/v2-6541a3263f76f32a2987e9d399ec803d_r.jpg\"/\u003e\u003cfigcaption\u003e根据qwen代码原创绘制\u003c/figcaption\u003e\u003c/figure\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"F22XFdFp\"\u003e以上3组视频帧经过vit的处理后，就变成了paper图例中的那3个小狗帧，小狗帧里每一个虚线框就表示喂给qwenvl decoder的一个vision token。结合图例，我们就不难理解它在h和w方向上的position_id赋值了。\u003c/li\u003e\u003cli data-pid=\"8r-M8pN3\"\u003e而在t时刻上，\u003cb\u003eqwenvl2是默认2个相邻小狗帧之间的间隔为1的\u003c/b\u003e，因此你可以在图例中发现，t方向上是0-2顺次递增。\u003c/li\u003e\u003cli data-pid=\"D4AB_5yI\"\u003e在qwenvl2中，\u003cb\u003e新模态的position_id初始值来自上一个模态的position_id (t, h, w)中的最大值 + 1\u003c/b\u003e，我们举一些例子：\u003cbr/\u003e\u003c/li\u003e\u003cul\u003e\u003cli data-pid=\"kHXkoZ8U\"\u003e在上图中，vision模态\u003cb\u003e所有tokens\u003c/b\u003e的 max(t,h,w) = 3，所以text token的初始值就是3+1 = 4，因此第一个text token是(4, 4, 4)\u003c/li\u003e\u003cli data-pid=\"epVFA3hz\"\u003e在上图中，到最后一个text token为止，max(t, h, w) = 12。此时如果在这个text token后我们继续接一条vision数据，我们先不考虑什么初始值设置，只考虑这条vision数据自己的话，那么这个vision数据的tokens的position_id应该是(0, 0, 0),(0, 0, 1), (0, 0, 2)...， 也就是和我们图例中画着小狗的那个vision数据编码方式是一样的。但是现在我们必须考虑整条input，所以新vision数据的position_id初始值 = 12 + 1 = 13，那么最终的它的position_id应该是(0+13, 0+13, 0+13), (0+13, 0+13, 1+13), (0+13, 0+13, 2+13), ...以此类推。\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"zDwKzXer\"\u003e\u003cbr/\u003e到这里我们就介绍完了qwenvl2的 3D position_id，现在我们来看qwenvl2.5，\u003cb\u003e这两者之间大部分的position id处理逻辑都是一致的，只有一个地方不同，那就是对于vision数据的 t 维度的position_id赋值。其实这个逻辑也很简单：\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"1_oEwDJ2\"\u003e\u003cb\u003e我们之前介绍过\u003ccode\u003esecond_per_grid_t\u003c/code\u003e的概念\u003c/b\u003e，如上图所示，它表示T维度上2个相邻的视频帧group之间的绝对时间间隔（单位：s）。\u003c/li\u003e\u003cli data-pid=\"l0Mf94du\"\u003e\u003cb\u003e现在我们再引入\u003ccode\u003etokens_per_second\u003c/code\u003e这个概念\u003c/b\u003e，它是个可以人为配置的参数（来自config.json文件，默认值为2）。这个参数让我困惑了好一阵，当前我的理解是这样的，我们再次回到视频数据进入vit前的样子：\u003c/li\u003e\u003c/ul\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-086d610a300f59b3257bf1ff71829359_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1732\" data-rawheight=\"380\" data-original-token=\"v2-81df83c5e4122d1b84d8c2609ef37fad\" class=\"origin_image zh-lightbox-thumb\" width=\"1732\" data-original=\"https://pic2.zhimg.com/v2-086d610a300f59b3257bf1ff71829359_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"ZUW-nvMF\"\u003e不难发现，从原始的视频数据来看，在T方向上，从第1个视频帧group到第2个视频帧group间，会经过2个token（注意，这里的token不是送给qwenvl decoder部分的token，你可以理解成是原始视频帧里的一个14*14的patch），从这个意义上来说，\u003ccode\u003etokens_per_second\u003c/code\u003e似乎就是\u003ccode\u003etemporal_patch_size\u003c/code\u003e的有意思。但是考虑到它本身也是一个人为可配置的参数，他可能表示”人觉得在1s的范围内，在T维度上应该要经过多少个tokens”才合理，所以默认情况下它才和\u003ccode\u003etemporal_patch_size\u003c/code\u003e取值一致（这里是我个人的解读，不确定理解地对不对）。\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e好，基于这些理解，则现在\u003ccode\u003esecond_per_grid_t * tokens_per_second\u003c/code\u003e这个乘积，就表示基于视频帧的绝对时间间隔（单位s），去修复它在T方向上理论上每秒应该走过的tokens数，所以这个乘积最终就是T维度上的position_id，用来替换原来的默认值1。举个例子：\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"jqE8rRgG\"\u003e假设原始T方向上的position_id分别是[0,1,2]（因为默认间隔是1）\u003c/li\u003e\u003cli data-pid=\"SKgEvbmT\"\u003e现在\u003ccode\u003esecond_per_grid_t * tokens_per_second\u003c/code\u003e = 50（比原来的默认值1大得多了，说明相邻2个视频帧group间间隔时间很长，很有可能是最原始的视频非常长，但你设定的抽帧数量比较少，导致相邻2帧间相距的时间太远，而50比1更能反应这种“远”）\u003c/li\u003e\u003cli data-pid=\"IHRE4I1a\"\u003e那么position_id = [0,1,2]*50 = [0, 50, 100]\u003c/li\u003e\u003c/ul\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003e6.2 3D mrope\u003c/h3\u003e\u003cp data-pid=\"U288stky\"\u003e现在来看，假设我们已经知道一个token的position_id是（t, h, w），那么它的rope矩阵应该是怎么样的？\u003cb\u003e其实这里和5.2节非常相似\u003c/b\u003e，\u003cb\u003e我们假设head_dim = 128，那么我们可以从配置文件中决定，这128维里分别有多少用来做t，h，w维度的rope\u003c/b\u003e。在默认配置中，我们设:\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"k3DCpca7\"\u003e\u003cb\u003eT = 16 * 2 = 32，即32维负责做T位置的旋转\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"GqGcAD83\"\u003e\u003cb\u003eH = 24 * 2 = 48，即48维负责做H位置的旋转\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"Zse3t-JD\"\u003e\u003cb\u003eW= 24 * 2 = 48，即48维负责做W位置的旋转\u003c/b\u003e\u003c/li\u003e\u003cli data-pid=\"X7ob_uTq\"\u003e我觉得这里在配置时，我们应该尽量保证H和W所用的维度数是一致的。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"eb6O2pDG\"\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e那么对于某个token，它的旋转矩阵可以写成：\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-b7658c166a07d63af591ab38875a493a_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1050\" data-rawheight=\"1280\" data-original-token=\"v2-4f642c2ef33c0fe931d6434e988a9363\" class=\"origin_image zh-lightbox-thumb\" width=\"1050\" data-original=\"https://pic1.zhimg.com/v2-b7658c166a07d63af591ab38875a493a_r.jpg\"/\u003e\u003cfigcaption\u003e嗨，又是我的丑字\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":1303,"thumbnails":["https://picx.zhimg.com/v2-fa725e5f69c2994059b74b6cf094888d.jpg?source=7e7ef6e2\u0026needBackground=1","https://picx.zhimg.com/50/v2-2893da1bbfc9774cb576b2db925455c1_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-f849c8fe7947cf07ca2bff3e4a957e8a_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-3f577ad08a26f0545b29a367d428b47b_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-38b41e6932bafe9e42d3c9a56aecbcfd_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-afd41d4360289d77a68c5a9c5ba6c9a8_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-5d2beaa26abe1f7698bca18f3adfffae_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-9235f4ea221b9ad53dcb82540afc43aa_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-91a8fe8fc8f59df98ce0aac7fc625097_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-71de5affda3d8de64cf6626e29143dcd_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-b15d41d1b9d50b379a6d20de8259e7df_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-3939a1ad581e54a23bcba23f428c1d62_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-c420bf519ca909cf6af0e96ecaa73d4c_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-bc646385d87ff4a7957c864de2220532_720w.jpg?source=b6762063"],"favorite_count":115,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1921289925552210138}","attached_info":"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","action_card":false},{"id":"87_1750898770.34","type":"feed","offset":87,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898770,"updated_time":1750898770,"target":{"id":"1921339466062214788","type":"answer","url":"https://api.zhihu.com/answers/1921339466062214788","author":{"id":"706e9070b3bf0001bfc8ce209f52b504","url":"https://api.zhihu.com/people/706e9070b3bf0001bfc8ce209f52b504","user_type":"people","url_token":"92-89-59-72","name":"超级侧卫","headline":"好读书，不求甚解","avatar_url":"https://picx.zhimg.com/50/v2-d4362cf4cc1128719d513f7880e94114_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"identity_people","description":"教师资格证持证人"}],"followers_count":37,"is_following":false,"is_followed":false},"created_time":1750863025,"updated_time":1750863200,"voteup_count":2,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"5712070673","type":"question","url":"https://api.zhihu.com/questions/5712070673","author":{"id":"6eded75307e379a204f5a14887505a8e","url":"https://api.zhihu.com/people/6eded75307e379a204f5a14887505a8e","user_type":"people","url_token":"determination-19","name":"新人","headline":"","avatar_url":"https://pic1.zhimg.com/50/v2-94e49440978e7dc59978fd378a3b46d5_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":2,"is_following":false,"is_followed":false},"title":"哪一本书的思想给你极大的震撼和影响？","created":1733107654,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[113,1176,4181,49975],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"《爱因斯坦文集》。商务印书馆出版，许良英、李宝恒、赵中立、范岱年编译。1976年1月第1版。 这套三卷本的书，在上世纪七十年代末八十年代初意味着什么呢？那是中国改革开放的黎明和随后的曙光。我所能看到的思想类的书，除了马恩列著作及毛选外，就只有什么艾思奇的《大众哲学》，苏联翻译过来的哲学词典之类。想睁眼看看广阔的世界，没门。 《爱因斯坦文集》第一卷，我看到了爱因斯坦眼中的有意义的生活，认识论，物理学的宏大…","excerpt_new":"《爱因斯坦文集》。商务印书馆出版，许良英、李宝恒、赵中立、范岱年编译。1976年1月第1版。 这套三卷本的书，在上世纪七十年代末八十年代初意味着什么呢？那是中国改革开放的黎明和随后的曙光。我所能看到的思想类的书，除了马恩列著作及毛选外，就只有什么艾思奇的《大众哲学》，苏联翻译过来的哲学词典之类。想睁眼看看广阔的世界，没门。 《爱因斯坦文集》第一卷，我看到了爱因斯坦眼中的有意义的生活，认识论，物理学的宏大…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"tIRiW9Eg\"\u003e《爱因斯坦文集》。商务印书馆出版，许良英、李宝恒、赵中立、范岱年编译。1976年1月第1版。\u003c/p\u003e\u003cp data-pid=\"EiDvy4t1\"\u003e这套三卷本的书，在上世纪七十年代末八十年代初意味着什么呢？那是中国改革开放的黎明和随后的曙光。我所能看到的思想类的书，除了马恩列著作及毛选外，就只有什么艾思奇的《大众哲学》，苏联翻译过来的哲学词典之类。想睁眼看看广阔的世界，没门。\u003c/p\u003e\u003cp data-pid=\"aMfqUXhJ\"\u003e《爱因斯坦文集》第一卷，我看到了爱因斯坦眼中的有意义的生活，认识论，物理学的宏大世界，宇宙宗教感情。他这个人本以各种零碎的消息传入我脑中，已是十分的向往了；经此比较完整的阅读了解，似有宗教上开悟了的感觉。\u003c/p\u003e\u003cp data-pid=\"tH9aP8vN\"\u003e第二卷，爱因斯坦的科学论文集。向我展示了他从经典力学出发，提出统力学（别人已提出，但爱因斯坦又独立创造一次），从而把热力学统一在经典力学中；通过狭义相对论，从运动学上统一了经典力学与电动力学；通过广义相对论，统一了狭义相对论与引力。后来致力于统一场论，从统一电磁力与引力入手，目标是建立物理学的基础（包括量子力学作为其推论），虽未竟其功，我也为他切实而宏大的宇宙图景所折服。至于光电效应等等一干成果，是他自己说的“在解决大问题时顺手解决的”。\u003c/p\u003e\u003cp data-pid=\"C0J1mSvY\"\u003e第三卷，社会政治言论。本着从物理世界得到的和谐有序的启发，他主张人类社会也如此，比如他支持建立“世界政府”，某种程度上就是后来的联合国。\u003c/p\u003e\u003cp data-pid=\"Q3MMS0vj\"\u003e哦，十分感谢编译出版这样一个人物的著作，使我看到了真实的物理世界是多么的广阔和美好；感谢爱因斯坦，作为一个榜样，使有形而上情结的人找到了心中的灯塔。\u003c/p\u003e\u003cp data-pid=\"J760kuNB\"\u003e记得有人说过，地球，因有了爱因斯坦而变得不同。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":81,"favorite_count":6,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921339466062214788}","attached_info":"CuUFCOfTv8rry6mZjQEQBBoJNzM0MDEzMDg5ILGZ8MIGKAIwAEBXSiMKGFRTX1NPVVJDRV9XQVJNX1VQX0JPT1NUMRIBMBgAIAA6AEovCiRUU19TT1VSQ0VfV0FSTVVQX1RXT1RPV0VSX0VYUFYyX1RFWFQSATAYACAAOgBaCTExMjExODk5MmIgYjFlYTFmZThjMjk1ZDk3ZGRjNTE2N2MzNmRkMTY3NjdyEzE5MjEzMzk0NjYwNjIyMTQ3ODiKAQo1NzEyMDcwNjczqgEJcmVjb21tZW5kwgEgNzA2ZTkwNzBiM2JmMDAwMWJmYzhjZTIwOWY1MmI1MDTyAQoIDBIGTm9ybWFs8gEoCAoSJDFmYzJjNjhkLThkZTctNGQ4Ni1hZjViLTBkYzg5ZTdkOTJhM/IBBggLEgIxNYICAIgC9oXMzfoykgIgNzA2ZTkwNzBiM2JmMDAwMWJmYzhjZTIwOWY1MmI1MDSaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIVVXNlckxjbkV4aXRXZWlnaHRSdWxl2gIYVFNfU09VUkNFX1dBUk1fVVBfQk9PU1Qx6AID+gILTk9STUFMX0ZMT1eKAyAwOTZlOGI0Mjg5NTY0ZWMxOTk5NGI3Yzg2MjczMWMyM5oDDQoCdjIQABoFb3RoZXKoA1HYAwDqAy5jb250ZW50V2FybXVwVHdvVG93ZXJUdnBUZXh0Qm9vc3RFeHBWMlJlY2FsbGVy+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAJhacIEAzQwMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAoBfTqT+BBQAAAAAAAAAAiQV+opJji5bSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUPkAYAoAZdqAYAkgIuCgk3MzQwMTMwODkSEzE5MjEzMzk0NjYwNjIyMTQ3ODgYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"88_1750898770.175","type":"feed","offset":88,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898770,"updated_time":1750898770,"target":{"id":"1921294818920871537","type":"answer","url":"https://api.zhihu.com/answers/1921294818920871537","author":{"id":"94fb954a939e6315dd8db8540734bc88","url":"https://api.zhihu.com/people/94fb954a939e6315dd8db8540734bc88","user_type":"people","url_token":"sheng-wu-han-si-wu-ju","name":"生无憾死无惧","headline":"","avatar_url":"https://picx.zhimg.com/50/v2-fc8004b7edbaa2e9e4296062ca8feccf_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":2,"is_following":false,"is_followed":false},"created_time":1750852380,"updated_time":1750852380,"voteup_count":1,"thanks_count":0,"comment_count":0,"is_copyable":true,"question":{"id":"12128564106","type":"question","url":"https://api.zhihu.com/questions/12128564106","author":{"id":"41617fb3af9738d71a86d4fa35591744","url":"https://api.zhihu.com/people/41617fb3af9738d71a86d4fa35591744","user_type":"people","url_token":"KevinCheng","name":"Serendipity C","headline":"工程師","avatar_url":"https://picx.zhimg.com/50/v2-d616f712820cfcf499877ad5ddc1133e_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":204,"is_following":false,"is_followed":false},"title":"在当下AI不断发展进步的年代，普通人可以做些什么来避免被时代抛弃？","created":1739434070,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[350,188574,2396255,3095498,3844268],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"人类社会没有动物世界那么残酷，不至于物竞天择适者生存。 但是，AI的出现其实跟很多年前蒸汽时代到来是一样的，肯定会影响到一部分人，但是也会释放一部分生产力，因为AI到来而衍生的新岗位、新机会肯定也有，甚至说夸张点，AI还是有机会引起一些国家社会的财产资源再分配。 对于愿意学习，愿意改变的人来说肯定是好事。 比如我身边的程序员朋友，很多都用上了AI coding，他们公司去年裁员了一部分，但是留下来的创造的成果也不…","excerpt_new":"人类社会没有动物世界那么残酷，不至于物竞天择适者生存。 但是，AI的出现其实跟很多年前蒸汽时代到来是一样的，肯定会影响到一部分人，但是也会释放一部分生产力，因为AI到来而衍生的新岗位、新机会肯定也有，甚至说夸张点，AI还是有机会引起一些国家社会的财产资源再分配。 对于愿意学习，愿意改变的人来说肯定是好事。 比如我身边的程序员朋友，很多都用上了AI coding，他们公司去年裁员了一部分，但是留下来的创造的成果也不…","preview_type":"default","preview_text":"","reshipment_settings":"allowed","content":"\u003cp data-pid=\"A3Uuu8jd\"\u003e人类社会没有动物世界那么残酷，不至于物竞天择适者生存。\u003c/p\u003e\u003cp data-pid=\"Ug5p6S0w\"\u003e但是，AI的出现其实跟很多年前蒸汽时代到来是一样的，肯定会影响到一部分人，但是也会释放一部分生产力，因为AI到来而衍生的新岗位、新机会肯定也有，甚至说夸张点，AI还是有机会引起一些国家社会的财产资源再分配。\u003c/p\u003e\u003cp data-pid=\"-yLYtuIA\"\u003e对于愿意学习，愿意改变的人来说肯定是好事。\u003c/p\u003e\u003cp data-pid=\"azlv83Yi\"\u003e比如我身边的程序员朋友，很多都用上了AI coding，他们公司去年裁员了一部分，但是留下来的创造的成果也不比当时人多的时候少，而且他们人均到手也更多了，用他的话来说，房贷压力轻了一点，但是也有危机感，害怕跟不上时代被淘汰。\u003c/p\u003e\u003cp data-pid=\"Pxid8YsN\"\u003e所以，对于普通人来说到底是危机还是机遇，都看你的选择。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":true,"visited_count":20,"favorite_count":1,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921294818920871537}","attached_info":"CvgFCOfTv8rry6mZjQEQBBoJNzMzOTkxNDk1IJzG78IGKAEwAEBYSiQKGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDISATAYACAAOgBaCTExMzQwMTY0NGIgYjFlYTFmZThjMjk1ZDk3ZGRjNTE2N2MzNmRkMTY3NjdyEzE5MjEyOTQ4MTg5MjA4NzE1MzeKAQsxMjEyODU2NDEwNqoBCXJlY29tbWVuZMIBIDk0ZmI5NTRhOTM5ZTYzMTVkZDhkYjg1NDA3MzRiYzg48gEKCAwSBk5vcm1hbPIBKAgKEiQ4OTMxNmJkMi1hMThiLTQwZWMtYTUyYy05NTM3YmRkODgzNGTyAQYICxICMTWCAgCIAveFzM36MpICIDk0ZmI5NTRhOTM5ZTYzMTVkZDhkYjg1NDA3MzRiYzg4mgIAygIWU2hvckludGVyZXN0V2VpZ2h0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCGENvbnRlbnRXYXJtVXBCcmVha0luUnVsZdoCGVRTX1NPVVJDRV9XQVJNX1VQX05PUk1BTDLoAgL6AgtOT1JNQUxfRkxPV4oDIDA5NmU4YjQyODk1NjRlYzE5OTk0YjdjODYyNzMxYzIzkgMXemhpaHU6Ly90b3BpY3MvMjY2OTE4OTWaAw0KAnYyEAAaBW90aGVyqAMU2AMA6gMfdGV4dF8xMmhvdXJfdW5pZmluc2hlZF9yZWNhbGxlcvoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAMDGTJw/gQUAAAAAAAAAAIkFfqKSY4uW0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFD5AGAKAGXqgGAZICLgoJNzMzOTkxNDk1EhMxOTIxMjk0ODE4OTIwODcxNTM3GAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"89_1750898770.276","type":"feed","offset":89,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898770,"updated_time":1750898770,"target":{"id":"1917670117212422864","type":"answer","url":"https://api.zhihu.com/answers/1917670117212422864","author":{"id":"922e15ff6c91bd084d6b055d00bdf9f4","url":"https://api.zhihu.com/people/922e15ff6c91bd084d6b055d00bdf9f4","user_type":"people","url_token":"keep-56-42","name":"居里富人","headline":"人在感到幸福的时候是不会被过去绊住的 ​​​。","avatar_url":"https://picx.zhimg.com/50/v2-fdf9b2a48886e42dedff28bb89dd1a10_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":1929,"is_following":false,"is_followed":false},"created_time":1749988184,"updated_time":1749988352,"voteup_count":1527,"thanks_count":84,"comment_count":132,"is_copyable":false,"question":{"id":"267775628","type":"question","url":"https://api.zhihu.com/questions/267775628","author":{"id":"1c1144a59bc498e535664e78efed7ce9","url":"https://api.zhihu.com/people/1c1144a59bc498e535664e78efed7ce9","user_type":"people","url_token":"xuan-ran-37","name":"Kaiser Young","headline":"健身教练兼网络写手","avatar_url":"https://pic1.zhimg.com/50/v2-0ee5c6a40007954e5be5d2c8df7fce37_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":24,"is_following":false,"is_followed":false},"title":"你从什么时候开始感受到所谓的“资本的力量”？","created":1519614075,"answer_count":0,"follower_count":0,"comment_count":54,"bound_topic_ids":[1546,5582,10895,12475,37596],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"资本最擅长的戏法，就是先夺走我们本该拥有的日常，再包装成“稀缺”卖回。 上网要花钱买“无广告”，想看大自然要花钱进景区，餐厅现炒该是标配吧，现在倒成了值得标榜的卖点。 番茄从沙瓤甜润变成适合长途运输的硬疙瘩，以前咬一口香甜爆汁，现在难吃还死贵。 甚至连该怎么活，都有一套标准流程：买房、买车、考名校、进大厂、考公、结婚……好像不这么干，人生就失败了。 世界在创造需求，大家都逃不过，但我们可以记住： 不…","excerpt_new":"资本最擅长的戏法，就是先夺走我们本该拥有的日常，再包装成“稀缺”卖回。 上网要花钱买“无广告”，想看大自然要花钱进景区，餐厅现炒该是标配吧，现在倒成了值得标榜的卖点。 番茄从沙瓤甜润变成适合长途运输的硬疙瘩，以前咬一口香甜爆汁，现在难吃还死贵。 甚至连该怎么活，都有一套标准流程：买房、买车、考名校、进大厂、考公、结婚……好像不这么干，人生就失败了。 世界在创造需求，大家都逃不过，但我们可以记住： 不…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"p_kUd60e\"\u003e资本最擅长的戏法，就是先夺走我们本该拥有的日常，再包装成“稀缺”卖回。\u003c/p\u003e\u003cp data-pid=\"ZF84qjj0\"\u003e上网要花钱买“无广告”，想看大自然要花钱进景区，餐厅现炒该是标配吧，现在倒成了值得标榜的卖点。\u003c/p\u003e\u003cp data-pid=\"p5n990q-\"\u003e番茄从沙瓤甜润变成适合长途运输的硬疙瘩，以前咬一口香甜爆汁，现在难吃还死贵。\u003c/p\u003e\u003cp data-pid=\"QwJiNI0x\"\u003e甚至连该怎么活，都有一套标准流程：买房、买车、考名校、进大厂、考公、结婚……好像不这么干，人生就失败了。\u003c/p\u003e\u003cp data-pid=\"jF-8mKaQ\"\u003e世界在创造需求，大家都逃不过，但我们可以记住：\u003c/p\u003e\u003cp data-pid=\"Uulj9VKF\"\u003e不要信“消费即幸福”的鬼话，很多让自己开心的事情都不需要花钱，钓鱼、画画、看书、逛公园，找到让自己舒服的节奏，攥紧不被售卖的人生。\u003c/p\u003e\u003cp data-pid=\"96P6fnSO\"\u003e以前电视打开就能看，现在电视还要买视频会员，买就算了，手机会员和电视会员不通用，你想看的东西一会在芒果一会在腾讯，一会在爱奇艺，一会在优酷，叹为观止。\u003c/p\u003e\u003cp data-pid=\"RDu8KXi-\"\u003e他们先用工作和低薪和随时失业让渡风险让你焦虑，再用茶多酚咖啡因让你快乐，以及创造短视频奶头乐让你暂时忘记一瞬间现实。\u003c/p\u003e\u003cp data-pid=\"q-v1_C1A\"\u003e原本我们可以不焦虑、不喝奶茶、不喝咖啡、不看短视频也很开心的。\u003c/p\u003e\u003cp data-pid=\"5QcgcjDg\"\u003e我感觉现在很多人确实想明白了。\u003c/p\u003e\u003cp data-pid=\"XVeN6eMT\"\u003e资本：宁愿把牛奶倒掉也不给穷人喝。\u003c/p\u003e\u003cp data-pid=\"UwBQJOgm\"\u003e打工人：宁愿在家躺着也不干996月薪2000的奴才活。\u003c/p\u003e\u003cp data-pid=\"TvRqablu\"\u003e包括现在卖很贵的有机蔬菜，也仅仅只是我们本来就该吃到的，所谓昂贵的“有机蔬菜”“串收番茄”“羽衣甘蓝”就是该正常种植出的蔬菜。\u003c/p\u003e\u003cp data-pid=\"iGdDvLeE\"\u003e最近去了一家公司附近的餐厅，主打“现炒菜”，你能看到三个不同的窗口，每个窗口背后都是站着一个厨师，面前两口锅，前面是几盆不同的菜，厨师在你面前炒菜。\u003c/p\u003e\u003cp data-pid=\"tmBOo1_9\"\u003e门口的服务员经常强调：“我们没有用预制菜哦，是现炒的才好吃！”\u003c/p\u003e\u003cp data-pid=\"y28AWerG\"\u003e听到这句我突然怔住了：为什么 “现炒”这个以前再普通不过的做法，现在反而成了一个值得拿出来标榜的“卖点”?\u003c/p\u003e\u003cp data-pid=\"KJjdIVpG\"\u003e我又联想起在地铁上看到的一家叫“农耕记”的餐厅，广告语赫然写着：“坚决不用预制菜！”\u003c/p\u003e\u003cp data-pid=\"XKMYAsRD\"\u003e但是等一下，在不知道几年以前，所有餐厅本来就没有预制菜啊！\u003c/p\u003e\u003cp data-pid=\"huSgfk1P\"\u003e让“正常”变成“稀有”，这就是资本最荒谬的地方。\u003c/p\u003e\u003cp data-pid=\"tO_MMAb4\"\u003e它们最擅长的，好像就是先悄悄夺走你本来就有的东西，然后再包装成“稀缺资源”，再卖给你。\u003c/p\u003e\u003cp data-pid=\"OkicbrYs\"\u003e再比如:\u003c/p\u003e\u003cp data-pid=\"1aFNkiy_\"\u003e曾经我们都可以自由地上网冲浪，现在却要花钱买“纯净无广告”的服务才能不被骚扰。\u003c/p\u003e\u003cp data-pid=\"0GQAcSL6\"\u003e地球资源本来就是大家共享的，很多自然景点现在却要花钱才能打卡。\u003c/p\u003e\u003cp data-pid=\"_khtoUAK\"\u003e甚至“日出、日落和星空”都成了一种奢侈，得要加班，还有无处不在的光污染。\u003c/p\u003e\u003cp data-pid=\"_PhARYc1\"\u003e资本的本质，就是把生活分解成一块一块的“产品”，让你为每一个本来理所应当的东西付费。\u003c/p\u003e\u003cp data-pid=\"RsXTtned\"\u003e它夺走了原本属于我们的正常，然后用一个个“看似更好”的版本卖回来。\u003c/p\u003e\u003cp data-pid=\"iAhj7LI6\"\u003e我们努力赚钱，结果买回来的，不过是我们早就该拥有的日常。\u003cbr/\u003e\u003cbr/\u003e说到底，我们不是在追求更好，而是在努力夺回原本的生活。\u003c/p\u003e\u003cp data-pid=\"U35SCRbT\"\u003e这世界就好像一片草原，牛本来可以自由自在吃草，突然来了一个农场主把牛圈起来养，告诉牛你必须给我干活才能吃草。\u003c/p\u003e\u003cp data-pid=\"daCuYzXM\"\u003e为了不让牛发现可以直接吃到草又创造出货币，只有拼命干活得到钱才能用钱换草。\u003c/p\u003e\u003cp data-pid=\"z13u7uv2\"\u003e后来农场主觉得还不够，把草染成各种颜色，告诉牛黑色的草吃了健康，粉色的草让你时尚，金色的草更贵能给你优越感……\u003c/p\u003e\u003cp data-pid=\"yuCHKmln\"\u003e真正害怕走出这个栅栏的是农场主而不是牛。\u003c/p\u003e\u003cp data-pid=\"NkMwsPW2\"\u003e而牛只会发现原来这才是真正地活着。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":65312,"favorite_count":961,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1917670117212422864}","attached_info":"CusGCOfTv8rry6mZjQEQBBoJNzMyMzQyODA1INjmusIGKPcLMIQBQFlKPwoqVFNfU09VUkNFX1pSRUNBTExfRkVFRFJFX05FV0JJRV9IT1VSTFlfUlVNEgEwGAAgADoKeyJyYXciOiIifVoIMjE1NDMzMzdiIGIxZWExZmU4YzI5NWQ5N2RkYzUxNjdjMzZkZDE2NzY3chMxOTE3NjcwMTE3MjEyNDIyODY0igEJMjY3Nzc1NjI4qgEJcmVjb21tZW5kwgEgOTIyZTE1ZmY2YzkxYmQwODRkNmIwNTVkMDBiZGY5ZjTyAQoIDBIGTm9ybWFs8gEoCAoSJGZkYWZlMmRhLTgyNmUtNDY5OS04MmY3LTdkZGY4M2NjM2E5YfIBBggLEgIxNYICAIgC94XMzfoykgIgOTIyZTE1ZmY2YzkxYmQwODRkNmIwNTVkMDBiZGY5ZjSaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCFlJldmlzaXRWYWx1ZVdlaWdodFJ1bGXKAhhQZXJpb2RJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZcoCHEJheWVzRmlyc3RMZXZlbElzb2xhdGlvblJ1bGXaAipUU19TT1VSQ0VfWlJFQ0FMTF9GRUVEUkVfTkVXQklFX0hPVVJMWV9SVU3oAgL6AgtOT1JNQUxfRkxPV4oDIDA5NmU4YjQyODk1NjRlYzE5OTk0YjdjODYyNzMxYzIzmgMNCgJ2MhAAGgVvdGhlcqgDoP4D2AMA6gMQbmV3YmllX2ZlZWRyZV92MvoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQGbWFudWFswgQDMTYwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAADgUT66P4EFAAAAAAAAAACJBX6ikmOLltI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBQ+QBgCgBl+oBgCSAi4KCTczMjM0MjgwNRITMTkxNzY3MDExNzIxMjQyMjg2NBgEIgpJTUFHRV9URVhU","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=89\u0026desktop=true\u0026end_offset=95\u0026page_number=16\u0026session_token=b1ea1fe8c295d97ddc5167c36dd16767","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=89\u0026desktop=true\u0026end_offset=95\u0026page_number=16\u0026session_token=b1ea1fe8c295d97ddc5167c36dd16767","totals":0},"fresh_text":"推荐已更新"}
