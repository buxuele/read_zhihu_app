{"data":[{"id":"150_1750898556.43","type":"feed","offset":150,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750898556,"updated_time":1750898556,"target":{"id":"1917228976725812658","type":"article","url":"https://api.zhihu.com/articles/1917228976725812658","author":{"id":"0a88f90026c27b264a94ab28aa14b529","url":"https://api.zhihu.com/people/0a88f90026c27b264a94ab28aa14b529","user_type":"people","url_token":"zfinancenews","name":"Z Finance","headline":"我们相信认知跨越阶层。","avatar_url":"https://picx.zhimg.com/50/v2-f13c6f0775cae1417a3097d7bf29cd3b_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":336,"is_following":false,"is_followed":false},"title":"深度｜红杉资本：95%的AI创业和传统创业别无二致，在AI无限产出的时代，品味将成为最后的壁垒","comment_permission":"all","created":1750122810,"updated":1750122810,"voteup_count":43,"voting":0,"comment_count":0,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"当算力、模型、分发路径与用户习惯同时成熟，AI 应用的黄金时代悄然来临。真正的主战场，已经从“训练最强模型”转向“谁能让AI真正用起来”。在红杉资本的这场闭门分享会上，Pat Grady、Sonia Verma 和 Constantine Vassilev 共同抛出四个关键问题——What is it、So what、Why now、Now what？——试图为这场史无前例的智能浪潮划定航向。 他们给出的判断令人震撼：AI不仅正在颠覆服务业，还将在十年内撕裂整个软件产业的利润…","excerpt_new":"当算力、模型、分发路径与用户习惯同时成熟，AI 应用的黄金时代悄然来临。真正的主战场，已经从“训练最强模型”转向“谁能让AI真正用起来”。在红杉资本的这场闭门分享会上，Pat Grady、Sonia Verma 和 Constantine Vassilev 共同抛出四个关键问题——What is it、So what、Why now、Now what？——试图为这场史无前例的智能浪潮划定航向。 他们给出的判断令人震撼：AI不仅正在颠覆服务业，还将在十年内撕裂整个软件产业的利润…","preview_type":"default","preview_text":"","content":"\u003cp\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-c805b3c859afc5ee9d5da3cf28bd83e7_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"828\" data-rawheight=\"465\" data-original-token=\"v2-eef8db1fc6d50e16ddaf328cd146eebc\" class=\"origin_image zh-lightbox-thumb\" width=\"828\" data-original=\"https://pic4.zhimg.com/v2-c805b3c859afc5ee9d5da3cf28bd83e7_r.jpg\"/\u003e\u003cfigcaption\u003e图片来源：Sequoia Capital\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Q5f1M-x1\"\u003e\u003ci\u003e当算力、模型、分发路径与用户习惯同时成熟，AI 应用的黄金时代悄然来临。真正的主战场，已经从“训练最强模型”转向“谁能让AI真正用起来”。在红杉资本的这场闭门分享会上，Pat Grady、Sonia Verma 和 Constantine Vassilev 共同抛出四个关键问题——What is it、So what、Why now、Now what？——试图为这场史无前例的智能浪潮划定航向。\u003c/i\u003e\u003c/p\u003e\u003cp data-pid=\"MiBeo2QR\"\u003e\u003cbr/\u003e\u003ci\u003e他们给出的判断令人震撼：\u003cb\u003eAI不仅正在颠覆服务业，还将在十年内撕裂整个软件产业的利润结构\u003c/b\u003e。传统的工具型企业将被AI重构为结果导向型组织——不是“卖软件”，而是“卖结果”；不是“雇佣人”，而是“调用Agent”。\u003c/i\u003e\u003c/p\u003e\u003cp data-pid=\"ZKugWxYo\"\u003e\u003cbr/\u003e\u003ci\u003e更具争议的是红杉的核心观点之一：\u003cb\u003eAI创业的决定性战役将发生在应用层\u003c/b\u003e，而非底层模型。这与过往“大模型赢家通吃”的逻辑背道而驰，却也解释了为何 OpenEvidence、Glean、Harvey 等应用型AI公司迅速崛起。\u003c/i\u003e\u003c/p\u003e\u003cp data-pid=\"0_cENY-N\"\u003e\u003cbr/\u003e\u003ci\u003e\u003cb\u003e“智能体即公司，应用层即未来。”\u003c/b\u003e 他们断言，未来的商业世界将被由AI驱动的智能体网络重构，每个企业职能都将由AI智能体替代，人类角色则退居为策略协调者与风险管理者。而AI Agent的杀手级场景，极可能出现在医疗、法律、教育、客服、DevOps等“高决策+高复杂度”的垂直领域。\u003c/i\u003e\u003c/p\u003e\u003cp data-pid=\"TGodqPwr\"\u003e\u003cbr/\u003e\u003cb\u003e\u003ci\u003e以下是全文翻译。\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003ch2\u003e\u003cbr/\u003e\u003cb\u003e应用层才是AI的主战场\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"UrVNvJ-x\"\u003e\u003cbr/\u003e\u003cb\u003ePat Grady\u003c/b\u003e：我是Pat Grady，红杉资本的合伙人。今天由我、Sonia、Constantine以及所有红杉合伙人共同主持这场活动。在进入正题之前，我们三人想先分享一些过去一年中积累的观察与思考。我们当然知道，自己只是开胃菜而不是主菜，我们理解大家真正期待的是什么，但在那之前，请允许我们先提出几个思考框架。\u003cbr/\u003e首先，我们希望做一个校准：我们怎么看当前AI领域正在发生的事情？\u003cb\u003e这是我们用来理解市场的一个简单分析框架：“What is it?”这是唐·Valentine式的提问。“So what?”它有何意义？“Why now?”为什么偏偏是现在？或许这不可避免，但它是否真的已迫在眉睫？最后是“Now what?”我们该做什么？怎样把握机遇？如何赢得胜利？\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e这几年来我们逐一探讨过这些问题，今天我们来更新一下我们的思考。说实话，我原本想聊一个重磅主题，但Constantine委婉地提醒我，在满屋子AI专家面前讲“AI不是什么”可能不太合适。所以我们直接讲重点。\u003cbr/\u003e去年我们曾分享过一个三阶段的转型模型，它将云计算与AI的发展进行了对比：左边代表过去，中间是现在，右边预示未来。我们当时观察到，云转型初期的市场规模就已经达到了4000亿美元，甚至超越了整个全球软件市场。如果用类似的逻辑来看AI，\u003cb\u003e眼下我们所面对的市场起点，至少比过去的云服务市场大一个数量级。展望未来十年或二十年，这个市场的潜力极其庞大。\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-c754586bb16cb6b8db3b822a8d02fcc0_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"828\" data-rawheight=\"434\" data-original-token=\"v2-77293f4be884c4affd4110bfa58af9a2\" class=\"origin_image zh-lightbox-thumb\" width=\"828\" data-original=\"https://pic3.zhimg.com/v2-c754586bb16cb6b8db3b822a8d02fcc0_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"rcOmRLja\"\u003e\u003cbr/\u003e\u003cb\u003e我们现在的认知是：AI所颠覆的远远不止服务市场，它也在冲击整个软件产业。这意味着两个最大的利润池：服务与软件都在同步遭遇变革。\u003c/b\u003e我们已经看到很多企业从传统软件起步，逐步走向智能化：先像辅助驾驶那样为人类协作，再像自动驾驶那样自主执行。最终，企业不再只是销售工具，而是开始把软件预算转化为销售成果，进一步演化为人力成本上的节省。\u003c/p\u003e\u003cp data-pid=\"ZlKjdVm3\"\u003e\u003cbr/\u003e换句话说，\u003cb\u003eAI正让这两个核心市场（TAM）在同时开放，利润池的争夺正处于初期阶段\u003c/b\u003e。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-caef1a157124616521a82bebaf93a118_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" data-original-token=\"v2-05c1724ac429414f12816e9403dff803\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-caef1a157124616521a82bebaf93a118_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"cuxisQaQ\"\u003e\u003cbr/\u003e过去我们曾总结过技术浪潮的演进轨迹，\u003cb\u003e其中有两个核心观点：\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"hwxdYJR-\"\u003e\u003cbr/\u003e\u003cb\u003e第一，AI已不再是遥远的未来，而是真正进入了临界点。算力、网络、数据、分布式架构和人才等所有关键条件如今已全部到位。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"niZZEflq\"\u003e\u003cbr/\u003e\u003cb\u003e第二，不同的技术浪潮往往是叠加发生的，而这一次的AI浪潮，不仅规模远超以往，来临的速度也更加惊人\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"el-p6cGA\"\u003e\u003cbr/\u003e老实讲，我不太喜欢用那种趋势图表，X轴是时间，Y轴是所谓的“虚荣指标”。大家总用这些图表为各种不当行为找借口。但它揭示的现象是对的：事件的发生速度确实越来越快。然而，真正去思考其背后动力的人并不多。我们不妨换个角度看问题，用传播物理学的思维来理解。\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e任何技术扩散其实只需要三个条件：人们知道你的产品、想要你的产品、并且有能力获得你的产品，仅此而已。\u003c/b\u003e还记得早年间云计算刚启动时吗？根本没有人关注，Ben Horowitz甚至需要靠游击营销引发注意。而AI完全不同。2022年11月30日，ChatGPT的横空出世让全世界的目光都聚焦到了AI。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-6cfa4eb2ec7f6da392757e74a2a9ff12_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"560\" data-original-token=\"v2-9f5709fb650860e679aa334b0f22d4ac\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-6cfa4eb2ec7f6da392757e74a2a9ff12_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"iA8bmCfe\"\u003e\u003cbr/\u003e在云计算和移动互联网兴起的早期阶段，像Reddit和Twitter这样的社交平台还尚未出现。而现在，它们已成为亿级用户聚集的传播渠道。与此同时，全球互联网用户从20亿增长到了56亿，几乎覆盖了每一个家庭和企业。这说明基础设施已经就位，当下再无采用障碍。这并非AI独有的现象，而是整个技术分发模式的新常态，底层规则已经改变。铁轨已经铺好。\u003c/p\u003e\u003cp data-pid=\"FmXoEGMK\"\u003e\u003cbr/\u003e那么，问题来了：我们该如何应对？我们的决胜点又在哪里？有两个核心要素值得关注：\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e第一，虽然很多空白市场如今已有企业进入，但仍存在大量未被开发的领域；\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"8qkI5CQu\"\u003e\u003cbr/\u003e\u003cb\u003e第二，在以往的技术转型中，真正做到十亿级营收的企业，大多集中在应用层。\u003c/b\u003e这正是我们关注的焦点。\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e我们始终认为，AI的最大价值也将在应用层实现。\u003c/b\u003e但这条路径并不容易，你将面临激烈竞争。你需要理解“第二增长曲线”的逻辑，拥有测试算力，具备工具推理与智能体通信能力，才能让基础模型深入渗透到应用层。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-36f236e1f596909427e71e78a3af5772_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" data-original-token=\"v2-c0d47c39ea59603cba412d323a8c06cb\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-36f236e1f596909427e71e78a3af5772_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"NdG6vAWY\"\u003e\u003cbr/\u003e\u003cb\u003e那作为非垂直整合的初创公司，你该怎么做？答案是：从用户需求出发，专注垂直场景，解决那些仍需人工参与的复杂问题。这才是竞争的本质所在，价值最终将汇聚于此。\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-44b8f3bf3032aeb375f3140b9ac702f1_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"589\" data-original-token=\"v2-02a4c473341ca36432bedf19a33c3ca3\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-44b8f3bf3032aeb375f3140b9ac702f1_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"YZFwjLQX\"\u003e\u003cbr/\u003e那具体要怎么赢？95%的AI创业其实和传统创业别无二致：你还是需要用独特方式解决真正的问题，还是要建立一支优秀的团队。这是基本功。真正属于AI的那5%，则体现在三个方面：\u003cbr/\u003e\u003c/p\u003e\u003cb\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-cbae92fd409f329effdb6cd36f78450d_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"504\" data-original-token=\"v2-aff77e5f97014578f023c99d41abf078\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-cbae92fd409f329effdb6cd36f78450d_r.jpg\"/\u003e\u003c/figure\u003e\u003c/b\u003e\u003cp data-pid=\"p-NpkbCK\"\u003e\u003cbr/\u003e\u003cb\u003e第一是营收。请远离虚浮营收，别被假象欺骗。\u003c/b\u003e看起来像是“我们赚翻了”的繁荣背后，可能只是昙花一现。你要问自己：这是否真正改变了用户行为？是否有参与率、留存率、活跃度来支撑？\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e第二是利润率。不担心你当前的毛利率高低，因为token成本在过去18个月里已经下降了99%，未来只会更低。你需要的是沿价值链不断向上，提升定价权和利润空间，构建一个可以持续盈利的业务模型。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Z4VEUncq\"\u003e\u003cbr/\u003e\u003cb\u003e第三是数据飞轮。\u003c/b\u003e请认真思考：你是否真正建立了一个能驱动核心业务指标的数据飞轮？如果连它推动了什么都说不清，那它多半并不重要。真正有价值的数据飞轮，必须与业务成果紧密挂钩，这是你最有可能构建出的核心护城河之一。\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e最后，我想强调一点：市场永远厌恶真空。\u003c/b\u003e当你还在观望时，别人就已经上场。现在我们看到的是巨大的虹吸效应，所有宏观经济的噪音都不重要，技术采用的趋势将淹没一切市场波动。换句话说，如果你现在不冲刺，别人就会抢跑。而你必须全速前进，从现在开始就保持极限速度。\u003c/p\u003e\u003ch2\u003e\u003cbr/\u003e\u003cb\u003e智能体即新公司：构建人与AI共生的机器网络\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"x_dF_1Js\"\u003e\u003cbr/\u003e\u003cb\u003eSonia Verma：\u003c/b\u003e谢谢Pat。接下来我想聚焦于我们正在经历的一些关键变化。我们将从客户反馈和技术演进两个维度，快速回顾AI在过去一年中的真实进展。首先是一个年度观察。2023年我们曾提出一个判断：AI原生应用的用户活跃度远低于传统移动应用，日活与月活的比值极低，炒作明显大于实际使用。但现在我们很高兴地宣布，这个判断发生了重大转变。\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e以ChatGPT为例，它的日活与月活比率曲线已迅速上升，接近Reddit等成熟平台，令人惊叹。这无疑是个积极信号。越来越多的人正在从AI中获得实际价值，大家都在探索如何将其真正融入日常生活。\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e很多时候，这种使用本身也很有趣，我自己就曾为了尝试各种“Jibilify”风格生成，烧掉了相当多GPU资源。虽然这种现象带有明显的娱乐性和病毒式传播特征，但更令人兴奋的是，我们刚刚触及的深层应用可能性。广告行业正在用生成式AI打造精准文案，教育领域能即时将抽象概念可视化，医疗场景中如OpenEvidence等工具也正在辅助医生实现更高精度的诊断。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-f128e61e54f4753bbe7aa3b877f180da_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"582\" data-original-token=\"v2-f6279ac09c4f8bb4eefa40762a63bc0d\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-f128e61e54f4753bbe7aa3b877f180da_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"D52dwDe9\"\u003e\u003cbr/\u003e我们眼下所见的，不过是这场巨大能力释放的起点而已。说到人与AI的关系，不知道大家有没有看过电影《Her》？虽然我们还没迎来AI版的Scarlett Johansson，但2024年确实可以被称作语音生成的“Her时刻”。语音技术终于全面跨越了“恐怖谷效应”，进入了真正自然流畅的体验阶段。\u003c/p\u003e\u003cp data-pid=\"gKygVkEE\"\u003e\u003cbr/\u003e有人提醒我要制造点悬念，那我们就来看看这些新技术是否真的能颠覆你对AI的认知。《Her》这部片子真是经典，Joaquin Phoenix那段爱上操作系统的表演深入人心。\u003cb\u003e而现在，包括Sesame在内的语音系统已经达到了令人难以置信的水平。科幻与现实之间的缝隙，正以惊人的速度消失。\u003c/b\u003e图灵测试似乎已经悄然成为现实，这一观点来自Jim Fan的一条推文，我也借来作为演讲的切入。\u003c/p\u003e\u003cp data-pid=\"nbmIc5eb\"\u003e\u003cbr/\u003e回到过去一年的技术爆发点，今年增长最迅猛的AI应用类别是编程工具。Anthropic发布的Claude 3.5 Sonnet，自去年秋季上线以来，彻底改变了整个开发生态。\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e我们已经看到开发者用AI构建自己的文档工具，编程效率出现了10倍提升。而对于初学者来说，AI更是大幅降低了入门门槛。它正在根本上重塑软件创造的方式：提高可及性、加快速度、降低成本。\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-bf1c1e584590203db06d64fc974c009a_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" data-original-token=\"v2-d232d86461bdb78ab6509fe262e7351b\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-bf1c1e584590203db06d64fc974c009a_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"jmxeKCoL\"\u003e\u003cbr/\u003e当然，技术层面也有坏消息，预训练的进展正在放缓。自AlexNet以来，我们已经把模型规模提升了9到10个数量级，这也意味着最容易的突破机会基本都已用尽。研究界开始转向寻找新的增长路径。其中最重要的突破是OpenAI在“推理能力”方面的进展。去年，我们邀请了Strawberry团队的Noam Brown在AI Ascent大会做了一场精彩预演。今年则将由Dan Roberts带来关于o3模型与推理进化的演讲。\u003c/p\u003e\u003cp data-pid=\"wCLNW251\"\u003e\u003cbr/\u003e但这不只是关于“推理”的故事，它还涵盖了合成数据的生成、工具链的调用能力、AI脚手架等多个新兴模块。这些技术共同组成了一个“智能扩展”的新范式。Anthropic所打造的MCP系统，已初步形成了强大的生态结构，我们非常期待它如何进一步加速“工具型AI”的落地。\u003c/p\u003e\u003cp data-pid=\"PyQSQpey\"\u003e\u003cbr/\u003e我们观察到，一些大型基础模型正在整合推理、工具调用、主动思考等多个模块，逐步形成具备执行复杂任务能力的系统。虽然Meter基准测试提供了量化衡量这些能力的指标，但我们更看重的是：有哪些新事物，只有通过o3、Operator、Notebook LM或Claude Sonnet才变得可行。\u003c/p\u003e\u003cp data-pid=\"v2lB7Cp4\"\u003e\u003cbr/\u003e在我们看来，过去一年中最具突破性的产品就包括Deep Research和Notebook LM。这两款产品的创始人今天也来到了现场：Risa和Jason来自Notebook团队（他们也正在启动一家新公司Hux），Issa Hulford则来自OpenAI。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-f97472c0b8b75ce2ee5327744204bda7_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" data-original-token=\"v2-779155c75dbe15da208c0005a93884e7\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-f97472c0b8b75ce2ee5327744204bda7_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"Nt18igV9\"\u003e\u003cbr/\u003e接下来，我想转向AI技术栈中的价值分布问题。\u003c/p\u003e\u003cp data-pid=\"4ou9Cq-u\"\u003e\u003cbr/\u003e回想过去，我曾和红杉内部的几位合伙人就此进行过一场有趣的辩论。当时我就像那个坐在图表中间、陷入犹豫的“中等智者”，对“GPG rapper”模型的看法拿不定主意。我还记得Pat是当中最坚定的一位，他坚信价值最终会集中在应用层。当时我心里还嘀咕着：“好吧Pat，祝你好运。”但几年下来，事实证明Pat是对的。他站在了胜利者一边，值得致敬。\u003c/p\u003e\u003cp data-pid=\"BuWIngKG\"\u003e\u003cbr/\u003e我们之所以这么说，是因为我们亲眼看到Harvey和OpenEvidence等公司正在真实地为客户创造价值。\u003cb\u003e也正是这些案例，让我们更加确信：AI技术栈的价值终点，确实是应用层。尽管基础模型层的竞争日益激烈，但在真正的客户关系、商业转化与产品落地中，应用层拥有压倒性的价值承载力。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"SY_lLDQy\"\u003e\u003cbr/\u003e说个玩笑话吧，其实我们这些搞应用层的也都被现实打脸了。因为技术栈中真正的“无冕之王”，其实是Jensen Huang“GOAT”本人，他几乎把整个产业的钱都赚走了。我们很期待稍后能听到他的分享。回到应用层，我们认为第一波杀手级AI应用已经到来。无论是ChatGPT、Harvey、Glean、Sierra、Cursor、Bridge，还是Listen Labs和OpenEvidence，这些公司都正在不同终端市场快速崛起。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-fb388e7ec46f4e73f726aa495cd0c367_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" data-original-token=\"v2-6bcf7e03699e5a09a4914c935c11ac44\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-fb388e7ec46f4e73f726aa495cd0c367_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"RECm2XRg\"\u003e\u003cbr/\u003e今天我们也邀请了其中不少代表来到现场。\u003cb\u003e我们观察到，新一代AI公司中，大多数都是以“智能Agent”作为核心产品。\u003c/b\u003e这些Agent系统将从当下拼凑型的原型产品，进化为真正可靠、可部署的智能系统。企业在构建智能Agent时通常采用两种路径：一是通过严密的测试机制，对流程进行精细编排；二是专注于端到端任务，直接对Agent系统进行调优。今天你们也会听到来自LangChain的Harrison和OpenAI的Issa对这两种路径的深入探讨。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-49f0fccefa5c940311af0e8efb3ee6d5_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"602\" data-original-token=\"v2-18935c5f58289d6dc12ea4328761281c\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-49f0fccefa5c940311af0e8efb3ee6d5_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"HzErEhSL\"\u003e\u003cbr/\u003e\u003cb\u003e我们对2025年Agent公司的形态也有一个预测：垂直领域智能Agent将成为主流。对那些深耕特定行业的创业者来说，这将是一次绝佳机会。\u003c/b\u003e我们已经看到一些企业通过合成数据强化学习、结合真实用户数据训练，构建出聚焦于特定工作流的智能Agent系统。这些迹象让我们感到非常乐观。例如，安全行业的Expo展示了能超越人类渗透测试员的AI；在DevOps领域，Traversal的AI故障排查系统已经优于最顶级工程师；网络运维方面，Meter同样表现优异。\u003cbr/\u003e\u003cbr/\u003e\u003c/p\u003e\u003cb\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-b414091622a84f8ec41aa79d9c2c5230_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"599\" data-original-token=\"v2-1a9ebd0c75b9dc22b74772d8f17b6b03\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-b414091622a84f8ec41aa79d9c2c5230_r.jpg\"/\u003e\u003c/figure\u003e\u003c/b\u003e\u003cp data-pid=\"EYFY9fty\"\u003e\u003cbr/\u003e\u003cb\u003e虽然这些案例还处在早期，但它们让我们确信：专注解决具体任务的垂直Agent，完全有可能超越人类专家，成为关键执行力。我们对Agent系统的最终预测是：我们正步入“丰饶时代”。代码是第一个被颠覆的市场，这将成为“丰饶时代”的预演。\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e当劳动力变得几乎无成本且无限可扩展时，会发生什么？会不会迎来大量AI垃圾内容的涌现？当“审美”变成稀缺资源时，又将如何定义创意？我们拭目以待编程Agent的持续演进，因为这不仅会重塑整个软件产业，也会成为未来其他行业AI化进程的先兆。\u003c/p\u003e\u003ch2\u003e\u003cbr/\u003e\u003cb\u003e智能体经济将至：重构个体、组织与经济的未来\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"Hz-jTSex\"\u003e\u003cbr/\u003e\u003cb\u003eConstantine Vassilev：大家早上好，感谢Sonia，也感谢Pat。刚才我们探讨了当前AI的发展现状及其近期趋势。现在，我想带大家退后一步，从更长远的视角出发，看看未来的可能路径。今天我们这部分内容将分为三部分展开：首先，我们将介绍我们眼中的下一波技术浪潮；其次，探讨实现这一浪潮所需的关键技术支撑；最后，我们来看看这些变化将如何重塑每个人的日常生活。\u003c/b\u003e\u003cbr/\u003e\u003cbr/\u003e\u003cb\u003e一年前，在AI Ascent峰会上，我们提出“Agent”是技术栈中的关键构件。\u003c/b\u003e当时我们预测，这些尚在早期商业化阶段的AI助手，将逐步聚合成互联互通的机器网络。\u003cb\u003e今天，这一趋势正在显现，这些网络现在被称为“智能体集群”，它们已经在许多企业中落地并发挥作用，逐步成为AI技术堆栈中的核心基础设施。\u003c/b\u003e这些智能体之间不仅能够协作，还能展开对抗、进行推理，并将在未来几年进一步演化为一种新的经济形态——智能体经济。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-4f418611c777b2ab714794c467886db6_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" data-original-token=\"v2-b9ec8bf465d6eb5e99715e1c72af93b8\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4f418611c777b2ab714794c467886db6_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"lgRN3tEX\"\u003e\u003cbr/\u003e在这种智能体经济中，AI智能体的角色远不止信息传递。它们还能在系统中转移资源、完成交易、进行关系追踪，并建立对信任与可靠性的基本理解，从而构成一个独立且可运行的经济系统。需要强调的是，这并不是一个以“取代人类”为目标的系统。相反，它始终是以人为核心构建的。智能体经济的真正形态，是人与AI之间的深度协同。但要实现这一愿景，我们仍需解决几个关键的技术挑战。今天，我想聚焦其中三项，它们是任何构建者都无法回避的问题。\u003c/p\u003e\u003cp data-pid=\"8wlGHR4t\"\u003e\u003cbr/\u003e\u003cb\u003e首先，是持久身份认证。\u003c/b\u003e这个问题分为两个层面：一是智能体自身的持久性。如果一个商业协作者的性格与认知天天变，长期信任和合作就无从谈起。智能体必须保持稳定、连贯的“个性”和记忆结构。第二，是对用户持久性的理解。试想，如果你的AI每次都记不住你是谁，对你没有任何长期记忆，那信任感也无从建立。我们已经尝试了RAG（检索增强生成）、向量数据库、超长上下文窗口等技术手段，但要实现真正的“长期记忆”与基于记忆的个性化学习，依然面临巨大挑战。换句话说，关键问题是：如何在保持智能体个性和差异化的同时，实现关键环节的一致性？\u003c/p\u003e\u003cp data-pid=\"sSqSpdC7\"\u003e\u003cbr/\u003e\u003cb\u003e第二个技术挑战，是构建无缝通信协议。\u003c/b\u003e令人鼓舞的是，整个行业似乎正在同步朝这个方向努力。想象一下，没有TCP/IP的个人计算将无法连上互联网；而没有通信协议的智能体生态，同样无法建立。我们正在构建这一层“协议堆栈”，其中最令人振奋的进展，是围绕MCP（Multi-chain Protocol，多链协议）的发展。这个协议不仅承载信息传输功能，还能实现价值转移与信任传递。看到众多行业头部力量开始联手推动这项基础设施建设，我们感到非常鼓舞。\u003c/p\u003e\u003cp data-pid=\"Bkvdj73T\"\u003e\u003cbr/\u003e\u003cb\u003e第三个挑战，是安全性。\u003c/b\u003e相信大家都已经感受到这一议题的重要性。当人与商业伙伴无法面对面互动时，安全和信任的价值就会被无限放大。在智能体之间的交互场景中尤其如此。我们预判，围绕“信任与安全”的整个产业链将快速成型，其重要性甚至将超越现有经济体系中的传统安全架构。这不仅是技术挑战，更是基础建设。\u003c/p\u003e\u003cp data-pid=\"5chbRqnZ\"\u003e\u003cbr/\u003e以上是我们对“智能体经济”所需底层能力的简要回顾。接下来，我们谈谈这场变革对每一个人的影响。\u003c/p\u003e\u003cp data-pid=\"ocHdCpLM\"\u003e\u003cbr/\u003e\u003cb\u003e首先，它将改变我们的思维方式。\u003c/b\u003e事实上，在座各位已经具备了一种我们称之为“随机性思维”的能力。这种思维方式有别于我们过去几十年熟悉的“确定性计算”。很多人之所以迷上计算机科学，是因为它的逻辑清晰、结果可控：你写一段代码，机器就按部就班执行，哪怕是bug也能复现。但我们正在进入一个“随机计算时代”。\u003c/p\u003e\u003cp data-pid=\"qPWXznEh\"\u003e\u003cbr/\u003e比如说，你今天告诉AI数字是73，它明天可能记得是73，也可能记成72、74，甚至是下一个质数79。更有可能，它完全忘记了。这背后揭示的是一个本质区别：\u003cb\u003eAI计算将越来越接近人类的“记忆偏差”与“语义跳跃”，这要求我们必须彻底转变认知方式。\u003c/b\u003e\u003cbr/\u003e\u003c/p\u003e\u003cb\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-e6efcae3170eee7416ff56cca45f988c_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"588\" data-original-token=\"v2-1d3ab9fb154030765ad3dec07bf58c86\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-e6efcae3170eee7416ff56cca45f988c_r.jpg\"/\u003e\u003c/figure\u003e\u003c/b\u003e\u003cp data-pid=\"RTO7z2eV\"\u003e\u003cbr/\u003e\u003cb\u003e第二个转变，是管理范式的改变。过去的层级式、静态组织结构，将让位于更具弹性和适应性的动态协同网络。\u003c/b\u003e管理者将不再只是管理人，而是学会与具有一定自主性的智能体共事。这对组织治理提出了新挑战：既要理解算法的不确定性，又要建立起一套可控、可审计的人机问责机制。\u003c/p\u003e\u003cp data-pid=\"jnuK_Dg2\"\u003e\u003cbr/\u003e其中的关键是：你必须知道你的智能体“能做什么，不能做什么”。这就像工程师和工程经理的差别一样，懂技术是一回事，会管理是一回事。未来的组织，必须理解这一变化，并据此构建新的协作模型。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-e5ada2eea2594a2f03d851a0bfe886a7_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"595\" data-original-token=\"v2-fb8439f1ad0b38e9ede6b0069af623c6\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e5ada2eea2594a2f03d851a0bfe886a7_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"Dj45fCoU\"\u003e\u003cbr/\u003e\u003cb\u003e第三个转变，是前两者的结合结果：我们将进入一个“高杠杆、低确定性”的时代。你能做的事情会变得更多，但相应地，你也必须更擅长处理风险与不确定性。\u003c/b\u003e而在座的各位，正处于这个新世界的最佳位置。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-e9a2a63e50cb6ed8d6403883b0526b24_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" data-original-token=\"v2-019082b277371b3587fbfadd326b0b9e\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-e9a2a63e50cb6ed8d6403883b0526b24_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"vPzpMVja\"\u003e\u003cbr/\u003e我们一年前就在AI Ascent大会提出过预测：\u003cb\u003e未来每个组织职能都将配备专属的AI智能体。\u003c/b\u003e而现在，我们进一步认为，这些职能将不再是分散的，而是像神经元一样聚合成集群，由智能体协同完成完整的流程。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-a04c569723d766b814443841ca3eddd8_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"498\" data-original-token=\"v2-59675856559a583206712a3369a5df44\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-a04c569723d766b814443841ca3eddd8_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"35hdu84D\"\u003e\u003cbr/\u003e我们甚至曾大胆设想，未来可能会诞生首个“单人独角兽企业”，一个人通过智能体实现对整个企业流程的掌控。虽然这还未真正发生，但我们已经看到一些公司用极少的人力，实现了前所未有的业务扩张。\u003c/p\u003e\u003cp data-pid=\"HtM1xRTl\"\u003e\u003cbr/\u003e我们可以确信：这个时代的“杠杆效应”正在达到历史最高水平。最终，随着流程与智能体深度融合，我们将迎来像神经网络那样的“嵌套系统”，重构一切。它将颠覆个体的工作模式，重塑企业的组织架构，乃至重建整个经济体系的运转方式。感谢各位的到来。今天，我们将共同见证一场非凡的AI盛会。\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-466e6240dd84ac3ecbbe1115b0554cca_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" data-original-token=\"v2-d969f771b4d239e8a1a0f05eb49be75a\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pica.zhimg.com/v2-466e6240dd84ac3ecbbe1115b0554cca_r.jpg\"/\u003e\u003c/figure\u003e\u003cp data-pid=\"S2AIp5Zm\"\u003e\u003cbr/\u003e \u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic2.zhimg.com/v2-f4797b19d130bf5b0317e6274429a921_1440w.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1700\" data-rawheight=\"2640\" data-original-token=\"v2-7b3526f51357c487c97525c8a225e870\" class=\"origin_image zh-lightbox-thumb\" width=\"1700\" data-original=\"https://pic2.zhimg.com/v2-f4797b19d130bf5b0317e6274429a921_r.jpg\"/\u003e\u003c/figure\u003e\u003cp\u003e\u003c/p\u003e","is_labeled":false,"visited_count":2244,"thumbnails":["https://pica.zhimg.com/50/v2-63819f6c8497aab5f7dc253b2011d597_720w.jpg?source=b6762063"],"favorite_count":111,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 1917228976725812658}","attached_info":"CtkNCN6arYPvwbuY8QEQBxoJMjU5MDgxNDY5ILqCw8IGKCswAECWAUpBCixUU19TT1VSQ0VfVFdPVE9XRVJfU0hPUlRJTlRFUkVTVF9SRUNBTExfVEVYVBIBMBgAIAA6CnsicmF3IjoiIn1iIDU5MDE5OTM4OTdiMmI2NzE1MzRiNDFkZmUxNzAzOTQ3chMxOTE3MjI4OTc2NzI1ODEyNjU4qgEJcmVjb21tZW5kwgEgMGE4OGY5MDAyNmMyN2IyNjRhOTRhYjI4YWExNGI1MjnyAQoIDBIGTm9ybWFs8gEoCAoSJGJkOWJmNTVkLTUyYTQtNDE5Zi04NmU3LTU1OWU3MGVkZWY1OPIBBggLEgIyNoICAIgCmPu+zfoykgIgMGE4OGY5MDAyNmMyN2IyNjRhOTRhYjI4YWExNGI1MjmaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCFlJldmlzaXRWYWx1ZVdlaWdodFJ1bGXKAhhQZXJpb2RJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXaAixUU19TT1VSQ0VfVFdPVE9XRVJfU0hPUlRJTlRFUkVTVF9SRUNBTExfVEVYVOgCBPoCC05PUk1BTF9GTE9XigMgODE2YjNlNTYzM2ZmNDgxMDhiNzlmMDdkYjQ0MDc1MTSaAw0KAnYyEAAaBW90aGVyqAPEEdgDAOoDGmZlZWRfYXR0bV90d290b3dlcl92Ml90ZXh0+gPLBxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREU6LQgDELwGGNEDIiN2Mi1lZWY4ZGIxZmM2ZDUwZTE2ZGRhZjMyOGNkMTQ2ZWViYzotCAIQvAYYsgMiI3YyLTc3MjkzZjRiZTg4NGM0YWZmZDQxMTBiZmE1OGFmOWEyOi0IAhC4CBjcBCIjdjItMDVjMTcyNGFjNDI5NDE0ZjEyODE2ZTk0MDNkZmY4MDM6LQgCELgIGLAEIiN2Mi05ZjU3MDlmYjY1MDg2MGU2NzlhYTMzNGIwZjIyZDRhYzotCAIQuAgY3QQiI3YyLWMwZDQ3YzM5ZWE1OTYwM2NiYTQxMmQzMjNhOGMwNmNiOi0IAhC4CBjNBCIjdjItMDJhNGM0NzMzNDFjYTM2NDMyYmVkZjE5YTMzYzNjYTM6LQgCELgIGPgDIiN2Mi1hZmY3N2U1Zjk3MDE0NTc4ZjAyM2M5OWQ0MWFiZjA3ODotCAMQuAgYxgQiI3YyLWY2Mjc5YWMwOWM0ZjhiYjRlZWZhNDA3NjJhNjNiYzBkOi0IBBC4CBjgBCIjdjItZDIzMmQ4NjQ2MWJkYjc4YWI2NTA5ZmUyNjJlNzM1MWI6LQgCELgIGN8EIiN2Mi03NzkxNTVjNzVkYmUxNWRhMjA4YzAwMDVhOTM4ODRlNzotCAIQuAgY3gQiI3YyLTZiY2Y3ZTAzNjk5ZTVhMDlhNDkxNGM5MzVjMTFhYzQ0Oi0IAhC4CBjaBCIjdjItMTg5MzVjNWY1ODI4OWQ2ZGMxMmVhNDMyODc2MTI4MWM6LQgCELgIGNcEIiN2Mi0xYTllYmQwYzc1YjlkYzIyYjc0NzcyZDhmMTdiNmIwMzotCAIQuAgY3AQiI3YyLWI5ZWM4YmY0NjVkNmViNWU5OTcxNWUxYzcyYWY5M2I4Oi0IAhC4CBjMBCIjdjItMWQzYWI5ZmIxNTQwMzA3NjVhZDNkZWMwN2JmNThjODY6LQgCELgIGNMEIiN2Mi1mYjg0MzlmMWFkMGIzOGU5ZWRlNmIwMDY5YWY2MjNjNjotCAIQuAgY3AQiI3YyLTAxOTA4MmIyNzczNzFiMzU4N2ZiZmFkZDMyNmIwYjllOi0IAhC4CBjyAyIjdjItNTk2NzU4NTY1NTlhNTgzMjA2NzEyYTMzNjlhNWRmNDQ6LQgEELgIGOEEIiN2Mi1kOTY5Zjc3MWI0ZDIzOWU4YTFhMGYwNWViNDliZTc1YTotCAIQpA0Y0BQiI3YyLTdiMzUyNmY1MTM1N2M0ODdjOTc1MjVjOGEyMjVlODcwgAQAiAQAkgQGTm9ybWFsmgQBNKAEAKgEALAEALoEBm1hbnVhbMIEAzExMMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAQF3xuz+BBQAAAAAAAAAAiQWwJcccLZzSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUakAYAoAaaAagGAZICLgoJMjU5MDgxNDY5EhMxOTE3MjI4OTc2NzI1ODEyNjU4GAciCklNQUdFX1RFWFQ=","action_card":false},{"id":"151_1750898556.357","type":"feed","offset":151,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898556,"updated_time":1750898556,"target":{"id":"53600984683","type":"answer","url":"https://api.zhihu.com/answers/53600984683","author":{"id":"396ef596ff23fe7eecb8d317da651822","url":"https://api.zhihu.com/people/396ef596ff23fe7eecb8d317da651822","user_type":"people","url_token":"gou-yu-60","name":"苟渝","headline":"认知行为心理咨询师","avatar_url":"https://picx.zhimg.com/50/6850f877e3c502759e790c42533bf730_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"best_answerer","description":"心理咨询等 3 个话题下的优秀答主","topic_names":["心理咨询","心理学","心理健康"],"topic_ids":[3877,404,5993]}],"followers_count":200988,"is_following":false,"is_followed":false},"created_time":1733977405,"updated_time":1734919653,"voteup_count":342,"thanks_count":32,"comment_count":31,"is_copyable":false,"question":{"id":"667214790","type":"question","url":"https://api.zhihu.com/questions/667214790","author":{"id":"416453cba7d30de57799c06142b819f7","url":"https://api.zhihu.com/people/416453cba7d30de57799c06142b819f7","user_type":"people","url_token":"qing-xiang-wei-xiao","name":"清清读书探险","headline":"喜阅读，持续写，持续积累，记录美好生活，分享即成长","avatar_url":"https://picx.zhimg.com/50/v2-907b990778df500742f4c607dc5feb1f_l.jpg?source=b6762063","is_org":false,"gender":0,"badge":[{"type":"identity_people","description":"独立理财顾问专业能力证书持证人"}],"followers_count":45,"is_following":false,"is_followed":false},"title":"现在好多自媒体博主都回去上班了，朋友账号几十万粉丝，视频收入越来越少，不知要不要换赛道?","created":1726364009,"answer_count":0,"follower_count":0,"comment_count":2,"bound_topic_ids":[645,602967,411538,32797],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"好多人搞自媒体都弄不清楚这玩意到底是个啥东西。同样是自媒体账号，简单粗暴地可以分为三类（注意这个分类并不严谨，主要只是为了说明粉丝为什么关注，以及获取流量的渠道）： 第一类是整活休闲类。像奥德彪剪辑，搞笑段子，真人秀之类的，都属于这一类。 此类账号确实容易粉丝多，就像以前智能手机还不流行时，学生上课总喜欢多带几本故事会一样，精神粮食没人嫌多，有人想要“屯粮”就会有关注，粉丝成本低，但忠诚度也不高。…","excerpt_new":"好多人搞自媒体都弄不清楚这玩意到底是个啥东西。同样是自媒体账号，简单粗暴地可以分为三类（注意这个分类并不严谨，主要只是为了说明粉丝为什么关注，以及获取流量的渠道）： 第一类是整活休闲类。像奥德彪剪辑，搞笑段子，真人秀之类的，都属于这一类。 此类账号确实容易粉丝多，就像以前智能手机还不流行时，学生上课总喜欢多带几本故事会一样，精神粮食没人嫌多，有人想要“屯粮”就会有关注，粉丝成本低，但忠诚度也不高。…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"XAIjpTqK\"\u003e好多人搞自媒体都弄不清楚这玩意到底是个啥东西。同样是自媒体账号，简单粗暴地可以分为三类（注意这个分类并不严谨，主要只是为了说明粉丝为什么关注，以及获取流量的渠道）：\u003c/p\u003e\u003cp data-pid=\"4BT0e5Ew\"\u003e\u003cb\u003e第一类是整活休闲类。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"idsnTdox\"\u003e像奥德彪剪辑，搞笑段子，真人秀之类的，都属于这一类。\u003c/p\u003e\u003cp data-pid=\"Md6OTcUc\"\u003e此类账号确实容易粉丝多，就像以前智能手机还不流行时，学生上课总喜欢多带几本故事会一样，精神粮食没人嫌多，有人想要“屯粮”就会有关注，粉丝成本低，但忠诚度也不高。\u003c/p\u003e\u003cp data-pid=\"QyCyUdIk\"\u003e这种账号的变现途径只有一个：广告或带货（其实带货的本质依然是帮人打广告）。\u003c/p\u003e\u003cp data-pid=\"F3y9d-Pf\"\u003e本质上就跟电视节目一样，制造关注，利用关注转化成信息发布渠道，然后向其他需要借渠道发布信息的人收过路费。\u003c/p\u003e\u003cp data-pid=\"EewSGVHc\"\u003e市场环境好的时候，广告客户多，这一类账号就能赚钱。\u003c/p\u003e\u003cp data-pid=\"iEeZzAon\"\u003e市场不好，那就大部分喝西北风。\u003c/p\u003e\u003cp data-pid=\"3bP55RSq\"\u003e而且如果整活内容过于重复，还有可能导致观众审美疲劳，就像看完的故事会，即使丢在家里不拿去卖废纸，也不会有人再去翻开看。\u003c/p\u003e\u003cp data-pid=\"XBvprgEQ\"\u003e所以开这一类账号除非能得到资本青睐而壮大和转型，比如小杨哥和陈翔六点半。否则就只能是赚个快钱。\u003c/p\u003e\u003cp data-pid=\"1H5PuQW9\"\u003e甚至有人专门就搬运别人的视频做剪辑养号，养好了直接卖掉。\u003c/p\u003e\u003cp data-pid=\"cUCq5vg1\"\u003e这一类账号的变现陷阱就是：流量不一定等于未来收益。因为粉丝对账号博主的信任度并不高，不一定会产生消费欲望。\u003c/p\u003e\u003cp data-pid=\"BgE9va3b\"\u003e很多明星带货翻车都是因为这个原因，知道你的人多不代表就一定要到你这里消费。\u003c/p\u003e\u003cp data-pid=\"cM7xYUHl\"\u003e除非明星本身的气质和产品特性吻合，能引起特定消费群体的信任，这才会有形象代言的效果来促进消费。\u003c/p\u003e\u003cp data-pid=\"yuP5-zJV\"\u003e很多开在地铁站门口的服装店不赚钱也是这个道理，你流量是多，但有哪个上班族会有闲心在赶地铁的时候去逛衣服？\u003c/p\u003e\u003cp data-pid=\"haKRWdNO\"\u003e\u003cb\u003e第二类是功能类。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"lFH4WpvN\"\u003e像知识博主，美妆、评测博主之类的账号，都属于功能类。\u003c/p\u003e\u003cp data-pid=\"mr0DnI1f\"\u003e功能类账号不像第一类那么有适应力，通常有特定的受众，比如喜欢电脑或有购置计划的人，才会去关注电脑博主来搜集硬件资讯。\u003c/p\u003e\u003cp data-pid=\"O7VDOujw\"\u003e功能类账号虽然粉丝增长相对缓慢，但忠诚度较高。\u003c/p\u003e\u003cp data-pid=\"6Nec-7ZU\"\u003e就像很多关注知乎心理博主的人，肯定都是对心理学有一定需求的，如果博主推荐一些心理书籍和课程，是很容易成交的。\u003c/p\u003e\u003cp data-pid=\"2Zx_8MyT\"\u003e我们也很容易看到有不少人会在美妆和穿搭博主的评论区求链接。\u003c/p\u003e\u003cp data-pid=\"e_VTa-fe\"\u003e如果功能类账号有平台流量加持，威力是很恐怖的，比如一些带货大网红，人们关注他只为了便宜又可靠的商品信息，本质上这些网红是站在消费者端口跟商家谈判的“律师”，这也一种服务功能。\u003c/p\u003e\u003cp data-pid=\"L5rRGC7F\"\u003e通常这一类账号的寿命会比较长，转化能力也可观，但要求博主有相对高的专业能力，起号门槛比较高的。\u003c/p\u003e\u003cp data-pid=\"xPgLPw2-\"\u003e比如某些带货博主因为企业管理技术落后，品控不好，于是就老翻车。因为怕律师函，就不点名了。\u003c/p\u003e\u003cp data-pid=\"wSygk3F8\"\u003e\u003cb\u003e第三类是地摊类。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"bXNgz88P\"\u003e直播账号就属于此类。\u003c/p\u003e\u003cp data-pid=\"5gWXCYmz\"\u003e这一类账号本质上就是在平台租借了一个虚拟的摊位，凭吆喝吸引流量来消费。过路的人偶尔刷到了，看到正好是自己需要的东西，就可能进来买，但不会关注。\u003c/p\u003e\u003cp data-pid=\"iwrruaeC\"\u003e这一类账号的粉丝通常不会多，毕竟没人会给自己关注一堆全是广告的账号。\u003c/p\u003e\u003cp data-pid=\"BFwUgsnp\"\u003e但这一类账号有平台算法加持和流量扶持，能保证经常出现在可能产生消费的群体面前，比如一到晚上我们就会发现短视频里的食品广告变多了。\u003c/p\u003e\u003cp data-pid=\"D3YH-S4_\"\u003e三农账号本质上就属于这一类，跟在农贸市场卖菜很像，出摊吆喝就有生意，不吆喝就没生意，卖的主要商品就是自己的农产品。\u003c/p\u003e\u003cp data-pid=\"naZVNbH1\"\u003e如果吆喝得好，也可以达到第一类整活账号的效果，比如竹鼠兄弟，背景太假老哥。但不管多火，背后都必须有产品支撑。\u003c/p\u003e\u003cp data-pid=\"iQBEPUDT\"\u003e账号的存在就是为其自身产品服务的。（严格来说，能自己制作课程的功能性账号也可以跨界分到地摊类，这就是我说这个分类并不严谨的原因）\u003c/p\u003e\u003cp data-pid=\"U30zHXhx\"\u003e这一类账号其实门槛也不低，因为你需要有自己的产品，这依然是一个专业门槛，否则就变成纯整活的账号，寿命长不了。\u003c/p\u003e\u003cp data-pid=\"q4emvl9A\"\u003e人们对这一类账号的忠诚度其实也挺高，别看粉丝关注可能很难涨，但这就跟你家楼下卖了10多年早餐的早餐铺一样，只要老老实实干，形象价值和情感价值是摆在那的，变现并不困难。\u003c/p\u003e\u003chr/\u003e\u003cp data-pid=\"oQMMc4BN\"\u003e账号变现困难，要么就是进入了第一类快消品账号的赛道，结果却没赶上风口。比如很多千万网红停更，就是因为经济下行期已经没有广告满天飞的风口了。\u003c/p\u003e\u003cp data-pid=\"M94GRJMj\"\u003e要么是在其他两个类型的赛道，却把主要精力拿去搏流量了。比如明明是美妆博主，不好好介绍美妆技巧，天天整花活，短期可能确实有流量，但却没什么可交易的价值物。\u003c/p\u003e\u003cp data-pid=\"LZBxzv_N\"\u003e会导致自媒体变现困难的问题肯定还有很多，但无论如何也要记住：\u003c/p\u003e\u003cp data-pid=\"ajTSZYoc\"\u003e我们的资源和精力是有限的，观众也一样。所以，不要想着把所有东西都做好，而是要先集中突破你的涨粉和变现核心渠道。\u003c/p\u003e\u003cp data-pid=\"K7f4CtQG\"\u003e你是整活的，就专心学编剧，学戏剧冲突，用你的专业戏剧性去吸引观众，不要总想着哗众取宠。\u003c/p\u003e\u003cp data-pid=\"f6DiSOCC\"\u003e你是功能性的，就专心提升自己的专业能力，其他什么拍摄方式、噱头，统统不重要，只要你的内容够硬，你拿着手机在街上边走边说，和你正儿八经坐在演播室里说，其实并没有太大区别。\u003c/p\u003e\u003cp data-pid=\"v1Kxwodx\"\u003e你是摆地摊的，就好好做好自己的产品，每天出摊拿着你的小喇叭尽量把自己宣传给所有路过的人，至于你的吆喝好不好听，那都是其次，只要产品好，就不怕吆喝不到人。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":28178,"favorite_count":613,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 53600984683}","attached_info":"CvYFCN6arYPvwbuY8QEQBBoJNzAzODM2MjMyIL3K6boGKNYCMB9AlwFKKAodVFNfU09VUkNFX05FQVJMSU5FX0NPTlRFTlRfVjISATAYACAAOgBaCTExMDU0NzU3M2IgNTkwMTk5Mzg5N2IyYjY3MTUzNGI0MWRmZTE3MDM5NDdyCzUzNjAwOTg0NjgzigEJNjY3MjE0NzkwqgEJcmVjb21tZW5kwgEgMzk2ZWY1OTZmZjIzZmU3ZWVjYjhkMzE3ZGE2NTE4MjLyAQoIDBIGTm9ybWFs8gEoCAoSJDlkNWNkNjY3LWY0MzYtNGY2OS1iNDVmLTIwMTU1ZWYwMDgxNfIBBggLEgIyNoICAIgCmPu+zfoykgIgMzk2ZWY1OTZmZjIzZmU3ZWVjYjhkMzE3ZGE2NTE4MjKaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxl2gIdVFNfU09VUkNFX05FQVJMSU5FX0NPTlRFTlRfVjLoAgX6AgtOT1JNQUxfRkxPV4oDIDgxNmIzZTU2MzNmZjQ4MTA4Yjc5ZjA3ZGI0NDA3NTE0mgMNCgJ2MhAAGgVvdGhlcqgDktwB2AMA+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATWgBACoBACwBAC6BAZtYW51YWzCBAMxNzDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAGB4Tsk/gQUAAAAAAAAAAIkFsCXHHC2c0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFGpAGAKAGmwGoBgOSAiYKCTcwMzgzNjIzMhILNTM2MDA5ODQ2ODMYBCIKSU1BR0VfVEVYVA==","action_card":false},{"id":"152_1750898556.621","type":"feed","offset":152,"verb":"TOPIC_ACKNOWLEDGED_ARTICLE","created_time":1750898556,"updated_time":1750898556,"target":{"id":"655402388","type":"article","url":"https://api.zhihu.com/articles/655402388","author":{"id":"02877c4fde3b2e9dbd938f1a8eef598d","url":"https://api.zhihu.com/people/02877c4fde3b2e9dbd938f1a8eef598d","user_type":"people","url_token":"li-bo-jie","name":"李博杰","headline":"中科大与MSRA联培计算机博士，AI创业者","avatar_url":"https://picx.zhimg.com/50/0308d1c0d4faf17553045ce43f445899_l.jpg?source=b6762063","is_org":false,"gender":1,"badge":[{"type":"zhihu_yearly_answerer","description":"新知答主"},{"type":"best_answerer","description":"人工智能话题下的优秀答主","topic_names":["人工智能"],"topic_ids":[350]},{"type":"identity_people","description":"中国科学技术大学 计算机软件与理论博士"}],"followers_count":51246,"is_following":false,"is_followed":false},"title":"A100/H100 太贵，何不用 4090？","comment_permission":"all","created":1694335372,"updated":1698102894,"voteup_count":3390,"voting":0,"comment_count":262,"linkbox":{"category":"","pic":"","title":"","url":""},"excerpt":"（长文预警：本文按 Word 计数法 16000 字，按知乎计数法 24000 字） 这是一个好问题。先说结论， 大模型的训练用 4090 是不行的，但推理（inference/serving）用 4090 不仅可行，在性价比上还能比 H100 稍高。4090 如果极致优化，性价比甚至可以达到 H100 的 2 倍。事实上， H100/A100 和 4090 最大的区别就在通信和内存上，算力差距不大。H100A1004090Tensor FP16 算力989 Tflops312 Tflops330 TflopsTensor FP32 算力495 Tflo…","excerpt_new":"（长文预警：本文按 Word 计数法 16000 字，按知乎计数法 24000 字） 这是一个好问题。先说结论， 大模型的训练用 4090 是不行的，但推理（inference/serving）用 4090 不仅可行，在性价比上还能比 H100 稍高。4090 如果极致优化，性价比甚至可以达到 H100 的 2 倍。事实上， H100/A100 和 4090 最大的区别就在通信和内存上，算力差距不大。H100A1004090Tensor FP16 算力989 Tflops312 Tflops330 TflopsTensor FP32 算力495 Tflo…","preview_type":"default","preview_text":"","column":{"id":"c_1670745607722639360","type":"column","url":"https://api.zhihu.com/columns/c_1670745607722639360","author":{"id":"","url":"","user_type":"people","url_token":"","name":"匿名用户","headline":"","avatar_url":"https://pic1.zhimg.com/v2-d41c2ceaed8f51999522f903672a521f_l.jpg?source=b6762063","is_org":false,"gender":-1,"followers_count":0,"is_following":false,"is_followed":false},"title":"大模型操作系统之路","imageUrl":"https://picx.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b","comment_permission":"private","intro":"","updated":1691116794,"is_following":false},"content":"\u003cp data-pid=\"E8fgO5J2\"\u003e（长文预警：本文按 Word 计数法 16000 字，按知乎计数法 24000 字）\u003c/p\u003e\u003cp data-pid=\"aNEiHN4m\"\u003e这是一个好问题。先说结论，\u003cb\u003e大模型的训练用 4090 是不行的，但推理（inference/serving）用 4090 不仅可行，在性价比上还能比 H100 稍高。4090 如果极致优化，性价比甚至可以达到 H100 的 2 倍。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"q5POhAnQ\"\u003e事实上，\u003cb\u003eH100/A100 和 4090 最大的区别就在通信和内存上，算力差距不大。\u003c/b\u003e\u003c/p\u003e\u003ctable data-draft-node=\"block\" data-draft-type=\"table\" data-size=\"normal\" data-row-style=\"normal\"\u003e\u003ctbody\u003e\u003ctr\u003e\u003cth\u003e\u003c/th\u003e\u003cth\u003eH100\u003c/th\u003e\u003cth\u003eA100\u003c/th\u003e\u003cth\u003e4090\u003c/th\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eTensor FP16 算力\u003c/td\u003e\u003ctd\u003e989 Tflops\u003c/td\u003e\u003ctd\u003e312 Tflops\u003c/td\u003e\u003ctd\u003e330 Tflops\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eTensor FP32 算力\u003c/td\u003e\u003ctd\u003e495 Tflops\u003c/td\u003e\u003ctd\u003e156 Tflops\u003c/td\u003e\u003ctd\u003e83 Tflops\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e内存容量\u003c/td\u003e\u003ctd\u003e80 GB\u003c/td\u003e\u003ctd\u003e80 GB\u003c/td\u003e\u003ctd\u003e24 GB\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e内存带宽\u003c/td\u003e\u003ctd\u003e3.35 TB/s\u003c/td\u003e\u003ctd\u003e2 TB/s\u003c/td\u003e\u003ctd\u003e1 TB/s\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e通信带宽\u003c/td\u003e\u003ctd\u003e900 GB/s\u003c/td\u003e\u003ctd\u003e900 GB/s\u003c/td\u003e\u003ctd\u003e64 GB/s\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e通信时延\u003c/td\u003e\u003ctd\u003e~1 us\u003c/td\u003e\u003ctd\u003e~1 us\u003c/td\u003e\u003ctd\u003e~10 us\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e售价\u003c/td\u003e\u003ctd\u003e$30000~$40000\u003c/td\u003e\u003ctd\u003e$15000\u003c/td\u003e\u003ctd\u003e$1600\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cp data-pid=\"NtfvZPl_\"\u003eNVIDIA 的算力表里面油水很多，比如 H100 TF16 算力写的是 1979 Tflops，但那是加了 sparsity（稀疏）的，稠密的算力只有一半；4090 官方宣传 Tensor Core 算力高达 1321 Tflops，但那是 int8 的，FP16 直只有 330 Tflops。这篇文章的第一版就是用了错的数据，H100 和 4090 的数据都用错了，得到的结论非常离谱。\u003c/p\u003e\u003cp data-pid=\"8ekStfPS\"\u003e\u003cb\u003eH100 这个售价其实是有 10 倍以上油水的。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"vUmNx5L7\"\u003e2016 年我在 MSRA 的时候，见证了\u003ca href=\"https://link.zhihu.com/?target=https%3A//01.me/2017/01/microsoft-fpga/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e微软给每块服务器部署了 FPGA，把 FPGA 打到了沙子的价格\u003c/a\u003e，甚至成为了供应商 Altera 被 Intel 收购的重要推手。2017 年我还\u003ca href=\"https://link.zhihu.com/?target=https%3A//01.me/2023/08/phd-summary-3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e自己挖过矿\u003c/a\u003e，知道什么显卡最划算。后来在华为，我也是鲲鹏、昇腾生态软件研发的核心参与者。因此，一个芯片成本多少，我心里大概是有数的。\u003c/p\u003e\u003cp data-pid=\"IgtuI9Wh\"\u003e鲲鹏的首席架构师夏 Core 有一篇知名文章《\u003ca href=\"https://zhuanlan.zhihu.com/p/639181571\" class=\"internal\" target=\"_blank\"\u003e谈一下英伟达帝国的破腚\u003c/a\u003e》，很好的分析了 H100 的成本：\u003c/p\u003e\u003cblockquote data-pid=\"367jDXCP\"\u003e把他的成本打开，SXM 的成本不会高于 300$，封装的 Substrate 及 CoWoS 大约也需要 $300，中间的 Logic Die 最大颗，看上去最高贵 ：） 那是 4nm 的一颗 814mm2 的 Die，TSMC 一张 12 英寸 Wafer 大致上可以制造大约 60 颗这个尺寸的 Die，Nvidia 在 Partial Good 上一向做得很好（他几乎不卖 Full Good），所以这 60 颗大致能有 50 颗可用，Nvidia 是大客户，从 TSMC 手上拿到的价格大约是 $15000，所以这个高贵的 Die 大约只需要 $300。哦，只剩下 HBM 了，当前 DRAM 市场疲软得都快要死掉一家的鬼样了，即使是 HBM3 大抵都是亏本在卖，差不多只需要 $15/GB，嗯，80GB 的容量成本是 $1200。 TSMC 曾经讲过一个故事。台湾同胞辛辛苦苦攒钱建厂，一张 4nm 那么先进的工艺哦，才能卖到 $15000，但是那某个客户拿去噢，能卖出 $1500000（$30000*50）的货啦，机车，那样很讨厌耶。你懂我意思吗？ 就如最开始说的，在这个世界的商业规则下，$2000 成本的东西卖 $30000，只有一家，销售量还很大，这是不符合逻辑的，这种金母鸡得有航母才守得住。\u003c/blockquote\u003e\u003cp data-pid=\"4ZhR8X4t\"\u003e据说微软和 OpenAI 包下了 H100 2024 年产能的一半，猜猜他们会不会发挥当年跟 Altera 砍价的传统艺能？会真的花 $40,000 * 500,000 = 200 亿美金去买卡？\u003c/p\u003e\u003cp data-pid=\"7DS7Xu6Y\"\u003e咱们再分析下 4090 的成本，5nm 的 609mm2 Die，大约成本是 $250。GDDR6X，24 GB，按照 1 GB $10 算，$240。PCIe Gen4 这种便宜东西就算 $100 吧。封装和风扇这些东西，算它 $300。总成本最多 $900，这样的东西卖 $1600，算是良心价了，因为研发成本也是钱啊，更何况 NVIDIA 的大部分研发人员可是在世界上程序员平均薪酬最高的硅谷。\u003c/p\u003e\u003cp data-pid=\"XNeijZwG\"\u003e可以说，H100 就像是中国一线城市的房子，本身钢筋水泥不值多少钱，房价完全是被供求关系吹起来的。我在 LA 已经住了两周，公司租的房子使用面积是我北京房子的 4 倍，但售价只贵了 30%，还带个小院，相当于单位面积的房价是北京的 1/3。我跟本地的老外聊天，他们都很吃惊，你们的平均收入水平比 LA 低这么多，怎么买得起北京的房子的？\u003c/p\u003e\u003cp data-pid=\"-44v6QT3\"\u003e问题来了，如果 4090 这么香的话，为啥大家还要争着买 H100，搞得 H100 都断货了？甚至 H100 都要对华禁售，搞出个 H800 的阉割版？\u003c/p\u003e\u003ch2\u003e\u003cb\u003e大模型训练为什么不能用 4090\u003c/b\u003e\u003c/h2\u003e\u003ch3\u003e\u003cb\u003eGPU 训练性能和成本对比\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"9mW7mk0p\"\u003e\u003ca href=\"https://link.zhihu.com/?target=https%3A//lambdalabs.com/gpu-benchmarks\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eLambdaLabs 有个很好的 GPU 单机训练性能和成本对比\u003c/a\u003e，在此摘录如下。\u003c/p\u003e\u003cp data-pid=\"RBK2RkBR\"\u003e首先看吞吐量，看起来没有什么违和的，在单卡能放下模型的情况下，确实是 H100 的吞吐量最高，达到 4090 的两倍。看算力和内存也能看出来，H100 的 FP16 算力大约是 4090 的 3 倍，内存带宽是 3.35 倍，训练过程中由于 batch size 比较大，大多数算子是 compute bound（计算密集型），少数算子是 memory bound（内存密集型），这个结果是不意外的。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-61b22f89e47f1d2387ced2846e1f0a00_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2202\" data-rawheight=\"708\" data-original-token=\"v2-0d6fb62fa5d91689c86a647e2476e4aa\" class=\"origin_image zh-lightbox-thumb\" width=\"2202\" data-original=\"https://pica.zhimg.com/v2-61b22f89e47f1d2387ced2846e1f0a00_r.jpg\"/\u003e\u003cfigcaption\u003eLambdaLabs PyTorch 单卡训练吞吐量对比图\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://picx.zhimg.com/v2-03e6c6d3326283b61c2007372431f33d_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2260\" data-rawheight=\"1580\" data-original-token=\"v2-e6f724b260e67d0beb56078ab4895b01\" class=\"origin_image zh-lightbox-thumb\" width=\"2260\" data-original=\"https://picx.zhimg.com/v2-03e6c6d3326283b61c2007372431f33d_r.jpg\"/\u003e\u003cfigcaption\u003eLambdaLabs PyTorch 单卡训练吞吐量对比表\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"XGM_fIyP\"\u003e\u003cb\u003e然后看性价比，就有意思了，原来排在榜首的 H100 现在几乎垫底了，而且 4090 和 H100 的差距高达接近 10 倍。\u003c/b\u003e这就是因为 H100 比 4090 贵太多了。\u003c/p\u003e\u003cp data-pid=\"AgZPzmzz\"\u003e由于 H100 货源紧张，云厂商的 H100 租用价格就更黑了，按照标价大约 7 个月就可以回本。就算大客户价能便宜一半，一年半也足够回本了。\u003c/p\u003e\u003cp data-pid=\"DQ7AKiM-\"\u003e在价格战中过惯了苦日子的 IaaS 云服务商看到这样的 H100 回本速度，估计要感叹，这真是比区块链挖矿回本还快呐。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-4d34549ae8d97201a3a80aaaeae69d6a_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2268\" data-rawheight=\"736\" data-original-token=\"v2-d3fe62568821aa41bdb8b81cd2bcaf2b\" class=\"origin_image zh-lightbox-thumb\" width=\"2268\" data-original=\"https://pic3.zhimg.com/v2-4d34549ae8d97201a3a80aaaeae69d6a_r.jpg\"/\u003e\u003cfigcaption\u003eLambdaLabs PyTorch 单卡训练单位成本吞吐量对比图\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-ea3d0bc05e73b4d6ed22ed6e60406a4a_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2292\" data-rawheight=\"1592\" data-original-token=\"v2-00b3cb3a4db3922af234dc34640d5f1f\" class=\"origin_image zh-lightbox-thumb\" width=\"2292\" data-original=\"https://pic1.zhimg.com/v2-ea3d0bc05e73b4d6ed22ed6e60406a4a_r.jpg\"/\u003e\u003cfigcaption\u003eLambdaLabs PyTorch 单卡训练单位成本吞吐量对比表\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e\u003cb\u003e大模型训练的算力需求\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"00yhn4iV\"\u003e既然 4090 单卡训练的性价比这么高，为啥不能用来做大模型训练呢？抛开不允许游戏显卡用于数据中心这样的许可证约束不谈，从技术上讲，根本原因是大模型训练需要高性能的通信，但 4090 的通信效率太低。\u003c/p\u003e\u003cp data-pid=\"f3EJAmow\"\u003e大模型训练需要多少算力？训练总算力（Flops）= 6 * 模型的参数量 * 训练数据的 token 数。\u003c/p\u003e\u003cp data-pid=\"xdVyxBrp\"\u003e我今年初第一次看到有人煞有介事地讲这个公式的时候，觉得这不是显然的吗？又看到 OpenAI 的高级工程师能拿 90 多万美金的年薪，顿时整个人都不好了，还是 AI 香呀。之前我也面试过一些做 AI 的工程师，包括一些做 AI 系统优化的专家，连 Q、K、V 是啥都说不清楚，LLaMA 每个 tensor 的大小也算不出来，就这样还能拿到 offer。\u003c/p\u003e\u003cp data-pid=\"kx-6o0qz\"\u003eAPNet 2023 panel 的主题是 Network, AI, and Foundational Models: Opportunties and Challenges。前面几个问题都中规中矩的，panelists 有点放不开，我就提了一个问题，网络历史上的重要成就基本上都基于对应用场景深刻的理解，但我们现在做网络的很多都不了解 AI，甚至连每个 tensor 的大小和每个 step 传输的数据量都不知道，如何让 network community 更了解 AI 呢？\u003c/p\u003e\u003cp data-pid=\"v9wiw08u\"\u003e这下热闹了，台下的谭博首先发言，说我在华为肯定能知道所有这些东西；然后传雄老师也跟了一句，要是做网络的懂了太多 AI，那可能他就变成一个 AI guy 了。接着主持人陈凯教授问，你们有谁真的训练过大模型？沉默了一会儿，阿里的兄弟先说，我算是半个训练过大模型的，我们做的东西是支撑阿里大模型 infra 的。后面又有 panelist 说，做 AI 系统的网络优化是否有必要自己懂 AI 呢，是不是只要会做 profiling 就行了？\u003c/p\u003e\u003cp data-pid=\"N7yQb3oo\"\u003e我个人观点仍然是，AI 并不难学，要想做好 AI 系统优化，可以不懂 attention 的 softmax 里面为什么要除以 sqrt(d_k)，但\u003cb\u003e不能不会计算模型所需的算力、内存带宽、内存容量和通信数据量\u003c/b\u003e。Jeff Dean 就有个很有名的 \u003ca href=\"https://link.zhihu.com/?target=http%3A//static.googleusercontent.com/media/research.google.com/en/us/people/jeff/stanford-295-talk.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eNumbers Every Programmer Should Know\u003c/a\u003e，数量级的估算对任何系统优化来说都很关键，不然根本不知道瓶颈在哪里。\u003c/p\u003e\u003cp data-pid=\"xxJXGU26\"\u003e回到大模型训练所需的总算力，其实很简单，\u003cb\u003e6 * 模型的参数量 * 训练数据的 token 数就是所有训练数据过一遍所需的算力。这里的 6 就是每个 token 在模型正向传播和反向传播的时候所需的乘法、加法计算次数。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"BgFNDcaJ\"\u003e一堆矩阵相乘，简单来想就是左边若干个神经元，右边若干个神经元，组成一个完全二分图。选出其中任意一个左边的神经元 l 和右边的神经元 r，正向传播的时候：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"3Pubm59o\"\u003el 把它的输出乘上 l 和 r 之间的权重 w，发给 r；\u003c/li\u003e\u003cli data-pid=\"7_u126Q4\"\u003er 不可能只连一个神经元吧，总要把多个 l 的加到一起，这就是 reduce，需要一次加法。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"bbhJCLJP\"\u003e反向传播的时候：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"0pAbPNhw\"\u003er 把它收到的梯度乘上 l 和 r 之间的权重 w，发给 l；\u003c/li\u003e\u003cli data-pid=\"cX9sTYyu\"\u003el 也不可能只连一个 r，需要把梯度 reduce 一下，做个加法；\u003c/li\u003e\u003cli data-pid=\"HkGT5g74\"\u003e别忘了权重 w 需要更新，那就要计算 w 的梯度，把 r 收到的梯度乘上 l 正向传播的输出（activation）；\u003c/li\u003e\u003cli data-pid=\"Ick_WRmn\"\u003e一个 batch 一般有多个 sample，权重 w 的更新需要把这些 sample 的梯度加到一起。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"sx5XdAOd\"\u003e一共 3 次乘法，3 次加法，不管 Transformer 多复杂，矩阵计算就是这么简单，其他的向量计算、softmax 之类的都不是占算力的主要因素，估算的时候可以忽略。\u003c/p\u003e\u003cp data-pid=\"THoO44Mv\"\u003e想起来我 2019 年刚加入 MindSpore 团队的时候，领导让我开发一个正向算子的反向版本，我求导给求错了，搞得算子的计算结果总是不对，还以为是我们的编译器出 bug 了。当发现求导求错的时候，领导像以为我没学过微积分一样看着我，确实我的微积分学的不好，这也是我从数学专业转到计算机专业的原因之一。\u003c/p\u003e\u003cp data-pid=\"du9WnmLe\"\u003e在 MindSpore 的时候，自动微分一共就不到 1000 行代码，按照微分公式递归计算下去就行了，但自动微分作为一个重要特性被吹了半天，我都感觉不好意思了。\u003c/p\u003e\u003cp data-pid=\"fpdg_n8E\"\u003e模型的参数量和训练数据的 token 数之间也有个比例关系，这也很容易理解，只要把模型想象成数据的压缩版本就行了，压缩比总是有极限的。模型的参数量太小，就吃不下训练数据里面所有的知识；模型的参数量如果大于训练数据的 token 数，那又浪费，还容易导致 over-fitting。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e训练 LLaMA-2 70B 需要多少张卡\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"Npoxfl87\"\u003e有了模型训练所需的总算力，除以每个 GPU 的理论算力，再除以 GPU 的有效算力利用比例，就得到了所需的 GPU-hours，这块已经有很多开源数据。LLaMA 2 70B 训练需要 1.7M GPU hours（A100），要是用 1 个 GPU，那得算 200 年。要在一个月这种比较能接受的时间周期内训练出来，就得至少有 2400 块 A100。\u003c/p\u003e\u003cp data-pid=\"5-18xCjH\"\u003e如果用 4090，单卡 FP16 算力是跟 A100 差不多（330 vs 312 Tflops），但是内存带宽比 A100 低一半（1 vs 2 TB/s），内存容量更是差好几倍（24 vs 80 GB），计算梯度时需要使用的 TF32 算力也低一半（83 vs 156 Tflops），综合起来 4090 单卡的训练速度还比 A100 稍低（参考前面 LambdaLabs 的评测）。\u003c/p\u003e\u003cp data-pid=\"2vVWCd2k\"\u003e就按照 2048 块 4090 算吧，这 2048 块 4090 之间的通信就成了最大的问题。\u003c/p\u003e\u003cp data-pid=\"-V_QP8yi\"\u003e为什么？\u003cb\u003e一般有 tensor parallelism、pipeline parallelism、data parallelism 几种并行方式，分别在模型的层内、模型的层间、训练数据三个维度上对 GPU 进行划分。\u003c/b\u003e三个并行度乘起来，就是这个训练任务总的 GPU 数量。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-99dec9651bf9500520360f24fd4d851a_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"606\" data-rawheight=\"350\" data-original-token=\"v2-275dec5a40311ac73abdc44948a33806\" class=\"origin_image zh-lightbox-thumb\" width=\"606\" data-original=\"https://pica.zhimg.com/v2-99dec9651bf9500520360f24fd4d851a_r.jpg\"/\u003e\u003cfigcaption\u003e三种并行方式从三个维度划分计算空间的示意图，来源：DeepSpeed\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3\u003e\u003cb\u003eData parallelism（数据并行）\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"1roxteAn\"\u003e数据并行是最容易想到的并行方式。每个 GPU 分别计算不同的输入数据，计算各自的梯度（也就是模型参数的改变量），再把梯度汇总起来，取个平均值，广播给各个 GPU 分别更新。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-195ecab95b13e83b95a0ebe46c60ac90_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1776\" data-rawheight=\"1686\" data-original-token=\"v2-4afbf60cd21e89f2f42a2384b79fd050\" class=\"origin_image zh-lightbox-thumb\" width=\"1776\" data-original=\"https://pica.zhimg.com/v2-195ecab95b13e83b95a0ebe46c60ac90_r.jpg\"/\u003e\u003cfigcaption\u003eData Parallelism 示意图，来源：Colossal AI\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"Yn_JDO_M\"\u003e但只用数据并行是肯定不行的，因为一块 GPU 放不下整个 LLaMA 70B 模型。\u003c/p\u003e\u003cp data-pid=\"KPqohea1\"\u003e就模型训练需要多少 GPU 内存，我发现能算清楚的人就不多。有的人甚至以为只需要把模型的参数和反向传播的梯度存下来就够了。事实上，\u003cb\u003e训练需要的内存包括模型参数、反向传播的梯度、优化器所用的内存、正向传播的中间状态（activation）。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"2DS982xs\"\u003e优化器所用的内存其实也很简单，如果用最经典的 Adam 优化器，它需要用 32 位浮点来计算，否则单纯使用 16 位浮点来计算的误差太大，模型容易不收敛。因此，每个参数需要存 4 字节的 32 位版本（正向传播时用 16 位版本，优化时用 32 位版本，这叫做 mixed-precision），还需要存 4 字节的 momentum 和 4 字节的 variance，一共 12 字节。如果是用类似 SGD 的优化器，可以不存 variance，只需要 8 字节。\u003c/p\u003e\u003cp data-pid=\"-I_41dr7\"\u003e正向传播的中间状态（activation）是反向传播时计算梯度必需的，而且跟 batch size 成正比。Batch size 越大，每次读取模型参数内存能做的计算就越多，这样对 GPU 内存带宽的压力就越小。可是不要忘了，正向传播的中间状态数量是跟 batch size 成正比的，GPU 内存容量又会成为瓶颈。\u003c/p\u003e\u003cp data-pid=\"JYhlDYOR\"\u003e大家也发现\u003cb\u003e正向传播中间状态占的内存太多了，可以玩一个用算力换内存的把戏\u003c/b\u003e，就是不要存储那么多梯度和每一层的正向传播的中间状态，而是在计算到某一层的时候再临时从头开始重算正向传播的中间状态，这样这层的正向传播中间状态就不用保存了。如果每一层都这么干，那么就只要 2 个字节来存这一层的梯度。但是计算中间状态的算力开销会很大。因此实际中一般是把整个 Transformer 分成若干组，一组有若干层，只保存每组第一层的中间状态，后面的层就从该组第一层开始重新计算，这样就平衡了算力和内存的开销。\u003c/p\u003e\u003cp data-pid=\"A1fAs2QL\"\u003e如果还是算不清楚，可以读读这篇论文：\u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2205.05198.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eReducing Activation Recomputation in Large Transformer Models\u003c/a\u003e。\u003c/p\u003e\u003cp data-pid=\"oHFBJlv1\"\u003e当然有人说，GPU 内存放不下可以换出到 CPU 内存，但是就目前的 PCIe 速度，换出到 CPU 内存的代价有时候还不如在 GPU 内存里重算。如果是像 Grace Hopper 那种极高带宽的统一内存，那么换入换出倒是一个不错的主意，不管训练的正向传播中间状态还是 KV Cache，都有很多优化的空间。\u003c/p\u003e\u003ch3\u003e\u003cb\u003ePipeline parallelism（流水线并行）\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"A1xnR-ug\"\u003e既然一块 GPU 放不下，用多块 GPU 总行了吧？这就是 \u003cb\u003emodel parallelism（模型并行），可以大致分为 pipeline parallelism 和 tensor parallelism\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"FapRMbkM\"\u003e大家最容易想到的并行方式就是 \u003cb\u003epipeline parallelism\u003c/b\u003e，模型不是有很多层吗，那就分成几组，每组算连续的几层，穿成一条链。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-2cff2fba24423e599f8af4e44b20fe8b_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1563\" data-rawheight=\"1053\" data-original-token=\"v2-d77f609a299de61c36e6c063050bccf2\" class=\"origin_image zh-lightbox-thumb\" width=\"1563\" data-original=\"https://pic4.zhimg.com/v2-2cff2fba24423e599f8af4e44b20fe8b_r.jpg\"/\u003e\u003cfigcaption\u003ePipeline Parallelism 示意图，来源：Colossal AI\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"0uXN9ICn\"\u003e这样就有个问题，\u003cb\u003e一条链上只有一个 GPU 在干活，剩下的都在干等\u003c/b\u003e。当然聪明的你一定也想到了，既然叫 pipeline，那就可以流水线处理，可以把一个 batch 分为若干个 mini-batch，每个 mini-batch 分别计算。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic1.zhimg.com/v2-dee255f0ae2e06e73e3f993ed2dd6f38_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"678\" data-rawheight=\"204\" data-original-token=\"v2-61f730f65e706afb5a50354ad2a544eb\" class=\"origin_image zh-lightbox-thumb\" width=\"678\" data-original=\"https://pic1.zhimg.com/v2-dee255f0ae2e06e73e3f993ed2dd6f38_r.jpg\"/\u003e\u003cfigcaption\u003ePipeline Parallelism 示意图，来源：GPipe\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"717d1eBx\"\u003e这可好，是不是把 pipeline 搞的越深越好，每个 GPU 只算一层？\u003c/p\u003e\u003cp data-pid=\"-FkKS5II\"\u003e首先，正向传播中间状态（activation）的存储容量会成倍增加，加剧内存容量不足的问题。比如流水线的第一级算出了正向传播的中间状态，如果有 N 个流水级，那就要正向流过后面的 N - 1 个流水级，再等反向传播 N - 1 个流水级，也就是 2N - 2 轮之后才能用到这个正向传播的中间状态。不要忘了每一轮都会产生这么多中间状态，因此一共是保存了 2N - 1 个中间状态。如果 N 比较大，这个存储容量是非常恐怖的。\u003c/p\u003e\u003cp data-pid=\"XDSHCcwO\"\u003e其次，pipeline 的相邻流水级（pipeline stage）之间是要通信的，级数越多，通信的总数据量和总时延就越高。\u003c/p\u003e\u003cp data-pid=\"sZBjMgBO\"\u003e最后，要让这样的 pipeline 流起来，batch size 需要等于 Transformer 里面的层数，一般是几十，再乘以 data parallelism 的并行数，batch size 会很大，影响模型收敛的速度或模型收敛后的精度。\u003c/p\u003e\u003cp data-pid=\"1iYm35Dw\"\u003e因此，\u003cb\u003e在内存容量足够的情况下，最好还是少划分一些流水级\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"3mZXiMrj\"\u003e对于 LLaMA-2 70B 模型，模型参数需要 140 GB，反向传播的梯度需要 140 GB，优化器的状态（如果用 Adam）需要 840 GB。\u003c/p\u003e\u003cp data-pid=\"JhaFEiyZ\"\u003e正向传播的中间状态跟 batch size 和选择性重新计算的配置有关，我们在算力和内存之间取一个折中，那么正向传播的中间状态需要 token 长度 * batch size * hidden layer 的神经元数量 * 层数 * (10 + 24/张量并行度) 字节。假设 batch size = 8，不用张量并行，那么 LLaMA-2 70B 模型的正向传播中间状态需要 4096 * 8 * 8192 * 80 * (10 + 24) byte = 730 GB，是不是很大？\u003c/p\u003e\u003cp data-pid=\"A_Nq7qiN\"\u003e总共需要 140 + 140 + 840 + 730 = 1850 GB，这可比单放模型参数的 140 GB 大多了。一张 A100/H100 卡也只有 80 GB 内存，这就至少要 24 张卡；如果用 4090，一张卡 24 GB 内存，就至少需要 78 张卡。\u003c/p\u003e\u003cp data-pid=\"lcoDGik1\"\u003eLLaMA-2 模型一共就只有 80 层，一张卡放一层，是不是正好？这样就有 80 个流水级，单是流水线并行就有 80 个并行的 batch 才能填满流水线。\u003c/p\u003e\u003cp data-pid=\"SiTE9z2Q\"\u003e这样，\u003cb\u003e正向传播的中间状态存储就会大到无法忍受\u003c/b\u003e，这可是 80 * 2 = 160 轮的中间状态，翻了 160 倍。就算是使用选择性重新计算，比如把 80 层分成 8 组，每组 10 层，中间状态存储仍然是翻了 16 倍。\u003c/p\u003e\u003cp data-pid=\"ap9A6wyw\"\u003e除非是用最极端的完全重新计算，反向传播到每一层都重新从头开始计算正向传播的中间结果，但这样计算开销可是随模型层数平方级别的增长，第 1 层算 1 层，第 2 层算 2 层，一直到第 80 层算 80 层，一共算了 3240 层，计算开销可是比正常算一次 80 层翻了 40 倍，这还能忍？\u003c/p\u003e\u003cp data-pid=\"O3Ko3vqe\"\u003e中间状态存储的问题就已经够大了，再看这 2048 张卡之间的通信开销。按照一张卡放一层，并且用不同的输入数据让它完全流水起来的做法，这 2048 张卡分别在计算自己的 mini-batch，可以认为是独立参与到 data parallelism 里面了。前面讲过，在数据并行中，每一轮需要传输的是它计算出的梯度和全局平均后的梯度，梯度的数据量就等于模型的参数数量。\u003c/p\u003e\u003cp data-pid=\"WE-xe5yE\"\u003e把 70B 模型分成 80 层，每一层大约有 1B 参数，由于优化器用的是 32 bit 浮点数，这就需要传输 4 GB 数据。那么一轮计算需要多久呢？总的计算量 = batch size * token 数量 * 6 * 参数量 = 8 * 4096 * 6 * 1B = 196 Tflops，在 4090 上如果假定算力利用率 100%，只需要 0.6 秒。而通过 PCIe Gen4 传输这 4 GB 数据就已经至少需要 0.12 秒了，还需要传两遍，也就是先传梯度，再把平均梯度传过来，这 0.24 秒的时间相比 0.6 秒来说，是占了比较大的比例。\u003c/p\u003e\u003cp data-pid=\"RVvI-WpR\"\u003e当然我们也可以做个优化，让每个 GPU 在 pipeline parallelism 中处理的 80 组梯度数据首先在内部做个聚合，这样理论上一个 training step 就需要 48 秒，通信占用的时间不到 1 秒，通信开销就可以接受了。当然，通信占用时间不到 1 秒的前提是机器上插了足够多的网卡，能够把 PCIe Gen4 的带宽都通过网络吐出去，否则网卡就成了瓶颈。假如一台机器上插了 8 块 GPU，这基本上需要 8 块 ConnectX-6 200 Gbps RDMA 网卡才能满足我们的需求。\u003c/p\u003e\u003cp data-pid=\"NmFxPQmH\"\u003e最后再看 batch size，整个 2048 张卡的集群跑起来，每个 GPU 的 mini-batch 我们刚才设置为 8，那可真是 batch size = 16384，已经是大规模训练中比较大的 batch size 了，如果再大，可能就影响模型的收敛速度或收敛后的精度了。\u003c/p\u003e\u003cp data-pid=\"1z_9z9og\"\u003e因此，\u003cb\u003e单纯使用流水线并行和数据并行训练大模型的最大问题在于流水线并行级数过多，导致正向传播中间状态（activation）存储容量不足\u003c/b\u003e。\u003c/p\u003e\u003ch3\u003e\u003cb\u003eTensor parallelism（张量并行）\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"_6S3OwQn\"\u003e那就没办法了吗？我们还有最后一招，就是 Tensor parallelism（张量并行）。它也是模型并行的一种，但不像流水线并行那样是在模型的层间划分，而是在模型的层内划分，也就是把一层内的 attention 计算和 Feed Forward Network 划分到多个 GPU 上处理。\u003c/p\u003e\u003cp data-pid=\"Dwo-2j4x\"\u003e\u003cb\u003e有了张量并行，就可以缓解 GPU 放不下模型导致的流水级太多的问题\u003c/b\u003e。分到 80 个 GPU 才能放下的模型，如果用单机 8 卡张量并行，就只需要划分 10 个流水级。同时，张量并行还可以降低 batch size，因为张量并行的几个 GPU 是在算同一个输入数据。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-7a5d85c3eb206e5180b8eafc8cf0159e_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"2048\" data-rawheight=\"1230\" data-original-token=\"v2-e22b98d54a6bd0fc18037421e78f9745\" class=\"origin_image zh-lightbox-thumb\" width=\"2048\" data-original=\"https://pic3.zhimg.com/v2-7a5d85c3eb206e5180b8eafc8cf0159e_r.jpg\"/\u003e\u003cfigcaption\u003eTensor、Pipeline、Data 三种并行方式从模型层内、模型层间、训练数据三个维度上划分计算空间，来源：DeepSpeed\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"t6eNStWk\"\u003eAttention 的计算过程是比较容易并行的，因为有多个 head，用来关注输入序列中的不同位置的，那么把这些 head 分别拆开就行了。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic3.zhimg.com/v2-25cc062678afa29d82802a4352fbeb20_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1436\" data-rawheight=\"804\" data-original-token=\"v2-3cd76d3e0d8a20d87dfa586b56cc1ad3\" class=\"origin_image zh-lightbox-thumb\" width=\"1436\" data-original=\"https://pic3.zhimg.com/v2-25cc062678afa29d82802a4352fbeb20_r.jpg\"/\u003e\u003cfigcaption\u003eAttention 的计算过程，来源：The Illustrated Transformer\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"0Qodptd0\"\u003e\u003cb\u003e但是我们做任何并行计算的时候都不要忘记通信开销。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"b5SEEh6y\"\u003e每个 head 里面的 Q、K 两个矩阵的大小是 batch size * token 长度 * key 的大小，V 矩阵的大小是 batch size * token 长度 * value 的大小。key/value 的大小一般等于 embedding size / heads 数量，例如在 LLaMA-2 70B 中就是 8192 / 64 = 128，矩阵大小是 batch size * 4096 * 8192 / 64（注意，这只是一个 head 的）。而 Q、K、V 参数矩阵在每个 head 上的大小是 embedding size * embedding size / heads num = 8192 * 8192 / 64。\u003c/p\u003e\u003cp data-pid=\"JuBu11aV\"\u003e我们前面推导过，正向的计算量基本上就是每个 token 过一遍所有参数的计算量，2 * 3 (Q, K, V) * batch size * token 长度 * 参数个数 = 2 * 3 * batch size * 4096 * 8192 * 8192 / 64。可以跟矩阵的大小对一下，看看有没有算错。\u003c/p\u003e\u003cp data-pid=\"FId00xgT\"\u003e那么通信量是多少呢？输出矩阵 Z 是由每个 head 拼起来的，每个 head 的大小是 batch size * token 长度 * embedding size / heads num = batch size * 4096 * 8192 / 64。输入矩阵 X 的大小是 batch size * token 长度 * embedding size = batch size * 4096 * 8192。注意这里的 X 大小跟所有 heads 合并在一起后的 Z 大小是一致的，而我们在这里算的是每个 head 的 Z 大小。这里的单位是参数数量，如果按照字节算，还要乘以每个参数的大小。\u003c/p\u003e\u003cp data-pid=\"U2t67Stf\"\u003e如果我们采用最极端的方式，每个 head 交给一个 GPU 去算，那么计算量和通信量的比例是多少？大概是 2 * 3 * embedding size / heads num / bytes per param = 2 * 3 * 8192 / 64 / 2 = 384。代入 4090 的 330 Tflops，如果想让通信不成为瓶颈，那么通信带宽至少需要是 330T / 384 = 859 GB/s，发送接收双向还得乘以 2，就是 1.7 TB/s。太大了，远远超过 PCIe Gen4 x16 的 64 GB/s，就算 NVLink 的 900 GB/s 都撑不住。\u003c/p\u003e\u003cp data-pid=\"OGBqtGQD\"\u003e所以，\u003cb\u003etensor parallelism 不能切得太细，每个 GPU 需要多算几个 heads\u003c/b\u003e。如果每个 GPU 多算几个 attention heads，输入矩阵 X 就是这些 heads 共享的了，因此输入矩阵的通信开销就被多个 heads 平摊了，计算量和通信量的比例就可以提高。\u003c/p\u003e\u003cp data-pid=\"2-hv4S32\"\u003e还是按照 4090 的算力 / 单向通信带宽 = 330T / (64GB/s / 2) 来算，计算量和通信量的比例最少需要是 10000，也就是 2 * 3 * (embedding size / 张量并行 GPU 数量) / bytes per param = 2 * 3 * 8192 / 张量并行 GPU 数量 / 2 \u0026gt;= 10000，解得：张量并行 GPU 数量 \u0026lt;= 2.4。也就是告诉你，要是用了张量并行，最多用 2 个 GPU，如果用更多的 GPU，算力就肯定跑不满理论值。这让我怎么玩？\u003c/p\u003e\u003cp data-pid=\"CETGZ62K\"\u003e但是，如果把 H100 的参数代入进去，马上就不一样了。H100 的峰值算力是 989 Tflops，NVLink 双向带宽是 900 GB/s，计算量和通信量的比例最少需要是 1100，也就是 2 * 3 * (embedding size / 张量并行 GPU 数量) / bytes per param = 2 * 3 * 8192 / 张量并行 GPU 数量 / 2 \u0026gt;= 1100，解得：张量并行 GPU 数量 \u0026lt;= 11，也就是单机 8 卡做张量并行，对于 embedding size = 8192 的模型，刚刚好，通信不会成为瓶颈！\u003c/p\u003e\u003cp data-pid=\"t-MNM5oc\"\u003e\u003cb\u003e阉割版的 H800 相比 H100 卡的就是网络带宽\u003c/b\u003e，把网络带宽从 900 GB/s 降到 400 GB/s 了。我们再代入一次，计算量和通信量比例最少需要是 5000，那么张量并行 GPU 数量 \u0026lt;= 4.8。这样单机 8 卡做张量并行，就会导致网络成为瓶颈。当然，计算量 989 Tflops 是理论值，并行切分方式也可以优化，因此实际训练 70B 的模型 8 卡 H800 网络不一定真的是瓶颈。\u003cb\u003e这就是 H800 精准打击大模型训练，让张量并行过得不舒服。\u003c/b\u003e\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pica.zhimg.com/v2-22875ee495b0e612a653cc30cd2d8ee0_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1400\" data-rawheight=\"1083\" data-original-token=\"v2-d4da1eb6e696af3ea3f0bf29e750e564\" class=\"origin_image zh-lightbox-thumb\" width=\"1400\" data-original=\"https://pica.zhimg.com/v2-22875ee495b0e612a653cc30cd2d8ee0_r.jpg\"/\u003e\u003cfigcaption\u003eFeed Forward Network 的计算过程，虽然这是 encoder 的，但 decoder 也差不多，来源：Step-by-Step Illustrated Explanations of Transformer\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"qITaza78\"\u003e如果在 Feed Forward Network 这里做张量并行，也是可以做类似的推导，在这里就不赘述了。大凡神经网络里的矩阵乘法，M*N 的矩阵乘上 N*K 的矩阵，总的计算量是 M*N*K，输入输出的总大小是 (M*N + N*K)，多摞几个矩阵那也是常数（就像 Q、K、V），也就是计算和通信的比例跟矩阵的边长（dimension）是一个量级的。\u003c/p\u003e\u003cp data-pid=\"5_JEzMEU\"\u003e这么分析完了，\u003cb\u003e如果你是要做大规模大模型训练，你还会买 A100/H100/H800 的 PCIe 版吗？\u003c/b\u003ePCIe Gen5 虽然比 Gen 4 快一倍，但对 H100 而言，计算量和通信量的比例仍然最少需要是 989T / (128G / 2) = 15000，解出来张量并行 GPU 数量 \u0026lt;= 1.6，也就是只要用了张量并行，就是损失算力的！\u003c/p\u003e\u003cp data-pid=\"-nxFF5eE\"\u003e等到 H100 的下一代出来了，比如 GH200，算力又翻了一倍，NVLink 还是 900 GB/s，这时候 NVLink 就也开始有点吃力了。所以 GH200 不失时机的推出了统一大内存，号称 144 TB，就是为了更好的做换入换出，用内存换网络通信。如果禁令保持不变，国内版本还是卡住 400 GB/s 的通信，那性能差距会有多大？\u003c/p\u003e\u003cp data-pid=\"l1DuWz-9\"\u003e上面的推导当然都是简化的，实际上可能不会这么夸张，但数量级是差不多的。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e训练部分小结\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"NyH0GiiK\"\u003e4090 不容易做大模型训练的原因除了前面分析的\u003cb\u003e内存小，通信慢，license 不支持数据中心\u003c/b\u003e，还有很多其他问题。\u003c/p\u003e\u003cp data-pid=\"djMc5k39\"\u003e比如，A100/H100 支持 ECC 显存容错，据说 4090 也支持 ECC，但是不知道故障率会不会比 A100/H100 更高。不要小看了容错，2048 张卡的集群就算每张卡 1 个月出一次故障，平均 20 分钟就会有一张卡出故障！要是没有自动化的故障恢复方式，炼丹师就别想睡觉了。\u003c/p\u003e\u003cp data-pid=\"iNoRnTAe\"\u003e就算是自动从上一个 checkpoint 恢复，这可是要时间的，如果不考虑丢弃故障 GPU 梯度这种比较暴力的方式，当前这个 step 就算是白算了，还要从上一个 checkpoint 加载梯度，一般需要 10 来分钟的时间才能搞定。这样，每 20 分钟就浪费 10 分钟，这 10 分钟恢复过程中可能又有新的卡故障，总的算下来要浪费掉一半的有效算力。\u003c/p\u003e\u003cp data-pid=\"CreMjLBE\"\u003e因此，\u003cb\u003e保持大规模训练集群的低故障率是非常重要的\u003c/b\u003e，这些 GPU 卡都非常金贵，可不能像挖矿机房那样，动不动就过热死机了。\u003c/p\u003e\u003cp data-pid=\"VTjLpGXh\"\u003e据说 3090 是支持 NVLink 的，但 4090 就把 NVLink 给砍掉了。更老的卡，甚至还有支持 PCIe P2P 的，现在也都被砍掉了。谁感兴趣可以测一测 3090 的 NVLink 性能怎么样，是不是真的能达到标称的 600 GB/s，如果真的能达到的话，是否又可以用来做大模型训练了呢。\u003c/p\u003e\u003cp data-pid=\"wjTyBxkf\"\u003e我们年会的时候，海哥讲了个段子，我们找老婆都希望又漂亮，又能挣钱，还一心一意爱自己。可同时满足这三个条件的老婆就很难找到了。类似的，在分布式系统中，我们都希望性能又高，通用性又强，成本还低。这三个条件的交集也很小。海哥讲到这里，谭博补充了一句，同时满足这三个条件的分布式系统根本就不存在。\u003c/p\u003e\u003cp data-pid=\"UtdHr9Xi\"\u003e\u003cb\u003eTensor、Pipeline、Data Parallelism 就像是这样的不可能三角，相互牵制，只要集群规模够大，模型结构仍然是 Transformer，就很难逃出内存容量和网络带宽的魔爪。\u003c/b\u003e\u003c/p\u003e\u003ch2\u003e\u003cb\u003e大模型推理为什么 4090 很香\u003c/b\u003e\u003c/h2\u003e\u003cp data-pid=\"4nIsVnpq\"\u003e推理和训练有什么区别？\u003c/p\u003e\u003cp data-pid=\"GFdKsLMB\"\u003e首先，训练不仅需要存储模型参数，还需要存储梯度、优化器状态、正向传播每一层的中间状态（activation），后面几个比参数更大，对模型内存的需求量也更大。\u003c/p\u003e\u003cp data-pid=\"2qVVA_OM\"\u003e其次，训练任务是一个整体，流水线并行的正向传播中间结果是需要存下来给反向传播用的。为了节约内存而使用流水线并行，流水级越多，要存储的中间状态也就更多，反而加剧内存的不足。而推理任务中的各个输入数据之间并没有关系，正向传播每一层的中间状态也不需要保存下来，因此流水线并行不需要存储很多中间状态。\u003c/p\u003e\u003cp data-pid=\"JaQEn99_\"\u003e首先我们需要计算一下推理需要多少算力。前面针对训练算力的估算，为了简单起见，忽略了两个事情，首先是没有考虑 KV Cache，其次是没有考虑内存带宽。\u003c/p\u003e\u003ch3\u003e\u003cb\u003eKV Cache\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"WPpbWcG6\"\u003e什么是 \u003ca href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2211.05102.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eKV Cache\u003c/a\u003e？对于每个输入的 prompt，在计算第一个 token 输出的时候，每个 token 的 attention 肯定是都要从头计算。但是在后续 token 的生成中，都需要计算 self-attention，也就是输入 prompt 以及前面输出的 token 的 attention。这是就需要用到前面每一个 token 的 K 和 V，由于每一层的参数矩阵是不变的，此时只有刚生成的那个 token 的 K 和 V 需要从头计算，输入 prompt 和之前生成的 token 的 K 和 V 其实是跟上一轮一样的。\u003c/p\u003e\u003cp data-pid=\"w3L35WZc\"\u003e这时，我们就可以\u003cb\u003e把每一层的 K、V 矩阵缓存起来，生成下一个 token 的时候不再需要重新计算，这就是所谓的 KV Cache\u003c/b\u003e。Q 矩阵每次都不一样，没有缓存的价值。前面讲的训练中的选择性保存正向 activation 是个拿计算换内存的把戏，这里的 KV Cache 就是一个拿内存换计算的把戏。\u003c/p\u003e\u003cp data-pid=\"87IljeY7\"\u003eKV Cache 需要多少存储容量呢？每一层，每个 token 的 K、V 矩阵都是 embedding size 这么大，再乘上 token 数量和 batch size，就是这一层的 KV Cache 所需的存储容量了。一定要记住 batch size，在正向和反向传播的几乎所有阶段，都不会涉及到对 batch size 中各个 sample 的合并处理，因此它始终是存储量和计算量计算中的一个系数。\u003c/p\u003e\u003cp data-pid=\"D6fSiw0b\"\u003e例如，如果 batch size = 8，在 LLaMA 2 70B 中，假设输入和输出的 token 数量达到了模型的极限 4096，80 层的 KV Cache 一共需要 2 (K, V) * 80 * 8192 * 4096 * 8 * 2B = 80 GB。如果 batch size 更大，那么 KV Cache 占据的空间将超过参数本身占的 140 GB。\u003c/p\u003e\u003cp data-pid=\"wCVGieDX\"\u003eKV Cache 能省下来多少计算量？每一层计算 K、V 矩阵一共需要 2 (K, V) * 2 (mult, add) * embedding size * embedding size = 4 * 8192 * 8192 这么多计算量，乘以之前输入过的 token 数量、层数和 batch size，就是 4096 * 80 * 8 * 4 * 8192 * 8192 = 640 Tflops。相当于每存储 1 个字节，节约了 16K 次计算，还是很划算的。\u003c/p\u003e\u003cp data-pid=\"T5Oxw3Kr\"\u003e事实上，KV Cache 节约的远远不止这些。计算 K、V 矩阵的过程是个典型的内存密集型过程，它需要加载每一层的 K、V 参数矩阵。也就是如果不做任何缓存，假设 prompt 长度很短而输出长度接近 token 的最大长度 4096，到了最后一个 token 的时候，单是重复计算前面每个 token 的 K、V 矩阵，就需要读取内存 4096 * 80 * 2 * 8192 * 8192 = 40T 次，每次 2 个字节，要知道 H100 的内存带宽只有 3.35 TB/s，4090 更是只有 1 TB/s，这单是最后一个 token 就得耗掉一张卡几十秒的时间来做重复计算。这样，token 的输出就会越来越慢，整个输出时间是输出长度平方级别的，根本没法用。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e推理是计算密集还是存储密集\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"ZBGqs2mH\"\u003e接下来我们就可以计算推理所需的计算量了。总的算力很好算，前面讲过，大概就是 \u003cb\u003e2 * 输出 token 数量 * 参数数量 flops\u003c/b\u003e。如果想看细节，可以看下面这张图，\u003ca href=\"https://link.zhihu.com/?target=https%3A//le.qun.ch/en/blog/2023/05/13/transformer-batching/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e来源是这里\u003c/a\u003e。\u003c/p\u003e\u003cfigure data-size=\"normal\"\u003e\u003cimg src=\"https://pic4.zhimg.com/v2-0702b1490860b4b88295f3307eb9e3c1_1440w.jpg\" data-size=\"normal\" data-rawwidth=\"1662\" data-rawheight=\"1844\" data-qrcode-action=\"none\" data-original-token=\"v2-b5aa472e3034e9098fefd4913dc8cd9b\" class=\"origin_image zh-lightbox-thumb\" width=\"1662\" data-original=\"https://pic4.zhimg.com/v2-0702b1490860b4b88295f3307eb9e3c1_r.jpg\"/\u003e\u003cfigcaption\u003eTransformer 推理过程中每一步的矩阵形状、所需算力和内存访问量，来源：Lequn Chen，Dissecting Batching Effects in GPT Inference\u003c/figcaption\u003e\u003c/figure\u003e\u003cp data-pid=\"pBpOmbcw\"\u003e但算力并不能说明一切，\u003cb\u003e模型还需要访问 GPU 内存，内存带宽也可能成为瓶颈\u003c/b\u003e。至少需要把参数从内存里面读出来吧？事实上，内存带宽的估算就这么简单，内存访问量 = 参数数量 * 2 bytes。中间结果有一部分是可以放在缓存里面的，缓存放不下的部分也需要占内存带宽，我们先不算。\u003c/p\u003e\u003cp data-pid=\"fycO-8v2\"\u003e如果不做任何批量输入，也就是模型专门服务一个 prompt，batch size = 1，整个 context 的长度很短（例如只有 128），那么整个推理过程中，每载入一个参数（2 字节），就只进行 128 次乘法和加法计算，那么计算 flops 和访问内存 bytes 的比例就只有 128。基本上任何 GPU 在这种情况下都会变成 memory bound，时间都耗在加载内存上了。\u003c/p\u003e\u003cp data-pid=\"HXVbt41d\"\u003e对于 4090 来说，计算 flops 和内存带宽之比是 330 / 1 = 330；对于 H100 来说，计算 flops 和内存带宽之比是 989 / 3.35 = 295。也就是说，如果 context 中的 token 数量小于 330 或者 295，那么内存访问就会成为瓶颈。\u003c/p\u003e\u003cp data-pid=\"pb-7Tcni\"\u003e虽然 LLaMA 2 的理论上限是 4096 个 token，但很多输入 prompt 用不了这么多，因此内存访问是有可能成为瓶颈的。此时，就需要靠 batch size 来补足了。推理中的批量处理，就是把几乎同时到达后端服务的 prompt 放到一起处理。不用担心，batch 里面的不同 prompt 的处理是完全独立的，不用担心会互相干扰。但这些 prompt 的输出是步调整齐划一的，每一轮整个 batch 中的每个 prompt 都会输出一个 token，因此如果有的 prompt 先输出完了，那就只能等其他的输出结束，造成一定的算力浪费。\u003c/p\u003e\u003cp data-pid=\"OjKOr34C\"\u003e有的人问，批量处理所需的算力跟分别单独处理所需的算力是一样的呀，那推理时为什么需要批量处理？答案就在访问内存的带宽上。\u003c/p\u003e\u003cp data-pid=\"Q5Kx-RS7\"\u003e\u003cb\u003e如果同时到达服务器的 prompt 很多，是不是 batch size 越大越好？\u003c/b\u003e也不是，因为 KV Cache 的大小可是正比于 batch size 的，batch size 大了，KV Cache 占据的 GPU 内存容量就很可观，比如在 LLaMA-2 70B 中，每个 prompt 都要占据 5 GB 的 KV Cache，如果 batch size 搞到 32，那么 KV Cache 就会占掉 160 GB 的 GPU 内存，比参数都大了。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e70B 推理需要多少张卡？\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"VJK0egkU\"\u003e总的存储容量也很好算，推理的时候最主要占内存的就是参数、KV Cache 和当前层的中间结果。当 batch size = 8 时，中间结果所需的大小是 batch size * token length * embedding size = 8 * 4096 * 8192 * 2B = 0.5 GB，相对来说是很小的。\u003c/p\u003e\u003cp data-pid=\"19L1jRcU\"\u003e70B 模型的参数是 140 GB，不管 A100/H100 还是 4090 都是单卡放不下的。那么 2 张 H100 够吗？看起来 160 GB 是够了，但是剩下的 20 GB 如果用来放 KV Cache，要么把 batch size 压缩一半，要么把 token 最大长度压缩一半，听起来是不太明智。因此，至少需要 3 张 H100。\u003c/p\u003e\u003cp data-pid=\"rTBYu5sj\"\u003e对于 4090，140 GB 参数 + 40 GB KV Cache = 180 GB，每张卡 24 GB，8 张卡刚好可以放下。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e推理用流水线并行可以吗？\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"EmsMJSMw\"\u003e\u003cb\u003e推理使用流水线并行，最主要的问题是串行处理的推理延迟，网络延迟倒是小问题。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Xlfuren8\"\u003e首先是推理延迟。虽然流水线的不同阶段可以塞进不同的 prompt，但同一个 prompt 的处理仍然永远在单个 GPU 上轮转，这样相比 Tensor parallelism 而言，单个 prompt 的延迟就增大了。\u003c/p\u003e\u003cp data-pid=\"V0wQ8VIM\"\u003e对于很小的 batch size，GPU 内存带宽是瓶颈，此时每张卡计算每个 token 的时延就是 2 byte * 参数量 / 卡的数量 / 内存带宽，例如 8 卡 4090 跑 LLaMA-2 70B，就是 2 * 70G / 8 / 1 TB/s = 0.0175 秒。这里没有考虑 KV Cache 带来的节约。注意，8 张卡是串行处理的，因此每个 token 的时延还要乘以 8，也就是 0.14 秒。\u003cb\u003e每秒只能输出 7 个 token\u003c/b\u003e，对于 70B 这么小的模型来说是有点慢了。\u003c/p\u003e\u003cp data-pid=\"ee9_yCX2\"\u003e对于很大的 batch size，GPU 算力是瓶颈，此时每张卡计算每个 token 的时延就是 batch size * 2 * 参数量 / 卡的数量 / 算力，例如 batch size = 1024，同样的 8 卡例子，就是 1024 * 2 * 70G / 8 / 330 Tflops = 0.0543 秒。事实上，对于这么大的 batch size，KV Cache 和正向传播的中间结果先把 GPU 内存给吃满了。\u003c/p\u003e\u003cp data-pid=\"wf8ilxwZ\"\u003e那么\u003cb\u003e要平衡利用 GPU 算力和内存带宽，batch size 需要是多少呢？\u003c/b\u003e这就是 2 byte * 参数量 / 卡的数量 / 内存带宽 = batch size * 2 * 参数量 / 卡的数量 / 算力，左右两边参数量和卡的数量互相抵消，得到 batch size = 算力 / 内存带宽。对于 4090，就是 330 / 1 = 330；对于 H100，就是 989 / 3.35 = 295。也就是说，对 4090 而言，batch size 小于 330 的时候 GPU 内存带宽是瓶颈，大于 330 的时候 GPU 算力是瓶颈。当 batch size = 330 的时候，理想情况下，内存带宽和算力恰好都打满，每张卡处理每个 token 的时间就是 17.5 ms。\u003c/p\u003e\u003cp data-pid=\"WpVoqzQH\"\u003e其次是\u003cb\u003e网络延迟\u003c/b\u003e。流水线并行相比张量并行的优点就是网络传输量小，流水级之间只需要传输 batch size * embedding size 这么多数据。例如 batch size = 8，embedding size = 8192，只需要传输 128 KB 数据，在 32 GB/s 的 PCIe Gen4 x16 上，只需要 4 us 就可以传输完成。当然，还需要考虑到通信库本身的开销，加上 4090 不支持 GPU 之间 P2P 传输，需要通过 CPU 中转，实际上需要几十 us 的时间，相比计算部分动辄几十 ms 的时延，可以忽略不计。\u003c/p\u003e\u003cp data-pid=\"OMh_EV5J\"\u003e即使 batch size = 330，这 5.28 MB 数据在 PCIe 上也只需要传输 0.16 ms，相比计算部分的 17.5 ms 仍然可以忽略不计。\u003c/p\u003e\u003cp data-pid=\"JN7mQH54\"\u003e\u003cb\u003e如果可以忍受流水线并行的推理延迟，甚至可以用多台主机来做流水线并行\u003c/b\u003e。我们假设主机间只有 1 Gbps 的普通以太网络，每台主机只有一张 4090。对于 batch size = 1，16 KB 数据需要 0.25 ms 才能传输完成，再加上 0.25 ms 两端网络协议栈的处理时间，每个流水级就需要 0.5 ms 的时延，8 张卡花在通信上的时间只有 4 ms，相比整体计算时延 140 ms 来说可以忽略，不会显著影响系统的推理延迟。\u003c/p\u003e\u003cp data-pid=\"mBMf1Xn9\"\u003e当 batch size 很小时，流水线推理中的网络流量是突发性（bursty）的，每过 18 ms 只会进行 0.25 ms 数据传输，只有 1/72 的占空比，不用担心流水线推理把局域网全部给占满了，搞得没法正常上网了。\u003c/p\u003e\u003cp data-pid=\"VywROiIj\"\u003e如果为了充分利用算力，把 batch size 设置得很大，比如 330，那么 16 KB * 330 = 5.28 MB 数据需要传输 41 ms，8 张卡花在通信上的时间高达 0.33 秒，这样就只有 3 token/s 的输出速度了，难以忍受。因此，\u003cb\u003e如果用主机间通信来做流水线并行，主机间又没有很高的通信带宽，就势必需要牺牲一定的吞吐量。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"-TUZBrCQ\"\u003e例如，我们设置输出速度不小于 5 token/s，这时留给通信的时间是 60 ms，每个流水级至多 7.5 ms，1 Gbps 网络可以传输 960 KB 数据，这时 batch size 至多设置为 60，也就是这 8 张 4090 的总吞吐量是 2400 token/s。此时的有效算力利用率只有不到 20%。\u003c/p\u003e\u003cp data-pid=\"OiqzMtUM\"\u003e最近有一个比较火的 \u003ca href=\"https://link.zhihu.com/?target=https%3A//github.com/bigscience-workshop/petals\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ePetals 开源项目\u003c/a\u003e，就是利用流水线并行，把 GPU 做成了一个类似 BitTorrent 的分布式网络。虽然推理延迟确实比较高，但至少说明了分布式 GPU 推理的可行性。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e推理用张量并行怎么样？\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"AioBiSmE\"\u003e前面讲到，流水线并行的最大缺点是 GPU 串行处理，延迟较高，导致输出 token 比较慢。而张量并行的最大缺点是传输数据量大，网络带宽低的设备不一定 hold 得住。\u003c/p\u003e\u003cp data-pid=\"d17Ph2L-\"\u003e\u003cb\u003e但是推理要传输的数据量跟训练要传输的数据量可不是一回事啊！\u003c/b\u003e推理只需要传输正向传播的中间结果（activation），而训练还需要传输所有参数的梯度，梯度才是数据量的大头。\u003c/p\u003e\u003cp data-pid=\"7g9_3nat\"\u003e在推理中，如果使用张量并行，Transformer 的每一层都需要传输把自己负责的结果向量（大小为 batch size * embedding size / num GPUs）广播给其他所有 GPU，并接受来自所有其他 GPU 广播来的数据。计算 attention 的时候需要传输一次，计算 feed-forward network 的时候又需要传输一次，也就是总共需要传输 2 * 层数这么多次。\u003c/p\u003e\u003cp data-pid=\"gWZzdsqS\"\u003e每次发送就是 batch size * embedding size（发送和接收是不同的方向，不能算两次），对于 batch size = 1, embedding size = 8192，只需要传输 16 KB 数据，在 32 GB/s 的 PCIe Gen4 上传输只需要 1 us。当然，考虑到前面讨论的 CPU 中转开销，还是需要大约 30 us 的。一共 160 次传输，需要 4.8 ms。\u003c/p\u003e\u003cp data-pid=\"CG2KBjB9\"\u003e我们再考虑计算的开销。还是考虑 batch size = 1 的情形，GPU 内存带宽是瓶颈，此时每张卡计算每个 token 的时延就是 2 byte * 参数量 / 卡的数量 / 内存带宽，代入我们前面的数值，仍然是 17.5 ms。但是这里 8 张卡是并行处理的，因此总的处理时长就是计算时间 + 通信时间 = 17.5 ms + 4.8 ms = 22.3 ms。这就意味着\u003cb\u003e每秒可以生成 45 个 token\u003c/b\u003e，这个 token 生成速度已经很不错了，至少人类的阅读速度是很难赶上生成的速度了。\u003c/p\u003e\u003cp data-pid=\"5r2msQaw\"\u003e如果 batch size 更大会怎样？例如 batch size = 330，把 GPU 算力和内存带宽都充分利用起来，每次需要传输的数据量是 330 * 8192 * 2 = 5.4 MB，在 32 GB/s 的 PCIe Gen4 上需要 0.17 ms。一共 160 次传输，就是 27 ms。这下网络通信开销成了延迟的大头，总处理时长为 27 + 17.5 = 44.5 ms，\u003cb\u003e每秒只能生成 22 个 token 了\u003c/b\u003e，但也不算慢。\u003c/p\u003e\u003cp data-pid=\"tGSOiRBZ\"\u003e注意，不管用多少个 GPU 做并行推理，只要用的是张量并行，网络传输的总数据量是相同的，因此\u003cb\u003e增加 GPU 的数量只能加速计算，不能加速通信。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"aHVcJKJJ\"\u003e因此，\u003cb\u003eA100/H100 的 NVLink 在降低推理延迟方面还是有很大作用的\u003c/b\u003e。如果用 H100，取 batch size = 295 达到算力和带宽的平衡利用，这 4.72 MB 数据只需要 4.72 MB / 450 GB/s = 0.01 ms。一共 160 次传输，也只有 1.6 ms。由于内存带宽大了，计算时间也可以大幅缩短，例如 H100 的计算时间为 2 * 70G / 8 / 3.35 TB/s = 5.2 ms。总处理时长只有 5.2 ms + 1.6 ms = 6.8 ms，\u003cb\u003e每秒可以生成 147 个 token\u003c/b\u003e，非常棒！\u003c/p\u003e\u003cp data-pid=\"reJ1qTSc\"\u003e可以说，如果论单个 prompt 的 token 生成速度，无论用多少块 4090 也追不上 8 卡 H100。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e用 4090 做推理的成本怎么样？\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"Eoi8oJhd\"\u003e\u003cb\u003e对于推理，不管用流水线并行还是张量并行，batch size 不算高到太离谱的情况下内存带宽都是瓶颈。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"uaUUe1XM\"\u003e假如 batch size 能够高到把算力 100% 利用起来，并且还能解决 KV Cache 不够大的问题，能解决中间结果占用内存过多的问题，那么这 8 张 4090 可以达到多少吞吐量？\u003c/p\u003e\u003cp data-pid=\"OAVsjrPl\"\u003e当然，这两个问题都不好解决，因此\u003cb\u003e推理优化才是一个热门的研究领域，存在很多的 trade-off 和奇技淫巧\u003c/b\u003e。如果只是用标准的 PyTorch，那推理性能距离把算力 100% 利用起来还远得很呐。\u003c/p\u003e\u003cp data-pid=\"6BTgqaWh\"\u003e假设都解决了，在张量并行的通信过程中我们可以利用 double buffer 做另外一个 batch 的计算，也就是计算和通信并行，进一步提高吞吐量。通信和计算分别是 27 ms 和 17.5 ms，传输的 27 ms 是瓶颈，也就是每 27 ms 输出一组 token，一个 batch 330 个 prompt，那这 8 张 4090 真是可以达到每秒 330 / 0.027 = 12.2K token 的吞吐量。\u003c/p\u003e\u003cp data-pid=\"HCFqDLBL\"\u003e8 张 4090 的成本是 12800 美金，8 卡 PCIe Gen4 服务器本身要 2 万美金，加上网络设备，平均每台 4 万美金的设备成本。固定资产按照 3 年摊销，每小时 1.52 美元。整机功耗大约 400W * 8 + 2 kW = 5 kW，按照 0.1 美元一度电算，每小时 0.5 美元。一个机架可以放 4 台这样的 8 卡服务器，数据中心机柜租用成本（不含电费）一个月 1500 美元算贵的了，合每小时 0.5 美元。这 2.5 美元一小时的机器，满打满算能生成 12.2K * 3600 = 44M tokens，也就是说 \u003cb\u003e1 美元能生成 17.6M tokens\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"UYqWdkhV\"\u003e是不是比 GPT-3.5 Turbo 的 $0.002 / 1K tokens，也就是 1 美元 0.5M tokens \u003cb\u003e便宜 35 倍\u003c/b\u003e？当然，账不能这么算。\u003c/p\u003e\u003cul\u003e\u003cli data-pid=\"wZmbxu59\"\u003e首先，GPU 的算力利用率到不了 100%；\u003c/li\u003e\u003cli data-pid=\"unN0sZx9\"\u003e其次，如同所有 SaaS 服务一样，用户的请求数量有波峰有波谷，用户是按量付费的，平台提供方可是不管有没有人用都在烧钱的；\u003c/li\u003e\u003cli data-pid=\"p670bDEN\"\u003e此外，每个 batch 中不同 prompt 的长度和响应 token 数量都不同，消耗的算力是 batch 中最大的那个，但收的钱是用户实际用的 token 数；\u003c/li\u003e\u003cli data-pid=\"OwUSlYl5\"\u003e再次，GPT-3.5 是 175B 的模型，比 70B 的 LLaMA 很可能推理成本更高；\u003c/li\u003e\u003cli data-pid=\"nZ-o6lNv\"\u003e最后，OpenAI 开发 GPT-3.5 是烧了不知道多少钱的，人家至少要赚回训练成本和研发人员的工资吧。\u003c/li\u003e\u003c/ul\u003e\u003cp data-pid=\"W8UASbiG\"\u003e其实 GPT-3.5 Turbo 的 $0.002 / 1K tokens 真的挺良心的，有的卖 API 的，LLaMA-2 70B 都敢比 GPT-3.5 Turbo 卖得贵。\u003c/p\u003e\u003cp data-pid=\"urjyzSRj\"\u003e如果换成用 H100 做推理，重新算一下这笔账。一张 H100 至少要 3 万美金，一台 8 卡 H100 高配服务器加上配套的 IB 网络，起码要 30 万美金，同样按照 3 年摊销，每小时 11.4 美元。10 kW 功耗，电费每小时 1 美元。一个普通供电和散热的机架只能放 2 台 8 卡 H100，机柜租用成本（不含电费）还按 1500 美元算，合每小时 1 美元。\u003cb\u003e一共 13.4 美元一小时\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"xHIDqg4g\"\u003e这其实已经是非常良心的价格了，你在任何云服务商都不可能租得到这么便宜的 8 卡 H100。所以说从云服务商租卡卖没有护城河的 SaaS 服务，比如开源模型的推理 API，除非有一种提高推理性能的独门绝技，很难赚得了什么大钱，二房东的生意不是这么好做的。\u003c/p\u003e\u003cp data-pid=\"LALhBKp2\"\u003e再算算这台 8 卡 H100 机器的吞吐量，张量并行也采用传输和计算并行，H100 的通信比较快，因此计算是瓶颈，每 5.2 ms 可以输出一组 token，一个 batch 295 个 prompt，满打满算可以达到每秒 295 / 0.0052 = 56K token 的吞吐量。理想情况下，一小时能生成 204M tokens，也就是 1 美元能生成 15.2M tokens，\u003cb\u003eH100 单位 token 的成本比 4090 仅仅高 16%，可以算打个平手吧。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"hX0kJPLD\"\u003e\u003cb\u003e为什么 8 卡 H100 机器是 4090 机器生命周期价格的 5 倍，性价比却跟 4090 差不多？\u003c/b\u003e因为一张 H100 的算力是 4090 的 3 倍，内存带宽是 4090 的 3.35 倍，不管按延迟还是按带宽算，单卡的性能就基本上是 3 倍。而且，H100 比 4090 的网络带宽强太多了，导致 4090 在张量并行中网络通信成了瓶颈，浪费了有效算力。因此，同样的 8 卡机器吞吐量可以达到 4090 的 4.6 倍。虽然一张 H100 卡的价格是 4090 的 20 倍以上，但算上服务器本身的成本、电费和数据中心托管费用，整机的成本只是 5 倍左右。\u003c/p\u003e\u003ch3\u003e\u003cb\u003e用最便宜的设备搞出最高的推理性能\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"LQMIMBsB\"\u003e我们发现在 8 卡 4090 机器中，3 万美金的设备成本，GPU 卡只占了 1.28 万美金，不像 H100 机器那样 GPU 成本占了大头。还有办法进一步降低吗？\u003c/p\u003e\u003cp data-pid=\"JTG4D0sH\"\u003e如果我们可以忍受 5 token/s 的输出速度，甚至\u003cb\u003e可以利用流水线并行，用家用台式机和 4090 攒出个推理集群来。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Ed9VfYb3\"\u003e遥想我当年在 MSRA 的时候，\u003ca href=\"https://link.zhihu.com/?target=https%3A//01.me/2023/01/phd-summary-2/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e在一台只用 1000 美金攒出来的机器上插了 10 块 FPGA\u003c/a\u003e，做出个世界最快的 Key-Value Store。其实如果让我去设计一个性价比最高的 4090 推理集群，有很多种方案可以尝试：\u003c/p\u003e\u003col\u003e\u003cli data-pid=\"-fKhoSdB\"\u003e\u003cb\u003e用流水线并行，台式机 + 10 Gbps 网卡\u003c/b\u003e，足够在 5 ms 内传输 batch size = 330 的 5.28 MB 数据了，通信 40 ms，计算 140 ms，达到 5 token/s 的单 prompt 输出速度，同时又能充分利用 4090 的算力。10 Gbps 的网卡和交换机都很便宜，Intel X710 网卡只要 150 美金，20 口交换机只要 1500 美金（每 8 个口 750 美金），一台家用台式机 700 美金，这只要 2 万美金就可以搞定原本需要 4 万美金的设备。  \u003c/li\u003e\u003cli data-pid=\"mbP9O7-V\"\u003e\u003cb\u003e用张量并行，台式机 + 200 Gbps ConnectX-6 网卡\u003c/b\u003e，上 RoCE，可以把 batch size = 330 的 5.28 MB 数据在 0.22 ms 内传完，160 次传输是 35 ms，加上计算的 17.5 ms，一个 token 52.5 ms，可以达到 19 token/s 的单 prompt 输出速度，这个速度已经不错了。网卡 1000 美金，200G 交换机 2 万美金 40 个端口，平均每 8 个端口 4000 美金，一台家用台式机 700 美金，这只要 3 万美金就能搞定原本 4 万美金的设备。  \u003c/li\u003e\u003cli data-pid=\"WcQLBwUh\"\u003e\u003cb\u003e主机内用张量并行，主机间用流水线并行\u003c/b\u003e，4 卡 PCIe Gen4 服务器主板只要 1000 美金而且能跑满 PCIe 带宽（因为 8 卡就需要 PCIe switch 了，价格会贵很多），两台主机之间用 25 Gbps 网卡直连，主机内张量并行的时延是 27 ms，主机间流水线并行只需 2 次 8 ms 的传输（注意 25G 的网络带宽是 4 张 GPU 卡共享的），加上两次流水线计算各 17.5 ms，总共 78 ms，可以达到 13 token/s 的单 prompt 输出速度。网卡 300 美金 * 2，服务器 3000 美金 * 2，这只要 1.95 万美金就可以搞定原本需要 4 万美金的设备。\u003c/li\u003e\u003c/ol\u003e\u003cp data-pid=\"RdT44Cvm\"\u003e2 万美金按照 3 年摊销是每小时 0.76 美元。按照 0.1 美元/度的电价，每小时的电费都要 0.5 美元，接近设备成本了，这有点挖矿的味道了。矿场里面可没有中央空调和 UPS，只有暴力风扇，托管费用比数据中心低很多，整机的成本是有可能压到 1.5 美元/小时的。如果跑满了 44M tokens 的吞吐量，\u003cb\u003e1 美元能生成 30M tokens，正好是 8 卡 H100 的 15M token per dollar 的 2 倍\u003c/b\u003e。\u003c/p\u003e\u003cp data-pid=\"IF-pIVEY\"\u003e为什么 H100 以 20 倍于 4090 的 GPU 价格，性价比却只差一倍？首先是因为能耗成本更低，8 卡 H100 的功耗是 10 kW，但 9 台 8 卡 4090 的功耗是 45 kW；其次是因为主机和网络设备成本更低，一台 8 卡 H100 准系统虽然贵，但只占整机价格的 20% 左右；但 4090 因为卡多，除非像 GPU 矿机那样压成本，只要还是用数据中心级的设备，准系统价格就要占到 35% 以上。\u003c/p\u003e\u003cp data-pid=\"3sA_1vCa\"\u003e其实，这个世界上不止有 A100/H100 和 4090，还有 A10、A40 等计算卡和 3090 等游戏卡，还有 AMD 的 GPU 和很多其他厂商的 AI 芯片。\u003cb\u003eH100 和 4090 大概率都不是性价比的最优解，例如 A10、A40 和 AMD GPU 的性价比有可能就更高。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"Zu1NipnE\"\u003e我都想搞一个推理性价比挑战赛，看谁能用最便宜的设备搞出最强的推理吞吐量，同时延迟不能太高；或者用最便宜的设备搞出最低的推理延迟，同时吞吐量不能太低。\u003c/p\u003e\u003cp data-pid=\"u5XR4yH1\"\u003e这一切都是在假设使用 LLaMA-2 70B 模型，没有做量化压缩的前提下。如果做了量化压缩，那性能就更高，甚至\u003ca href=\"https://link.zhihu.com/?target=https%3A//mlc.ai/mlc-llm/docs/get_started/try_out.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e在 Unified Memory 够大的 MacBook Pro 上都能单机跑了\u003c/a\u003e。\u003c/p\u003e\u003ch3\u003e\u003cb\u003eLicense 问题怎么办？\u003c/b\u003e\u003c/h3\u003e\u003cp data-pid=\"5K8iAfHh\"\u003e我把这个问题放到最后。\u003ca href=\"https://link.zhihu.com/?target=https%3A//www.nvidia.com/en-us/drivers/geforce-license/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003eNVIDIA Geforce driver 的 License\u003c/a\u003e 里写道：\u003c/p\u003e\u003cblockquote data-pid=\"m6NqBvY-\"\u003eNo Datacenter Deployment. The SOFTWARE is not licensed for datacenter deployment, except that blockchain processing in a datacenter is permitted.\u003c/blockquote\u003e\u003cp data-pid=\"IPfGk2-H\"\u003e既然机器都是用台式机攒起来的，这能叫 data center 吗？还是叫矿场比较合适吧。人家也说了，4090 用来做区块链是允许的。\u003c/p\u003e\u003cp data-pid=\"DAbOET2B\"\u003e我有一个大胆的想法，\u003cb\u003e如果未来的区块链不再用挖矿来做 proof of work，而是用大模型推理来做 proof of work，这是不是很有意思？\u003c/b\u003e每个人买几块显卡，接到矿池上，既可以自己用来玩游戏，闲时又可以贡献算力。矿池直接就是个卖大模型推理 SaaS 服务的公司，提供前所未有的低价 API。甚至需要大模型推理服务的人可以在区块链里自己 P2P 玩起来，谁要用大模型就付点 gas。\u003c/p\u003e\u003cp data-pid=\"qO3MaKUB\"\u003e当然，目前的 proof of work 都是计算很复杂，验证很简单的。如果真用大模型推理做 proof of work，必须防止用户随意编造一个结果交上去。当然这也是有解决方案的，就像 BitTorrent 和其他一些去中心化网络一样，采用信用机制，新人只能做验证别人计算结果的工作，积攒信用；老人每次算错了，都有比较严厉的惩罚。\u003c/p\u003e\u003cp data-pid=\"FmiI41cj\"\u003e从另一个角度看，\u003cb\u003e家庭局域网络的速度也越来越快\u003c/b\u003e，比如我家就自己部署了 10 Gbps 的网络。家中的智能设备越来越多，算力越来越强。光纤入户也越来越普遍，小区和城市的运营商机房里部署了越来越多的边缘计算节点。前面我们用 1 Gbps 的网络就足以把多台主机上的 GPU 组成流水线并行，那么在未来的家庭高速网络中，流水线并行甚至张量并行都将成为可能。\u003c/p\u003e\u003cp data-pid=\"pb2Phjyb\"\u003e大多数搞 AI 推理的都只关心数据中心，忽略了家中的分布式算力。\u003cb\u003e只要解决了安全、隐私和经济动机问题，我家的 Siri，也许就跑在邻居家里的 GPU 上。\u003c/b\u003e\u003c/p\u003e\u003cp data-pid=\"FAFOFZ2p\"\u003e很多人都在说要 democratize AI。但现在大模型平民化的最大障碍就是成本，而成本最大的来源又是 GPU 市场上计算卡和游戏卡价格的剪刀差。这并不是指责某家公司，其他做 AI 芯片的公司，AI 芯片的算力也并不便宜。毕竟芯片、软件和生态的研发都是白花花的银子。\u003c/p\u003e\u003cp data-pid=\"4LO7yca2\"\u003e就像本文开头提到的微软给每台服务器部署 FPGA 一样，大规模量产的芯片价格就像沙子一样。到时候，能限制大模型推理算力的就只有能源了，就像区块链挖矿和通用 CPU 的云计算一样，都在找最便宜的电力供应。我在之前的一个采访中就表示，\u003ca href=\"https://link.zhihu.com/?target=https%3A//01.me/2023/05/zhizaogongshe-interview/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003e长期来看，能源和材料可能是制约大模型发展的关键\u003c/a\u003e。让我们期待廉价的大模型走进千家万户，真正改变人们的生活。\u003c/p\u003e","is_labeled":false,"visited_count":182764,"thumbnails":["https://picx.zhimg.com/50/v2-2a0fb8ef098bd6125c6b76aa5a6ce4d9_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-16df830817b34a5b29490a17d257ec95_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-dda2c926f0fd1d0589c2f85ab92b71d7_720w.jpg?source=b6762063","https://pica.zhimg.com/50/v2-8619dfcbdb6ff0801d37b2f994166b67_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-65ec0f13b1d35b52215f069095b8bf49_720w.jpg?source=b6762063","https://pic1.zhimg.com/50/v2-4446e63867a705a8001c5ede82839c2d_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-ea1fb44d0831c402e0eedade4be69c17_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-2a5981867455673203e3893d03841c9d_720w.jpg?source=b6762063","https://picx.zhimg.com/50/v2-0f92251bad105b10623cd4a035826167_720w.jpg?source=b6762063"],"favorite_count":5188,"article_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"article\", \"id\": 655402388}","attached_info":"CpMKCN6arYPvwbuY8QEQBxoJMjMzNzczNjAyIIyD9qcGKL4aMIYCQJgBSkEKLFRTX1NPVVJDRV9UV09UT1dFUl9TSE9SVElOVEVSRVNUX1JFQ0FMTF9URVhUEgEwGAAgADoKeyJyYXciOiIifVoIMTMzMzg1MzdiIDU5MDE5OTM4OTdiMmI2NzE1MzRiNDFkZmUxNzAzOTQ3cgk2NTU0MDIzODiKARVjXzE2NzA3NDU2MDc3MjI2MzkzNjCqAQlyZWNvbW1lbmTCASAwMjg3N2M0ZmRlM2IyZTlkYmQ5MzhmMWE4ZWVmNTk4ZPIBCggMEgZOb3JtYWzyASgIChIkODAyMjMzNmYtOTI4Ny00OGU2LWE1MTUtZmI1ZGZhMmQ1ODhl8gEGCAsSAjI2ggIAiAKY+77N+jKSAiAwMjg3N2M0ZmRlM2IyZTlkYmQ5MzhmMWE4ZWVmNTk4ZJoCAMoCFlNob3JJbnRlcmVzdFdlaWdodFJ1bGXKAhVVc2VyTGNuRXhpdFdlaWdodFJ1bGXKAhRDb250ZW50QWdlV2VpZ2h0UnVsZcoCF1Rlc3RlZEFuZFdvcmtXZWlnaHRSdWxl2gIsVFNfU09VUkNFX1RXT1RPV0VSX1NIT1JUSU5URVJFU1RfUkVDQUxMX1RFWFToAgX6AgtOT1JNQUxfRkxPV4oDIDgxNmIzZTU2MzNmZjQ4MTA4Yjc5ZjA3ZGI0NDA3NTE0mgMNCgJ2MhAAGgVvdGhlcqgD7JML2AMA6gMaZmVlZF9hdHRtX3R3b3Rvd2VyX3YyX3RleHT6A6QEEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERTotCAIQmhEYxAUiI3YyLTBkNmZiNjJmYTVkOTE2ODljODZhNjQ3ZTI0NzZlNGFhOi0IAhDUERisDCIjdjItZTZmNzI0YjI2MGU2N2QwYmViNTYwNzhhYjQ4OTViMDE6LQgCENwRGOAFIiN2Mi1kM2ZlNjI1Njg4MjFhYTQxYmRiOGI4MWNkMmJjYWYyYjotCAIQ9BEYuAwiI3YyLTAwYjNjYjNhNGRiMzkyMmFmMjM0ZGMzNDY0MGQ1ZjFmOi0IBBDeBBjeAiIjdjItMjc1ZGVjNWE0MDMxMWFjNzNhYmRjNDQ5NDhhMzM4MDY6LQgEEPANGJYNIiN2Mi00YWZiZjYwY2QyMWU4OWYyZjQyYTIzODRiNzlmZDA1MDotCAQQmwwYnQgiI3YyLWQ3N2Y2MDlhMjk5ZGU2MWMzNmU2YzA2MzA1MGJjY2YyOi0IBBCmBRjMASIjdjItNjFmNzMwZjY1ZTcwNmFmYjVhNTAzNTRhZDJhNTQ0ZWI6LQgEEIAQGM4JIiN2Mi1lMjJiOThkNTRhNmJkMGZjMTgwMzc0MjFlNzhmOTc0NTotCAIQnAsYpAYiI3YyLTNjZDc2ZDNlMGQ4YTIwZDg3ZGZhNTg2YjU2Y2MxYWQzOi0IBBD4Chi7CCIjdjItZDRkYTFlYjZlNjk2YWYzZWEzZjBiZjI5ZTc1MGU1NjSABACIBACSBAZOb3JtYWyaBAE1oAQAqAQAsAQAugQGbWFudWFswgQDMTcwyAQA0gQP5o6o6I2Q5bey5pu05paw2AQA8AQA+QQAAAAgKY2zP4EFAAAAAAAAAACJBbAlxxwtnNI/kgUAmgUDZGZ0ogUDZGZ0sgUBMbkFAAAAAAAAAADQBQDgBQDoBQDwBRqQBgCgBpwBqAYAkgIkCgkyMzM3NzM2MDISCTY1NTQwMjM4OBgHIgpJTUFHRV9URVhU","action_card":false},{"id":"153_1750898556.862","type":"feed","offset":153,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898556,"updated_time":1750898556,"target":{"id":"3258161033","type":"answer","url":"https://api.zhihu.com/answers/3258161033","author":{"id":"55102c956e92ad7b7b250f781b45c825","url":"https://api.zhihu.com/people/55102c956e92ad7b7b250f781b45c825","user_type":"people","url_token":"NLSZ-an-hao-xin","name":"安好心","headline":"微博id：安好心","avatar_url":"https://pic1.zhimg.com/50/v2-45f2eb4f8cba4cef2c07ec64db7e984e_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":247115,"is_following":false,"is_followed":false},"created_time":1697819203,"updated_time":1747693612,"voteup_count":5795,"thanks_count":806,"comment_count":195,"is_copyable":false,"question":{"id":"589709354","type":"question","url":"https://api.zhihu.com/questions/589709354","author":{"id":"1baef3eb882ffc34912e507778c353c4","url":"https://api.zhihu.com/people/1baef3eb882ffc34912e507778c353c4","user_type":"people","url_token":"tang-zuo-56","name":"汤左","headline":"学生","avatar_url":"https://pic1.zhimg.com/50/70f8c06d76817d7420bf4d6499ce6ab4_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":16,"is_following":false,"is_followed":false},"title":"如何看待“霍金来了也要起身陪酒”的言论？","created":1678857506,"answer_count":0,"follower_count":0,"comment_count":96,"bound_topic_ids":[16295],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"我来给你们讲讲萝莉岛本质是什么玩意吧。 这是“资源整合局”的终极形态，而它的最小单位，恰恰就是一次敬酒。 什么是资源整合局？ 可以说除了你的专业以外，提升你一切有关你做成事必要条件的社交活动。小到求人办事，大到合伙创业。闲时财散人聚，急时财聚人散。可能是你批条子，我出银子，他出点子。也可能是我缺人指路，你恰好有这方面消息。更有可能谁一辈子也用不上谁，但交个朋友彼此就像财产在那放着，死前花不掉也不至…","excerpt_new":"我来给你们讲讲萝莉岛本质是什么玩意吧。 这是“资源整合局”的终极形态，而它的最小单位，恰恰就是一次敬酒。 什么是资源整合局？ 可以说除了你的专业以外，提升你一切有关你做成事必要条件的社交活动。小到求人办事，大到合伙创业。闲时财散人聚，急时财聚人散。可能是你批条子，我出银子，他出点子。也可能是我缺人指路，你恰好有这方面消息。更有可能谁一辈子也用不上谁，但交个朋友彼此就像财产在那放着，死前花不掉也不至…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"sG2n-n8l\"\u003e我来给你们讲讲萝莉岛本质是什么玩意吧。\u003c/p\u003e\u003cp data-pid=\"V2I2I3Xh\"\u003e这是“资源整合局”的终极形态，而它的最小单位，恰恰就是一次敬酒。\u003c/p\u003e\u003cp data-pid=\"7XT4fDAz\"\u003e什么是资源整合局？\u003c/p\u003e\u003cp data-pid=\"wfi_ckGx\"\u003e可以说除了你的专业以外，提升你一切有关你做成事必要条件的社交活动。小到求人办事，大到合伙创业。闲时财散人聚，急时财聚人散。可能是你批条子，我出银子，他出点子。也可能是我缺人指路，你恰好有这方面消息。更有可能谁一辈子也用不上谁，但交个朋友彼此就像财产在那放着，死前花不掉也不至于烧了。\u003c/p\u003e\u003cp data-pid=\"3U03zEhv\"\u003e如果你还不懂，那简单的说俩字：“王林”。\u003c/p\u003e\u003cp data-pid=\"ZuzYOEwi\"\u003e想想参加抓蛇局的人，那个被证监会查的，那个被反垄断罚的。他们是真对特异功能感兴趣吗？还是想借这个局认识彼此呢？\u003c/p\u003e\u003cp data-pid=\"YbYNJvdd\"\u003e王林只是一个抓蛇的神棍吗？想想他的杀人罪，杀到什么级别的人了，自己查查。这就是这类脏局掮客的能量。\u003c/p\u003e\u003cp data-pid=\"zJawRjnY\"\u003e虽然他的杀人罪最终还是判了，但也正因如此，我们离资源整合局终极形态还远，你要感到庆幸。\u003c/p\u003e\u003cp data-pid=\"OTu5sxaO\"\u003e人家萝莉岛是单独一个岛的事吗？这个岛背后的大佬，说把谁灭口就灭口，全国都没人敢查了。\u003c/p\u003e\u003cp data-pid=\"4pp5z4DP\"\u003e你说他们是统治阶级，想统治国家？根本没这回事。\u003c/p\u003e\u003cp data-pid=\"rqeFlS7i\"\u003e他们之所以隐在幕后，就是为了剥离了统治者的责任，只想掏空这个国家。\u003c/p\u003e\u003cp data-pid=\"riL-gTEy\"\u003e迎接美乐帝的不只有敞篷车外的鲜花，还有子弹。\u003c/p\u003e\u003cp data-pid=\"D4tAHMgv\"\u003e迎接拜登晚年的不只有40%带尿布休假时间，还有剩下60%说胡话发呆出丑时间。\u003c/p\u003e\u003cp data-pid=\"WX-w2b48\"\u003e但你要退居幕后，迎接你的就只剩萝莉了。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"fZDOb6lp\"\u003e局和局不一样，我们大多数人都是普通人，拿酒精玩玩服从性测试就是上限了，也不是不能挣钱。\u003c/p\u003e\u003cp data-pid=\"SBGXkM52\"\u003e某些做销售跑业务的，可能会被叫到商务KTV和夜总会一起玩脏的，当然更多的是你要付钱攒局。\u003c/p\u003e\u003cp data-pid=\"0B_0l7Tq\"\u003e没人会逼你，给不给面子你看着办，如果你从了，那十有八九就会背叛老婆。如果你不从，事就办不成。\u003c/p\u003e\u003cp data-pid=\"I2_kLn-I\"\u003e虽然到这一层，还通常是商圈的活动。但你要讨好的人已经拥的特权对你发出了灵魂拷问：“这活儿为什么给你不给他”。同流合污是一个隐形门票。即使你同流合污了，也不一定看得上你。但无论如何，都不是因为你的专业性。\u003c/p\u003e\u003cp data-pid=\"FJRI2ukY\"\u003e好，那问题来了，我们谈论的既然是“对科研人员的重视”，而你被选择的理由却不是专业性，你不觉得这里有什么不对吗？？？？？\u003c/p\u003e\u003cp data-pid=\"DxovY2Mn\"\u003e你一个科研工作者今天要是抛开专业能力不谈，那我建议你还是练练怎么敬酒比较好，腰也练练，不要再愤青了。\u003c/p\u003e\u003cp class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"2b3_Ex5V\"\u003e再讲深点吧。\u003c/p\u003e\u003cp data-pid=\"l93DV9vO\"\u003e资源整合局往上走，会避不开一个东西。叫做“特权表演”，这是网上搜不到的东西。\u003c/p\u003e\u003cp data-pid=\"rZ_gAB1D\"\u003e这种局去多了，你就知道我说的是什么了。\u003c/p\u003e\u003cp data-pid=\"bKRUCuUi\"\u003e说个年少时的亲身经历。\u003c/p\u003e\u003cp data-pid=\"j4nWC9WC\"\u003e某年，某地，某局。饭吃得差不多了，东家打电话叫来一个艺人。是一个上过不少节目的艺人“变脸王”，全国有一号，全省称王，宗脉传到他那一代正被他发扬光大。他被邀请来给饭桌上的我们近距离演的变脸，惟妙惟肖，要快有快的，要慢有慢的，丝毫没破绽。穿梭于饭桌之间，一点走位的生疏都看不出来。好像这里比电视台上更像他的主场。\u003c/p\u003e\u003cp data-pid=\"ZxfX34Hd\"\u003e当时做东的大叔不仅要强调自己是一个电话让这位变脸王屁颠颠来在我们饭局上表演，强调他推了多少多少活动，还要用手拨弄人家脑袋让人家过来以示尊卑。半开玩笑的尴尬，令人不耐。但变脸王没有生气，最多是笑有些不自然，但也可能是带着面具的缘故。年少的我对此感到生理不适，恨不得起身离席，但还是坐了下来。\u003c/p\u003e\u003cp data-pid=\"FKcca4ii\"\u003e当时看不出这样侮辱艺人有什么必要，人家已经成名，为什么不给点面子呢？你这不是得罪人吗？长大之后才明白，就是因为成名，才要摆弄。不一定要在资源整合局里真的进行整合，第一次见通常是交个朋友而已。但时长展示特权是免不了的。后来我也懂了那位艺人为何顺从，他可能参透了自己的价值，不是真的来变脸的，就是来让大佬拍脑袋的显示尊卑的。\u003c/p\u003e\u003cp data-pid=\"j6C5aKGi\"\u003e现在看懂我要说什么了么？人的尊严，是这类局用来把玩的东西。把玩的狠了就涉及违法乱纪，把玩的再庆也不会留下一具没被捏过的自尊。\u003c/p\u003e\u003cp data-pid=\"jXA2WUcx\"\u003e所以平时我们从党内的一些通报批评也能看出来问题。组织上看见你搞比较过分的“特权展示”，太过违背公序良俗的话马上就说你“作风有问题”，不是组织希望你要想个儒家定义的君子，而是大家都心知肚明你搞这种局肯定没憋好屁，想办法制止，制止不了那也不能让这些人升官。所以体制内因为这些作风问题没被提拔，听起来很土，但实际上很正确，是为法律所不能限制的腐败风险做补充，外行不知道罢了。你还以为铺张浪费是为了铺张浪费本身......还以为三公消费用了之后自己能高兴得满地打滚？？？？得了吧，他们大鱼大肉早吃腻了，挥霍是为了展示，展示是获取背后更大的利益。\u003c/p\u003e\u003cp data-pid=\"rKylgacm\"\u003e酒是这里重要元素，但象征性更大，实际意义不大。不然为啥组织内不禁酒呢？我们天天觉得体制内各种规章制度死板、过时，结果你们一个个恨不得把敬酒这点破事骂上天，对小萝莉服侍视而不见，你岂不是更迂腐？\u003c/p\u003e\u003cp data-pid=\"gwV9uLjm\"\u003e其实我参加的那个局还有很多烂事，特权免票、警灯开道等等，不方便一一道来。感受到那种张狂后，带我去的长辈便不再与之来往了。果然过些年，苍蝇老虎一起打，他们的也确实都没有好下场，这也拯救了我的三观。\u003c/p\u003e\u003cp data-pid=\"CHSQaK-A\"\u003e这类局中，明明都是不缺钱的人，为什么要用特权免票？明明不用闯封锁区的人，为什么要打电话给管理人员的领导，强行过境？明明可以开慢点差不多几分钟.......明明太多事没必要，却冒着组织上惩罚的危险非要去做，还假装是为了方便大家一起玩，好像多仗义似的。而且这些能力不出自一个人，而是八仙过海各显神通。\u003c/p\u003e\u003cp data-pid=\"PXLcsdwD\"\u003e但没办法，这都是资源整合局中的“特权表演”。你可以理解为一部漫画里，东家招来包括主人公的各种奇人异事要执行一个任务。还没开始行动呢，剧情就总会安排这些人无可避免得漏一手，以便你很快记住这些角色。而在现实中，为了让彼此知道自己的价值，做出一些才艺展示。今天只是交朋友，说不定哪天就用上了彼此。\u003c/p\u003e\u003cp data-pid=\"Hhx3Dzuh\" class=\"ztext-empty-paragraph\"\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp data-pid=\"ryCQG8Wg\"\u003e腐败是个很宽泛的概念，如果不谈法律，最好不要执着于文字定义。因为有的国家直接把这些罪行合法化了，但玩得比我们更花，字面上让你不许说它腐败，但腐败实质已经登峰造极。萝莉岛的罪恶在于，它是这资源整合局的最终形态。它可以不是萝莉，可以是别的什么人什么事，但一定是非常违背人伦的，挑战法律的。 这就好像腐败官员吃个饭还非得点个二级保护动物似的。你说他吃得出好来么？不吃会死吗？不会，但不跟你装x，就会。在那个年代，装x是让你心理评估他的价值呢。\u003c/p\u003e\u003cp data-pid=\"lhIcJF8Y\"\u003e好了，现在又回到那个问题。一个能展示违反人伦的资源整合局，它想从你身上获取的价值，是从其他正规渠道也能得到的吗？如果正规渠道可以得到，那花那么大功夫，是吃饱了撑的？上岛来整合资源，就意味着跳出正规竞争，就意味着掏空国家，就意味着凑出非常规也无法公开的力量。这是毫无疑问的，这是萝莉局性质决定的。\u003c/p\u003e\u003cp data-pid=\"PfhRvHfT\"\u003e那你说，萝莉岛展示的力量如何？你被服侍完，掂量了一下这岛的背景深浅，看了看各路来宾，最后得出的结论是“不怎么样”？也许爱泼斯坦本人确实不怎么样，但这个局不是。因为他们为了灭口，当着全世界的面儿弄死了爱泼斯坦，没有证据，没人追查，全世界最强国，自称最民主最自由的老百姓干看着，什么都做不了。都这样了，你还不服吗？\u003c/p\u003e\u003cp data-pid=\"Gc_l8Pzi\"\u003e所以你思考一下，如果你是受邀者，若你要从这个局里获取力量，那你要付出什么？\u003c/p\u003e\u003cp data-pid=\"B1HD3_h_\"\u003e霍金是那种典型的名声大于学术地位的人。请他上岛，该不会是女装克林顿忽然对黑洞蒸发与霍金辐射感兴趣。肯定是看上他一些别的实力，至少让他付出的，不会是帮忙写几篇论文。真正需要他帮忙的时候，更可能是关键时刻在他们的触手伸向科研领域的时候，他能站出来站台说话。事实上霍金已经为民主党说过话抨击过特朗普了，类似的上岛名流都有相同政治倾向。还没有展现更恶劣的政治站台，那是因为没机会了，岛的事曝光，人也寿终正寝了。\u003c/p\u003e\u003cp data-pid=\"ieDZJeYD\"\u003e但你可千万别误会一点，非法萝莉是“展示”，不是贿赂。她们被当做物品和牲口一样在这里接受剥削，根本不被考虑其“使用成本”。因为展示的就是这个拿人不当人的“用法”。如果不进行性剥削，人家还以为你是从外面请来的妞呢，觉得你是个只会拿钱解决问题的废物，瞧不起你的手腕怎么办？这就是为什么，她们被抓来服侍顶级人物，反而受到的对待更丧尽天良的原因，就得让你来被虐待的。\u003c/p\u003e\u003cp data-pid=\"2pbX7_Ou\"\u003e这类局不会马上要求别人拿出资源互相交换，大家交个朋友而已，那时大家凑局更像是游戏里凑队伍，找个T，找个奶，找俩DPS，然后大家一起如下副本一样聊天，话题遍布各行各业。一有事再找彼此帮忙，更容易成事，如果不喜欢这个局面，以后可以找借口不来。我们没有证据说霍金真的有求于岛主，也想不出他还有什么非得要岛上人才能帮助解决的，哪怕是搞定一些自愿的萝莉。（当然他不喜欢萝莉，他本就是换妻俱乐部的vip）\u003c/p\u003e\u003cp data-pid=\"Agzw9dui\"\u003e所以我们更多人才觉得霍金躺在轮椅上都要上岛，更像是在不得不给“资产阶级领导”敬酒。他敬了个“大酒”。就像我一开始说的，酒局上的一次敬酒从最小单位，他所处的局是到终极形态。他已经经历了那种我们想象中的身不由己，死后至今都因此事名节有亏。\u003c/p\u003e\u003cp data-pid=\"4x22Xzj4\"\u003e更令我厌恶的是。这类资源整合局到处找名流来上岛的架势，更常见于犹太大佬控制世界舆论的手段。更像是几个一起打高尔夫的华尔街CEO联合起来不让某些支持巴勒斯坦的学生找到工作一样。这些人联合起来，翻手为云覆手为雨。我们的世界之所以遭受的创伤，贫富差距之所以那么大，所学的一身本领之所以总是用不上，离不开这些人在背后交换着资源和利益。让他们的人得到舆论支持，让他们的人承包项目，最后让他们认识的外行指挥内行。\u003c/p\u003e\u003cp data-pid=\"5ywplquQ\"\u003e他们不可能干正常人干的事。因为他们都很有钱，去正常渠道什么都可以买到。既然还要开辟不正常的小脏局，肯定没好事。\u003c/p\u003e\u003cp data-pid=\"jToWAM0a\"\u003e你说美国科技发达，比中国强，那好。马斯克可以代表美国科技企业了吧。他能挣钱理由有很多，有个人能力，有美国自身的营商环境，也有美国对外贸易霸权等等，但唯独不能说被萝莉岛上的“领导”看中了并栽培。\u003c/p\u003e\u003cp data-pid=\"vHMFJXvN\"\u003e马斯克说自己被暗杀率飙升的时候，他所指的，正是民主党系杀手。就是环希拉里神秘死亡和环爱泼斯坦案神秘死亡的那批杀手。美国全国都知道他在说谁，但谁都无法拿这帮人有办法。你说这是阴谋论？那你去查查，看你敢不敢就完了。\u003c/p\u003e\u003cp data-pid=\"jFKfuxAo\"\u003e这种肮脏的小圈子对国家内在实力是摧毁性的，对马斯克这种实实在在的帝国根基，是随时有致命威胁的。世上总有人造血，总有人吸血。马斯克是前者，萝莉岛系毫无疑问是后者。被后者祝福的萝岛神选，到底对这个国家有什么用？没帮倒忙都算他出淤泥而不染。\u003c/p\u003e\u003cp data-pid=\"6pCeROgF\"\u003e人类是一定要会和这种践踏别人尊严的小圈子战斗到底的。\u003c/p\u003e\u003cp data-pid=\"n6Ym11PD\"\u003e从小处看，如果该地区该行业的大部分单子，必须喝酒何出病来才能拿到的话，地区必然贫富差距拉大，行业必然没落萎靡。\u003c/p\u003e\u003cp data-pid=\"F7DRtPxj\"\u003e从大处看，正规途径讲不明白，上岛见识违法乱纪的展示后才能讲明白的事，一定是损害正道利益的。他对科学家是重视了，但对科学不重视，对你我不重视，对国家利益不重视。你确定要舍本逐末？\u003c/p\u003e\u003cp data-pid=\"7Ruj-XSn\"\u003e这些资源整合，往往是超越规则的，破坏公正的，其破坏力的系数，恰恰可以看做是东家对你做出的“特权表演”。\u003c/p\u003e\u003cp data-pid=\"HvYuyayV\"\u003e有小特权，就有小腐败。\u003c/p\u003e\u003cp data-pid=\"T4jAa7gW\"\u003e有大特权，就有大腐败。\u003c/p\u003e\u003cp data-pid=\"ZDzkEwo-\"\u003e有未成年性剥削给你玩，那局里就有人能当着全世界人面灭口的终极腐败。\u003c/p\u003e\u003cp data-pid=\"dni-zuWg\"\u003e这个定律是非常铁的，只是光靠搜集资料来写文章的人是很难懂的。\u003c/p\u003e\u003cp data-pid=\"SDrdEJ8u\"\u003e当你踏入一个服侍你的女性都眼神都很恐惧很不情愿的地方，当你发现这里充满罪恶却没人敢反抗没人敢报道的时候，你脑子如果是正常的，第一个感受到的不是什么统治阶级接纳了同志这种诡异的事。而是意识到自己踏入了特权核心，并将付出自己专业以外的代价与之攀交。说不定还要献出你的灵魂。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":414320,"favorite_count":4776,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 3258161033}","attached_info":"CocGCN6arYPvwbuY8QEQBBoJNjIyNDcyNjYxIMPUyqkGKKMtMMMBQJkBSi8KBkl0ZW1DRhIfZG9jX3R5cGU6IEFuc3dlcgppZDogNjI5OTQwNTc0ChgAIAA6AFoIOTMzMjgxMzhiIDU5MDE5OTM4OTdiMmI2NzE1MzRiNDFkZmUxNzAzOTQ3cgozMjU4MTYxMDMzigEJNTg5NzA5MzU0qgEJcmVjb21tZW5kwgEgNTUxMDJjOTU2ZTkyYWQ3YjdiMjUwZjc4MWI0NWM4MjXyAQoIDBIGTm9ybWFs8gEoCAoSJDI5MWE0YTJjLTI4ZjUtNGIzYS1iOGYzLTliNDU2MWMyNjMwNPIBBggLEgIyNoICAIgCmPu+zfoykgIgNTUxMDJjOTU2ZTkyYWQ3YjdiMjUwZjc4MWI0NWM4MjWaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxl2gIGSXRlbUNG6AID+gILTk9STUFMX0ZMT1eKAyA4MTZiM2U1NjMzZmY0ODEwOGI3OWYwN2RiNDQwNzUxNJoDDQoCdjIQABoFb3RoZXKoA/CkGdgDAOoDH3RleHRBbGxTaXRlTXZIaWdoQWN0aW9uSXRlbUNGVjH6Ax8SDFVOS05PV05fTU9ERSAAKg1OT19JTUFHRV9NT0RFgAQAiAQAkgQGTm9ybWFsmgQBM6AEAKgEALAEALoEBm1hbnVhbMIEAzE2MMgEANIED+aOqOiNkOW3suabtOaWsNgEAPAEAPkEAAAAYNWixD+BBQAAAAAAAAAAiQWwJcccLZzSP5IFAJoFA2RmdKIFA2RmdLIFATG5BQAAAAAAAAAA0AUA4AUA6AUA8AUakAYAoAadAagGAJICJQoJNjIyNDcyNjYxEgozMjU4MTYxMDMzGAQiCklNQUdFX1RFWFQ=","action_card":false},{"id":"154_1750898556.470","type":"feed","offset":154,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898556,"updated_time":1750898556,"target":{"id":"1921299069315299041","type":"answer","url":"https://api.zhihu.com/answers/1921299069315299041","author":{"id":"78e454b3f6068c910147818705902bb5","url":"https://api.zhihu.com/people/78e454b3f6068c910147818705902bb5","user_type":"people","url_token":"sky-25-29","name":"sky","headline":"视觉设计 自学成材 野生插画爱好者","avatar_url":"https://picx.zhimg.com/50/v2-dd3a412ac54901675ad4fad5a5d5ab12_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":4980,"is_following":false,"is_followed":false},"created_time":1750853394,"updated_time":1750853394,"voteup_count":0,"thanks_count":0,"comment_count":0,"is_copyable":false,"question":{"id":"1921003724714799614","type":"question","url":"https://api.zhihu.com/questions/1921003724714799614","author":{"id":"12201d4f2717aadd7ccbd12618ec1347","url":"https://api.zhihu.com/people/12201d4f2717aadd7ccbd12618ec1347","user_type":"people","url_token":"ju-wai-ren-50-93","name":"镜子别酷","headline":"一个笑起来居然有一个酒窝的00后创业者","avatar_url":"https://picx.zhimg.com/50/v2-c193f7b464f55419373d049bc3662055_l.jpg?source=b6762063","is_org":false,"gender":1,"followers_count":4,"is_following":false,"is_followed":false},"title":"有哪些是你百用不爽的AI提示词？","created":1750782978,"answer_count":0,"follower_count":0,"comment_count":0,"bound_topic_ids":[2502371,2758397],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"喜欢用ai画复杂图案首饰道具，为了方便抠图，每个提示词最后都会加上，“垂直平铺，黑色背景”。不论是图案还是实物道具都是正面平铺的。百试不爽，几乎能一一键抠图。","excerpt_new":"喜欢用ai画复杂图案首饰道具，为了方便抠图，每个提示词最后都会加上，“垂直平铺，黑色背景”。不论是图案还是实物道具都是正面平铺的。百试不爽，几乎能一一键抠图。","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"JZkVbZnh\"\u003e喜欢用ai画复杂图案首饰道具，为了方便抠图，每个提示词最后都会加上，“垂直平铺，黑色背景”。不论是图案还是实物道具都是正面平铺的。百试不爽，几乎能一一键抠图。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":true,"visited_count":33,"favorite_count":1,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 1921299069315299041}","attached_info":"CrQFCN6arYPvwbuY8QEQBBoJNzMzOTkzNDA5IJLO78IGKAAwAECaAUokChlUU19TT1VSQ0VfV0FSTV9VUF9OT1JNQUwyEgEwGAAgADoAWgkxMTU1Mzg0MzliIDU5MDE5OTM4OTdiMmI2NzE1MzRiNDFkZmUxNzAzOTQ3chMxOTIxMjk5MDY5MzE1Mjk5MDQxigETMTkyMTAwMzcyNDcxNDc5OTYxNKoBCXJlY29tbWVuZMIBIDc4ZTQ1NGIzZjYwNjhjOTEwMTQ3ODE4NzA1OTAyYmI18gEKCAwSBk5vcm1hbPIBKAgKEiRlYjQ3ZGI5Mi1lMDJiLTRmMzItOWM1My1lZjNlM2M2NjAyNTLyAQYICxICMjaCAgCIApj7vs36MpICIDc4ZTQ1NGIzZjYwNjhjOTEwMTQ3ODE4NzA1OTAyYmI1mgIAygIVVXNlckxjbkV4aXRXZWlnaHRSdWxlygIYQ29udGVudFdhcm1VcEJyZWFrSW5SdWxl2gIZVFNfU09VUkNFX1dBUk1fVVBfTk9STUFMMugCAvoCC05PUk1BTF9GTE9XigMgODE2YjNlNTYzM2ZmNDgxMDhiNzlmMDdkYjQ0MDc1MTSaAw0KAnYyEAAaBW90aGVyqAMh2AMA6gMfdGV4dF8xMmhvdXJfdW5pZmluc2hlZF9yZWNhbGxlcvoDHxIMVU5LTk9XTl9NT0RFIAAqDU5PX0lNQUdFX01PREWABACIBACSBAZOb3JtYWyaBAEyoAQAqAQAsAQAugQCYWnCBAM0MDDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAAA2BqI/gQUAAAAAAAAAAIkFsCXHHC2c0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFGpAGAKAGngGoBgGSAi4KCTczMzk5MzQwORITMTkyMTI5OTA2OTMxNTI5OTA0MRgEIgpJTUFHRV9URVhU","action_card":false},{"id":"155_1750898556.985","type":"feed","offset":155,"verb":"TOPIC_ACKNOWLEDGED_ANSWER","created_time":1750898556,"updated_time":1750898556,"target":{"id":"96148469096","type":"answer","url":"https://api.zhihu.com/answers/96148469096","author":{"id":"503b6f2bdebb10979f92d0d97372b42f","url":"https://api.zhihu.com/people/503b6f2bdebb10979f92d0d97372b42f","user_type":"people","url_token":"skudl-hu","name":"静娃","headline":"作者在思考 ","avatar_url":"https://picx.zhimg.com/50/ee56c33ad3df55d5d96f81741655d874_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":2534,"is_following":false,"is_followed":false},"created_time":1739026755,"updated_time":1748434511,"voteup_count":180,"thanks_count":11,"comment_count":12,"is_copyable":false,"question":{"id":"20263920","type":"question","url":"https://api.zhihu.com/questions/20263920","author":{"id":"1633c229e1f77214fe4babbb39b3a675","url":"https://api.zhihu.com/people/1633c229e1f77214fe4babbb39b3a675","user_type":"people","url_token":"chen-xiao-diao","name":"陈小调","headline":"","avatar_url":"https://pic1.zhimg.com/50/v2-1ca53f98629fb5669e7900d6f924f882_l.jpg?source=b6762063","is_org":false,"gender":0,"followers_count":60,"is_following":false,"is_followed":false},"title":"一个人创业有可能吗？","created":1338210206,"answer_count":0,"follower_count":0,"comment_count":3,"bound_topic_ids":[1740,7129],"is_following":false,"excerpt":"","relationship":{"is_author":false},"detail":"","question_type":"normal"},"excerpt":"我身边创业成功的人，基本都是个体创业。 美国目前大概有400万家小规模公司，有多小呢，只有4名及以下员工。而美国目前一共才只有600多万家公司。 中国也是如此，中国的工商登记主体数量为1.7亿户，其中个体工商户数量为1.14亿户，占所有市场主体数量约三分之二。 我今天和前同事说，你知道个体创业成功的关键是什么？ ①关键是可以用最低成本启动创业。 我身边先上商学院，再租一个场地，再招聘一堆员工创业的人，大部分都失败…","excerpt_new":"我身边创业成功的人，基本都是个体创业。 美国目前大概有400万家小规模公司，有多小呢，只有4名及以下员工。而美国目前一共才只有600多万家公司。 中国也是如此，中国的工商登记主体数量为1.7亿户，其中个体工商户数量为1.14亿户，占所有市场主体数量约三分之二。 我今天和前同事说，你知道个体创业成功的关键是什么？ ①关键是可以用最低成本启动创业。 我身边先上商学院，再租一个场地，再招聘一堆员工创业的人，大部分都失败…","preview_type":"default","preview_text":"","reshipment_settings":"disallowed","content":"\u003cp data-pid=\"0pu4HWBS\"\u003e我身边创业成功的人，基本都是个体创业。\u003c/p\u003e\u003cp data-pid=\"4OdaaOL2\"\u003e美国目前大概有400万家小规模公司，有多小呢，只有4名及以下员工。而美国目前一共才只有600多万家公司。\u003c/p\u003e\u003cp data-pid=\"Rmhu0004\"\u003e中国也是如此，中国的工商登记主体数量为1.7亿户，其中个体工商户数量为1.14亿户，占所有市场主体数量约三分之二。\u003c/p\u003e\u003cp data-pid=\"87FwvMIT\"\u003e我今天和前同事说，你知道个体创业成功的关键是什么？\u003c/p\u003e\u003cp data-pid=\"1T45dxE5\"\u003e①关键是可以用最低成本启动创业。\u003c/p\u003e\u003cp data-pid=\"zA6SeQnY\"\u003e我身边先上商学院，再租一个场地，再招聘一堆员工创业的人，大部分都失败了，现在都快成为被执行失信人。\u003c/p\u003e\u003cp data-pid=\"V2hIaDTa\"\u003e很多人创业没有思路，想透过商学院走捷径，实际上商学院抛出来的概念适合世界五百强试一试，未必适合个体创业者。\u003c/p\u003e\u003cp data-pid=\"3_H8RqXk\"\u003e就拿被刘润、张琦等商业顾问炒作很火的概念“私域运营”来说，称之为流量生态的第三次打通。我早在2021年就认识一个埃森哲高管出来创业做私域代运营，接了很多世界五百强企业和国内知名消费品牌的项目，到2022年我知道绝大多数公域转私域商业模式没有跑通，公司本身也很难盈利。里面一个总监前段时间换工作请我吃饭打探行业信息，她说去过很多公司做私域总监，模式几乎都没有跑通，但是架不住这个概念目前很火，等火过这几年，职业发展不知何去何从。\u003c/p\u003e\u003cp data-pid=\"WA3xeK8i\"\u003e世界五百强企业花个上千万试一试，没跑通，无伤大雅，而对于个体创业者花了几百万没跑通就要命了。\u003c/p\u003e\u003cp data-pid=\"OAzBwfoB\"\u003e如果不是在你原先就很熟悉的领域里面创业。那创业是与未知相遇，没有什么捷径可以走，就是一点点去试，去试出一条路。\u003c/p\u003e\u003cp data-pid=\"3kQ9TJwy\"\u003e你认为行得通的路子，可能一落地压根行不通，你认为不一定行得通的路子，可能一试也有用。这就需要一个过程和时间，就要挺得住见到光明的那一天。\u003c/p\u003e\u003cp data-pid=\"nC-HJDSU\"\u003e商业世界的成功不是一直成功，而是不断尝试，不断试错，用最小代价持续试错，直到最终走向成功。\u003c/p\u003e\u003cp data-pid=\"P6QQ-7NL\"\u003e同样开培训机构，我身边一上来就投资场地、招聘团队和老师，基本上撑不了一年，因为没有熬过前期的试错期和跑通模式期，资金链要断了要关门倒闭了。\u003c/p\u003e\u003cp data-pid=\"UM6ckdca\"\u003e而我身边好几个老师出身，前期用最低成本启动创业的基本都成功了。我有个年入百万的女友跟我说，创业第一年就是勉强没有饿死，那个时候因为拿不出钱一起供婚房，谈的恋爱也分手了。\u003c/p\u003e\u003cp data-pid=\"ZinWuwwW\"\u003e②如果一个行业大部分人都能赚到钱，你也能赚到钱，如果大部分人赚不到钱，那你也往往赚不到钱。\u003c/p\u003e\u003cp data-pid=\"DSHh5ApU\"\u003e我们绝大多数人都是普通人，我们的智力、能力都在平均值，没有什么太特别。\u003c/p\u003e\u003cp data-pid=\"dg5Se9r0\"\u003e如果这个行业大多数人都可以赚到钱，说明还在红利期，如果这个行业已经大部分人都已经赚不到钱，说明已经在下行阶段，你进去也好不到哪里去。\u003c/p\u003e\u003cp data-pid=\"8d_1mFj8\"\u003e我在接管客户门店代运营，要先看看同一个商圈同行赚不赚钱，同行赚不到钱，大概率也赚不到钱。因为赚钱的背后一定有符合赚钱逻辑的底层要素，比如当地是否有对标的客户画像人群、消费力。\u003c/p\u003e\u003cp data-pid=\"eDAKAJHD\"\u003e很多情况下成功的决定因素就是：这件事别人的成功概率是多少。\u003c/p\u003e\u003cp data-pid=\"sJM-gWL_\"\u003e我和前同事说，你今天和我说的创业点子，大多数我已经想过试过，我上班的时候我的团队已经想过试过，这个行业很多机构也想过试过，那么就要关注成功的可能性多高，人均产值多高，成本利润多高，再看是否值得投入。\u003c/p\u003e\u003cp data-pid=\"zngMUWIl\"\u003e③推荐一本书《小而美》\u003c/p\u003e\u003cp data-pid=\"8L7Zm2ym\"\u003e怎么找到自己的赚钱能力呢？一句话：把自己活成一个解决方案。而对于创业者来说，还可以把这句话进一步扩展一下，那就是：是社群把你引向问题，问题把你引向方案；流程化的方案就成了产品，能赚钱的产品就成了生意。\u003c/p\u003e\u003cp data-pid=\"HMQu4PG8\"\u003e一时赚钱还不够，怎么能持续赚钱呢？关键在于牢记一个很简单的公式，利润=收入-成本。想要保持赚钱能力，就要始终让成本小于收入；发展不求快，但求稳；不求风头最盛，但求笑得最久。\u003c/p\u003e\u003cp data-pid=\"QQjGwbiR\"\u003e经济生态系统已经改变，旧的物种会消失，新的物种会崛起，未来的新物种很可能不是像恐龙和鲸鱼这样的庞然大物，而是像麻雀这样能迅速找到新的生态位的聪明物种。\u003c/p\u003e","relationship":{"is_thanked":false,"is_nothelp":false,"voting":0},"is_labeled":false,"visited_count":13588,"favorite_count":355,"answer_type":"normal","is_navigator":false,"navigator_vote":false,"vote_next_step":"vote"},"brief":"{\"source\": \"TS\", \"type\": \"answer\", \"id\": 96148469096}","attached_info":"CusGCN6arYPvwbuY8QEQBBoJNzEyMzQ1MjYwIMPinb0GKLQBMAxAmwFKQQosVFNfU09VUkNFX1RXT1RPV0VSX1NIT1JUSU5URVJFU1RfUkVDQUxMX1RFWFQSATAYACAAOgp7InJhdyI6IiJ9WgYyODU3NTViIDU5MDE5OTM4OTdiMmI2NzE1MzRiNDFkZmUxNzAzOTQ3cgs5NjE0ODQ2OTA5NooBCDIwMjYzOTIwqgEJcmVjb21tZW5kwgEgNTAzYjZmMmJkZWJiMTA5NzlmOTJkMGQ5NzM3MmI0MmbyAQoIDBIGTm9ybWFs8gEoCAoSJDVhYzc3NWViLWRmZWEtNDlkOC04MGM4LTY0NzJlZmVhMTJjZfIBBggLEgIyNoICAIgCmPu+zfoykgIgNTAzYjZmMmJkZWJiMTA5NzlmOTJkMGQ5NzM3MmI0MmaaAgDKAhZTaG9ySW50ZXJlc3RXZWlnaHRSdWxlygIWQWN0aW9uU2hvckludGVyZXN0UnVsZcoCG0ludGVyYWN0aW9uU2hvckludGVyZXN0UnVsZcoCGFBlcmlvZEludGVyZXN0V2VpZ2h0UnVsZcoCFVVzZXJMY25FeGl0V2VpZ2h0UnVsZcoCFENvbnRlbnRBZ2VXZWlnaHRSdWxlygITRW1iU2ltSXNvbGF0aW9uUnVsZcoCHEJheWVzRmlyc3RMZXZlbElzb2xhdGlvblJ1bGXaAixUU19TT1VSQ0VfVFdPVE9XRVJfU0hPUlRJTlRFUkVTVF9SRUNBTExfVEVYVOgCA/oCC05PUk1BTF9GTE9XigMgODE2YjNlNTYzM2ZmNDgxMDhiNzlmMDdkYjQ0MDc1MTSaAw0KAnYyEAAaBW90aGVyqAOUatgDAOoDGmZlZWRfYXR0bV90d290b3dlcl92Ml90ZXh0+gMfEgxVTktOT1dOX01PREUgACoNTk9fSU1BR0VfTU9ERYAEAIgEAJIEBk5vcm1hbJoEATOgBACoBACwBAC6BAZtYW51YWzCBAMxNzDIBADSBA/mjqjojZDlt7Lmm7TmlrDYBADwBAD5BAAAAABoE9I/gQUAAAAAAAAAAIkFsCXHHC2c0j+SBQCaBQNkZnSiBQNkZnSyBQExuQUAAAAAAAAAANAFAOAFAOgFAPAFGpAGAKAGnwGoBgCSAiYKCTcxMjM0NTI2MBILOTYxNDg0NjkwOTYYBCIKSU1BR0VfVEVYVA==","action_card":false}],"paging":{"is_end":false,"is_start":false,"next":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=down\u0026ad_interval=-10\u0026after_id=155\u0026desktop=true\u0026end_offset=159\u0026page_number=27\u0026session_token=5901993897b2b671534b41dfe1703947","previous":"https://www.zhihu.com/api/v3/feed/topstory/recommend?action=pull\u0026ad_interval=-10\u0026before_id=155\u0026desktop=true\u0026end_offset=159\u0026page_number=27\u0026session_token=5901993897b2b671534b41dfe1703947","totals":0},"fresh_text":"推荐已更新"}
